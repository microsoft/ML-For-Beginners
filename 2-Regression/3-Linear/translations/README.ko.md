# Scikit-learnì„ ì‚¬ìš©í•œ regression ëª¨ë¸ ë§Œë“¤ê¸°: regression 2ê°€ì§€ ë°©ì‹

![Linear vs polynomial regression infographic](.././images/linear-polynomial.png)
> Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)

## [ê°•ì˜ ì „ í€´ì¦ˆ](https://white-water-09ec41f0f.azurestaticapps.net/quiz/13/)

### ì†Œê°œ 

ì§€ê¸ˆê¹Œì§€ ì´ ê°•ì˜ì—ì„œ ì‚¬ìš©í•  í˜¸ë°• ê°€ê²© ë°ì´í„°ì…‹ì—ì„œ ëª¨ì€ ìƒ˜í”Œ ë°ì´í„°ë¡œ regressionì´ ë¬´ì—‡ì¸ì§€ ì°¾ì•„ë³´ì•˜ìŠµë‹ˆë‹¤. Matplotlibì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.

ì´ì œ MLì˜ regressionì— ëŒ€í•˜ì—¬ ë” ê¹Šê²Œ íŒŒê³  ë“¤ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê°•ì˜ì—ì„œ, 2ê°€ì§€ íƒ€ì…ì˜ regressionì— ëŒ€í•´ ë°°ì›ë‹ˆë‹¤: ì´ ê¸°ìˆ ì˜ ê¸°ë°˜ì´ ë˜ëŠ” ìˆ˜í•™ì˜ ì¼ë¶€ì™€ í•¨ê»˜, _basic linear regression_ ê³¼ _polynomial regression_.


> ì´ ì»¤ë¦¬í˜ëŸ¼ ëŒ€ë¶€ë¶„ì— ê±¸ì³ì„œ, ìˆ˜í•™ì— ëŒ€í•œ ìµœì†Œí•œì˜ ì§€ì‹ì„ ê°€ì •í•˜ê³ , ë‹¤ë¥¸ í•„ë“œì—ì„œ ì˜¨ í•™ìƒë“¤ì´ ìˆ˜í•™ì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•˜ë¯€ë¡œ, ì´í•´ë¥¼ ë•ê¸° ìœ„í•˜ì—¬ ë…¸íŠ¸, ğŸ§® callouts, ë‹¤ì´ì–´ê·¸ë¨ê³¼ ê¸°íƒ€ í•™ìŠµ ë„êµ¬ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.

### í•„ìš”í•œ ì¡°ê±´

ì§€ê¸ˆì¦ˆìŒ ì¡°ì‚¬í•˜ê³  ìˆë˜ í˜¸ë°• ë°ì´í„°ì˜ êµ¬ì¡°ì— ìµìˆ™í•´ì§‘ë‹ˆë‹¤. ì´ ê°•ì˜ì˜ _notebook.ipynb_ íŒŒì¼ì—ì„œ preloadedì™€ pre-cleanedëœ ê²ƒì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŒŒì¼ì—ì„œ, í˜¸ë°• ê°€ê²©ì€ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì—ì„œ bushel perë¡œ ë³´ì—¬ì§‘ë‹ˆë‹¤. Visual Studio Codeì˜ ì»¤ë„ì—ì„œ ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì§€ í™•ì¸í•©ë‹ˆë‹¤.

### ì¤€ë¹„í•˜ê¸°

ì°¸ê³ í•˜ìë©´, ì´ëŸ¬í•œ ì§ˆë¬¸ì„ ë¬¼ì–´ë³´ê¸° ìœ„í•´ì„œ ì´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤.

- í˜¸ë°•ì„ ì‚¬ê¸° ì¢‹ì€ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?
- ì‘ì€ í˜¸ë°• ì¼€ì´ìŠ¤ì˜ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?
- half-bushel ë°”êµ¬ë‹ˆ ë˜ëŠ” 1 1/9 bushel ë°•ìŠ¤ë¡œ ì‚¬ì•¼ í•˜ë‚˜ìš”?

ì´ ë°ì´í„°ë¡œ ê³„ì† íŒŒë´…ì‹œë‹¤.

ì´ì „ ê°•ì˜ì—ì„œ, Pandas ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“¤ê³  ì›ë³¸ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë¥¼ ì±„ì› ìœ¼ë©°, bushelë¡œ ê°€ê²©ì„ í‘œì¤€í™”í–ˆìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ í–ˆì§€ë§Œ, ê°€ì„ì—ë§Œ 400ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ëª¨ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

ì´ ê°•ì˜ì— ì²¨ë¶€ëœ notebookì—ì„œ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ë´…ì‹œë‹¤. ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¤ê³  ì´ˆê¸° scatterplot(ì‚°ì ë„)ì´ ì›” ë°ì´í„°ë¥¼ ë³´ì—¬ì£¼ë„ë¡ ì°¨íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤. ë” ì •ë¦¬í•˜ë©´ ë°ì´í„°ì˜ íŠ¹ì„±ì— ëŒ€í•˜ì—¬ ì¡°ê¸ˆ ë” ìì„¸íˆ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Linear regression ë¼ì¸

1 ê°•ì˜ì—ì„œ ë°°ì› ë˜ ê²ƒì²˜ëŸ¼, linear regression ì—°ìŠµì˜ ëª©í‘œëŠ” ë¼ì¸ì„ ê·¸ë¦´ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

- **ë³€ìˆ˜ ê´€ê³„ ë³´ì´ê¸°**. ë³€ìˆ˜ ì‚¬ì´ ê´€ê²Œ ë³´ì´ê¸°
- **ì˜ˆìƒí•˜ê¸°**. ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ê°€ ë¼ì¸ê³¼ ê´€ë ¨í•´ì„œ ì–´ë””ì— ìˆëŠ”ì§€ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
 
ì´ëŸ° íƒ€ì…ì˜ ì„ ì„ ê·¸ë¦¬ëŠ” ê²ƒì€ **Least-Squares Regression** ì˜ ì „í˜•ì ì…ë‹ˆë‹¤. 'least-squares'ì´ë¼ëŠ” ë§ì€ regression ë¼ì¸ì„ ë‘ë¥¸ ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ê°€ ì œê³±ëœ ë‹¤ìŒì— ë”í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ìƒì ìœ¼ë¡œ, ì ì€ ìˆ˜ì˜ ì˜¤ë¥˜, ë˜ëŠ” `least-squares`ë¥¼ ì›í•˜ê¸° ë•Œë¬¸ì—, ìµœì¢… í•©ê³„ëŠ” ê°€ëŠ¥í•œ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.

ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ ëˆ„ì  ê±°ë¦¬ê°€ ê°€ì¥ ì§§ì€ ë¼ì¸ì„ ëª¨ë¸ë§í•˜ê¸° ì›í•©ë‹ˆë‹¤. ë°©í–¥ë³´ë‹¤ í¬ê¸°ì— ê´€ì‹¬ìˆì–´ì„œ í•­ì„ ë”í•˜ê¸° ì „ì— ì œê³±í•©ë‹ˆë‹¤.

> **ğŸ§® Show me the math** 
> 
> _line of best fit_ ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ì´ ì„ ì€, [an equation](https://en.wikipedia.org/wiki/Simple_linear_regression)ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
> 
> ```
> Y = a + bX
> ```
>
> `X` ëŠ” 'ë…ë¦½(ì„¤ëª…) ë³€ìˆ˜'ì…ë‹ˆë‹¤. `Y`ëŠ” 'ì¢…ì† ë³€ìˆ˜'ì…ë‹ˆë‹¤. ë¼ì¸ì˜ ê¸°ìš¸ê¸°ëŠ” `b`ì´ê³  `a`ëŠ” y-ì ˆí¸ì´ë©°, `X = 0`ì¼ ë–„ `Y`ì˜ ê°’ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
>
>![calculate the slope](../images/slope.png)
>
> ìš°ì„ , ê¸°ìš¸ê¸° `b`ë¥¼ êµ¬í•©ë‹ˆë‹¤. Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> ì¦‰, í˜¸ë°•ì˜ ì›ë³¸ ì§ˆë¬¸ì„ ì°¸ì¡°í•´ë´…ë‹ˆë‹¤ : "predict the price of a pumpkin per bushel by month", `X`ëŠ” ê°€ê²©ì„ ë‚˜íƒ€ë‚´ê³  `Y`ëŠ” íŒë§¤í•œ ë‹¬ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
>
>![complete the equation](../images/calculation.png)
>
> Yì˜ ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ë§Œì•½ 4ë‹¬ëŸ¬ ì •ë„ ì¤€ë‹¤ë©´, 4ì›”ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤! Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> ë¼ì¸ì„ êµ¬í•˜ëŠ” ìˆ˜í•™ì€ ì ˆí¸, ë˜ëŠ” `X = 0`ì¼ ë•Œ `Y`ê°€ ìœ„ì¹˜í•œ ê³³ì— ë”°ë¼, ë‹¬ë¼ì§€ëŠ” ë¼ì¸ì˜ ê¸°ìš¸ê¸°ë¥¼ ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
>
> [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê°’ì„ êµ¬í•˜ëŠ” ë°©ì‹ì„ ì§€ì¼œë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html)ë¥¼ ì°¾ì•„ê°€ì„œ ìˆ«ì ê°’ì´ ë¼ì¸ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ” ì§€ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ìƒê´€ ê´€ê³„

ì´í•´í•  í•˜ë‚˜ì˜ ìš©ì–´ëŠ” ì£¼ì–´ì§„ Xì™€ Y ë³€ìˆ˜ ì‚¬ì´ì˜ **Correlation Coefficient**ì…ë‹ˆë‹¤. scatterplot(ì‚°ì ë„)ë¥¼ ì‚¬ìš©í•´ì„œ, ì´ ê³„ìˆ˜ë¥¼ ë¹ ë¥´ê²Œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê¹”ë”í•œ ë¼ì¸ìœ¼ë¡œ í©ì–´ ë‘” plotì€ ìƒê´€ ê´€ê³„ê°€ ë†’ì§€ë§Œ, ë°ì´í„° í¬ì¸íŠ¸ê°€  Xì™€ Y ì‚¬ì´ ì–´ë””ì—ë‚˜ í©ì–´ì§„ plotì€ ìƒê´€ ê´€ê³„ê°€ ë‚®ìŠµë‹ˆë‹¤.

ì¢‹ì€ linear regression ëª¨ë¸ì€ regression ë¼ì¸ê³¼ ê°™ì´ Least-Squares Regression ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ (0 ë³´ë‹¤ 1ì— ê°€ê¹Œìš´) ë†’ì€ ìƒê´€ ê³„ìˆ˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

âœ… ì´ ê°•ìœ„ì—ì„œ ê°™ì´ ì£¼ëŠ” ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ê³  City to Priceì˜ scatterplot (ì‚°ì ë„)ë¥¼ ë´…ë‹ˆë‹¤. scatterplot (ì‚°ì ë„)ì˜ ì‹œê°ì  í•´ì„ì— ë”°ë¥´ë©´, í˜¸ë°• íŒë§¤ë¥¼ ë„ì‹œì™€ ê°€ê²©ì— ì—°ê´€ì§€ìœ¼ë©´ ë°ì´í„°ê°€ ë†’ê±°ë‚˜ ë‚®ì€ ìƒê´€ ê´€ê³„ë¥¼ ë³´ì´ëŠ” ê²ƒ ê°™ë‚˜ìš”?


## Regressionë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„í•˜ê¸°

ì§€ê¸ˆë¶€í„° ì—°ìŠµì— ê¸°ë°˜í•œ ìˆ˜í•™ì„ ì´í•´í–ˆìœ¼ë¯€ë¡œ, Regression ëª¨ë¸ì„ ë§Œë“¤ì–´ì„œ í˜¸ë°• ê°€ê²©ì´ ê´œì°®ì€ í˜¸ë°• íŒ¨í‚¤ì§€ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ì§€ ë´…ë‹ˆë‹¤. holiday pumpkin patchë¥¼ ìœ„í•´ì„œ í˜¸ë°•ì„ ì‚¬ëŠ” ì‚¬ëŒì€ ì´ ì •ë³´ë¡œ íŒ¨ì¹˜ìš© í˜¸ë°• íŒ¨í‚¤ì§€ë¥¼ ìµœì ìœ¼ë¡œ ì‚¬ê³  ì‹¶ìŠµë‹ˆë‹¤.

Scikit-learnì„ ì‚¬ìš©í•  ì˜ˆì •ì´ê¸° ë•Œë¬¸ì—, (í•  ìˆ˜ ìˆì§€ë§Œ) ì†ìœ¼ë¡œ ì§ì ‘ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì—… ë…¸íŠ¸ë¶ì˜ ì£¼ ë°ì´í„°-ì²˜ë¦¬ ë¸”ë¡ì—ì„œ, Scikit-learnì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë“  ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ìë™ ë³€í™˜í•©ë‹ˆë‹¤:

```python
from sklearn.preprocessing import LabelEncoder

new_pumpkins.iloc[:, 0:-1] = new_pumpkins.iloc[:, 0:-1].apply(LabelEncoder().fit_transform)
```

new_pumpkins ë°ì´í„°í”„ë ˆì„ì„ ë³´ë©´, ëª¨ë“  ë¬¸ìì—´ì€ ì´ì œ ìˆ«ìë¡œ ë³´ì…ë‹ˆë‹¤. ì§ì ‘ ì½ê¸°ëŠ” í˜ë“¤ì§€ë§Œ Scikit-learnì€ ë”ìš± ë” ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤!
ì§€ê¸ˆë¶€í„° regressionì— ì˜ ë§ëŠ” ë°ì´í„°ì— ëŒ€í•˜ì—¬ (scatterplot(ì‚°ì ë„) ì§€ì¼œë³´ëŠ” ê²ƒ ë§ê³ ë„) êµìœ¡ì ì¸ ê²°ì •ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì ì¬ì ìœ¼ë¡œ ì¢‹ì€ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“œë ¤ë©´ ë°ì´í„°ì˜ ë‘ í¬ì¸íŠ¸ ì‚¬ì´ ì¢‹ì€ ìƒê´€ ê´€ê³„ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ë„ì‹œì™€ ê°€ê²© ì‚¬ì´ì—ëŠ” ì•½í•œ ìƒê´€ ê´€ê³„ë§Œ ìˆë‹¤ëŠ”, ì‚¬ì‹¤ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤:

```python
print(new_pumpkins['City'].corr(new_pumpkins['Price']))
0.32363971816089226
```

í•˜ì§€ë§Œ íŒ¨í‚¤ì§€ì™€ ê°€ê²© ì‚¬ì´ì—ëŠ” ì¡°ê¸ˆ ë” í° ìƒê´€ ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì´í•´ê°€ ë˜ë‚˜ìš”? ì¼ë°˜ì ìœ¼ë¡œ, ë†ì‚°ë¬¼ ë°•ìŠ¤ê°€ í´ìˆ˜ë¡, ê°€ê²©ë„ ë†’ìŠµë‹ˆë‹¤.

```python
print(new_pumpkins['Package'].corr(new_pumpkins['Price']))
0.6061712937226021
```

ë°ì´í„°ì— ë¬¼ì–´ë³´ê¸° ì¢‹ì€ ì§ˆë¬¸ì€ ì´ë ‡ìŠµë‹ˆë‹¤: 'What price can I expect of a given pumpkin package?'

regression ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ë‹ˆë‹¤

## linear ëª¨ë¸ ë§Œë“¤ê¸°

ëª¨ë¸ì„ ë§Œë“¤ê¸° ì „ì—, ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì •ë¦¬í•©ë‹ˆë‹¤. Null ë°ì´í„°ë¥¼ ë“œëí•˜ê³  ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë³´ì´ëŠ” ì§€ ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤.

```python
new_pumpkins.dropna(inplace=True)
new_pumpkins.info()
```

ê·¸ëŸ¬ë©´, ìµœì†Œ ì…‹ì—ì„œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“¤ê³  ì¶œë ¥í•©ë‹ˆë‹¤:

```python
new_columns = ['Package', 'Price']
lin_pumpkins = new_pumpkins.drop([c for c in new_pumpkins.columns if c not in new_columns], axis='columns')

lin_pumpkins
```

```output
	Package	Price
70	0	13.636364
71	0	16.363636
72	0	16.363636
73	0	15.454545
74	0	13.636364
...	...	...
1738	2	30.000000
1739	2	28.750000
1740	2	25.750000
1741	2	24.000000
1742	2	24.000000
415 rows Ã— 2 columns
```

1. ì´ì œ Xì™€ Y ì¢Œí‘œ ë°ì´í„°ë¥¼ ëŒ€ì…í•©ë‹ˆë‹¤:

   ```python
   X = lin_pumpkins.values[:, :1]
   y = lin_pumpkins.values[:, 1:2]
   ```
âœ… ì–´ë–¤ ì¼ì´ ìƒê¸°ë‚˜ìš”? [Python slice notation](https://stackoverflow.com/questions/509211/understanding-slice-notation/509295#509295)ìœ¼ë¡œ `X` ì™€ `y`ë¥¼ ì±„ìš¸ ë°°ì—´ì„ ìƒì„±í•©ë‹ˆë‹¤.

2. ë‹¤ìŒìœ¼ë¡œ, regression model-building ë£¨í‹´ì„ ì‹œì‘í•©ë‹ˆë‹¤:

   ```python
   from sklearn.linear_model import LinearRegression
   from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
   from sklearn.model_selection import train_test_split

   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
   lin_reg = LinearRegression()
   lin_reg.fit(X_train,y_train)

   pred = lin_reg.predict(X_test)

   accuracy_score = lin_reg.score(X_train,y_train)
   print('Model Accuracy: ', accuracy_score)
   ```

   ìƒê´€ ê´€ê³„ê°€ ì¢‹ì§€ ëª»í•´ì„œ, ë§Œë“¤ì–´ì§„ ëª¨ë¸ì€ ë”±íˆ ì •í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

   ```output
   Model Accuracy:  0.3315342327998987
   ```

3. í”„ë¡œì„¸ìŠ¤ì—ì„œ ê·¸ë ¤ì§„ ë¼ì¸ì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

   ```python
   plt.scatter(X_test, y_test,  color='black')
   plt.plot(X_test, pred, color='blue', linewidth=3)

   plt.xlabel('Package')
   plt.ylabel('Price')

   plt.show()
   ```
   ![A scatterplot showing package to price relationship](.././images/linear.png)

4. ê°€ìƒì˜ Varietyì— ëŒ€í•˜ì—¬ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:

   ```python
   lin_reg.predict( np.array([ [2.75] ]) )
   ```
   
   ì „ì„¤ì  Varietyì˜ ë°˜í’ˆëœ ê°€ê²©ì…ë‹ˆë‹¤:

   ```output
   array([[33.15655975]])
   ```

regression ë¼ì¸ì˜ ë¡œì§ì´ ì‚¬ì‹¤ì´ë¼ë©´, ìˆ«ìëŠ” ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.

ğŸƒ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤. ë°©ê¸ˆ ì „ì— ëª‡ í˜¸ë°• ì¢…ì˜ ê°€ê²© ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. holiday pumpkin patchëŠ” ì•„ë¦…ë‹µìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë” ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

## Polynomial regression

linear regressionì˜ ë˜ ë‹¤ë¥¸ íƒ€ì…ì€ polynomial regression ì…ë‹ˆë‹¤. ë•Œë•Œë¡œ ë³€ìˆ˜ ì‚¬ì´ linear ê´€ê³„ê°€ ìˆì§€ë§Œ - í˜¸ë°• ë³¼ë¥¨ì´ í´ìˆ˜ë¡, ê°€ê²©ì´ ë†’ì•„ì§€ëŠ” - ì´ëŸ° ê´€ê³„ë¥¼ í‰ë©´ ë˜ëŠ” ì§ì„ ìœ¼ë¡œ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

âœ… polynomial regressionì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ [some more examples](https://online.stat.psu.edu/stat501/lesson/9/9.8)ì…ë‹ˆë‹¤.

ì´ì „ plotì—ì„œ ë‹¤ì–‘ì„±ê³¼ ê°€ê²© ì‚¬ì´ ê´€ê³„ë¥¼ ë´…ë‹ˆë‹¤. scatterplot(ì‚°ì ë„)ì´ ë°˜ë“œì‹œ ì§ì„ ìœ¼ë¡œ ë¶„ì„ë˜ì–´ì•¼ í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ë‚˜ìš”? ì•„ë§ˆ ì•„ë‹ê²ë‹ˆë‹¤. ì´ ì¼€ì´ìŠ¤ì—ì„œ, polynomial regressionì„ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

âœ… PolynomialsëŠ” í•˜ë‚˜ ë˜ëŠ” ë” ë§ì€ ë³€ìˆ˜ì™€ ê³„ìˆ˜ë¡œ ì´ë£¨ì–´ ì§ˆ ìˆ˜ ìˆëŠ” ìˆ˜í•™ì  í‘œí˜„ì‹ì…ë‹ˆë‹¤.

Polynomial regressionì€ nonlinear ë°ì´í„°ì— ë” ë§ëŠ” ê³¡ì„ ì„ ë§Œë“­ë‹ˆë‹¤.

1. ì›ë³¸ í˜¸ë°• ë°ì´í„°ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì±„ì›Œì§„ ë°ì´í„°í”„ë ˆì„ì„ ë‹¤ì‹œ ë§Œë“­ë‹ˆë‹¤:

   ```python
   new_columns = ['Variety', 'Package', 'City', 'Month', 'Price']
   poly_pumpkins = new_pumpkins.drop([c for c in new_pumpkins.columns if c not in new_columns], axis='columns')

   poly_pumpkins
   ```

ë°ì´í„°í”„ë ˆì„ì˜ ë°ì´í„° ì‚¬ì´ ìƒê´€ ê´€ê³„ë¥¼ ì‹œê°í™”í•˜ëŠ” ì¢‹ì€ ë°©ì‹ì€ 'coolwarm' ì°¨íŠ¸ì— ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤:

2. ì¸ìˆ˜ ê°’ìœ¼ë¡œ `coolwarm`ì„ `Background_gradient()` ë©”ì†Œë“œì— ì‚¬ìš©í•©ë‹ˆë‹¤:

   ```python
   corr = poly_pumpkins.corr()
   corr.style.background_gradient(cmap='coolwarm')
   ```
   ì´ ì½”ë“œë¡œ heatmapì„ ë§Œë“­ë‹ˆë‹¤:
   ![A heatmap showing data correlation](.././images/heatmap.png)

ì´ ì°¨íŠ¸ë¥¼ ë³´ê³  ìˆìœ¼ë©´, íŒ¨í‚¤ì§€ì™€ ê°€ê²© ì‚¬ì´ ì¢‹ì€ ìƒê´€ ê´€ê³„ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ì „ì˜ ëª¨ë¸ë³´ë‹¤ ì•½ê°„ ì¢‹ê²Œ ë§Œë“¤ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

### íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸°

Scikit-learnì—ëŠ” polynomial regression ëª¨ë¸ì„ ë§Œë“¤ ë•Œ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” APIê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ - the `make_pipeline` [API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline). ì¶”ì •ëŸ‰ì˜ ì²´ì¸ì¸ 'pipeline'ì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ì´ ì¼€ì´ìŠ¤ëŠ”, íŒŒì´í”„ë¼ì¸ì— polynomial features, ë˜ëŠ” nonlinear ê²½ë¡œë¥¼ ë§Œë“¤ ì˜ˆì¸¡ì´ í¬í•¨ë©ë‹ˆë‹¤.

1. X ì™€ y ì—´ì„ ì‘ì„±í•©ë‹ˆë‹¤:

   ```python
   X=poly_pumpkins.iloc[:,3:4].values
   y=poly_pumpkins.iloc[:,4:5].values
   ```

2. `make_pipeline()` ë©”ì†Œë“œë¥¼ ë¶ˆëŸ¬ì„œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤:

   ```python
   from sklearn.preprocessing import PolynomialFeatures
   from sklearn.pipeline import make_pipeline

   pipeline = make_pipeline(PolynomialFeatures(4), LinearRegression())

   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

   pipeline.fit(np.array(X_train), y_train)

   y_pred=pipeline.predict(X_test)
   ```

### ì‹œí€€ìŠ¤ ë§Œë“¤ê¸°

ì´ ì§€ì ì—ì„œ, íŒŒì´í”„ë¼ì¸ì´ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ _sorted_ ë°ì´í„°ë¡œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“¤ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

í•´ë‹¹ ì½”ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:

   ```python
   df = pd.DataFrame({'x': X_test[:,0], 'y': y_pred[:,0]})
   df.sort_values(by='x',inplace = True)
   points = pd.DataFrame(df).to_numpy()

   plt.plot(points[:, 0], points[:, 1],color="blue", linewidth=3)
   plt.xlabel('Package')
   plt.ylabel('Price')
   plt.scatter(X,y, color="black")
   plt.show()
   ```

`pd.DataFrame`ì„ ë¶ˆëŸ¬ì„œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“­ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ `sort_values()`ë„ ë¶ˆëŸ¬ì„œ ê°’ì„ ì •ë ¬í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ polynomial plotì„ ë§Œë“­ë‹ˆë‹¤:

![A polynomial plot showing package to price relationship](.././images/polynomial.png)

ë°ì´í„°ì— ë” ë§ëŠ” ê³¡ì„ ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•©ì‹œë‹¤:

   ```python
   accuracy_score = pipeline.score(X_train,y_train)
   print('Model Accuracy: ', accuracy_score)
   ```

   ê·¸ë¦¬ê³  ì§ !

   ```output
   Model Accuracy:  0.8537946517073784
   ```

ë” ì¢‹ìŠµë‹ˆë‹¤! ê°€ê²©ì„ ì˜ˆì¸¡í•´ë´…ì‹œë‹¤:

### ì˜ˆì¸¡í•˜ê¸°

ìƒˆë¡œìš´ ê°’ì„ ë„£ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆë‚˜ìš”?

`predict()`ë¥¼ ë¶ˆëŸ¬ì„œ ì˜ˆì¸¡í•©ë‹ˆë‹¤:
 
   ```python
   pipeline.predict( np.array([ [2.75] ]) )
   ```

   ì´ë ‡ê²Œ ì˜ˆì¸¡ë©ë‹ˆë‹¤:

   ```output
   array([[46.34509342]])
   ```

ì£¼ì–´ì§„ plotì—ì„œ, ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤! ê·¸ë¦¬ê³ , ì´ì „ë³´ë‹¤ ëª¨ë¸ì´ ë” ì¢‹ì•„ì¡Œë‹¤ë©´, ê°™ì€ ë°ì´í„°ë¥¼ ë³´ê³ , ë” ë¹„ì‹¼ í˜¸ë°•ì„ ìœ„í•œ ì˜ˆì‚°ì´ í•„ìš”í•©ë‹ˆë‹¤!

ğŸ† ì¢‹ìŠµë‹ˆë‹¤! ì´ ê°•ì˜ì—ì„œ ë‘ê°€ì§€ regression ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. regressionì˜ ë§ˆì§€ë§‰ ì„¹ì…˜ì—ì„œ, ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•œ logistic regressionì— ëŒ€í•˜ì—¬ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.

---
## ğŸš€ ë„ì „

ë…¸íŠ¸ë¶ì—ì„œ ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•˜ë©´ì„œ ìƒê´€ ê´€ê³„ê°€ ëª¨ë¸ ì •í™•ë„ì— ì–´ë–»ê²Œ ëŒ€ì‘ë˜ëŠ” ì§€ ë´…ë‹ˆë‹¤.

## [ê°•ì˜ í›„ í€´ì¦ˆ](https://white-water-09ec41f0f.azurestaticapps.net/quiz/14/)

## ê²€í†  & ìê¸°ì£¼ë„ í•™ìŠµ

ì´ ê°•ì˜ì—ì„œ Linear Regressionì— ëŒ€í•˜ì—¬ ë°°ì› ìŠµë‹ˆë‹¤. Regressionì˜ ë‹¤ë¥¸ ì¤‘ìš” íƒ€ì…ì´ ìˆìŠµë‹ˆë‹¤. Stepwise, Ridge, Lasso ì™€ Elasticnet ê¸°ìˆ ì— ëŒ€í•˜ì—¬ ì½ì–´ë´…ë‹ˆë‹¤. ë” ë°°ìš°ê¸° ìœ„í•´ì„œ ê³µë¶€í•˜ê¸° ì¢‹ì€ ì½”ìŠ¤ëŠ” [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)ì…ë‹ˆë‹¤.

## ê³¼ì œ 

[Build a Model](../assignment.md)