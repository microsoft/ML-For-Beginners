# æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹å…¬å¹³ã•
 
![æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹å…¬å¹³æ€§ã‚’ã¾ã¨ã‚ãŸã‚¹ã‚±ãƒƒãƒ](../../../sketchnotes/ml-fairness.png)
> [Tomomi Imura](https://www.twitter.com/girlie_mac)ã«ã‚ˆã‚‹ã‚¹ã‚±ãƒƒãƒ

## [Pre-lecture quiz](https://jolly-sea-0a877260f.azurestaticapps.net/quiz/5/)
 
## ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³

ã“ã®ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãŒç§ãŸã¡ã®æ—¥å¸¸ç”Ÿæ´»ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹ã‹ã‚’çŸ¥ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãŸã£ãŸä»Šã€åŒ»ç™‚ã®è¨ºæ–­ã‚„ä¸æ­£ã®æ¤œå‡ºãªã©ã€æ—¥å¸¸ã®æ„æ€æ±ºå®šã«ã‚·ã‚¹ãƒ†ãƒ ã‚„ãƒ¢ãƒ‡ãƒ«ãŒé–¢ã‚ã£ã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€èª°ã‚‚ãŒå…¬å¹³ãªçµæœã‚’å¾—ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã«ã¯ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ãŒã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

ã—ã‹ã—ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã€äººç¨®ã€æ€§åˆ¥ã€æ”¿æ²»çš„è¦‹è§£ã€å®—æ•™ãªã©ã®ç‰¹å®šã®å±æ€§ãŒæ¬ ã‘ã¦ã„ãŸã‚Šã€ãã®ã‚ˆã†ãªå±æ€§ãŒåã£ã¦ã„ãŸã‚Šã™ã‚‹ã¨ã€ä½•ãŒèµ·ã“ã‚‹ã‹æƒ³åƒã—ã¦ã¿ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒç‰¹å®šã®å±¤ã«æœ‰åˆ©ã«ãªã‚‹ã‚ˆã†ã«è§£é‡ˆã•ã‚ŒãŸå ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚ãã®çµæœã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’å—ã‘ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ

ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã¯ã€ä»¥ä¸‹ã®ã“ã¨ã‚’è¡Œã„ã¾ã™:

- æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹å…¬å¹³æ€§ã®é‡è¦æ€§ã«å¯¾ã™ã‚‹æ„è­˜ã‚’é«˜ã‚ã‚‹ã€‚
- å…¬å¹³æ€§ã«é–¢é€£ã™ã‚‹å•é¡Œã«ã¤ã„ã¦å­¦ã¶ã€‚
- å…¬å¹³æ€§ã®è©•ä¾¡ã¨ç·©å’Œã«ã¤ã„ã¦å­¦ã¶ã€‚

## å‰ææ¡ä»¶
å‰ææ¡ä»¶ã¨ã—ã¦ã€"Responsible AI Principles"ã®Learn Pathã‚’å—è¬›ã—ã€ã“ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢ã™ã‚‹ä»¥ä¸‹ã®ãƒ“ãƒ‡ã‚ªã‚’è¦–è´ã—ã¦ãã ã•ã„ã€‚

ã“ã¡ã‚‰ã®[Learning Path](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-15963-cxa)ã‚ˆã‚Šã€è²¬ä»»ã®ã‚ã‚‹AIã«ã¤ã„ã¦å­¦ã¶ã€‚

[![Microsoftã®è²¬ä»»ã‚ã‚‹AIã«å¯¾ã™ã‚‹å–ã‚Šçµ„ã¿](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsoftã®è²¬ä»»ã‚ã‚‹AIã«å¯¾ã™ã‚‹å–ã‚Šçµ„ã¿")

> ğŸ¥ ä¸Šã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å‹•ç”»ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ï¼šMicrosoftã®è²¬ä»»ã‚ã‚‹AIã«å¯¾ã™ã‚‹å–ã‚Šçµ„ã¿

## ãƒ‡ãƒ¼ã‚¿ã‚„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸å…¬å¹³ã•

> ã€Œãƒ‡ãƒ¼ã‚¿ã‚’é•·ãæ‹·å•ã™ã‚Œã°ã€ä½•ã§ã‚‚è‡ªç™½ã™ã‚‹ã‚ˆã†ã«ãªã‚‹ã€ - Ronald Coase

ã“ã®è¨€è‘‰ã¯æ¥µç«¯ã«èã“ãˆã¾ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ãŒã©ã‚“ãªçµè«–ã‚’ã‚‚è£ä»˜ã‘ã‚‹ã‚ˆã†ã«æ“ä½œã§ãã‚‹ã“ã¨ã¯äº‹å®Ÿã§ã™ã€‚ã—ã‹ã—ã€ãã®ã‚ˆã†ãªæ“ä½œã¯ã€æ™‚ã«æ„å›³ã›ãšã«è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚äººé–“ã¯èª°ã§ã‚‚ãƒã‚¤ã‚¢ã‚¹ã‚’æŒã£ã¦ãŠã‚Šã€è‡ªåˆ†ãŒã„ã¤ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚¢ã‚¹ã‚’å°å…¥ã—ã¦ã„ã‚‹ã‹ã‚’æ„è­˜çš„ã«çŸ¥ã‚‹ã“ã¨ã¯é›£ã—ã„ã“ã¨ãŒå¤šã„ã®ã§ã™ã€‚

AIã‚„æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹å…¬å¹³æ€§ã®ä¿è¨¼ã¯ã€ä¾ç„¶ã¨ã—ã¦è¤‡é›‘ãªç¤¾ä¼šæŠ€è¡“çš„èª²é¡Œã§ã™ã€‚ã¤ã¾ã‚Šã€ç´”ç²‹ã«ç¤¾ä¼šçš„ãªè¦–ç‚¹ã‚„æŠ€è¡“çš„ãªè¦–ç‚¹ã®ã©ã¡ã‚‰ã‹ã‚‰ã‚‚å¯¾å‡¦ã§ããªã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚

### å…¬å¹³æ€§ã«é–¢é€£ã—ãŸå•é¡Œ

ä¸å…¬å¹³ã¨ã¯ã©ã†ã„ã†æ„å‘³ã§ã™ã‹ï¼Ÿä¸å…¬å¹³ã¨ã¯ã€äººç¨®ã€æ€§åˆ¥ã€å¹´é½¢ã€éšœå®³ã®æœ‰ç„¡ãªã©ã§å®šç¾©ã•ã‚ŒãŸäººã€…ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«æ‚ªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã€ã‚ã‚‹ã„ã¯ã€è¢«å®³ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã™ã€‚

ä¸»ãªä¸å…¬å¹³ã«é–¢é€£ã™ã‚‹å•é¡Œã¯ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚:

- **ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³**ã€‚ã‚ã‚‹æ€§åˆ¥ã‚„æ°‘æ—ãŒä»–ã®æ€§åˆ¥ã‚„æ°‘æ—ã‚ˆã‚Šã‚‚å„ªé‡ã•ã‚Œã¦ã„ã‚‹å ´åˆã€‚
- **ã‚µãƒ¼ãƒ“ã‚¹ã®è³ª**ã€‚ã‚ã‚‹ç‰¹å®šã®ã‚·ãƒŠãƒªã‚ªã®ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ã—ã¦ã‚‚ã€ç¾å®ŸãŒã‚ˆã‚Šè¤‡é›‘ãªå ´åˆã«ã¯ã‚µãƒ¼ãƒ“ã‚¹ã®è³ªã®ä½ä¸‹ã«ã¤ãªãŒã‚Šã¾ã™ã€‚
- **å›ºå®šè¦³å¿µ**ã€‚ç‰¹å®šã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã‚ã‚‰ã‹ã˜ã‚å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸå±æ€§ã‚’é–¢é€£ã•ã›ã‚‹ã“ã¨ã€‚
- **èª¹è¬—ä¸­å‚·**ã€‚ä½•ã‹ã‚„èª°ã‹ã‚’ä¸å½“ã«æ‰¹åˆ¤ã—ãŸã‚Šã€ãƒ¬ãƒƒãƒ†ãƒ«ã‚’è²¼ã‚‹ã“ã¨ã€‚
- **éå‰°è¡¨ç¾ã¾ãŸã¯éå°è¡¨ç¾**ã€‚ç‰¹å®šã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒç‰¹å®šã®è·æ¥­ã«å°±ã„ã¦ã„ã‚‹å§¿ãŒè¦‹ã‚‰ã‚Œãšã€ãã‚Œã‚’å®£ä¼ã—ç¶šã‘ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚„æ©Ÿèƒ½ã¯è¢«å®³ã‚’åŠ©é•·ã—ã¦ã„ã‚‹ã¨ã„ã†è€ƒãˆã€‚

ãã‚Œã§ã¯ã€ã„ãã¤ã‹ä¾‹ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

### ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ­ãƒ¼ãƒ³ç”³è«‹ã‚’å¯©æŸ»ã™ã‚‹ä»®æƒ³çš„ãªã‚·ã‚¹ãƒ†ãƒ ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ä»–ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚ˆã‚Šã‚‚ç™½äººç”·æ€§ã‚’å„ªç§€ãªå€™è£œè€…ã¨ã—ã¦é¸ã¶å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚ãã®çµæœã€ç‰¹å®šã®ç”³è«‹è€…ã«ã¯ãƒ­ãƒ¼ãƒ³ãŒæä¾›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚

ã‚‚ã†ä¸€ã¤ã®ä¾‹ã¯ã€å¤§ä¼æ¥­ãŒå€™è£œè€…ã‚’å¯©æŸ»ã™ã‚‹ãŸã‚ã«é–‹ç™ºã—ãŸå®Ÿé¨“çš„ãªæ¡ç”¨ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€ã‚ã‚‹æ€§åˆ¥ã«é–¢é€£ã™ã‚‹è¨€è‘‰ã‚’å¥½ã‚€ã‚ˆã†ã«è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€ã‚ã‚‹æ€§åˆ¥ã‚’ã‚·ã‚¹ãƒ†ãƒ çš„ã«å·®åˆ¥ã—ã¦ã„ã¾ã—ãŸã€‚ãã®çµæœã€å±¥æ­´æ›¸ã«ã€Œå¥³å­ãƒ©ã‚°ãƒ“ãƒ¼ãƒãƒ¼ãƒ ã€ãªã©ã®å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å€™è£œè€…ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™ã‚‚ã®ã¨ãªã£ã¦ã„ã¾ã—ãŸã€‚

âœ… ã“ã“ã§ã€ä¸Šè¨˜ã®ã‚ˆã†ãªå®Ÿä¾‹ã‚’å°‘ã—èª¿ã¹ã¦ã¿ã¦ãã ã•ã„ã€‚

### ã‚µãƒ¼ãƒ“ã‚¹ã®è³ª

ç ”ç©¶è€…ã¯ã€ã„ãã¤ã‹ã®å¸‚è²©ã®ã‚¸ã‚§ãƒ³ãƒ€ãƒ¼åˆ†é¡æ³•ã¯ã€æ˜ã‚‹ã„è‚Œè‰²ã®ç”·æ€§ã®ç”»åƒã¨æ¯”è¼ƒã—ã¦ã€æš—ã„è‚Œè‰²ã®å¥³æ€§ã®ç”»åƒã§ã¯é«˜ã„ä¸æ­£è§£ç‡ã‚’ç¤ºã—ãŸã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚[å‚ç…§](https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/) 

ã¾ãŸã€è‚Œã®è‰²ãŒæš—ã„äººã‚’æ„ŸçŸ¥ã§ããªã‹ã£ãŸãƒãƒ³ãƒ‰ã‚½ãƒ¼ãƒ—ãƒ‡ã‚£ã‚¹ãƒšãƒ³ã‚µãƒ¼ã®ä¾‹ã‚‚æ‚ªã„æ„å‘³ã§æœ‰åã§ã™ã€‚[å‚ç…§](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)

### å›ºå®šè¦³å¿µ

æ©Ÿæ¢°ç¿»è¨³ã«ã¯ã€ã‚¹ãƒ†ãƒ¬ã‚ªã‚¿ã‚¤ãƒ—ãªæ€§åˆ¥è¦³ãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ã€Œå½¼ã¯ãƒŠãƒ¼ã‚¹ã§ã€å½¼å¥³ã¯åŒ»è€…ã§ã™ã€‚(â€œhe is a nurse and she is a doctorâ€)ã€ã¨ã„ã†æ–‡ã‚’ãƒˆãƒ«ã‚³èªã«ç¿»è¨³ã™ã‚‹éš›ã€å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒˆãƒ«ã‚³èªã¯å˜æ•°ã®ä¸‰äººç§°ã‚’è¡¨ã™ä»£åè©ã€Œoã€ãŒ1ã¤ã‚ã‚‹ã®ã¿ã§ã€æ€§åˆ¥ã®åŒºåˆ¥ã®ãªã„è¨€èªã§ã€ã“ã®æ–‡ç« ã‚’ãƒˆãƒ«ã‚³èªã‹ã‚‰è‹±èªã«ç¿»è¨³ã—ç›´ã™ã¨ã€ã€Œå½¼å¥³ã¯ãƒŠãƒ¼ã‚¹ã§ã€å½¼ã¯åŒ»è€…ã§ã™ã€‚(â€œshe is a nurse and he is a doctorâ€)ã€ã¨ã„ã†ã‚¹ãƒ†ãƒ¬ã‚ªã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹æ­£ã—ããªã„æ–‡ç« ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚

![ãƒˆãƒ«ã‚³èªã«å¯¾ã™ã‚‹ç¿»è¨³](../images/gender-bias-translate-en-tr.png)

![è‹±èªã«å¾©å…ƒã™ã‚‹ç¿»è¨³](../images/gender-bias-translate-tr-en.png)

### èª¹è¬—ä¸­å‚·

ç”»åƒãƒ©ãƒ™ãƒªãƒ³ã‚°æŠ€è¡“ã«ã‚ˆã‚Šã€è‚Œã®è‰²ãŒé»’ã„äººã®ç”»åƒã‚’ã‚´ãƒªãƒ©ã¨èª¤è¡¨ç¤ºã—ãŸã“ã¨ãŒæœ‰åã§ã™ã€‚èª¤è¡¨ç¤ºã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãŒå˜ã«é–“é•ã„ã‚’ã—ãŸã¨ã„ã†ã ã‘ã§ãªãã€é»’äººã‚’èª¹è¬—ä¸­å‚·ã™ã‚‹ãŸã‚ã«ã“ã®è¡¨ç¾ãŒæ„å›³çš„ã«ä½¿ã‚ã‚Œã¦ããŸé•·ã„æ­´å²ã‚’æŒã£ã¦ã„ãŸãŸã‚ã€æœ‰å®³ã§ã‚ã‚‹ã€‚

[![AI: è‡ªåˆ†ã¯å¥³æ€§ã§ã¯ãªã„ã®ï¼Ÿ](https://img.youtube.com/vi/QxuyfWoVV98/0.jpg)](https://www.youtube.com/watch?v=QxuyfWoVV98 "AI: è‡ªåˆ†ã¯å¥³æ€§ã§ã¯ãªã„ã®ï¼Ÿ")
> ğŸ¥ ä¸Šã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å‹•ç”»ãŒè¡¨ç¤ºã•ã‚Œã¾ã™: AI: è‡ªåˆ†ã¯å¥³æ€§ã§ã¯ãªã„ã®ï¼Ÿ - AIã«ã‚ˆã‚‹äººç¨®å·®åˆ¥çš„ãªèª¹è¬—ä¸­å‚·ã«ã‚ˆã‚‹è¢«å®³ã‚’ç¤ºã™ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### éå‰°è¡¨ç¾ã¾ãŸã¯éå°è¡¨ç¾
 
ç•°å¸¸ãªç”»åƒæ¤œç´¢ã®çµæœã¯ã“ã®å•é¡Œã®è‰¯ã„ä¾‹ã§ã™ã€‚ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„CEOãªã©ã€ç”·æ€§ã¨å¥³æ€§ã®å‰²åˆãŒåŒã˜ã‹ãã‚Œä»¥ä¸Šã®è·æ¥­ã®ç”»åƒã‚’æ¤œç´¢ã™ã‚‹ã¨ã€ã©ã¡ã‚‰ã‹ã®æ€§åˆ¥ã«å¤§ããåã£ãŸçµæœãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã§æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚

![Bingã§CEOã¨æ¤œç´¢](../images/ceos.png)
> This search on Bing for 'CEO' produces pretty inclusive results

ã“ã‚Œã‚‰ã®5ã¤ã®ä¸»è¦ãªã‚¿ã‚¤ãƒ—ã®å•é¡Œã¯ã€ç›¸äº’ã«æ’ä»–çš„ãªã‚‚ã®ã§ã¯ãªãã€1ã¤ã®ã‚·ã‚¹ãƒ†ãƒ ãŒè¤‡æ•°ã®ã‚¿ã‚¤ãƒ—ã®å®³ã‚’ç¤ºã™ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€ãã‚Œãã‚Œã®ã‚±ãƒ¼ã‚¹ã§ã¯ã€ãã®é‡å¤§æ€§ãŒç•°ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ã‚ã‚‹äººã«ä¸å½“ã«çŠ¯ç½ªè€…ã®ãƒ¬ãƒƒãƒ†ãƒ«ã‚’è²¼ã‚‹ã“ã¨ã¯ã€ç”»åƒã‚’èª¤ã£ã¦è¡¨ç¤ºã™ã‚‹ã“ã¨ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«æ·±åˆ»ãªå•é¡Œã§ã™ã€‚ã—ã‹ã—ã€æ¯”è¼ƒçš„æ·±åˆ»ã§ã¯ãªã„è¢«å®³ã§ã‚ã£ã¦ã‚‚ã€äººã€…ãŒç–å¤–æ„Ÿã‚’æ„Ÿã˜ãŸã‚Šã€ç‰¹åˆ¥è¦–ã•ã‚Œã¦ã„ã‚‹ã¨æ„Ÿã˜ãŸã‚Šã™ã‚‹ã“ã¨ãŒã‚ã‚Šã€ãã®ç´¯ç©çš„ãªå½±éŸ¿ã¯éå¸¸ã«æŠ‘åœ§çš„ãªã‚‚ã®ã«ãªã‚Šã†ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠãã“ã¨ã¯é‡è¦ã§ã—ã‚‡ã†ã€‚
 
âœ… **ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³**: ã„ãã¤ã‹ã®ä¾‹ã‚’å†æ¤œè¨ã—ã€ç•°ãªã‚‹å®³ã‚’ç¤ºã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ 

|                         | ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ | ã‚µãƒ¼ãƒ“ã‚¹ã®è³ª | å›ºå®šè¦³å¿µ | èª¹è¬—ä¸­å‚· | éå‰°è¡¨ç¾/éå°è¡¨ç¾ |
| ----------------------- | :--------: | :----------------: | :----------: | :---------: | :----------------------------: |
| æ¡ç”¨ã‚·ã‚¹ãƒ†ãƒ ã®è‡ªå‹•åŒ– |     x      |         x          |      x       |             |               x                |
| æ©Ÿæ¢°ç¿»è¨³     |            |                    |              |             |                                |
| å†™çœŸã®ãƒ©ãƒ™ãƒªãƒ³ã‚°          |            |                    |              |             |                                |


## ä¸å…¬å¹³ã®æ¤œå‡º

ã‚ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãŒä¸å…¬å¹³ãªå‹•ä½œã‚’ã™ã‚‹ç†ç”±ã¯ã•ã¾ã–ã¾ã§ã™ã€‚ä¾‹ãˆã°ã€ç¤¾ä¼šçš„ãªãƒã‚¤ã‚¢ã‚¹ãŒã€å­¦ç¿’ã«ä½¿ã‚ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åæ˜ ã•ã‚Œã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã§ã™ã—ã€éå»ã®ãƒ‡ãƒ¼ã‚¿ã«é ¼ã‚Šã™ããŸãŸã‚ã«ã€æ¡ç”¨ã®ä¸å…¬å¹³ãŒæ‚ªåŒ–ã—ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€10å¹´é–“ã«ä¼šç¤¾ã«æå‡ºã•ã‚ŒãŸå±¥æ­´æ›¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ©ç”¨ã—ã¦ã€ç”·æ€§ã‹ã‚‰ã®å±¥æ­´æ›¸ãŒå¤§åŠã‚’å ã‚ã¦ã„ãŸã“ã¨ã‹ã‚‰ã€ç”·æ€§ã®æ–¹ãŒé©æ ¼ã§ã‚ã‚‹ã¨åˆ¤æ–­ã—ã¾ã—ãŸã€‚

ç‰¹å®šã®ã‚°ãƒ«ãƒ¼ãƒ—ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã‚ã‚‹ã“ã¨ã‚‚ã€ä¸å…¬å¹³ã®åŸå› ã¨ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€è‚Œã®è‰²ãŒæ¿ƒã„äººã®ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã«ã€ç”»åƒåˆ†é¡ã«ãŠã„ã¦è‚Œã®è‰²ãŒæ¿ƒã„äººã®ç”»åƒã®ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ããªã‚Šã¾ã™ã€‚

ã¾ãŸã€é–‹ç™ºæ™‚ã®èª¤ã£ãŸä»®å®šã‚‚ä¸å…¬å¹³ã®åŸå› ã¨ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€äººã®é¡”ã®ç”»åƒã‹ã‚‰çŠ¯ç½ªã‚’çŠ¯ã™äººã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ãŸé¡”åˆ†æã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€æœ‰å®³ãªæ¨æ¸¬ã‚’ã—ã¦ã—ã¾ã†ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãã®çµæœã€èª¤ã£ãŸåˆ†é¡ã‚’ã•ã‚ŒãŸäººãŒå¤§ããªè¢«å®³ã‚’å—ã‘ã‚‹ã“ã¨ã«ãªã‚Šã‹ã­ã¾ã›ã‚“ã€‚

## ãƒ¢ãƒ‡ãƒ«ã‚’ç†è§£ã—ã€å…¬å¹³æ€§ã‚’æ§‹ç¯‰ã™ã‚‹
 
Although many aspects of fairness are not captured in quantitative fairness metrics, and it is not possible to fully remove bias from a system to guarantee fairness, you are still responsible to detect and to mitigate fairness issues as much as possible. 

When you are working with machine learning models, it is important to understand your models by means of assuring their interpretability and by assessing and mitigating unfairness.

Letâ€™s use the loan selection example to isolate the case to figure out each factor's level of impact on the prediction.

## Assessment methods

1. **Identify harms (and benefits)**. The first step is to identify harms and benefits. Think about how actions and decisions can affect both potential customers and a business itself.
  
1. **Identify the affected groups**. Once you understand what kind of harms or benefits that can occur, identify the groups that may be affected. Are these groups defined by gender, ethnicity, or social group?

1. **Define fairness metrics**. Finally, define a metric so you have something to measure against in your work to improve the situation.

### Identify harms (and benefits)

What are the harms and benefits associated with lending? Think about false negatives and false positive scenarios: 

**False negatives** (reject, but Y=1) - in this case, an applicant who will be capable of repaying a loan is rejected. This is an adverse event because the resources of the loans are withheld from qualified applicants.

**False positives** (accept, but Y=0) - in this case, the applicant does get a loan but eventually defaults. As a result, the applicant's case will be sent to a debt collection agency which can affect their future loan applications.

### Identify affected groups

The next step is to determine which groups are likely to be affected. For example, in case of a credit card application, a model might determine that women should receive much lower credit limits compared with their spouses who share household assets. An entire demographic, defined by gender, is thereby affected.

### Define fairness metrics
 
You have identified harms and an affected group, in this case, delineated by gender. Now, use the quantified factors to disaggregate their metrics. For example, using the data below, you can see that women have the largest false positive rate and men have the smallest, and that the opposite is true for false negatives.

âœ… In a future lesson on Clustering, you will see how to build this 'confusion matrix' in code

|            | False positive rate | False negative rate | count |
| ---------- | ------------------- | ------------------- | ----- |
| Women      | 0.37                | 0.27                | 54032 |
| Men        | 0.31                | 0.35                | 28620 |
| Non-binary | 0.33                | 0.31                | 1266  |

 
This table tells us several things. First, we note that there are comparatively few non-binary people in the data. The data is skewed, so you need to be careful how you interpret these numbers.

In this case, we have 3 groups and 2 metrics. When we are thinking about how our system affects the group of customers with their loan applicants, this may be sufficient, but when you want to define larger number of groups, you may want to distill this to smaller sets of summaries. To do that, you can add more metrics, such as the largest difference or smallest ratio of each false negative and false positive. 
 
âœ… Stop and Think: What other groups are likely to be affected for loan application? 
 
## Mitigating unfairness 
 
To mitigate unfairness, explore the model to generate various mitigated models and compare the tradeoffs it makes between accuracy and fairness to select the most fair model. 

This introductory lesson does not dive deeply into the details of algorithmic unfairness mitigation, such as post-processing and reductions approach, but here is a tool that you may want to try. 

### Fairlearn 
 
[Fairlearn](https://fairlearn.github.io/) is an open-source Python package that allows you to assess your systems' fairness and mitigate unfairness.  

The tool helps you to assesses how a model's predictions affect different groups, enabling you to compare multiple models by using fairness and performance metrics, and supplying a set of algorithms to mitigate unfairness in binary classification and regression. 

- Learn how to use the different components by checking out the Fairlearn's [GitHub](https://github.com/fairlearn/fairlearn/)

- Explore the [user guide](https://fairlearn.github.io/main/user_guide/index.html), [examples](https://fairlearn.github.io/main/auto_examples/index.html)

- Try some [sample notebooks](https://github.com/fairlearn/fairlearn/tree/master/notebooks). 
  
- Learn [how to enable fairness assessments](https://docs.microsoft.com/azure/machine-learning/how-to-machine-learning-fairness-aml?WT.mc_id=academic-15963-cxa) of machine learning models in Azure Machine Learning. 
  
- Check out these [sample notebooks](https://github.com/Azure/MachineLearningNotebooks/tree/master/contrib/fairness) for more fairness assessment scenarios in Azure Machine Learning. 

---
## ğŸš€ Challenge 
 
To prevent biases from being introduced in the first place, we should: 

- have a diversity of backgrounds and perspectives among the people working on systems 
- invest in datasets that reflect the diversity of our society 
- develop better methods for detecting and correcting bias when it occurs 

Think about real-life scenarios where unfairness is evident in model-building and usage. What else should we consider? 

## [Post-lecture quiz](https://jolly-sea-0a877260f.azurestaticapps.net/quiz/6/)
## Review & Self Study 
 
In this lesson, you have learned some basics of the concepts of fairness and unfairness in machine learning.  
 
Watch this workshop to dive deeper into the topics: 

- YouTube: Fairness-related harms in AI systems: Examples, assessment, and mitigation by Hanna Wallach and Miro Dudik [Fairness-related harms in AI systems: Examples, assessment, and mitigation - YouTube](https://www.youtube.com/watch?v=1RptHwfkx_k) 

Also, read: 

- Microsoftâ€™s RAI resource center: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Microsoftâ€™s FATE research group: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

Explore the Fairlearn toolkit

[Fairlearn](https://fairlearn.org/)

Read about Azure Machine Learning's tools to ensure fairness

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-15963-cxa) 

## Assignment

[Explore Fairlearn](assignment.md) 
