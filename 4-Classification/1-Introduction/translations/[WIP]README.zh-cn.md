# åˆ†ç±»çš„ä»‹ç»

åœ¨è¿™å››èŠ‚è¯¾ä¸­ï¼Œæ‚¨å°†æ¢ç´¢ç»å…¸æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåŸºæœ¬é‡ç‚¹â€”â€”_åˆ†ç±»_ ã€‚æˆ‘ä»¬å°†åˆ©ç”¨å„ç§åˆ†ç±»ç®—æ³•å¯¹æ³›äºšçš„ä½³è‚´æ•°æ®é›†è¿›è¡Œæ¼”ç»ƒã€‚å¸Œæœ›ä½ å¦‚é¥¥ä¼¼æ¸´äº†ï¼

![å°±æ”¾ä¸€ç‚¹å„¿ï¼](../images/pinch.png)

> åœ¨è¿™äº›è¯¾ç¨‹ä¸­äº«å—æ³›äºšç¾é£Ÿå§ï¼å›¾ç‰‡ç”± [Jen Looper](https://twitter.com/jenlooper) æä¾›

åˆ†ç±»æ˜¯[ç›‘ç£å­¦ä¹ ](https://wikipedia.org/wiki/Supervised_learning)çš„ä¸€ç§å½¢å¼ï¼Œä¸å›å½’æŠ€æœ¯æœ‰å¾ˆå¤šå…±åŒä¹‹å¤„ã€‚å¦‚æœè¯´æœºå™¨å­¦ä¹ å°±æ˜¯é€šè¿‡ä½¿ç”¨æ•°æ®é›†æ¥é¢„æµ‹äº‹ç‰©çš„å€¼æˆ–åç§°ï¼Œé‚£ä¹ˆåˆ†ç±»é€šå¸¸å°±æ˜¯å…¶ä¸­ä¸¤ç±»ï¼š_äºŒå…ƒåˆ†ç±»_ å’Œ _å¤šå…ƒåˆ†ç±»_ ã€‚

[![åˆ†ç±»çš„ä»‹ç»](https://img.youtube.com/vi/eg8DJYwdMyg/0.jpg)](https://youtu.be/eg8DJYwdMyg "åˆ†ç±»çš„ä»‹ç»")

> ğŸ¥ ç‚¹å‡»ä¸Šé¢çš„å›¾ç‰‡æ¥è§‚çœ‹ï¼šMITçš„ John Guttag å‘æ‚¨ä»‹ç»åˆ†ç±»

è¯·è®°ä½ï¼š

- **çº¿æ€§å›å½’** å¸®åŠ©æ‚¨é¢„æµ‹å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å‡†ç¡®é¢„æµ‹æ–°æ•°æ®ç‚¹ä¸è¯¥çº¿çš„å…³ç³»ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œæ‚¨å¯ä»¥é¢„æµ‹ *å—ç“œåœ¨ä¹æœˆä¸åäºŒæœˆçš„ä»·æ ¼* ã€‚
- **é€»è¾‘å›å½’** å¸®åŠ©æ‚¨å‘ç°â€œäºŒå…ƒå½’ç±»â€ï¼šåœ¨è¿™ä¸ªä»·ä½ä¸Šï¼Œ*è¿™ä¸ªå—ç“œæ˜¯æ©™è‰²è¿˜æ˜¯éæ©™è‰²* ï¼Ÿ

åˆ†ç±»åˆ©ç”¨å„ç§ç®—æ³•æä¾›äº†ç¡®å®šæ•°æ®ç‚¹çš„æ ‡ç­¾æˆ–ç±»åˆ«çš„å¦å¤–ä¸€ç§æ–¹æ³•ã€‚è®©æˆ‘ä»¬ä½¿ç”¨è¿™äº›ç¾é£Ÿæ•°æ®ï¼Œçœ‹çœ‹èƒ½å¦é€šè¿‡è§‚å¯Ÿèœè°±ï¼Œæ¥ç¡®å®šå…¶ç¾é£Ÿçš„æ¥æºã€‚

## [è¯¾å‰æµ‹éªŒ](https://jolly-sea-0a877260f.azurestaticapps.net/quiz/19/)

### ç®€ä»‹

åˆ†ç±»æ˜¯æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜å’Œæ•°æ®ç§‘å­¦å®¶çš„åŸºæœ¬æ´»åŠ¨ä¹‹ä¸€ã€‚ä»åŸºæœ¬çš„äºŒå…ƒåˆ†ç±»ï¼ˆâ€œè¿™æ˜¯å¦æ˜¯åƒåœ¾é‚®ä»¶ï¼Ÿâ€ï¼‰ï¼Œåˆ°ä½¿ç”¨è®¡ç®—æœºè§†è§‰çš„å¤æ‚å›¾åƒè¯†åˆ«ã€‚èƒ½å¤Ÿå°†æ•°æ®åˆ†ç±»å¹¶æå‡ºé—®é¢˜æ€»æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚

ç”¨æ›´ä¸¥è°¨çš„æ–¹å¼è¯´æ˜è¿™ä¸€è¿‡ç¨‹â€”â€”æ‚¨çš„åˆ†ç±»æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œä½¿æ‚¨èƒ½å¤Ÿå°†è¾“å…¥å˜é‡ä¹‹é—´çš„å…³ç³»æ˜ å°„åˆ°è¾“å‡ºå˜é‡ã€‚

![binary vs. multiclass classification](../images/binary-multiclass.png)

> é€‚åˆä½¿ç”¨åˆ†ç±»ç®—æ³•å¤„ç†çš„äºŒå…ƒä¸å¤šå…ƒé—®é¢˜ã€‚ç”± [Jen Looper](https://twitter.com/jenlooper) ç»˜åˆ¶çš„å›¾ç¤º

åœ¨å¼€å§‹ç€æ‰‹æ¸…ç†æ•°æ®ã€å¯è§†åŒ–ä»¥åŠä¸ºæœºå™¨å­¦ä¹ ä»»åŠ¡å‡†å¤‡æ•°æ®ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹æœºå™¨å­¦ä¹ ä¹‹ä¸­å¯ç”¨äºå¯¹æ•°æ®è¿›è¡Œåˆ†ç±»çš„å„ç§æ–¹å¼ã€‚

ä¾æ®[ç»´åŸºç™¾ç§‘çš„ç»Ÿè®¡](https://wikipedia.org/wiki/Statistical_classification)ï¼Œç»å…¸çš„æœºå™¨å­¦ä¹ åˆ†ç±»æ³•åˆ©ç”¨ä¸€äº›ç‰¹å¾ï¼Œå¦‚`å¸çƒŸ`ã€`ä½“é‡`ä¸`å¹´é¾„`ç­‰ï¼Œæ¥ç¡®å®šæ‚£æŸäº›ç–¾ç—…çš„å¯èƒ½æ€§ã€‚è·Ÿæ‚¨ä¹‹å‰åšè¿‡çš„çš„å›å½’ç»ƒä¹ ç±»ä¼¼ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•åˆ©ç”¨æ•°æ®é›†çš„ç±»åˆ«ï¼ˆæˆ–â€œç‰¹å¾â€ï¼‰ï¼Œæ¥è¿›è¡Œå½’ç±»å’Œé¢„æµ‹ï¼Œå¹¶å¾—å‡ºä¸€ä¸ªåˆ†ç±»æˆ–ç»“æœã€‚

âœ… è®©æˆ‘ä»¬èŠ±ç‚¹æ—¶é—´æ¥æ€è€ƒå…³äºç¾é£Ÿçš„æ•°æ®é›†ã€‚å¤šå…ƒæ¨¡å‹èƒ½å¤Ÿå›ç­”ä»€ä¹ˆï¼ŸäºŒå…ƒæ¨¡å‹èƒ½å¤Ÿå›ç­”ä»€ä¹ˆï¼Ÿå¦‚æœæ‚¨æƒ³ç¡®å®šç»™å®šçš„èœè‚´æ˜¯å¦ä¼šä½¿ç”¨èƒ¡èŠ¦å·´æ€ä¹ˆåŠï¼Ÿå¦‚æœæ‚¨æƒ³çŸ¥é“æ‹¥æœ‰ä¸€ä¸ªè£…æ»¡å…«è§’ã€æœé²œè“Ÿã€èŠ±æ¤°èœå’ŒèŠ¥æœ«çš„é£ŸæåŒ…ï¼Œæ‚¨æ˜¯å¦å¯ä»¥åˆ¶ä½œå‡ºå…¸å‹çš„å°åº¦èœï¼Ÿ

[![Crazy mystery baskets](https://img.youtube.com/vi/GuTeDbaNoEU/0.jpg)](https://youtu.be/GuTeDbaNoEU "Crazy mystery baskets")

> ğŸ¥ å•å‡»ä¸Šé¢çš„å›¾ç‰‡è§‚çœ‹è§†é¢‘ã€‚â€œç–¯å¨èƒ¡ç‚–ï¼ˆChoppedï¼‰â€èŠ‚ç›®ä¼šæ‹¿å‡ºä¸€ä¸ªâ€œç¥ç§˜ç¯®å­â€ï¼Œå…¶ä¸­è£…æœ‰ä¸€äº›éšæœºçš„é£Ÿæï¼Œå¨å¸ˆå¿…é¡»ç”¨è¿™äº›é£Ÿææ¥åˆ¶ä½œèœè‚´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœºå™¨å­¦ä¹ å½“ç„¶ä¼šå¾ˆæœ‰å¸®åŠ©ï¼
## ä½ å¥½å‘€ 'åˆ†ç±»å™¨'

å…³äºè¿™ä¸ªç¾é£Ÿæ•°æ®é›†çš„é—®é¢˜å®é™…ä¸Šæ˜¯ä¸€ä¸ª**å¤šå…ƒåˆ†ç±»**é—®é¢˜â€”â€”å¦‚æœæˆ‘ä»¬æœ‰ä¸€äº›é€‰å®šå›½å®¶çš„ç¾é£Ÿï¼ŒçŸ¥é“äº†å®ƒä»¬çš„é…æ–™ï¼Œæ¨æµ‹å‡ºä»–ä»¬æ˜¯å“ªä¸ªå›½å®¶çš„ç¾é£Ÿã€‚

Scikit-learn æä¾›äº†é€‚ç”¨äºä¸åŒé—®é¢˜ç±»å‹çš„å‡ ç§ç”¨äºå¯¹æ•°æ®è¿›è¡Œåˆ†ç±»çš„ç®—æ³•æ¨¡å‹ã€‚åœ¨æ¥ä¸‹æ¥çš„ä¸¤è¯¾ä¸­ï¼Œæ‚¨å°†äº†è§£å…¶ä¸­çš„å‡ ç§ç®—æ³•ã€‚

## ç»ƒä¹  - æ¸…ç†å’Œå¹³è¡¡æ‚¨çš„æ•°æ® 

The first task at hand, before starting this project, is to clean and **balance** your data to get better results. Start with the blank _notebook.ipynb_ file in the root of this folder.

The first thing to install is [imblearn](https://imbalanced-learn.org/stable/). This is a Scikit-learn package that will allow you to better balance the data (you will learn more about this task in a minute).

1. To install `imblearn`, run `pip install`, like so:

    ```python
    pip install imblearn
    ```

1. Import the packages you need to import your data and visualize it, also import `SMOTE` from `imblearn`.

    ```python
    import pandas as pd
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    import numpy as np
    from imblearn.over_sampling import SMOTE
    ```

    Now you are set up to read import the data next.

1. The next task will be to import the data:

    ```python
    df  = pd.read_csv('../data/cuisines.csv')
    ```

   Using `read_csv()` will read the content of the csv file _cusines.csv_ and place it in the variable `df`.

1. Check the data's shape:

    ```python
    df.head()
    ```

   The first five rows look like this:

    ```output
    |     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
    | --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
    | 0   | 65         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 1   | 66         | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 2   | 67         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 3   | 68         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 4   | 69         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
    ```

1. Get info about this data by calling `info()`:

    ```python
    df.info()
    ```

    Your out resembles:

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 2448 entries, 0 to 2447
    Columns: 385 entries, Unnamed: 0 to zucchini
    dtypes: int64(384), object(1)
    memory usage: 7.2+ MB
    ```

## Exercise - learning about cuisines

Now the work starts to become more interesting. Let's discover the distribution of data, per cuisine 

1. Plot the data as bars by calling `barh()`:

    ```python
    df.cuisine.value_counts().plot.barh()
    ```

    ![cuisine data distribution](images/cuisine-dist.png)

    There are a finite number of cuisines, but the distribution of data is uneven. You can fix that! Before doing so, explore a little more. 

1. Find out how much data is available per cuisine and print it out:

    ```python
    thai_df = df[(df.cuisine == "thai")]
    japanese_df = df[(df.cuisine == "japanese")]
    chinese_df = df[(df.cuisine == "chinese")]
    indian_df = df[(df.cuisine == "indian")]
    korean_df = df[(df.cuisine == "korean")]
    
    print(f'thai df: {thai_df.shape}')
    print(f'japanese df: {japanese_df.shape}')
    print(f'chinese df: {chinese_df.shape}')
    print(f'indian df: {indian_df.shape}')
    print(f'korean df: {korean_df.shape}')
    ```

    the output looks like so:

    ```output
    thai df: (289, 385)
    japanese df: (320, 385)
    chinese df: (442, 385)
    indian df: (598, 385)
    korean df: (799, 385)
    ```

## Discovering ingredients

Now you can dig deeper into the data and learn what are the typical ingredients per cuisine. You should clean out recurrent data that creates confusion between cuisines, so let's learn about this problem.

1. Create a function `create_ingredient()` in Python to create an ingredient dataframe. This function will start by dropping an unhelpful column and sort through ingredients by their count:

    ```python
    def create_ingredient_df(df):
        ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')
        ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]
        ingredient_df = ingredient_df.sort_values(by='value', ascending=False
        inplace=False)
        return ingredient_df
    ```

   Now you can use that function to get an idea of top ten most popular ingredients by cuisine.

1. Call `create_ingredient()` and plot it calling `barh()`:

    ```python
    thai_ingredient_df = create_ingredient_df(thai_df)
    thai_ingredient_df.head(10).plot.barh()
    ```

    ![thai](images/thai.png)

1. Do the same for the japanese data:

    ```python
    japanese_ingredient_df = create_ingredient_df(japanese_df)
    japanese_ingredient_df.head(10).plot.barh()
    ```

    ![japanese](images/japanese.png)

1. Now for the chinese ingredients:

    ```python
    chinese_ingredient_df = create_ingredient_df(chinese_df)
    chinese_ingredient_df.head(10).plot.barh()
    ```

    ![chinese](images/chinese.png)

1. Plot the indian ingredients:

    ```python
    indian_ingredient_df = create_ingredient_df(indian_df)
    indian_ingredient_df.head(10).plot.barh()
    ```

    ![indian](images/indian.png)

1. Finally, plot the korean ingredients:

    ```python
    korean_ingredient_df = create_ingredient_df(korean_df)
    korean_ingredient_df.head(10).plot.barh()
    ```

    ![korean](images/korean.png)

1. Now, drop the most common ingredients that create confusion between distinct cuisines, by calling `drop()`: 

   Everyone loves rice, garlic and ginger!

    ```python
    feature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)
    labels_df = df.cuisine #.unique()
    feature_df.head()
    ```

## Balance the dataset

Now that you have cleaned the data, use [SMOTE](https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTE.html) - "Synthetic Minority Over-sampling Technique" - to balance it.

1. Call `fit_resample()`, this strategy generates new samples by interpolation.

    ```python
    oversample = SMOTE()
    transformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)
    ```

    By balancing your data, you'll have better results when classifying it. Think about a binary classification. If most of your data is one class, a ML model is going to predict that class more frequently, just because there is more data for it. Balancing the data takes any skewed data and helps remove this imbalance. 

1. Now you can check the numbers of labels per ingredient:

    ```python
    print(f'new label count: {transformed_label_df.value_counts()}')
    print(f'old label count: {df.cuisine.value_counts()}')
    ```

    Your output looks like so:

    ```output
    new label count: korean      799
    chinese     799
    indian      799
    japanese    799
    thai        799
    Name: cuisine, dtype: int64
    old label count: korean      799
    indian      598
    chinese     442
    japanese    320
    thai        289
    Name: cuisine, dtype: int64
    ```

    The data is nice and clean, balanced, and very delicious! 

1. The last step is to save your balanced data, including labels and features, into a new dataframe that can be exported into a file:

    ```python
    transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')
    ```

1. You can take one more look at the data using `transformed_df.head()` and `transformed_df.info()`. Save a copy of this data for use in future lessons:

    ```python
    transformed_df.head()
    transformed_df.info()
    transformed_df.to_csv("../data/cleaned_cuisine.csv")
    ```

    This fresh CSV can now be found in the root data folder.

---

## ğŸš€Challenge

This curriculum contains several interesting datasets. Dig through the `data` folders and see if any contain datasets that would be appropriate for binary or multi-class classification? What questions would you ask of this dataset?

## [Post-lecture quiz](https://jolly-sea-0a877260f.azurestaticapps.net/quiz/20/)

## Review & Self Study

Explore SMOTE's API. What use cases is it best used for? What problems does it solve?

## Assignment 

[Explore classification methods](assignment.md)
