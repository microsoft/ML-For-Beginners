# ìš”ë¦¬ classifiers 1

ì´ ê°•ì˜ì—ì„œëŠ”, ìš”ë¦¬ì— ëŒ€í•˜ì—¬ ê· í˜•ì ì´ê³ , ê¹”ë”í•œ ë°ì´í„°ë¡œ ì±„ìš´ ì €ë²ˆ ê°•ì˜ì—ì„œ ì €ì¥í–ˆë˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë‹¤ì–‘í•œ classifiersì™€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ì„œ _ì¬ë£Œ ê·¸ë£¹ ê¸°ë°˜ìœ¼ë¡œ ì£¼ì–´ì§„ êµ­ë¯¼ ìš”ë¦¬ë¥¼ ì˜ˆì¸¡_ í•©ë‹ˆë‹¤. ì´ëŸ¬ëŠ” ë™ì•ˆ, classification ì‘ì—…ì— ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•  ëª‡ ë°©ì‹ì— ëŒ€í•´ ìì„¸íˆ ë°°ì›Œë³¼ ì˜ˆì •ì…ë‹ˆë‹¤.

## [ê°•ì˜ ì „ í€´ì¦ˆ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/21/)

## ì¤€ë¹„í•˜ê¸°

[Lesson 1](../../1-Introduction/README.md)ì„ ì™„ë£Œí–ˆë‹¤ê³  ê°€ì •í•˜ê³ , 4ê°€ì§€ ê°•ì˜ì˜ ìµœìƒë‹¨ `/data` í´ë”ì—ì„œ _cleaned_cuisines.csv_ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

## ì—°ìŠµ - êµ­ë¯¼ ìš”ë¦¬ ì˜ˆì¸¡í•˜ê¸°

1. ê°•ì˜ì˜ _notebook.ipynb_ í´ë”ì—ì„œ ì‘ì—…í•˜ê³ , Pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì´ íŒŒì¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    ë°ì´í„°ëŠ” ì´ë ‡ê²Œ ë³´ì…ë‹ˆë‹¤:


    |     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
    | --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
    | 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
    | 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |


1. ì§€ê¸ˆë¶€í„°, ì—¬ëŸ¬ê°€ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. í›ˆë ¨ì„ ìœ„í•œ 2ê°€ì§€ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ X ì™€ y ì¢Œí‘œë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤. `cuisine`ì€ ë¼ë²¨ í”„ë ˆì„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    ì´ë ‡ê²Œ ë³´ì¼ ì˜ˆì •ì…ë‹ˆë‹¤:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. `drop()`ì„ ë¶ˆëŸ¬ì„œ `Unnamed: 0` ì—´ê³¼ `cuisine` ì—´ì„ ë“œëí•©ë‹ˆë‹¤. í›ˆë ¨ ê°€ëŠ¥í•œ featuresë¡œ ë‚¨ê¸´ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    featuresëŠ” ì´ë ‡ê²Œ ë³´ì…ë‹ˆë‹¤:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

ì§€ê¸ˆë¶€í„° ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!

## classifier ê³ ë¥´ê¸°

ì´ì œ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìœ¼ë¯€ë¡œ, ì‘ì—…ì— ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ì„ ì •í•´ì•¼ í•©ë‹ˆë‹¤.

Scikit-learnì€ Supervised Learning ì•„ë˜ì— classification ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì—¬ìˆê³ , ì´ ì¹´í…Œê³ ë¦¬ì—ì„œ ë‹¤ì–‘í•œ ë¶„ë¥˜ ë°©ì‹ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [The variety](https://scikit-learn.org/stable/supervised_learning.html)ëŠ” ì²˜ìŒì— ê½¤ ë‹¹í™©ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ì‹ì— ëª¨ë“  classification ê¸°ìˆ ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- Linear ëª¨ë¸
- Support Vector Machines
- Stochastic Gradient Descent
- Nearest Neighbors
- Gaussian Processes
- Decision Trees
- Ensemble methods (voting Classifier)
- Multiclass ì™€ multioutput algorithms (multiclass ì™€ multilabel classification, multiclass-multioutput classification)

> [neural networks to classify data](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, ì´ ê°•ì˜ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.

### ì–´ë– í•œ classifier ì‚¬ìš©í•˜ë‚˜ìš”?

ê·¸ë˜ì„œ, ì–´ë–¤ classifierë¥¼ ê³¨ë¼ì•¼ í•˜ë‚˜ìš”? ìì£¼, ì—¬ëŸ¬ê°€ì§€ë¡œ ì‹¤í–‰í•˜ë©° ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ëŠ” ê²Œ í…ŒìŠ¤íŠ¸ ë°©ì‹ì…ë‹ˆë‹¤. Scikit-learnì€ KNeighbors, SVC ë‘ ë°©ì‹ìœ¼ë¡œ GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB ê·¸ë¦¬ê³  QuadraticDiscrinationAnalysis ì™€ ë¹„êµí•˜ì—¬ ë§Œë“  ë°ì´í„°ì…‹ì— ëŒ€í•œ [side-by-side comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)ì„ ì œê³µí•˜ê³ , ì‹œê°í™”ëœ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤:

![comparison of classifiers](../images/comparison.png)
> Plots generated on Scikit-learn's documentation

> AutoMLì€ í´ë¼ìš°ë“œì—ì„œ comparisonsì„ ì‹¤í–‰í•´ì„œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê¹”ë”í•˜ê²Œ í•´ê²°í–ˆìœ¼ë©°, ë°ì´í„°ì— ì ë‹¹í•œ ì•Œê³ ë¦¬ì¦˜ì„ ê³ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [here](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)ì—ì„œ ì‹œë„í•´ë´…ë‹ˆë‹¤.

### ë” ê´œì°®ì€ ì ‘ê·¼ë²•

ê·¸ëŸ¬ë‚˜, ì„±ê¸‰íˆ ì¶”ì¸¡í•˜ê¸°ë³´ë‹¤ ë” ê´œì°®ì€ ë°©ì‹ìœ¼ë¡œ, ë‚´ë ¤ë°›ì„ ìˆ˜ ìˆëŠ” [ML Cheat sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott)ì˜ ì•„ì´ë””ì–´ë¥¼ ë”°ë¥´ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°, multiclass ë¬¸ì œì— ëŒ€í•˜ì—¬, ëª‡ ì„ íƒ ì‚¬í•­ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

![cheatsheet for multiclass problems](../images/cheatsheet.png)
> multiclass classification ì˜µì…˜ì„ ì˜ ì„¤ëª…í•˜ëŠ”, Microsoftì˜ ì•Œê³ ë¦¬ì¦˜ ì¹˜íŠ¸ ì‹œíŠ¸ì˜ ì„¹ì…˜

âœ… ì¹˜íŠ¸ ì‹œíŠ¸ë¥¼ ë‚´ë ¤ë°›ê³ , ì¶œë ¥í•´ì„œ, ë²½ì— ê²ë‹ˆë‹¤!

### ì¶”ë¦¬í•˜ê¸°

ë§Œì•½ ì£¼ì–´ì§„ ì œì•½ ì‚¬í•­ì„ ê°ì•ˆí•´ì„œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ì§€ ë´…ë‹ˆë‹¤:

- **Neural networks ë§¤ìš° ë¬´ê²ìŠµë‹ˆë‹¤**. ê¹”ë”í•˜ì§€ë§Œ, ìµœì†Œ ë°ì´í„°ì…‹ê³¼, ë…¸íŠ¸ë¶ìœ¼ë¡œ ë¡œì»¬ì—ì„œ í›ˆë ¨í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ë³´ë©´, ì´ ì‘ì—…ì—ì„œ neural networksëŠ” ë§¤ìš° ë¬´ê²ìŠµë‹ˆë‹¤.
- **two-class classifier ì•„ë‹™ë‹ˆë‹¤**. one-vs-allë¥¼ ë¹¼ê¸° ìœ„í•´ì„œ, two-class classifierë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- **Decision tree ë˜ëŠ” logistic regression ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**. decision tree ë˜ëŠ”, multiclassë¥¼ ìœ„í•œ logistic regressionì´ ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Multiclass Boosted Decision Trees ë‹¤ë¥¸ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤**. multiclass boosted decision treeëŠ” nonparametric ì‘ì—…ì— ê°€ì¥ ì ë‹¹í•©ë‹ˆë‹¤. ì˜ˆì‹œë¡œ. ë­í‚¹ì„ ë§Œë“œë ¤ê³  ë””ìì¸ í–ˆìœ¼ë¯€ë¡œ, ìœ ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### Scikit-learn ì‚¬ìš©í•˜ê¸°

Scikit-learnìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•  ì˜ˆì •ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, Scikit-learnì—ëŠ” logistic regressionì„ ì‚¬ìš©í•  ë§ì€ ë°©ì‹ì´ ì¡´ì¬í•©ë‹ˆë‹¤. [parameters to pass](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression)ë¥¼ ì°¾ì•„ë´…ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ Scikit-learnì—ì„œ logistic regressionì„ í•˜ë„ë¡ ìš”ì²­í•  ë•Œ ì§€ì •í•  í•„ìš”ê°€ ìˆëŠ”, `multi_class` ì™€ `solver` ì¤‘ìš”í•œ ë‘ ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤. `multi_class` ê°’ì€ íŠ¹ì • ë™ì‘ì„ ì ìš©í•©ë‹ˆë‹¤. solverì˜ ê°’ì€ ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ëª¨ë“  solverê°€ ëª¨ë“  `multi_class` ê°’ë“¤ì„ ì—°ê²°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ë¬¸ì„œì— ë”°ë¥´ë©´, multiclass ì¼€ì´ìŠ¤ì¸ ê²½ìš°, í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:

- **one-vs-rest (OvR) ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤**, `multi_class` ì˜µì…˜ì„ `ovr`ë¡œ í•œ ê²½ìš°
- **cross-entropy lossë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤**, `multi_class` ì˜µì…˜ì„ `multinomial`ë¡œ í•œ ê²½ìš°. (í˜„ì¬ `multinomial` ì˜µì…˜ì€ â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™ ê·¸ë¦¬ê³  â€˜newton-cgâ€™ solversì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.)"

> ğŸ“ 'scheme'ëŠ” ì—¬ê¸°ì—ì„œ 'ovr' (one-vs-rest) í˜¹ì€ 'multinomial'ì¼ ê²ƒì…ë‹ˆë‹¤. logistic regressionì€ binary classificationì„ ì˜ ì§€ì›í•  ìˆ˜ ìˆë„ë¡ ë””ìì¸ ë˜ì—ˆìœ¼ë¯€ë¡œ, ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ë©´ multiclass classification ì‘ì—…ì„ ì˜ í•¸ë“¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [source](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> ğŸ“ 'solver'ëŠ” "the algorithm to use in the optimization problem"ë¡œ ì •ì˜ë©ë‹ˆë‹¤. [source](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learnì€ solversê°€ ë‹¤ì–‘í•œ ë°ì´í„° êµ¬ì¡°ì— ì œì‹œëœ ë‹¤ë¥¸ ë¬¸ì œ ë°©ì‹ì„ ì„¤ëª…í•˜ê³ ì ì´ í‘œë¥¼ ì œê³µí•©ë‹ˆë‹¤:

![solvers](../images/solvers.png)

## ì—°ìŠµ - ë°ì´í„° ë‚˜ëˆ„ê¸°

ì§€ë‚œ ê°•ì˜ì—ì„œ ìµœê·¼ì— ë°°ì› ìœ¼ë¯€ë¡œ ì²« í›ˆë ¨ ì‹œë„ì— ëŒ€í•œ logistic regressionìœ¼ë¡œ ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
`train_test_split()` ë¶ˆëŸ¬ì„œ ë°ì´í„°ë¥¼ í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## ì—°ìŠµ - logistic regression ì ìš©í•˜ê¸°

multiclass ì¼€ì´ìŠ¤ë¡œ, ì‚¬ìš©í•  _scheme_ ì™€ ì„¤ì •í•  _solver_ ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. í›ˆë ¨í•  multiclass ì„¸íŒ…ê³¼ **liblinear** solverì™€ í•¨ê»˜ LogisticRegressionì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

1. multi_classë¥¼ `ovr`ë¡œ ì„¤ì •í•˜ê³  solverë„ `liblinear`ë¡œ ì„¤ì •í•´ì„œ logistic regressionì„ ë§Œë“­ë‹ˆë‹¤:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    âœ… ê°€ë” ê¸°ë³¸ì ìœ¼ë¡œ ì„¤ì •ë˜ëŠ”, `lbfgs`ì²˜ëŸ¼ ë‹¤ë¥¸ solverë¥¼ ì‹œë„í•©ë‹ˆë‹¤

    > ë…¸íŠ¸, Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ í•„ìš”í•œ ìˆœê°„ì— ë°ì´í„°ë¥¼ í‰í‰í•˜ê²Œ í•ë‹ˆë‹¤.

    ì •í™•ë„ëŠ” **80%** ë³´ë‹¤ ì¢‹ìŠµë‹ˆë‹¤!

1. í•˜ë‚˜ì˜ í–‰ ë°ì´í„° (#50)ë¥¼ í…ŒìŠ¤íŠ¸í•˜ë©´ ëª¨ë¸ì´ ì‘ë™í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    ê²°ê³¼ê°€ ì¶œë ¥ë©ë‹ˆë‹¤:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   âœ… ë‹¤ë¥¸ í–‰ ë²ˆí˜¸ë¡œ ì‹œë„í•´ë³´ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

1. ê¹Šê²Œ íŒŒë³´ë©´, ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    ê²°ê³¼ê°€ ì¶œë ¥ë©ë‹ˆë‹¤ - ì¸ë„ ìš”ë¦¬ê°€ ê°€ì¥ ì¢‹ì€ í™•ë¥ ì— ìµœì„ ìœ¼ë¡œ ì¶”ì¸¡ë©ë‹ˆë‹¤.

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    âœ… ëª¨ë¸ì´ ì´ë¥¼ ì¸ë„ ìš”ë¦¬ë¼ê³  í™•ì‹ í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?

1. regression ê°•ì˜ì—ì„œ í–ˆë˜ í–‰ë™ì²˜ëŸ¼, classification ë¦¬í¬íŠ¸ë¥¼ ì¶œë ¥í•´ì„œ ìì„¸í•œ ì •ë³´ë¥¼ ì–»ìŠµë‹ˆë‹¤:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | precision | recall | f1-score | support |
    | ------------ | --------- | ------ | -------- | ------- |
    | chinese      | 0.73      | 0.71   | 0.72     | 229     |
    | indian       | 0.91      | 0.93   | 0.92     | 254     |
    | japanese     | 0.70      | 0.75   | 0.72     | 220     |
    | korean       | 0.86      | 0.76   | 0.81     | 242     |
    | thai         | 0.79      | 0.85   | 0.82     | 254     |
    | accuracy     | 0.80      | 1199   |          |         |
    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |
    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |

## ğŸš€ ë„ì „

ì´ ê°•ì˜ì—ì„œ, ì •ë¦¬ëœ ë°ì´í„°ë¡œ ì¬ë£Œì˜ ì‹œë¦¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ­ë¯¼ ìš”ë¦¬ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì‹œê°„ì„ íˆ¬ìí•´ì„œ Scikit-learnì´ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì½ì–´ë´…ë‹ˆë‹¤. ë¬´ëŒ€ ë’¤ì—ì„œ ìƒê¸°ëŠ” ì¼ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œ 'solver'ì˜ ê°œë…ì„ ê¹Šê²Œ íŒŒë´…ë‹ˆë‹¤.

## [ê°•ì˜ í›„ í€´ì¦ˆ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/22/)
## ê²€í†  & ìê¸°ì£¼ë„ í•™ìŠµ

[this lesson](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)ì—ì„œ logistic regression ë’¤ì˜ ìˆ˜í•™ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ íŒŒë´…ë‹ˆë‹¤.

## ê³¼ì œ 

[Study the solvers](../assignment.md)
