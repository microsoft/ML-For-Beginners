<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-08-29T14:13:40+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "ar"
}
-->
# عالم أكثر واقعية

في وضعنا الحالي، كان بيتر قادرًا على التحرك تقريبًا دون أن يشعر بالتعب أو الجوع. في عالم أكثر واقعية، عليه أن يجلس ويرتاح من وقت لآخر، وأيضًا أن يطعم نفسه. دعونا نجعل عالمنا أكثر واقعية من خلال تطبيق القواعد التالية:

1. عند الانتقال من مكان إلى آخر، يفقد بيتر **الطاقة** ويكتسب بعض **الإرهاق**.
2. يمكن لبيتر أن يكتسب المزيد من الطاقة عن طريق تناول التفاح.
3. يمكن لبيتر التخلص من الإرهاق عن طريق الراحة تحت الشجرة أو على العشب (أي المشي إلى موقع على اللوحة يحتوي على شجرة أو عشب - حقل أخضر).
4. يحتاج بيتر إلى العثور على الذئب وقتله.
5. لكي يتمكن بيتر من قتل الذئب، يجب أن تكون لديه مستويات معينة من الطاقة والإرهاق، وإلا سيخسر المعركة.

## التعليمات

استخدم [notebook.ipynb](notebook.ipynb) الأصلي كنقطة بداية لحلّك.

قم بتعديل وظيفة المكافأة أعلاه وفقًا لقواعد اللعبة، وشغّل خوارزمية التعلم المعزز لتعلم أفضل استراتيجية للفوز باللعبة، وقارن نتائج المشي العشوائي مع خوارزميتك من حيث عدد الألعاب التي تم الفوز بها والخسارة.

> **Note**: في عالمك الجديد، الحالة أكثر تعقيدًا، وبالإضافة إلى موقع الإنسان، تشمل أيضًا مستويات الإرهاق والطاقة. يمكنك اختيار تمثيل الحالة كـ tuple (Board,energy,fatigue)، أو تعريف فئة للحالة (يمكنك أيضًا أن تستمدها من `Board`)، أو حتى تعديل الفئة الأصلية `Board` داخل [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py).

في حلّك، يرجى الاحتفاظ بالكود المسؤول عن استراتيجية المشي العشوائي، وقارن نتائج خوارزميتك مع المشي العشوائي في النهاية.

> **Note**: قد تحتاج إلى ضبط المعاملات الفائقة لجعلها تعمل، خاصة عدد العصور. نظرًا لأن نجاح اللعبة (مواجهة الذئب) هو حدث نادر، يمكنك توقع وقت تدريب أطول بكثير.

## التقييم

| المعايير | ممتاز                                                                                                                                                                                                 | مقبول                                                                                                                                                                                  | يحتاج إلى تحسين                                                                                                                             |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
|          | يتم تقديم دفتر ملاحظات يحتوي على تعريف قواعد العالم الجديدة، خوارزمية Q-Learning وبعض الشروحات النصية. Q-Learning قادر على تحسين النتائج بشكل كبير مقارنة بالمشي العشوائي.                              | يتم تقديم دفتر ملاحظات، يتم تنفيذ Q-Learning ويحسن النتائج مقارنة بالمشي العشوائي، ولكن ليس بشكل كبير؛ أو أن دفتر الملاحظات موثق بشكل ضعيف والكود غير منظم جيدًا.                      | يتم بذل بعض المحاولات لإعادة تعريف قواعد العالم، ولكن خوارزمية Q-Learning لا تعمل، أو وظيفة المكافأة غير محددة بالكامل.

---

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حساسة أو هامة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.