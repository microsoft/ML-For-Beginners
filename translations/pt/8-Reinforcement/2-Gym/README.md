<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T08:49:47+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "pt"
}
-->
## Pr√©-requisitos

Nesta li√ß√£o, utilizaremos uma biblioteca chamada **OpenAI Gym** para simular diferentes **ambientes**. Pode executar o c√≥digo desta li√ß√£o localmente (por exemplo, no Visual Studio Code), caso em que a simula√ß√£o ser√° aberta numa nova janela. Ao executar o c√≥digo online, poder√° precisar de fazer alguns ajustes, conforme descrito [aqui](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Na li√ß√£o anterior, as regras do jogo e o estado foram definidos pela classe `Board`, que cri√°mos n√≥s mesmos. Aqui, utilizaremos um **ambiente de simula√ß√£o** especial, que simular√° a f√≠sica por tr√°s do equil√≠brio do poste. Um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizagem por refor√ßo √© chamado de [Gym](https://gym.openai.com/), mantido pela [OpenAI](https://openai.com/). Com este Gym, podemos criar diferentes **ambientes**, desde simula√ß√µes de CartPole at√© jogos de Atari.

> **Nota**: Pode ver outros ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

Primeiro, vamos instalar o Gym e importar as bibliotecas necess√°rias (bloco de c√≥digo 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Exerc√≠cio - inicializar um ambiente de CartPole

Para trabalhar com o problema de equil√≠brio do CartPole, precisamos de inicializar o ambiente correspondente. Cada ambiente est√° associado a:

- **Espa√ßo de observa√ß√£o**, que define a estrutura da informa√ß√£o que recebemos do ambiente. No problema do CartPole, recebemos a posi√ß√£o do poste, a velocidade e outros valores.

- **Espa√ßo de a√ß√£o**, que define as a√ß√µes poss√≠veis. No nosso caso, o espa√ßo de a√ß√£o √© discreto e consiste em duas a√ß√µes: **esquerda** e **direita**. (bloco de c√≥digo 2)

1. Para inicializar, escreva o seguinte c√≥digo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Para ver como o ambiente funciona, vamos executar uma simula√ß√£o curta de 100 passos. Em cada passo, fornecemos uma das a√ß√µes a serem realizadas - nesta simula√ß√£o, selecionamos aleatoriamente uma a√ß√£o do `action_space`.

1. Execute o c√≥digo abaixo e veja o resultado.

    ‚úÖ Lembre-se de que √© prefer√≠vel executar este c√≥digo numa instala√ß√£o local de Python! (bloco de c√≥digo 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Deve ver algo semelhante a esta imagem:

    ![CartPole sem equil√≠brio](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Durante a simula√ß√£o, precisamos de obter observa√ß√µes para decidir como agir. Na verdade, a fun√ß√£o step retorna as observa√ß√µes atuais, uma fun√ß√£o de recompensa e um indicador `done` que indica se faz sentido continuar a simula√ß√£o ou n√£o: (bloco de c√≥digo 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    No notebook, ver√° algo semelhante a isto:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    O vetor de observa√ß√£o retornado em cada passo da simula√ß√£o cont√©m os seguintes valores:
    - Posi√ß√£o do carrinho
    - Velocidade do carrinho
    - √Çngulo do poste
    - Taxa de rota√ß√£o do poste

1. Obtenha os valores m√≠nimos e m√°ximos desses n√∫meros: (bloco de c√≥digo 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Tamb√©m pode notar que o valor de recompensa em cada passo da simula√ß√£o √© sempre 1. Isto acontece porque o nosso objetivo √© sobreviver o maior tempo poss√≠vel, ou seja, manter o poste numa posi√ß√£o razoavelmente vertical pelo maior per√≠odo de tempo.

    ‚úÖ Na verdade, a simula√ß√£o do CartPole √© considerada resolvida se conseguirmos obter uma recompensa m√©dia de 195 ao longo de 100 tentativas consecutivas.

## Discretiza√ß√£o do estado

No Q-Learning, precisamos de construir uma Q-Table que define o que fazer em cada estado. Para isso, o estado precisa de ser **discreto**, ou seja, deve conter um n√∫mero finito de valores discretos. Assim, precisamos de alguma forma **discretizar** as nossas observa√ß√µes, mapeando-as para um conjunto finito de estados.

Existem algumas formas de fazer isto:

- **Dividir em intervalos**. Se conhecemos o intervalo de um determinado valor, podemos dividir este intervalo em v√°rios **bins** e substituir o valor pelo n√∫mero do bin ao qual pertence. Isto pode ser feito usando o m√©todo [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) do numpy. Neste caso, saberemos exatamente o tamanho do estado, pois depender√° do n√∫mero de bins que selecionarmos para a digitaliza√ß√£o.

‚úÖ Podemos usar interpola√ß√£o linear para trazer os valores para um intervalo finito (por exemplo, de -20 a 20) e, em seguida, converter os n√∫meros em inteiros arredondando-os. Isto d√°-nos um pouco menos de controlo sobre o tamanho do estado, especialmente se n√£o conhecermos os intervalos exatos dos valores de entrada. Por exemplo, no nosso caso, 2 dos 4 valores n√£o t√™m limites superiores/inferiores, o que pode resultar num n√∫mero infinito de estados.

No nosso exemplo, utilizaremos a segunda abordagem. Como poder√° notar mais tarde, apesar de os limites superiores/inferiores n√£o estarem definidos, esses valores raramente assumem valores fora de certos intervalos finitos, tornando os estados com valores extremos muito raros.

1. Aqui est√° a fun√ß√£o que ir√° receber a observa√ß√£o do nosso modelo e produzir um tuplo de 4 valores inteiros: (bloco de c√≥digo 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Vamos tamb√©m explorar outro m√©todo de discretiza√ß√£o usando bins: (bloco de c√≥digo 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Agora, execute uma simula√ß√£o curta e observe esses valores discretos do ambiente. Sinta-se √† vontade para experimentar tanto `discretize` quanto `discretize_bins` e veja se h√° alguma diferen√ßa.

    ‚úÖ `discretize_bins` retorna o n√∫mero do bin, que √© baseado em 0. Assim, para valores da vari√°vel de entrada pr√≥ximos de 0, retorna o n√∫mero do meio do intervalo (10). Em `discretize`, n√£o nos preocup√°mos com o intervalo dos valores de sa√≠da, permitindo que sejam negativos, e assim os valores do estado n√£o s√£o deslocados, e 0 corresponde a 0. (bloco de c√≥digo 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Descomente a linha que come√ßa com `env.render` se quiser ver como o ambiente √© executado. Caso contr√°rio, pode execut√°-lo em segundo plano, o que √© mais r√°pido. Utilizaremos esta execu√ß√£o "invis√≠vel" durante o nosso processo de Q-Learning.

## Estrutura da Q-Table

Na li√ß√£o anterior, o estado era um simples par de n√∫meros de 0 a 8, e assim era conveniente representar a Q-Table por um tensor numpy com uma forma de 8x8x2. Se utilizarmos a discretiza√ß√£o por bins, o tamanho do nosso vetor de estado tamb√©m ser√° conhecido, ent√£o podemos usar a mesma abordagem e representar o estado por um array com a forma 20x20x10x10x2 (aqui 2 √© a dimens√£o do espa√ßo de a√ß√£o, e as primeiras dimens√µes correspondem ao n√∫mero de bins que selecion√°mos para cada um dos par√¢metros no espa√ßo de observa√ß√£o).

No entanto, √†s vezes as dimens√µes precisas do espa√ßo de observa√ß√£o n√£o s√£o conhecidas. No caso da fun√ß√£o `discretize`, nunca podemos ter certeza de que o nosso estado permanece dentro de certos limites, porque alguns dos valores originais n√£o t√™m limites. Assim, utilizaremos uma abordagem ligeiramente diferente e representaremos a Q-Table por um dicion√°rio.

1. Use o par *(state,action)* como chave do dicion√°rio, e o valor corresponder√° ao valor da entrada na Q-Table. (bloco de c√≥digo 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aqui tamb√©m definimos uma fun√ß√£o `qvalues()`, que retorna uma lista de valores da Q-Table para um determinado estado que corresponde a todas as a√ß√µes poss√≠veis. Se a entrada n√£o estiver presente na Q-Table, retornaremos 0 como padr√£o.

## Vamos come√ßar o Q-Learning

Agora estamos prontos para ensinar o Peter a equilibrar!

1. Primeiro, vamos definir alguns hiperpar√¢metros: (bloco de c√≥digo 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aqui, `alpha` √© a **taxa de aprendizagem**, que define at√© que ponto devemos ajustar os valores atuais da Q-Table em cada passo. Na li√ß√£o anterior, come√ß√°mos com 1 e depois reduzimos `alpha` para valores mais baixos durante o treino. Neste exemplo, manteremos constante apenas por simplicidade, e pode experimentar ajustar os valores de `alpha` mais tarde.

    `gamma` √© o **fator de desconto**, que mostra at√© que ponto devemos priorizar a recompensa futura em rela√ß√£o √† recompensa atual.

    `epsilon` √© o **fator de explora√ß√£o/explora√ß√£o**, que determina se devemos preferir explorar ou explorar. No nosso algoritmo, em `epsilon` por cento dos casos, selecionaremos a pr√≥xima a√ß√£o de acordo com os valores da Q-Table, e no restante dos casos executaremos uma a√ß√£o aleat√≥ria. Isto permitir√° explorar √°reas do espa√ßo de busca que nunca vimos antes.

    ‚úÖ Em termos de equil√≠brio - escolher uma a√ß√£o aleat√≥ria (explora√ß√£o) seria como um empurr√£o aleat√≥rio na dire√ß√£o errada, e o poste teria de aprender a recuperar o equil√≠brio desses "erros".

### Melhorar o algoritmo

Podemos tamb√©m fazer duas melhorias ao nosso algoritmo da li√ß√£o anterior:

- **Calcular a recompensa cumulativa m√©dia**, ao longo de v√°rias simula√ß√µes. Iremos imprimir o progresso a cada 5000 itera√ß√µes e calcularemos a m√©dia da recompensa cumulativa nesse per√≠odo de tempo. Isto significa que, se obtivermos mais de 195 pontos, podemos considerar o problema resolvido, com qualidade ainda maior do que o necess√°rio.

- **Calcular o resultado cumulativo m√©dio m√°ximo**, `Qmax`, e armazenaremos a Q-Table correspondente a esse resultado. Quando executar o treino, notar√° que, √†s vezes, o resultado cumulativo m√©dio come√ßa a cair, e queremos manter os valores da Q-Table que correspondem ao melhor modelo observado durante o treino.

1. Colete todas as recompensas cumulativas em cada simula√ß√£o no vetor `rewards` para posterior plotagem. (bloco de c√≥digo 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

O que pode notar a partir desses resultados:

- **Perto do nosso objetivo**. Estamos muito pr√≥ximos de alcan√ßar o objetivo de obter 195 recompensas cumulativas ao longo de 100+ execu√ß√µes consecutivas da simula√ß√£o, ou podemos at√© ter alcan√ßado! Mesmo que obtenhamos n√∫meros menores, ainda n√£o sabemos, porque calculamos a m√©dia ao longo de 5000 execu√ß√µes, e apenas 100 execu√ß√µes s√£o necess√°rias no crit√©rio formal.

- **Recompensa come√ßa a cair**. √Äs vezes, a recompensa come√ßa a cair, o que significa que podemos "destruir" valores j√° aprendidos na Q-Table com os que pioram a situa√ß√£o.

Esta observa√ß√£o √© mais claramente vis√≠vel se plotarmos o progresso do treino.

## Plotar o progresso do treino

Durante o treino, colet√°mos o valor da recompensa cumulativa em cada uma das itera√ß√µes no vetor `rewards`. Aqui est√° como fica quando o plotamos contra o n√∫mero de itera√ß√µes:

```python
plt.plot(rewards)
```

![progresso bruto](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

A partir deste gr√°fico, n√£o √© poss√≠vel dizer nada, porque, devido √† natureza do processo de treino estoc√°stico, a dura√ß√£o das sess√µes de treino varia muito. Para dar mais sentido a este gr√°fico, podemos calcular a **m√©dia m√≥vel** ao longo de uma s√©rie de experimentos, digamos 100. Isto pode ser feito convenientemente usando `np.convolve`: (bloco de c√≥digo 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progresso do treino](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variar os hiperpar√¢metros

Para tornar o treino mais est√°vel, faz sentido ajustar alguns dos nossos hiperpar√¢metros durante o treino. Em particular:

- **Para a taxa de aprendizagem**, `alpha`, podemos come√ßar com valores pr√≥ximos de 1 e depois ir reduzindo o par√¢metro. Com o tempo, obteremos boas probabilidades na Q-Table e, assim, devemos ajust√°-las ligeiramente, e n√£o sobrescrever completamente com novos valores.

- **Aumentar epsilon**. Podemos querer aumentar o `epsilon` lentamente, para explorar menos e explorar mais. Provavelmente faz sentido come√ßar com um valor mais baixo de `epsilon` e aumentar at√© quase 1.
> **Tarefa 1**: Experimente alterar os valores dos hiperpar√¢metros e veja se consegue obter uma recompensa cumulativa mais alta. Est√° a conseguir ultrapassar 195?
> **Tarefa 2**: Para resolver formalmente o problema, √© necess√°rio alcan√ßar uma recompensa m√©dia de 195 ao longo de 100 execu√ß√µes consecutivas. Me√ßa isso durante o treino e certifique-se de que o problema foi resolvido formalmente!

## Ver o resultado em a√ß√£o

Seria interessante ver como o modelo treinado se comporta. Vamos executar a simula√ß√£o e seguir a mesma estrat√©gia de sele√ß√£o de a√ß√µes usada durante o treino, amostrando de acordo com a distribui√ß√£o de probabilidade na Q-Table: (bloco de c√≥digo 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Dever√° aparecer algo como isto:

![um cartpole equilibrado](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄDesafio

> **Tarefa 3**: Aqui, est√°vamos a usar a c√≥pia final da Q-Table, que pode n√£o ser a melhor. Lembre-se de que armazen√°mos a Q-Table com melhor desempenho na vari√°vel `Qbest`! Experimente o mesmo exemplo com a Q-Table de melhor desempenho, copiando `Qbest` para `Q`, e veja se nota alguma diferen√ßa.

> **Tarefa 4**: Aqui, n√£o est√°vamos a selecionar a melhor a√ß√£o em cada passo, mas sim a amostrar com a correspondente distribui√ß√£o de probabilidade. Faria mais sentido selecionar sempre a melhor a√ß√£o, com o valor mais alto na Q-Table? Isto pode ser feito utilizando a fun√ß√£o `np.argmax` para descobrir o n√∫mero da a√ß√£o correspondente ao maior valor na Q-Table. Implemente esta estrat√©gia e veja se melhora o equil√≠brio.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Tarefa
[Treinar um Mountain Car](assignment.md)

## Conclus√£o

Aprendemos agora como treinar agentes para alcan√ßar bons resultados apenas fornecendo-lhes uma fun√ß√£o de recompensa que define o estado desejado do jogo e dando-lhes a oportunidade de explorar inteligentemente o espa√ßo de busca. Aplic√°mos com sucesso o algoritmo de Q-Learning em casos de ambientes discretos e cont√≠nuos, mas com a√ß√µes discretas.

√â importante tamb√©m estudar situa√ß√µes em que o estado das a√ß√µes √© cont√≠nuo e quando o espa√ßo de observa√ß√£o √© muito mais complexo, como a imagem do ecr√£ de um jogo Atari. Nestes problemas, muitas vezes √© necess√°rio usar t√©cnicas de aprendizagem autom√°tica mais avan√ßadas, como redes neuronais, para alcan√ßar bons resultados. Esses t√≥picos mais avan√ßados ser√£o abordados no nosso pr√≥ximo curso de IA mais avan√ßado.

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original no seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas resultantes do uso desta tradu√ß√£o.