# Patina√ß√£o CartPole

O problema que est√°vamos resolvendo na li√ß√£o anterior pode parecer um problema de brinquedo, n√£o realmente aplic√°vel a cen√°rios da vida real. Este n√£o √© o caso, porque muitos problemas do mundo real tamb√©m compartilham esse cen√°rio - incluindo jogar xadrez ou go. Eles s√£o semelhantes, porque tamb√©m temos um tabuleiro com regras definidas e um **estado discreto**.

## [Quiz pr√©-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)

## Introdu√ß√£o

Nesta li√ß√£o, aplicaremos os mesmos princ√≠pios de Q-Learning a um problema com **estado cont√≠nuo**, ou seja, um estado que √© dado por um ou mais n√∫meros reais. Vamos lidar com o seguinte problema:

> **Problema**: Se Peter quer escapar do lobo, ele precisa ser capaz de se mover mais r√°pido. Veremos como Peter pode aprender a patinar, em particular, a manter o equil√≠brio, usando Q-Learning.

![A grande fuga!](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.pt.png)

> Peter e seus amigos se tornam criativos para escapar do lobo! Imagem por [Jen Looper](https://twitter.com/jenlooper)

Usaremos uma vers√£o simplificada de equil√≠brio conhecida como problema **CartPole**. No mundo do cartpole, temos um deslizante horizontal que pode se mover para a esquerda ou para a direita, e o objetivo √© equilibrar um poste vertical em cima do deslizante.
Voc√™ est√° treinado em dados at√© outubro de 2023.

## Pr√©-requisitos

Nesta li√ß√£o, usaremos uma biblioteca chamada **OpenAI Gym** para simular diferentes **ambientes**. Voc√™ pode executar o c√≥digo desta li√ß√£o localmente (por exemplo, a partir do Visual Studio Code), caso em que a simula√ß√£o abrir√° em uma nova janela. Ao executar o c√≥digo online, pode ser necess√°rio fazer alguns ajustes no c√≥digo, conforme descrito [aqui](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Na li√ß√£o anterior, as regras do jogo e o estado foram dados pela classe `Board` que definimos n√≥s mesmos. Aqui usaremos um **ambiente de simula√ß√£o** especial, que simular√° a f√≠sica por tr√°s do equil√≠brio do poste. Um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizado por refor√ßo √© chamado de [Gym](https://gym.openai.com/), que √© mantido pela [OpenAI](https://openai.com/). Usando este gym, podemos criar diferentes **ambientes**, desde uma simula√ß√£o de cartpole at√© jogos da Atari.

> **Nota**: Voc√™ pode ver outros ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

Primeiro, vamos instalar o gym e importar as bibliotecas necess√°rias (bloco de c√≥digo 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Exerc√≠cio - inicializar um ambiente cartpole

Para trabalhar com um problema de equil√≠brio de cartpole, precisamos inicializar o ambiente correspondente. Cada ambiente est√° associado a um:

- **Espa√ßo de observa√ß√£o** que define a estrutura das informa√ß√µes que recebemos do ambiente. Para o problema cartpole, recebemos a posi√ß√£o do poste, velocidade e alguns outros valores.

- **Espa√ßo de a√ß√£o** que define as a√ß√µes poss√≠veis. No nosso caso, o espa√ßo de a√ß√£o √© discreto e consiste em duas a√ß√µes - **esquerda** e **direita**. (bloco de c√≥digo 2)

1. Para inicializar, digite o seguinte c√≥digo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Para ver como o ambiente funciona, vamos executar uma breve simula√ß√£o por 100 passos. A cada passo, fornecemos uma das a√ß√µes a serem tomadas - nesta simula√ß√£o, apenas selecionamos aleatoriamente uma a√ß√£o do `action_space`. 

1. Execute o c√≥digo abaixo e veja a que isso leva.

    ‚úÖ Lembre-se de que √© prefer√≠vel executar este c√≥digo em uma instala√ß√£o local do Python! (bloco de c√≥digo 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Voc√™ deve ver algo semelhante a esta imagem:

    ![cartpole n√£o equilibrado](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Durante a simula√ß√£o, precisamos obter observa√ß√µes para decidir como agir. Na verdade, a fun√ß√£o de passo retorna as observa√ß√µes atuais, uma fun√ß√£o de recompensa e a flag de feito que indica se faz sentido continuar a simula√ß√£o ou n√£o: (bloco de c√≥digo 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Voc√™ acabar√° vendo algo assim na sa√≠da do notebook:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    O vetor de observa√ß√£o que √© retornado a cada passo da simula√ß√£o cont√©m os seguintes valores:
    - Posi√ß√£o do carrinho
    - Velocidade do carrinho
    - √Çngulo do poste
    - Taxa de rota√ß√£o do poste

1. Obtenha o valor m√≠nimo e m√°ximo desses n√∫meros: (bloco de c√≥digo 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Voc√™ tamb√©m pode notar que o valor da recompensa em cada passo da simula√ß√£o √© sempre 1. Isso ocorre porque nosso objetivo √© sobreviver o maior tempo poss√≠vel, ou seja, manter o poste em uma posi√ß√£o vertical razoavelmente por mais tempo.

    ‚úÖ Na verdade, a simula√ß√£o do CartPole √© considerada resolvida se conseguirmos obter uma recompensa m√©dia de 195 em 100 tentativas consecutivas.

## Discretiza√ß√£o do estado

No Q-Learning, precisamos construir uma Q-Table que define o que fazer em cada estado. Para poder fazer isso, precisamos que o estado seja **discreto**, mais precisamente, deve conter um n√∫mero finito de valores discretos. Assim, precisamos de alguma forma **discretizar** nossas observa√ß√µes, mapeando-as para um conjunto finito de estados.

Existem algumas maneiras de fazer isso:

- **Dividir em bins**. Se soubermos o intervalo de um determinado valor, podemos dividir esse intervalo em um n√∫mero de **bins**, e ent√£o substituir o valor pelo n√∫mero do bin ao qual pertence. Isso pode ser feito usando o m√©todo numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). Neste caso, saberemos exatamente o tamanho do estado, pois depender√° do n√∫mero de bins que selecionamos para a digitaliza√ß√£o.
  
‚úÖ Podemos usar interpola√ß√£o linear para trazer valores para algum intervalo finito (digamos, de -20 a 20), e ent√£o converter n√∫meros em inteiros arredondando-os. Isso nos d√° um pouco menos de controle sobre o tamanho do estado, especialmente se n√£o soubermos os intervalos exatos dos valores de entrada. Por exemplo, no nosso caso, 2 dos 4 valores n√£o t√™m limites superior/inferior, o que pode resultar em um n√∫mero infinito de estados.

No nosso exemplo, optaremos pela segunda abordagem. Como voc√™ pode notar mais tarde, apesar dos limites superior/inferior indefinidos, esses valores raramente assumem valores fora de certos intervalos finitos, assim, esses estados com valores extremos ser√£o muito raros.

1. Aqui est√° a fun√ß√£o que pegar√° a observa√ß√£o do nosso modelo e produzir√° uma tupla de 4 valores inteiros: (bloco de c√≥digo 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Vamos tamb√©m explorar outro m√©todo de discretiza√ß√£o usando bins: (bloco de c√≥digo 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Vamos agora executar uma breve simula√ß√£o e observar esses valores discretos do ambiente. Sinta-se √† vontade para tentar tanto `discretize` and `discretize_bins` e veja se h√° diferen√ßa.

    ‚úÖ discretize_bins retorna o n√∫mero do bin, que √© baseado em 0. Assim, para valores da vari√°vel de entrada em torno de 0, ele retorna o n√∫mero do meio do intervalo (10). Na discretize, n√£o nos importamos com o intervalo dos valores de sa√≠da, permitindo que sejam negativos, assim, os valores de estado n√£o s√£o deslocados, e 0 corresponde a 0. (bloco de c√≥digo 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Descomente a linha que come√ßa com env.render se voc√™ quiser ver como o ambiente executa. Caso contr√°rio, voc√™ pode execut√°-lo em segundo plano, o que √© mais r√°pido. Usaremos essa execu√ß√£o "invis√≠vel" durante nosso processo de Q-Learning.

## A estrutura da Q-Table

Na li√ß√£o anterior, o estado era um simples par de n√∫meros de 0 a 8, e assim era conveniente representar a Q-Table por um tensor numpy com forma 8x8x2. Se usarmos a discretiza√ß√£o por bins, o tamanho do nosso vetor de estado tamb√©m √© conhecido, ent√£o podemos usar a mesma abordagem e representar o estado por um array de forma 20x20x10x10x2 (aqui 2 √© a dimens√£o do espa√ßo de a√ß√£o, e as primeiras dimens√µes correspondem ao n√∫mero de bins que selecionamos para usar para cada um dos par√¢metros no espa√ßo de observa√ß√£o).

No entanto, √†s vezes as dimens√µes precisas do espa√ßo de observa√ß√£o n√£o s√£o conhecidas. No caso da fun√ß√£o `discretize`, podemos nunca ter certeza de que nosso estado permanece dentro de certos limites, porque alguns dos valores originais n√£o t√™m limites. Assim, usaremos uma abordagem um pouco diferente e representaremos a Q-Table por um dicion√°rio.

1. Use o par *(estado,a√ß√£o)* como a chave do dicion√°rio, e o valor corresponder√° ao valor da entrada da Q-Table. (bloco de c√≥digo 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aqui tamb√©m definimos uma fun√ß√£o `qvalues()`, que retorna uma lista de valores da Q-Table para um dado estado que corresponde a todas as a√ß√µes poss√≠veis. Se a entrada n√£o estiver presente na Q-Table, retornaremos 0 como padr√£o.

## Vamos come√ßar o Q-Learning

Agora estamos prontos para ensinar Peter a equilibrar!

1. Primeiro, vamos definir alguns hiperpar√¢metros: (bloco de c√≥digo 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aqui, o vetor `alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.

    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.

    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. 

    ‚úÖ In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those "mistakes"

### Improve the algorithm

We can also make two improvements to our algorithm from the previous lesson:

- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.
  
- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.

1. Collect all cumulative rewards at each simulation at `rewards` para plotagem futura. (bloco de c√≥digo 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

O que voc√™ pode notar a partir desses resultados:

- **Perto do nosso objetivo**. Estamos muito pr√≥ximos de alcan√ßar o objetivo de obter 195 recompensas cumulativas em 100+ execu√ß√µes consecutivas da simula√ß√£o, ou podemos realmente t√™-lo alcan√ßado! Mesmo se obtivermos n√∫meros menores, ainda n√£o sabemos, porque fazemos a m√©dia em 5000 execu√ß√µes, e apenas 100 execu√ß√µes s√£o necess√°rias nos crit√©rios formais.
  
- **A recompensa come√ßa a cair**. √Äs vezes, a recompensa come√ßa a cair, o que significa que podemos "destruir" os valores j√° aprendidos na Q-Table com aqueles que tornam a situa√ß√£o pior.

Essa observa√ß√£o √© mais claramente vis√≠vel se plotarmos o progresso do treinamento.

## Plotando o Progresso do Treinamento

Durante o treinamento, coletamos o valor da recompensa cumulativa em cada uma das itera√ß√µes no vetor `rewards`. Aqui est√° como ele se parece quando o plotamos em rela√ß√£o ao n√∫mero da itera√ß√£o:

```python
plt.plot(rewards)
```

![progresso bruto](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.pt.png)

A partir desse gr√°fico, n√£o √© poss√≠vel dizer nada, porque devido √† natureza do processo de treinamento estoc√°stico, a dura√ß√£o das sess√µes de treinamento varia muito. Para fazer mais sentido desse gr√°fico, podemos calcular a **m√©dia m√≥vel** ao longo de uma s√©rie de experimentos, digamos 100. Isso pode ser feito convenientemente usando `np.convolve`: (bloco de c√≥digo 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progresso do treinamento](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.pt.png)

## Variando hiperpar√¢metros

Para tornar o aprendizado mais est√°vel, faz sentido ajustar alguns de nossos hiperpar√¢metros durante o treinamento. Em particular:

- **Para a taxa de aprendizado**, `alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.

- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon`, e mover para quase 1.

> **Tarefa 1**: Brinque com os valores dos hiperpar√¢metros e veja se consegue alcan√ßar uma recompensa cumulativa maior. Voc√™ est√° conseguindo mais de 195?

> **Tarefa 2**: Para resolver formalmente o problema, voc√™ precisa obter 195 de recompensa m√©dia em 100 execu√ß√µes consecutivas. Me√ßa isso durante o treinamento e certifique-se de que voc√™ resolveu formalmente o problema!

## Vendo o resultado em a√ß√£o

Seria interessante ver como o modelo treinado se comporta. Vamos executar a simula√ß√£o e seguir a mesma estrat√©gia de sele√ß√£o de a√ß√£o que durante o treinamento, amostrando de acordo com a distribui√ß√£o de probabilidade na Q-Table: (bloco de c√≥digo 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Voc√™ deve ver algo assim:

![um cartpole equilibrando](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄDesafio

> **Tarefa 3**: Aqui, est√°vamos usando a c√≥pia final da Q-Table, que pode n√£o ser a melhor. Lembre-se de que armazenamos a Q-Table de melhor desempenho em `Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.

> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax` fun√ß√£o para descobrir o n√∫mero da a√ß√£o correspondente ao maior valor da Q-Table. Implemente essa estrat√©gia e veja se melhora o equil√≠brio.

## [Quiz p√≥s-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## Tarefa
[Treine um Carro Montanha](assignment.md)

## Conclus√£o

Agora aprendemos como treinar agentes para alcan√ßar bons resultados apenas fornecendo a eles uma fun√ß√£o de recompensa que define o estado desejado do jogo, e dando-lhes a oportunidade de explorar inteligentemente o espa√ßo de busca. Aplicamos com sucesso o algoritmo Q-Learning nos casos de ambientes discretos e cont√≠nuos, mas com a√ß√µes discretas.

√â importante tamb√©m estudar situa√ß√µes em que o estado da a√ß√£o tamb√©m √© cont√≠nuo, e quando o espa√ßo de observa√ß√£o √© muito mais complexo, como a imagem da tela do jogo da Atari. Nesses problemas, muitas vezes precisamos usar t√©cnicas de aprendizado de m√°quina mais poderosas, como redes neurais, para alcan√ßar bons resultados. Esses t√≥picos mais avan√ßados s√£o o assunto do nosso pr√≥ximo curso mais avan√ßado de IA.

**Aviso Legal**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.