<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T08:35:13+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "pt"
}
-->
# Construir um modelo de regress√£o usando Scikit-learn: quatro abordagens de regress√£o

![Infogr√°fico de regress√£o linear vs polinomial](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infogr√°fico por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

> ### [Esta li√ß√£o est√° dispon√≠vel em R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introdu√ß√£o 

At√© agora, exploraste o que √© regress√£o com dados de exemplo retirados do conjunto de dados de pre√ßos de ab√≥boras que utilizaremos ao longo desta li√ß√£o. Tamb√©m j√° os visualizaste utilizando o Matplotlib.

Agora est√°s pronto para mergulhar mais fundo na regress√£o para Machine Learning. Embora a visualiza√ß√£o permita compreender os dados, o verdadeiro poder do Machine Learning vem do _treinamento de modelos_. Os modelos s√£o treinados com dados hist√≥ricos para capturar automaticamente as depend√™ncias dos dados, permitindo prever resultados para novos dados que o modelo ainda n√£o viu.

Nesta li√ß√£o, aprender√°s mais sobre dois tipos de regress√£o: _regress√£o linear b√°sica_ e _regress√£o polinomial_, juntamente com algumas das matem√°ticas subjacentes a estas t√©cnicas. Esses modelos permitir√£o prever os pre√ßos das ab√≥boras dependendo de diferentes dados de entrada.

[![ML para iniciantes - Compreendendo a Regress√£o Linear](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para iniciantes - Compreendendo a Regress√£o Linear")

> üé• Clica na imagem acima para uma breve vis√£o geral sobre regress√£o linear.

> Ao longo deste curr√≠culo, assumimos um conhecimento m√≠nimo de matem√°tica e procuramos torn√°-lo acess√≠vel para estudantes de outras √°reas. Por isso, presta aten√ß√£o √†s notas, üßÆ chamadas, diagramas e outras ferramentas de aprendizagem para ajudar na compreens√£o.

### Pr√©-requisitos

Deves estar familiarizado, neste momento, com a estrutura dos dados de ab√≥boras que estamos a analisar. Podes encontr√°-los pr√©-carregados e pr√©-limpos no ficheiro _notebook.ipynb_ desta li√ß√£o. No ficheiro, o pre√ßo das ab√≥boras √© exibido por alqueire num novo _data frame_. Certifica-te de que consegues executar estes _notebooks_ em _kernels_ no Visual Studio Code.

### Prepara√ß√£o

Como lembrete, est√°s a carregar estes dados para fazer perguntas sobre eles.

- Qual √© a melhor altura para comprar ab√≥boras? 
- Que pre√ßo posso esperar por uma caixa de ab√≥boras miniatura?
- Devo compr√°-las em cestos de meio alqueire ou em caixas de 1 1/9 de alqueire?
Vamos continuar a explorar estes dados.

Na li√ß√£o anterior, criaste um _data frame_ com Pandas e preencheste-o com parte do conjunto de dados original, padronizando os pre√ßos por alqueire. No entanto, ao fazer isso, s√≥ conseguiste reunir cerca de 400 pontos de dados e apenas para os meses de outono.

D√° uma olhada nos dados que pr√©-carreg√°mos no _notebook_ que acompanha esta li√ß√£o. Os dados est√£o pr√©-carregados e um gr√°fico de dispers√£o inicial √© tra√ßado para mostrar os dados por m√™s. Talvez possamos obter mais detalhes sobre a natureza dos dados ao limp√°-los mais.

## Uma linha de regress√£o linear

Como aprendeste na Li√ß√£o 1, o objetivo de um exerc√≠cio de regress√£o linear √© ser capaz de tra√ßar uma linha para:

- **Mostrar rela√ß√µes entre vari√°veis**. Mostrar a rela√ß√£o entre vari√°veis.
- **Fazer previs√µes**. Fazer previs√µes precisas sobre onde um novo ponto de dados cairia em rela√ß√£o a essa linha.

√â t√≠pico da **Regress√£o dos M√≠nimos Quadrados** tra√ßar este tipo de linha. O termo 'm√≠nimos quadrados' significa que todos os pontos de dados ao redor da linha de regress√£o s√£o elevados ao quadrado e depois somados. Idealmente, essa soma final √© o menor poss√≠vel, porque queremos um n√∫mero baixo de erros, ou `m√≠nimos quadrados`.

Fazemos isso porque queremos modelar uma linha que tenha a menor dist√¢ncia cumulativa de todos os nossos pontos de dados. Tamb√©m elevamos os termos ao quadrado antes de som√°-los, pois estamos preocupados com a magnitude e n√£o com a dire√ß√£o.

> **üßÆ Mostra-me a matem√°tica** 
> 
> Esta linha, chamada de _linha de melhor ajuste_, pode ser expressa por [uma equa√ß√£o](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` √© a 'vari√°vel explicativa'. `Y` √© a 'vari√°vel dependente'. A inclina√ß√£o da linha √© `b` e `a` √© o intercepto em Y, que se refere ao valor de `Y` quando `X = 0`. 
>
>![calcular a inclina√ß√£o](../../../../2-Regression/3-Linear/images/slope.png)
>
> Primeiro, calcula-se a inclina√ß√£o `b`. Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> Em outras palavras, e referindo-se √† pergunta original sobre os dados das ab√≥boras: "prever o pre√ßo de uma ab√≥bora por alqueire por m√™s", `X` referir-se-ia ao pre√ßo e `Y` ao m√™s de venda. 
>
>![completar a equa√ß√£o](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Calcula o valor de Y. Se est√°s a pagar cerca de $4, deve ser abril! Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> A matem√°tica que calcula a linha deve demonstrar a inclina√ß√£o da linha, que tamb√©m depende do intercepto, ou onde `Y` est√° situado quando `X = 0`.
>
> Podes observar o m√©todo de c√°lculo desses valores no site [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Tamb√©m visita [este calculador de m√≠nimos quadrados](https://www.mathsisfun.com/data/least-squares-calculator.html) para ver como os valores dos n√∫meros impactam a linha.

## Correla√ß√£o

Outro termo a compreender √© o **Coeficiente de Correla√ß√£o** entre as vari√°veis X e Y fornecidas. Usando um gr√°fico de dispers√£o, podes visualizar rapidamente este coeficiente. Um gr√°fico com pontos de dados dispersos numa linha ordenada tem alta correla√ß√£o, mas um gr√°fico com pontos de dados espalhados por todo o lado entre X e Y tem baixa correla√ß√£o.

Um bom modelo de regress√£o linear ser√° aquele que tem um Coeficiente de Correla√ß√£o alto (mais pr√≥ximo de 1 do que de 0) usando o m√©todo de Regress√£o dos M√≠nimos Quadrados com uma linha de regress√£o.

‚úÖ Executa o _notebook_ que acompanha esta li√ß√£o e observa o gr√°fico de dispers√£o de M√™s para Pre√ßo. Os dados que associam M√™s ao Pre√ßo das vendas de ab√≥boras parecem ter alta ou baixa correla√ß√£o, de acordo com a tua interpreta√ß√£o visual do gr√°fico de dispers√£o? Isso muda se usares uma medida mais detalhada em vez de `M√™s`, por exemplo, *dia do ano* (ou seja, n√∫mero de dias desde o in√≠cio do ano)?

No c√≥digo abaixo, assumiremos que limp√°mos os dados e obtivemos um _data frame_ chamado `new_pumpkins`, semelhante ao seguinte:

ID | M√™s | DiaDoAno | Variedade | Cidade | Embalagem | Pre√ßo Baixo | Pre√ßo Alto | Pre√ßo
---|-----|----------|-----------|--------|-----------|-------------|------------|-------
70 | 9 | 267 | TIPO TORTA | BALTIMORE | caixas de 1 1/9 alqueires | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | TIPO TORTA | BALTIMORE | caixas de 1 1/9 alqueires | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | TIPO TORTA | BALTIMORE | caixas de 1 1/9 alqueires | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | TIPO TORTA | BALTIMORE | caixas de 1 1/9 alqueires | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | TIPO TORTA | BALTIMORE | caixas de 1 1/9 alqueires | 15.0 | 15.0 | 13.636364

> O c√≥digo para limpar os dados est√° dispon√≠vel em [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Realiz√°mos os mesmos passos de limpeza da li√ß√£o anterior e calcul√°mos a coluna `DiaDoAno` usando a seguinte express√£o: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Agora que tens uma compreens√£o da matem√°tica por tr√°s da regress√£o linear, vamos criar um modelo de Regress√£o para ver se conseguimos prever qual embalagem de ab√≥boras ter√° os melhores pre√ßos. Algu√©m que compra ab√≥boras para um campo de ab√≥boras de feriado pode querer esta informa√ß√£o para otimizar as suas compras de embalagens de ab√≥boras para o campo.

## Procurando por Correla√ß√£o

[![ML para iniciantes - Procurando por Correla√ß√£o: A Chave para a Regress√£o Linear](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para iniciantes - Procurando por Correla√ß√£o: A Chave para a Regress√£o Linear")

> üé• Clica na imagem acima para uma breve vis√£o geral sobre correla√ß√£o.

Na li√ß√£o anterior, provavelmente viste que o pre√ßo m√©dio para diferentes meses parece assim:

<img alt="Pre√ßo m√©dio por m√™s" src="../2-Data/images/barchart.png" width="50%"/>

Isto sugere que deve haver alguma correla√ß√£o, e podemos tentar treinar um modelo de regress√£o linear para prever a rela√ß√£o entre `M√™s` e `Pre√ßo`, ou entre `DiaDoAno` e `Pre√ßo`. Aqui est√° o gr√°fico de dispers√£o que mostra a √∫ltima rela√ß√£o:

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/scatter-dayofyear.png" width="50%" /> 

Vamos verificar se h√° correla√ß√£o usando a fun√ß√£o `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que a correla√ß√£o √© bastante pequena, -0.15 por `M√™s` e -0.17 por `DiaDoAno`, mas pode haver outra rela√ß√£o importante. Parece que h√° diferentes agrupamentos de pre√ßos correspondentes a diferentes variedades de ab√≥boras. Para confirmar esta hip√≥tese, vamos tra√ßar cada categoria de ab√≥bora usando uma cor diferente. Passando um par√¢metro `ax` para a fun√ß√£o de plotagem `scatter`, podemos tra√ßar todos os pontos no mesmo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/scatter-dayofyear-color.png" width="50%" /> 

A nossa investiga√ß√£o sugere que a variedade tem mais efeito no pre√ßo geral do que a data de venda. Podemos ver isso com um gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de pre√ßo vs variedade" src="images/price-by-variety.png" width="50%" /> 

Vamos focar-nos, por enquanto, apenas numa variedade de ab√≥bora, o 'tipo torta', e ver qual o efeito da data no pre√ßo:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Se agora calcularmos a correla√ß√£o entre `Pre√ßo` e `DiaDoAno` usando a fun√ß√£o `corr`, obteremos algo como `-0.27` - o que significa que treinar um modelo preditivo faz sentido.

> Antes de treinar um modelo de regress√£o linear, √© importante garantir que os nossos dados est√£o limpos. A regress√£o linear n√£o funciona bem com valores ausentes, por isso faz sentido eliminar todas as c√©lulas vazias:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Outra abordagem seria preencher esses valores vazios com valores m√©dios da coluna correspondente.

## Regress√£o Linear Simples

[![ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn")

> üé• Clica na imagem acima para uma breve vis√£o geral sobre regress√£o linear e polinomial.

Para treinar o nosso modelo de Regress√£o Linear, utilizaremos a biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Come√ßamos separando os valores de entrada (caracter√≠sticas) e a sa√≠da esperada (r√≥tulo) em arrays numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Nota que tivemos de realizar `reshape` nos dados de entrada para que o pacote de Regress√£o Linear os compreendesse corretamente. A Regress√£o Linear espera um array 2D como entrada, onde cada linha do array corresponde a um vetor de caracter√≠sticas de entrada. No nosso caso, como temos apenas uma entrada, precisamos de um array com forma N√ó1, onde N √© o tamanho do conjunto de dados.

Depois, precisamos dividir os dados em conjuntos de treino e teste, para que possamos validar o nosso modelo ap√≥s o treino:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, treinar o modelo de Regress√£o Linear propriamente dito leva apenas duas linhas de c√≥digo. Definimos o objeto `LinearRegression` e ajustamo-lo aos nossos dados usando o m√©todo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

O objeto `LinearRegression` ap√≥s o ajuste (`fit`) cont√©m todos os coeficientes da regress√£o, que podem ser acessados usando a propriedade `.coef_`. No nosso caso, h√° apenas um coeficiente, que deve ser em torno de `-0.017`. Isso significa que os pre√ßos parecem cair um pouco com o tempo, mas n√£o muito, cerca de 2 c√™ntimos por dia. Tamb√©m podemos acessar o ponto de interse√ß√£o da regress√£o com o eixo Y usando `lin_reg.intercept_` - ser√° em torno de `21` no nosso caso, indicando o pre√ßo no in√≠cio do ano.

Para ver qu√£o preciso √© o nosso modelo, podemos prever pre√ßos num conjunto de dados de teste e depois medir qu√£o pr√≥ximas est√£o as nossas previs√µes dos valores esperados. Isso pode ser feito usando a m√©trica de erro quadr√°tico m√©dio (MSE), que √© a m√©dia de todas as diferen√ßas ao quadrado entre o valor esperado e o previsto.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
O nosso erro parece estar em torno de 2 pontos, o que equivale a ~17%. N√£o √© muito bom. Outro indicador da qualidade do modelo √© o **coeficiente de determina√ß√£o**, que pode ser obtido assim:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Se o valor for 0, significa que o modelo n√£o considera os dados de entrada e age como o *pior preditor linear*, que √© simplesmente o valor m√©dio do resultado. O valor 1 significa que conseguimos prever perfeitamente todos os resultados esperados. No nosso caso, o coeficiente √© cerca de 0,06, o que √© bastante baixo.

Tamb√©m podemos tra√ßar os dados de teste juntamente com a linha de regress√£o para visualizar melhor como a regress√£o funciona no nosso caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Regress√£o linear" src="images/linear-results.png" width="50%" />

## Regress√£o Polinomial

Outro tipo de Regress√£o Linear √© a Regress√£o Polinomial. Embora √†s vezes exista uma rela√ß√£o linear entre vari√°veis - quanto maior o volume da ab√≥bora, maior o pre√ßo - em outras situa√ß√µes essas rela√ß√µes n√£o podem ser representadas como um plano ou linha reta.

‚úÖ Aqui est√£o [alguns exemplos adicionais](https://online.stat.psu.edu/stat501/lesson/9/9.8) de dados que poderiam usar Regress√£o Polinomial.

Observe novamente a rela√ß√£o entre Data e Pre√ßo. Este gr√°fico de dispers√£o parece que deveria ser analisado necessariamente por uma linha reta? Os pre√ßos n√£o podem flutuar? Neste caso, pode-se tentar a regress√£o polinomial.

‚úÖ Polin√≥mios s√£o express√µes matem√°ticas que podem consistir em uma ou mais vari√°veis e coeficientes.

A regress√£o polinomial cria uma linha curva para se ajustar melhor a dados n√£o lineares. No nosso caso, se incluirmos uma vari√°vel `DayOfYear` ao quadrado nos dados de entrada, deveremos ser capazes de ajustar os dados com uma curva parab√≥lica, que ter√° um m√≠nimo em determinado ponto do ano.

O Scikit-learn inclui uma [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) √∫til para combinar diferentes etapas de processamento de dados. Um **pipeline** √© uma cadeia de **estimadores**. No nosso caso, criaremos um pipeline que primeiro adiciona caracter√≠sticas polinomiais ao modelo e, em seguida, treina a regress√£o:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usar `PolynomialFeatures(2)` significa que incluiremos todos os polin√≥mios de segundo grau dos dados de entrada. No nosso caso, isso significar√° apenas `DayOfYear`<sup>2</sup>, mas, dado duas vari√°veis de entrada X e Y, isso adicionar√° X<sup>2</sup>, XY e Y<sup>2</sup>. Tamb√©m podemos usar polin√≥mios de grau superior, se quisermos.

Os pipelines podem ser usados da mesma forma que o objeto original `LinearRegression`, ou seja, podemos usar `fit` no pipeline e, em seguida, usar `predict` para obter os resultados da previs√£o. Aqui est√° o gr√°fico mostrando os dados de teste e a curva de aproxima√ß√£o:

<img alt="Regress√£o polinomial" src="images/poly-results.png" width="50%" />

Usando a Regress√£o Polinomial, conseguimos um MSE ligeiramente mais baixo e um coeficiente de determina√ß√£o mais alto, mas n√£o significativamente. Precisamos levar em conta outras caracter√≠sticas!

> Pode-se observar que os pre√ßos m√≠nimos das ab√≥boras ocorrem por volta do Halloween. Como explicaria isso?

üéÉ Parab√©ns, acabou de criar um modelo que pode ajudar a prever o pre√ßo das ab√≥boras para tartes. Provavelmente poderia repetir o mesmo procedimento para todos os tipos de ab√≥bora, mas isso seria trabalhoso. Vamos agora aprender como levar em conta a variedade de ab√≥bora no nosso modelo!

## Caracter√≠sticas Categ√≥ricas

No mundo ideal, queremos ser capazes de prever pre√ßos para diferentes variedades de ab√≥bora usando o mesmo modelo. No entanto, a coluna `Variety` √© um pouco diferente de colunas como `Month`, porque cont√©m valores n√£o num√©ricos. Essas colunas s√£o chamadas de **categ√≥ricas**.

[![ML para principiantes - Previs√µes com caracter√≠sticas categ√≥ricas usando Regress√£o Linear](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML para principiantes - Previs√µes com caracter√≠sticas categ√≥ricas usando Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo sobre o uso de caracter√≠sticas categ√≥ricas.

Aqui pode ver como o pre√ßo m√©dio depende da variedade:

<img alt="Pre√ßo m√©dio por variedade" src="images/price-by-variety.png" width="50%" />

Para levar a variedade em conta, primeiro precisamos convert√™-la para uma forma num√©rica, ou **codific√°-la**. Existem v√°rias formas de fazer isso:

* A **codifica√ß√£o num√©rica simples** criar√° uma tabela com as diferentes variedades e, em seguida, substituir√° o nome da variedade por um √≠ndice nessa tabela. Esta n√£o √© a melhor ideia para regress√£o linear, porque a regress√£o linear considera o valor num√©rico do √≠ndice e adiciona-o ao resultado, multiplicando por algum coeficiente. No nosso caso, a rela√ß√£o entre o n√∫mero do √≠ndice e o pre√ßo √© claramente n√£o linear, mesmo que garantamos que os √≠ndices estejam ordenados de uma forma espec√≠fica.
* A **codifica√ß√£o one-hot** substituir√° a coluna `Variety` por 4 colunas diferentes, uma para cada variedade. Cada coluna conter√° `1` se a linha correspondente for de uma determinada variedade, e `0` caso contr√°rio. Isso significa que haver√° quatro coeficientes na regress√£o linear, um para cada variedade de ab√≥bora, respons√°veis pelo "pre√ßo inicial" (ou melhor, "pre√ßo adicional") para essa variedade espec√≠fica.

O c√≥digo abaixo mostra como podemos codificar uma variedade usando one-hot encoding:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para treinar a regress√£o linear usando a variedade codificada como one-hot nos dados de entrada, s√≥ precisamos inicializar os dados `X` e `y` corretamente:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

O restante do c√≥digo √© o mesmo que usamos acima para treinar a Regress√£o Linear. Se experimentar, ver√° que o erro quadr√°tico m√©dio √© aproximadamente o mesmo, mas obtemos um coeficiente de determina√ß√£o muito mais alto (~77%). Para obter previs√µes ainda mais precisas, podemos levar em conta mais caracter√≠sticas categ√≥ricas, bem como caracter√≠sticas num√©ricas, como `Month` ou `DayOfYear`. Para obter um grande conjunto de caracter√≠sticas, podemos usar `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqui tamb√©m levamos em conta `City` e o tipo de `Package`, o que nos d√° um MSE de 2,84 (10%) e um coeficiente de determina√ß√£o de 0,94!

## Resumindo tudo

Para criar o melhor modelo, podemos usar dados combinados (categ√≥ricos codificados como one-hot + num√©ricos) do exemplo acima juntamente com a Regress√£o Polinomial. Aqui est√° o c√≥digo completo para sua conveni√™ncia:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Isso deve nos dar o melhor coeficiente de determina√ß√£o de quase 97% e MSE=2,23 (~8% de erro de previs√£o).

| Modelo | MSE | Determina√ß√£o |
|--------|-----|--------------|
| `DayOfYear` Linear | 2,77 (17,2%) | 0,07 |
| `DayOfYear` Polinomial | 2,73 (17,0%) | 0,08 |
| `Variety` Linear | 5,24 (19,7%) | 0,77 |
| Todas as caracter√≠sticas Linear | 2,84 (10,5%) | 0,94 |
| Todas as caracter√≠sticas Polinomial | 2,23 (8,25%) | 0,97 |

üèÜ Muito bem! Criou quatro modelos de Regress√£o numa √∫nica li√ß√£o e melhorou a qualidade do modelo para 97%. Na se√ß√£o final sobre Regress√£o, aprender√° sobre Regress√£o Log√≠stica para determinar categorias.

---
## üöÄDesafio

Teste v√°rias vari√°veis diferentes neste notebook para ver como a correla√ß√£o corresponde √† precis√£o do modelo.

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Revis√£o e Autoestudo

Nesta li√ß√£o aprendemos sobre Regress√£o Linear. Existem outros tipos importantes de Regress√£o. Leia sobre as t√©cnicas Stepwise, Ridge, Lasso e Elasticnet. Um bom curso para aprender mais √© o [curso de Aprendizagem Estat√≠stica de Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Tarefa

[Crie um Modelo](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante ter em conta que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.