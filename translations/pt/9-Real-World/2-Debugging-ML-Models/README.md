<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T08:42:52+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "pt"
}
-->
# P√≥s-escrito: Depura√ß√£o de Modelos de Machine Learning usando componentes do painel de IA Respons√°vel

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

## Introdu√ß√£o

O machine learning impacta as nossas vidas diariamente. A IA est√° a integrar-se em alguns dos sistemas mais importantes que nos afetam como indiv√≠duos e como sociedade, desde a sa√∫de, finan√ßas, educa√ß√£o e emprego. Por exemplo, sistemas e modelos est√£o envolvidos em tarefas de tomada de decis√£o di√°ria, como diagn√≥sticos m√©dicos ou dete√ß√£o de fraude. Consequentemente, os avan√ßos na IA, juntamente com a sua ado√ß√£o acelerada, est√£o a ser acompanhados por expectativas sociais em evolu√ß√£o e regulamenta√ß√µes crescentes em resposta. Continuamos a observar √°reas onde os sistemas de IA n√£o correspondem √†s expectativas; exp√µem novos desafios; e os governos est√£o a come√ßar a regulamentar solu√ß√µes de IA. Por isso, √© essencial que estes modelos sejam analisados para fornecer resultados justos, confi√°veis, inclusivos, transparentes e respons√°veis para todos.

Neste curr√≠culo, iremos explorar ferramentas pr√°ticas que podem ser usadas para avaliar se um modelo apresenta problemas relacionados com IA respons√°vel. As t√©cnicas tradicionais de depura√ß√£o de machine learning tendem a basear-se em c√°lculos quantitativos, como precis√£o agregada ou perda m√©dia de erro. Imagine o que pode acontecer quando os dados que est√° a usar para construir esses modelos carecem de certos grupos demogr√°ficos, como ra√ßa, g√©nero, vis√£o pol√≠tica, religi√£o, ou representam desproporcionalmente esses grupos demogr√°ficos. E quando a sa√≠da do modelo √© interpretada de forma a favorecer um grupo demogr√°fico? Isso pode introduzir uma representa√ß√£o excessiva ou insuficiente desses grupos sens√≠veis, resultando em problemas de justi√ßa, inclus√£o ou confiabilidade no modelo. Outro fator √© que os modelos de machine learning s√£o considerados caixas pretas, o que dificulta a compreens√£o e explica√ß√£o do que impulsiona as previs√µes de um modelo. Todos estes s√£o desafios enfrentados por cientistas de dados e desenvolvedores de IA quando n√£o possuem ferramentas adequadas para depurar e avaliar a justi√ßa ou confiabilidade de um modelo.

Nesta li√ß√£o, ir√° aprender a depurar os seus modelos usando:

- **An√°lise de Erros**: identificar onde na distribui√ß√£o dos seus dados o modelo apresenta taxas de erro elevadas.
- **Vis√£o Geral do Modelo**: realizar an√°lises comparativas entre diferentes coortes de dados para descobrir disparidades nas m√©tricas de desempenho do modelo.
- **An√°lise de Dados**: investigar onde pode haver representa√ß√£o excessiva ou insuficiente nos seus dados que podem enviesar o modelo para favorecer um grupo demogr√°fico em detrimento de outro.
- **Import√¢ncia das Features**: compreender quais features est√£o a impulsionar as previs√µes do modelo a n√≠vel global ou local.

## Pr√©-requisito

Como pr√©-requisito, reveja [Ferramentas de IA Respons√°vel para desenvolvedores](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif sobre Ferramentas de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## An√°lise de Erros

As m√©tricas tradicionais de desempenho de modelos usadas para medir a precis√£o s√£o, na maioria das vezes, c√°lculos baseados em previs√µes corretas versus incorretas. Por exemplo, determinar que um modelo √© preciso 89% das vezes com uma perda de erro de 0.001 pode ser considerado um bom desempenho. No entanto, os erros nem sempre est√£o distribu√≠dos uniformemente no seu conjunto de dados subjacente. Pode obter uma pontua√ß√£o de precis√£o de 89% no modelo, mas descobrir que existem diferentes regi√µes nos seus dados onde o modelo falha 42% das vezes. As consequ√™ncias desses padr√µes de falha em certos grupos de dados podem levar a problemas de justi√ßa ou confiabilidade. √â essencial compreender as √°reas onde o modelo est√° a ter um bom desempenho ou n√£o. As regi√µes de dados onde h√° um n√∫mero elevado de imprecis√µes no modelo podem revelar-se um grupo demogr√°fico importante.

![Analisar e depurar erros do modelo](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

O componente de An√°lise de Erros no painel de IA Respons√°vel ilustra como as falhas do modelo est√£o distribu√≠das entre v√°rios coortes com uma visualiza√ß√£o em √°rvore. Isto √© √∫til para identificar features ou √°reas onde h√° uma taxa de erro elevada no seu conjunto de dados. Ao ver de onde v√™m a maioria das imprecis√µes do modelo, pode come√ßar a investigar a causa raiz. Tamb√©m pode criar coortes de dados para realizar an√°lises. Esses coortes de dados ajudam no processo de depura√ß√£o para determinar porque o desempenho do modelo √© bom num coorte, mas apresenta erros noutro.

![An√°lise de Erros](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

Os indicadores visuais no mapa de √°rvore ajudam a localizar as √°reas problem√°ticas mais rapidamente. Por exemplo, quanto mais escuro for o tom de vermelho num n√≥ da √°rvore, maior ser√° a taxa de erro.

O mapa de calor √© outra funcionalidade de visualiza√ß√£o que os utilizadores podem usar para investigar a taxa de erro usando uma ou duas features para encontrar contribuintes para os erros do modelo em todo o conjunto de dados ou coortes.

![Mapa de Calor da An√°lise de Erros](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Use a an√°lise de erros quando precisar de:

* Obter uma compreens√£o profunda de como as falhas do modelo est√£o distribu√≠das num conjunto de dados e em v√°rias dimens√µes de entrada e features.
* Dividir as m√©tricas de desempenho agregadas para descobrir automaticamente coortes com erros e informar os seus passos de mitiga√ß√£o direcionados.

## Vis√£o Geral do Modelo

Avaliar o desempenho de um modelo de machine learning requer uma compreens√£o hol√≠stica do seu comportamento. Isto pode ser alcan√ßado ao rever mais de uma m√©trica, como taxa de erro, precis√£o, recall, precis√£o ou MAE (Erro Absoluto M√©dio), para encontrar disparidades entre as m√©tricas de desempenho. Uma m√©trica de desempenho pode parecer √≥tima, mas imprecis√µes podem ser expostas noutra m√©trica. Al√©m disso, comparar as m√©tricas para disparidades em todo o conjunto de dados ou coortes ajuda a esclarecer onde o modelo est√° a ter um bom desempenho ou n√£o. Isto √© especialmente importante para observar o desempenho do modelo entre features sens√≠veis versus insens√≠veis (por exemplo, ra√ßa, g√©nero ou idade de pacientes) para descobrir potenciais injusti√ßas que o modelo possa ter. Por exemplo, descobrir que o modelo √© mais impreciso num coorte que possui features sens√≠veis pode revelar potenciais injusti√ßas no modelo.

O componente Vis√£o Geral do Modelo do painel de IA Respons√°vel ajuda n√£o apenas a analisar as m√©tricas de desempenho da representa√ß√£o de dados num coorte, mas tamb√©m d√° aos utilizadores a capacidade de comparar o comportamento do modelo entre diferentes coortes.

![Coortes de conjunto de dados - vis√£o geral do modelo no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

A funcionalidade de an√°lise baseada em features do componente permite que os utilizadores reduzam subgrupos de dados dentro de uma feature espec√≠fica para identificar anomalias a um n√≠vel granular. Por exemplo, o painel possui intelig√™ncia integrada para gerar automaticamente coortes para uma feature selecionada pelo utilizador (ex., *"tempo_no_hospital < 3"* ou *"tempo_no_hospital >= 7"*). Isto permite que o utilizador isole uma feature espec√≠fica de um grupo de dados maior para ver se √© um influenciador chave dos resultados errados do modelo.

![Coortes de features - vis√£o geral do modelo no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

O componente Vis√£o Geral do Modelo suporta duas classes de m√©tricas de disparidade:

**Disparidade no desempenho do modelo**: Estes conjuntos de m√©tricas calculam a disparidade (diferen√ßa) nos valores da m√©trica de desempenho selecionada entre subgrupos de dados. Aqui est√£o alguns exemplos:

* Disparidade na taxa de precis√£o
* Disparidade na taxa de erro
* Disparidade na precis√£o
* Disparidade no recall
* Disparidade no erro absoluto m√©dio (MAE)

**Disparidade na taxa de sele√ß√£o**: Esta m√©trica cont√©m a diferen√ßa na taxa de sele√ß√£o (previs√£o favor√°vel) entre subgrupos. Um exemplo disso √© a disparidade nas taxas de aprova√ß√£o de empr√©stimos. A taxa de sele√ß√£o refere-se √† fra√ß√£o de pontos de dados em cada classe classificados como 1 (em classifica√ß√£o bin√°ria) ou √† distribui√ß√£o de valores de previs√£o (em regress√£o).

## An√°lise de Dados

> "Se torturar os dados o suficiente, eles confessar√£o qualquer coisa" - Ronald Coase

Esta afirma√ß√£o parece extrema, mas √© verdade que os dados podem ser manipulados para apoiar qualquer conclus√£o. Tal manipula√ß√£o pode, por vezes, acontecer de forma n√£o intencional. Como humanos, todos temos preconceitos, e √© frequentemente dif√≠cil saber conscientemente quando estamos a introduzir preconceitos nos dados. Garantir justi√ßa na IA e no machine learning continua a ser um desafio complexo.

Os dados s√£o um grande ponto cego para as m√©tricas tradicionais de desempenho de modelos. Pode ter pontua√ß√µes de precis√£o elevadas, mas isso nem sempre reflete o preconceito subjacente nos dados que pode estar no seu conjunto de dados. Por exemplo, se um conjunto de dados de funcion√°rios tem 27% de mulheres em posi√ß√µes executivas numa empresa e 73% de homens no mesmo n√≠vel, um modelo de IA de publicidade de emprego treinado com esses dados pode direcionar principalmente um p√∫blico masculino para posi√ß√µes de n√≠vel s√©nior. Ter este desequil√≠brio nos dados enviesou a previs√£o do modelo para favorecer um g√©nero. Isto revela um problema de justi√ßa onde h√° preconceito de g√©nero no modelo de IA.

O componente de An√°lise de Dados no painel de IA Respons√°vel ajuda a identificar √°reas onde h√° representa√ß√£o excessiva ou insuficiente no conjunto de dados. Ajuda os utilizadores a diagnosticar a causa raiz de erros e problemas de justi√ßa introduzidos por desequil√≠brios nos dados ou falta de representa√ß√£o de um grupo de dados espec√≠fico. Isto d√° aos utilizadores a capacidade de visualizar conjuntos de dados com base em resultados previstos e reais, grupos de erros e features espec√≠ficas. Por vezes, descobrir um grupo de dados sub-representado tamb√©m pode revelar que o modelo n√£o est√° a aprender bem, da√≠ as imprecis√µes elevadas. Ter um modelo com preconceito nos dados n√£o √© apenas um problema de justi√ßa, mas mostra que o modelo n√£o √© inclusivo ou confi√°vel.

![Componente de An√°lise de Dados no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Use a an√°lise de dados quando precisar de:

* Explorar as estat√≠sticas do seu conjunto de dados selecionando diferentes filtros para dividir os seus dados em diferentes dimens√µes (tamb√©m conhecidos como coortes).
* Compreender a distribui√ß√£o do seu conjunto de dados entre diferentes coortes e grupos de features.
* Determinar se as suas descobertas relacionadas com justi√ßa, an√°lise de erros e causalidade (derivadas de outros componentes do painel) s√£o resultado da distribui√ß√£o do seu conjunto de dados.
* Decidir em quais √°reas coletar mais dados para mitigar erros que surgem de problemas de representa√ß√£o, ru√≠do de r√≥tulos, ru√≠do de features, preconceito de r√≥tulos e fatores semelhantes.

## Interpretabilidade do Modelo

Os modelos de machine learning tendem a ser caixas pretas. Compreender quais features de dados chave impulsionam a previs√£o de um modelo pode ser desafiador. √â importante fornecer transpar√™ncia sobre porque um modelo faz uma determinada previs√£o. Por exemplo, se um sistema de IA prev√™ que um paciente diab√©tico est√° em risco de ser readmitido num hospital em menos de 30 dias, deve ser capaz de fornecer dados de suporte que levaram √† sua previs√£o. Ter indicadores de suporte traz transpar√™ncia para ajudar os cl√≠nicos ou hospitais a tomar decis√µes bem informadas. Al√©m disso, ser capaz de explicar porque um modelo fez uma previs√£o para um paciente individual permite responsabilidade com regulamentos de sa√∫de. Quando est√° a usar modelos de machine learning de formas que afetam a vida das pessoas, √© crucial compreender e explicar o que influencia o comportamento de um modelo. A explicabilidade e interpretabilidade do modelo ajudam a responder a perguntas em cen√°rios como:

* Depura√ß√£o do modelo: Por que o meu modelo cometeu este erro? Como posso melhorar o meu modelo?
* Colabora√ß√£o humano-IA: Como posso compreender e confiar nas decis√µes do modelo?
* Conformidade regulat√≥ria: O meu modelo cumpre os requisitos legais?

O componente Import√¢ncia das Features do painel de IA Respons√°vel ajuda a depurar e obter uma compreens√£o abrangente de como um modelo faz previs√µes. Tamb√©m √© uma ferramenta √∫til para profissionais de machine learning e tomadores de decis√£o explicarem e mostrarem evid√™ncias das features que influenciam o comportamento do modelo para conformidade regulat√≥ria. Em seguida, os utilizadores podem explorar explica√ß√µes globais e locais para validar quais features impulsionam a previs√£o do modelo. As explica√ß√µes globais listam as principais features que afetaram a previs√£o geral do modelo. As explica√ß√µes locais mostram quais features levaram √† previs√£o do modelo para um caso individual. A capacidade de avaliar explica√ß√µes locais tamb√©m √© √∫til na depura√ß√£o ou auditoria de um caso espec√≠fico para compreender e interpretar melhor porque um modelo fez uma previs√£o precisa ou imprecisa.

![Componente Import√¢ncia das Features no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Explica√ß√µes globais: Por exemplo, quais features afetam o comportamento geral de um modelo de readmiss√£o hospitalar para diabetes?
* Explica√ß√µes locais: Por exemplo, porque foi previsto que um paciente diab√©tico com mais de 60 anos e hospitaliza√ß√µes anteriores seria readmitido ou n√£o readmitido num hospital dentro de 30 dias?

No processo de depura√ß√£o ao examinar o desempenho de um modelo entre diferentes coortes, a Import√¢ncia das Features mostra o n√≠vel de impacto que uma feature tem entre os coortes. Ajuda a revelar anomalias ao comparar o n√≠vel de influ√™ncia que a feature tem em impulsionar previs√µes erradas do modelo. O componente Import√¢ncia das Features pode mostrar quais valores numa feature influenciaram positivamente ou negativamente o resultado do modelo. Por exemplo, se um modelo fez uma previs√£o imprecisa, o componente d√°-lhe a capacidade de aprofundar e identificar quais features ou valores de features impulsionaram a previs√£o. Este n√≠vel de detalhe ajuda n√£o apenas na depura√ß√£o, mas tamb√©m fornece transpar√™ncia e responsabilidade em situa√ß√µes de auditoria. Finalmente, o componente pode ajud√°-lo a identificar problemas de justi√ßa. Para ilustrar, se uma feature sens√≠vel como etnia ou g√©nero √© altamente influente em impulsionar a previs√£o do modelo, isso pode ser um sinal de preconceito de ra√ßa ou g√©nero no modelo.

![Import√¢ncia das features](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Use a interpretabilidade quando precisar de:

* Determinar qu√£o confi√°veis s√£o as previs√µes do seu sistema de IA ao compreender quais features s√£o mais importantes para as previs√µes.
* Abordar a depura√ß√£o do seu modelo ao compreend√™-lo primeiro e identificar se o modelo est√° a usar features saud√°veis ou apenas correla√ß√µes falsas.
* Descobrir potenciais fontes de injusti√ßa ao compreender se o modelo est√° a basear previs√µes em features sens√≠veis ou em features altamente correlacionadas com elas.
* Construir confian√ßa do utilizador nas decis√µes do modelo ao gerar explica√ß√µes locais para ilustrar os seus resultados.
* Completar uma auditoria regulat√≥ria de um sistema de IA para validar modelos e monitorizar o impacto das decis√µes do modelo nas pessoas.

## Conclus√£o

Todos os componentes do painel de IA Respons√°vel s√£o ferramentas pr√°ticas para ajud√°-lo a construir modelos de machine learning que sejam menos prejudiciais e mais confi√°veis para a sociedade. Melhoram a preven√ß√£o de amea√ßas aos direitos humanos; discrimina√ß√£o ou exclus√£o de certos grupos de oportunidades de vida; e o risco de danos f√≠sicos ou psicol√≥gicos. Tamb√©m ajudam a construir confian√ßa nas decis√µes do modelo ao gerar explica√ß√µes locais para ilustrar os seus resultados. Alguns dos potenciais danos podem ser classificados como:

- **Aloca√ß√£o**, se um g√©nero ou etnia, por exemplo, for favorecido em detrimento de outro.
- **Qualidade do servi√ßo**. Se treinar os dados para um cen√°rio espec√≠fico, mas a realidade for muito mais complexa, isso leva a um servi√ßo de desempenho inferior.
- **Estereotipagem**. Associar um determinado grupo a atributos pr√©-atribu√≠dos.
- **Denigra√ß√£o**. Criticar injustamente e rotular algo ou algu√©m.
- **Representa√ß√£o excessiva ou insuficiente**. A ideia √© que um determinado grupo n√£o seja visto em uma certa profiss√£o, e qualquer servi√ßo ou fun√ß√£o que continue a promover isso est√° a contribuir para o problema.

### Azure RAI dashboard

[Azure RAI dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) √© constru√≠do com ferramentas de c√≥digo aberto desenvolvidas por institui√ß√µes acad√©micas e organiza√ß√µes l√≠deres, incluindo a Microsoft, que s√£o fundamentais para cientistas de dados e desenvolvedores de IA compreenderem melhor o comportamento dos modelos, descobrirem e mitigarem problemas indesej√°veis nos modelos de IA.

- Aprenda a usar os diferentes componentes consultando a [documenta√ß√£o do RAI dashboard.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Veja alguns [notebooks de exemplo do RAI dashboard](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) para depurar cen√°rios de IA mais respons√°veis no Azure Machine Learning.

---
## üöÄ Desafio

Para evitar que vieses estat√≠sticos ou de dados sejam introduzidos desde o in√≠cio, devemos:

- ter uma diversidade de origens e perspetivas entre as pessoas que trabalham nos sistemas
- investir em conjuntos de dados que reflitam a diversidade da nossa sociedade
- desenvolver melhores m√©todos para detetar e corrigir vieses quando eles ocorrem

Pense em cen√°rios da vida real onde a injusti√ßa √© evidente na constru√ß√£o e utiliza√ß√£o de modelos. O que mais devemos considerar?

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)
## Revis√£o e Autoestudo

Nesta li√ß√£o, aprendeu algumas ferramentas pr√°ticas para incorporar IA respons√°vel em machine learning.

Veja este workshop para aprofundar os t√≥picos:

- Responsible AI Dashboard: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica, por Besmira Nushi e Mehrnoosh Sameki

[![Responsible AI Dashboard: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica")

> üé• Clique na imagem acima para ver o v√≠deo: Responsible AI Dashboard: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica, por Besmira Nushi e Mehrnoosh Sameki

Consulte os seguintes materiais para aprender mais sobre IA respons√°vel e como construir modelos mais confi√°veis:

- Ferramentas do RAI dashboard da Microsoft para depurar modelos de ML: [Recursos de ferramentas de IA respons√°vel](https://aka.ms/rai-dashboard)

- Explore o toolkit de IA respons√°vel: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Centro de recursos de IA respons√°vel da Microsoft: [Recursos de IA Respons√°vel ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de pesquisa FATE da Microsoft: [FATE: Justi√ßa, Responsabilidade, Transpar√™ncia e √âtica em IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Tarefa

[Explore o RAI dashboard](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original no seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas resultantes do uso desta tradu√ß√£o.