{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  },
  "colab": {
   "name": "lesson_14.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "coopTranslator": {
   "original_hash": "ad65fb4aad0a156b42216e4929f490fc",
   "translation_date": "2025-09-03T20:16:16+00:00",
   "source_file": "5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb",
   "language_code": "pt"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GULATlQXLXyR"
   },
   "source": [
    "## Explore K-Means clustering usando R e princ√≠pios de dados organizados.\n",
    "\n",
    "### [**Question√°rio pr√©-aula**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)\n",
    "\n",
    "Nesta li√ß√£o, vais aprender a criar clusters utilizando o pacote Tidymodels e outros pacotes do ecossistema R (vamos cham√°-los de amigos üßë‚Äçü§ù‚Äçüßë), e o conjunto de dados de m√∫sica nigeriana que importaste anteriormente. Vamos abordar os fundamentos do K-Means para Clustering. Lembra-te de que, como aprendeste na li√ß√£o anterior, existem v√°rias formas de trabalhar com clusters e o m√©todo que utilizas depende dos teus dados. Vamos experimentar o K-Means, pois √© a t√©cnica de clustering mais comum. Vamos come√ßar!\n",
    "\n",
    "Termos que vais aprender:\n",
    "\n",
    "-   Pontua√ß√£o de Silhueta\n",
    "\n",
    "-   M√©todo do Cotovelo\n",
    "\n",
    "-   In√©rcia\n",
    "\n",
    "-   Vari√¢ncia\n",
    "\n",
    "### **Introdu√ß√£o**\n",
    "\n",
    "[K-Means Clustering](https://wikipedia.org/wiki/K-means_clustering) √© um m√©todo derivado do dom√≠nio do processamento de sinais. √â utilizado para dividir e agrupar conjuntos de dados em `k clusters` com base em semelhan√ßas nas suas caracter√≠sticas.\n",
    "\n",
    "Os clusters podem ser visualizados como [Diagramas de Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), que incluem um ponto (ou 'semente') e a sua regi√£o correspondente.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/voronoi.png\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Infogr√°fico por Jen Looper</figcaption>\n",
    "\n",
    "\n",
    "O clustering K-Means segue os seguintes passos:\n",
    "\n",
    "1.  O cientista de dados come√ßa por especificar o n√∫mero desejado de clusters a serem criados.\n",
    "\n",
    "2.  Em seguida, o algoritmo seleciona aleatoriamente K observa√ß√µes do conjunto de dados para servir como os centros iniciais dos clusters (ou seja, os centr√≥ides).\n",
    "\n",
    "3.  Depois, cada uma das observa√ß√µes restantes √© atribu√≠da ao centr√≥ide mais pr√≥ximo.\n",
    "\n",
    "4.  A seguir, a nova m√©dia de cada cluster √© calculada e o centr√≥ide √© movido para essa m√©dia.\n",
    "\n",
    "5.  Agora que os centros foram recalculados, cada observa√ß√£o √© novamente verificada para ver se pode estar mais pr√≥xima de um cluster diferente. Todos os objetos s√£o novamente reatribu√≠dos utilizando as m√©dias atualizadas dos clusters. Os passos de atribui√ß√£o de clusters e atualiza√ß√£o dos centr√≥ides s√£o repetidos iterativamente at√© que as atribui√ß√µes de clusters deixem de mudar (ou seja, quando se alcan√ßa a converg√™ncia). Normalmente, o algoritmo termina quando cada nova itera√ß√£o resulta em um movimento insignificante dos centr√≥ides e os clusters tornam-se est√°ticos.\n",
    "\n",
    "<div>\n",
    "\n",
    "> Nota que, devido √† randomiza√ß√£o das observa√ß√µes iniciais k utilizadas como centr√≥ides iniciais, podemos obter resultados ligeiramente diferentes cada vez que aplicamos o procedimento. Por esta raz√£o, a maioria dos algoritmos utiliza v√°rios *in√≠cios aleat√≥rios* e escolhe a itera√ß√£o com o menor WCSS. Assim, √© fortemente recomendado executar o K-Means com v√°rios valores de *nstart* para evitar um *√≥timo local indesej√°vel.*\n",
    "\n",
    "</div>\n",
    "\n",
    "Esta curta anima√ß√£o utilizando a [arte](https://github.com/allisonhorst/stats-illustrations) de Allison Horst explica o processo de clustering:\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/kmeans.gif\"\n",
    "   width=\"550\"/>\n",
    "   <figcaption>Arte por @allison_horst</figcaption>\n",
    "\n",
    "\n",
    "\n",
    "Uma quest√£o fundamental que surge no clustering √© esta: como sabes quantos clusters deves separar nos teus dados? Uma desvantagem do uso do K-Means √© o facto de que precisas de estabelecer `k`, ou seja, o n√∫mero de `centr√≥ides`. Felizmente, o `m√©todo do cotovelo` ajuda a estimar um bom valor inicial para `k`. Vais experiment√°-lo em breve.\n",
    "\n",
    "### \n",
    "\n",
    "**Pr√©-requisito**\n",
    "\n",
    "Vamos come√ßar exatamente de onde par√°mos na [li√ß√£o anterior](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb), onde analis√°mos o conjunto de dados, fizemos v√°rias visualiza√ß√µes e filtr√°mos o conjunto de dados para observa√ß√µes de interesse. Certifica-te de que o revisitas!\n",
    "\n",
    "Vamos precisar de alguns pacotes para completar este m√≥dulo. Podes instal√°-los com: `install.packages(c('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork'))`\n",
    "\n",
    "Alternativamente, o script abaixo verifica se tens os pacotes necess√°rios para completar este m√≥dulo e instala-os para ti caso alguns estejam em falta.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ah_tBi58LXyi"
   },
   "source": [
    "suppressWarnings(if(!require(\"pacman\")) install.packages(\"pacman\"))\n",
    "\n",
    "pacman::p_load('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e--UCUTLXym"
   },
   "source": [
    "Vamos come√ßar com tudo!\n",
    "\n",
    "## 1. Uma dan√ßa com dados: Reduzir aos 3 g√©neros musicais mais populares\n",
    "\n",
    "Este √© um resumo do que fizemos na li√ß√£o anterior. Vamos explorar e analisar alguns dados!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ycamx7GGLXyn"
   },
   "source": [
    "# Load the core tidyverse and make it available in your current R session\n",
    "library(tidyverse)\n",
    "\n",
    "# Import the data into a tibble\n",
    "df <- read_csv(file = \"https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/data/nigerian-songs.csv\", show_col_types = FALSE)\n",
    "\n",
    "# Narrow down to top 3 popular genres\n",
    "nigerian_songs <- df %>% \n",
    "  # Concentrate on top 3 genres\n",
    "  filter(artist_top_genre %in% c(\"afro dancehall\", \"afropop\",\"nigerian pop\")) %>% \n",
    "  # Remove unclassified observations\n",
    "  filter(popularity != 0)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize popular genres using bar plots\n",
    "theme_set(theme_light())\n",
    "nigerian_songs %>%\n",
    "  count(artist_top_genre) %>%\n",
    "  ggplot(mapping = aes(x = artist_top_genre, y = n,\n",
    "                       fill = artist_top_genre)) +\n",
    "  geom_col(alpha = 0.8) +\n",
    "  paletteer::scale_fill_paletteer_d(\"ggsci::category10_d3\") +\n",
    "  ggtitle(\"Top genres\") +\n",
    "  theme(plot.title = element_text(hjust = 0.5))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5h5zmkPLXyp"
   },
   "source": [
    "ü§© Isso correu bem!\n",
    "\n",
    "## 2. Mais explora√ß√£o de dados.\n",
    "\n",
    "Qu√£o limpos est√£o estes dados? Vamos verificar a exist√™ncia de valores at√≠picos utilizando boxplots. Vamos concentrar-nos em colunas num√©ricas com menos valores at√≠picos (embora seja poss√≠vel limpar os valores at√≠picos). Os boxplots podem mostrar o intervalo dos dados e ajudar a escolher quais colunas usar. Note que os boxplots n√£o mostram a vari√¢ncia, um elemento importante para dados bem agrup√°veis. Por favor, veja [esta discuss√£o](https://stats.stackexchange.com/questions/91536/deduce-variance-from-boxplot) para leitura adicional.\n",
    "\n",
    "[Boxplots](https://en.wikipedia.org/wiki/Box_plot) s√£o usados para representar graficamente a distribui√ß√£o de dados `num√©ricos`, ent√£o vamos come√ßar por *selecionar* todas as colunas num√©ricas juntamente com os g√©neros musicais populares.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HhNreJKLLXyq"
   },
   "source": [
    "# Select top genre column and all other numeric columns\n",
    "df_numeric <- nigerian_songs %>% \n",
    "  select(artist_top_genre, where(is.numeric)) \n",
    "\n",
    "# Display the data\n",
    "df_numeric %>% \n",
    "  slice_head(n = 5)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYXrwJRaLXyq"
   },
   "source": [
    "Veja como o auxiliar de sele√ß√£o `where` torna isso f√°cil üíÅ? Explore outras fun√ß√µes semelhantes [aqui](https://tidyselect.r-lib.org/).\n",
    "\n",
    "Como vamos criar um boxplot para cada caracter√≠stica num√©rica e queremos evitar o uso de loops, vamos reformular os nossos dados para um formato *mais longo* que nos permitir√° tirar proveito de `facets` - subgr√°ficos que exibem cada subconjunto dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gd5bR3f8LXys"
   },
   "source": [
    "# Pivot data from wide to long\n",
    "df_numeric_long <- df_numeric %>% \n",
    "  pivot_longer(!artist_top_genre, names_to = \"feature_names\", values_to = \"values\") \n",
    "\n",
    "# Print out data\n",
    "df_numeric_long %>% \n",
    "  slice_head(n = 15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7tE1swnLXyv"
   },
   "source": [
    "Muito mais longo! Agora √© hora de alguns `ggplots`! Ent√£o, que `geom` vamos usar?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r88bIsyuLXyy"
   },
   "source": [
    "# Make a box plot\n",
    "df_numeric_long %>% \n",
    "  ggplot(mapping = aes(x = feature_names, y = values, fill = feature_names)) +\n",
    "  geom_boxplot() +\n",
    "  facet_wrap(~ feature_names, ncol = 4, scales = \"free\") +\n",
    "  theme(legend.position = \"none\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYVyKIUELXyz"
   },
   "source": [
    "Agora podemos ver que estes dados est√£o um pouco ruidosos: ao observar cada coluna como um boxplot, √© poss√≠vel identificar valores at√≠picos. Poder√≠amos percorrer o conjunto de dados e remover esses valores at√≠picos, mas isso tornaria os dados bastante reduzidos.\n",
    "\n",
    "Por agora, vamos escolher quais colunas utilizaremos para o nosso exerc√≠cio de clustering. Vamos selecionar as colunas num√©ricas com intervalos semelhantes. Poder√≠amos codificar o `artist_top_genre` como num√©rico, mas vamos descart√°-lo por enquanto.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wkpINyZLXy0"
   },
   "source": [
    "# Select variables with similar ranges\n",
    "df_numeric_select <- df_numeric %>% \n",
    "  select(popularity, danceability, acousticness, loudness, energy) \n",
    "\n",
    "# Normalize data\n",
    "# df_numeric_select <- scale(df_numeric_select)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7dLzgpqLXy1"
   },
   "source": [
    "## 3. Computa√ß√£o de clustering k-means em R\n",
    "\n",
    "Podemos calcular k-means em R utilizando a fun√ß√£o integrada `kmeans`, veja `help(\"kmeans()\")`. A fun√ß√£o `kmeans()` aceita um data frame com todas as colunas num√©ricas como seu argumento principal.\n",
    "\n",
    "O primeiro passo ao usar clustering k-means √© especificar o n√∫mero de clusters (k) que ser√£o gerados na solu√ß√£o final. Sabemos que existem 3 g√©neros musicais que extra√≠mos do conjunto de dados, ent√£o vamos tentar com 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uC4EQ5w7LXy5"
   },
   "source": [
    "set.seed(2056)\n",
    "# Kmeans clustering for 3 clusters\n",
    "kclust <- kmeans(\n",
    "  df_numeric_select,\n",
    "  # Specify the number of clusters\n",
    "  centers = 3,\n",
    "  # How many random initial configurations\n",
    "  nstart = 25\n",
    ")\n",
    "\n",
    "# Display clustering object\n",
    "kclust\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzfhscWrLXy-"
   },
   "source": [
    "O objeto kmeans cont√©m v√°rias informa√ß√µes que est√£o bem explicadas em `help(\"kmeans()\")`. Por agora, vamos concentrar-nos em algumas. Vemos que os dados foram agrupados em 3 clusters com tamanhos de 65, 110, 111. O resultado tamb√©m cont√©m os centros dos clusters (m√©dias) para os 3 grupos ao longo das 5 vari√°veis.\n",
    "\n",
    "O vetor de clustering √© a atribui√ß√£o de cluster para cada observa√ß√£o. Vamos usar a fun√ß√£o `augment` para adicionar a atribui√ß√£o de cluster ao conjunto de dados original.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0XwwpFGQLXy_"
   },
   "source": [
    "# Add predicted cluster assignment to data set\n",
    "augment(kclust, df_numeric_select) %>% \n",
    "  relocate(.cluster) %>% \n",
    "  slice_head(n = 10)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXIVXXACLXzA"
   },
   "source": [
    "Perfeito, acab√°mos de dividir o nosso conjunto de dados em 3 grupos. Ent√£o, qu√£o bom √© o nosso agrupamento ü§∑? Vamos dar uma olhada no `Silhouette score`.\n",
    "\n",
    "### **Silhouette score**\n",
    "\n",
    "[A an√°lise de Silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)) pode ser usada para estudar a dist√¢ncia de separa√ß√£o entre os clusters resultantes. Este score varia de -1 a 1, e se o score estiver pr√≥ximo de 1, o cluster √© denso e bem separado dos outros clusters. Um valor pr√≥ximo de 0 representa clusters sobrepostos com amostras muito pr√≥ximas da fronteira de decis√£o dos clusters vizinhos. [fonte](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam).\n",
    "\n",
    "O m√©todo de silhouette m√©dio calcula a m√©dia do silhouette das observa√ß√µes para diferentes valores de *k*. Um score m√©dio de silhouette elevado indica um bom agrupamento.\n",
    "\n",
    "A fun√ß√£o `silhouette` no pacote de cluster √© usada para calcular a largura m√©dia do silhouette.\n",
    "\n",
    "> O silhouette pode ser calculado com qualquer [m√©trica de dist√¢ncia](https://en.wikipedia.org/wiki/Distance \"Distance\"), como a [dist√¢ncia Euclidiana](https://en.wikipedia.org/wiki/Euclidean_distance \"Euclidean distance\") ou a [dist√¢ncia Manhattan](https://en.wikipedia.org/wiki/Manhattan_distance \"Manhattan distance\"), que discutimos na [li√ß√£o anterior](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jn0McL28LXzB"
   },
   "source": [
    "# Load cluster package\n",
    "library(cluster)\n",
    "\n",
    "# Compute average silhouette score\n",
    "ss <- silhouette(kclust$cluster,\n",
    "                 # Compute euclidean distance\n",
    "                 dist = dist(df_numeric_select))\n",
    "mean(ss[, 3])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyQRn97nLXzC"
   },
   "source": [
    "A nossa pontua√ß√£o √© **0,549**, ou seja, bem no meio. Isto indica que os nossos dados n√£o s√£o particularmente adequados para este tipo de agrupamento. Vamos ver se conseguimos confirmar esta suspeita visualmente. O [pacote factoextra](https://rpkgs.datanovia.com/factoextra/index.html) fornece fun√ß√µes (`fviz_cluster()`) para visualizar agrupamentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7a6Km1_FLXzD"
   },
   "source": [
    "library(factoextra)\n",
    "\n",
    "# Visualize clustering results\n",
    "fviz_cluster(kclust, df_numeric_select)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwCWt-0LXzD"
   },
   "source": [
    "A sobreposi√ß√£o nos clusters indica que os nossos dados n√£o s√£o particularmente adequados para este tipo de clustering, mas vamos continuar.\n",
    "\n",
    "## 4. Determinar o n√∫mero ideal de clusters\n",
    "\n",
    "Uma quest√£o fundamental que frequentemente surge no clustering K-Means √© esta: sem etiquetas de classe conhecidas, como saber quantos clusters usar para separar os seus dados?\n",
    "\n",
    "Uma forma de tentar descobrir √© usar uma amostra de dados para `criar uma s√©rie de modelos de clustering` com um n√∫mero crescente de clusters (por exemplo, de 1 a 10) e avaliar m√©tricas de clustering como o **Silhouette score.**\n",
    "\n",
    "Vamos determinar o n√∫mero ideal de clusters calculando o algoritmo de clustering para diferentes valores de *k* e avaliando o **Within Cluster Sum of Squares** (WCSS). O total de soma de quadrados dentro do cluster (WCSS) mede a compacidade do clustering, e queremos que seja o menor poss√≠vel, com valores mais baixos significando que os pontos de dados est√£o mais pr√≥ximos.\n",
    "\n",
    "Vamos explorar o efeito de diferentes escolhas de `k`, de 1 a 10, neste clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hSeIiylDLXzE"
   },
   "source": [
    "# Create a series of clustering models\n",
    "kclusts <- tibble(k = 1:10) %>% \n",
    "  # Perform kmeans clustering for 1,2,3 ... ,10 clusters\n",
    "  mutate(model = map(k, ~ kmeans(df_numeric_select, centers = .x, nstart = 25)),\n",
    "  # Farm out clustering metrics eg WCSS\n",
    "         glanced = map(model, ~ glance(.x))) %>% \n",
    "  unnest(cols = glanced)\n",
    "  \n",
    "\n",
    "# View clustering rsulsts\n",
    "kclusts\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7rS2U1eLXzE"
   },
   "source": [
    "Agora que temos o total da soma dos quadrados dentro dos clusters (tot.withinss) para cada algoritmo de clustering com centro *k*, utilizamos o [m√©todo do cotovelo](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) para encontrar o n√∫mero ideal de clusters. O m√©todo consiste em tra√ßar o WCSS como uma fun√ß√£o do n√∫mero de clusters e escolher o [cotovelo da curva](https://en.wikipedia.org/wiki/Elbow_of_the_curve \"Cotovelo da curva\") como o n√∫mero de clusters a ser utilizado.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o_DjHGItLXzF"
   },
   "source": [
    "set.seed(2056)\n",
    "# Use elbow method to determine optimum number of clusters\n",
    "kclusts %>% \n",
    "  ggplot(mapping = aes(x = k, y = tot.withinss)) +\n",
    "  geom_line(size = 1.2, alpha = 0.8, color = \"#FF7F0EFF\") +\n",
    "  geom_point(size = 2, color = \"#FF7F0EFF\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLYyt5XSLXzG"
   },
   "source": [
    "O gr√°fico mostra uma grande redu√ß√£o no WCSS (ou seja, maior *coer√™ncia*) √† medida que o n√∫mero de clusters aumenta de um para dois, e uma redu√ß√£o adicional not√°vel de dois para tr√™s clusters. Depois disso, a redu√ß√£o torna-se menos acentuada, resultando num `cotovelo` üí™ no gr√°fico por volta de tr√™s clusters. Isto √© uma boa indica√ß√£o de que existem dois a tr√™s clusters de pontos de dados razoavelmente bem separados.\n",
    "\n",
    "Podemos agora avan√ßar e extrair o modelo de clustering onde `k = 3`:\n",
    "\n",
    "> `pull()`: usado para extrair uma √∫nica coluna\n",
    ">\n",
    "> `pluck()`: usado para indexar estruturas de dados como listas\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JP_JPKBILXzG"
   },
   "source": [
    "# Extract k = 3 clustering\n",
    "final_kmeans <- kclusts %>% \n",
    "  filter(k == 3) %>% \n",
    "  pull(model) %>% \n",
    "  pluck(1)\n",
    "\n",
    "\n",
    "final_kmeans\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_PDTu8tLXzI"
   },
   "source": [
    "√ìtimo! Vamos visualizar os clusters obtidos. Que tal adicionar um pouco de interatividade usando `plotly`?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dNcleFe-LXzJ"
   },
   "source": [
    "# Add predicted cluster assignment to data set\n",
    "results <-  augment(final_kmeans, df_numeric_select) %>% \n",
    "  bind_cols(df_numeric %>% select(artist_top_genre)) \n",
    "\n",
    "# Plot cluster assignments\n",
    "clust_plt <- results %>% \n",
    "  ggplot(mapping = aes(x = popularity, y = danceability, color = .cluster, shape = artist_top_genre)) +\n",
    "  geom_point(size = 2, alpha = 0.8) +\n",
    "  paletteer::scale_color_paletteer_d(\"ggthemes::Tableau_10\")\n",
    "\n",
    "ggplotly(clust_plt)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JUM_51VLXzK"
   },
   "source": [
    "Talvez esper√°ssemos que cada grupo (representado por cores diferentes) tivesse g√©neros distintos (representados por formas diferentes).\n",
    "\n",
    "Vamos analisar a precis√£o do modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HdIMUGq7LXzL"
   },
   "source": [
    "# Assign genres to predefined integers\n",
    "label_count <- results %>% \n",
    "  group_by(artist_top_genre) %>% \n",
    "  mutate(id = cur_group_id()) %>% \n",
    "  ungroup() %>% \n",
    "  summarise(correct_labels = sum(.cluster == id))\n",
    "\n",
    "\n",
    "# Print results  \n",
    "cat(\"Result:\", label_count$correct_labels, \"out of\", nrow(results), \"samples were correctly labeled.\")\n",
    "\n",
    "cat(\"\\nAccuracy score:\", label_count$correct_labels/nrow(results))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C50wvaAOLXzM"
   },
   "source": [
    "A precis√£o deste modelo n√£o √© m√°, mas tamb√©m n√£o √© excelente. Pode ser que os dados n√£o sejam adequados para o K-Means Clustering. Estes dados s√£o demasiado desequilibrados, pouco correlacionados e h√° muita vari√¢ncia entre os valores das colunas para formar clusters eficazes. Na verdade, os clusters que se formam provavelmente s√£o fortemente influenciados ou enviesados pelas tr√™s categorias de g√©nero que definimos acima.\n",
    "\n",
    "Ainda assim, foi um processo de aprendizagem interessante!\n",
    "\n",
    "Na documenta√ß√£o do Scikit-learn, pode-se ver que um modelo como este, com clusters pouco bem definidos, tem um problema de 'vari√¢ncia':\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/problems.png\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Infogr√°fico do Scikit-learn</figcaption>\n",
    "\n",
    "\n",
    "\n",
    "## **Vari√¢ncia**\n",
    "\n",
    "A vari√¢ncia √© definida como \"a m√©dia das diferen√ßas quadradas em rela√ß√£o √† m√©dia\" [fonte](https://www.mathsisfun.com/data/standard-deviation.html). No contexto deste problema de clustering, refere-se aos dados em que os n√∫meros do nosso conjunto tendem a divergir um pouco demais da m√©dia.\n",
    "\n",
    "‚úÖ Este √© um √≥timo momento para pensar em todas as formas de corrigir este problema. Ajustar os dados um pouco mais? Usar colunas diferentes? Utilizar um algoritmo diferente? Dica: Experimente [escalar os seus dados](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) para normaliz√°-los e testar outras colunas.\n",
    "\n",
    "> Experimente este '[calculador de vari√¢ncia](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' para compreender melhor o conceito.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **üöÄDesafio**\n",
    "\n",
    "Dedique algum tempo a este notebook, ajustando os par√¢metros. Consegue melhorar a precis√£o do modelo ao limpar mais os dados (removendo outliers, por exemplo)? Pode usar pesos para dar mais import√¢ncia a determinadas amostras de dados. O que mais pode fazer para criar clusters melhores?\n",
    "\n",
    "Dica: Experimente escalar os seus dados. H√° c√≥digo comentado no notebook que adiciona escalonamento padr√£o para fazer com que as colunas de dados se assemelhem mais em termos de intervalo. Vai perceber que, embora o score de silhueta diminua, o 'cotovelo' no gr√°fico suaviza-se. Isto acontece porque deixar os dados sem escala permite que dados com menos vari√¢ncia tenham mais peso. Leia mais sobre este problema [aqui](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).\n",
    "\n",
    "## [**Question√°rio p√≥s-aula**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)\n",
    "\n",
    "## **Revis√£o & Autoestudo**\n",
    "\n",
    "-   D√™ uma olhada num Simulador de K-Means [como este](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Pode usar esta ferramenta para visualizar pontos de dados de amostra e determinar os seus centr√≥ides. Pode editar a aleatoriedade dos dados, o n√∫mero de clusters e o n√∫mero de centr√≥ides. Isto ajuda-o a ter uma ideia de como os dados podem ser agrupados?\n",
    "\n",
    "-   Al√©m disso, veja [este documento sobre K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) da Stanford.\n",
    "\n",
    "Quer testar as suas novas compet√™ncias de clustering em conjuntos de dados que se adaptam bem ao K-Means clustering? Veja:\n",
    "\n",
    "-   [Treinar e Avaliar Modelos de Clustering](https://rpubs.com/eR_ic/clustering) usando Tidymodels e amigos\n",
    "\n",
    "-   [An√°lise de Cluster K-Means](https://uc-r.github.io/kmeans_clustering), UC Business Analytics R Programming Guide\n",
    "\n",
    "- [Clustering K-Means com princ√≠pios de dados organizados](https://www.tidymodels.org/learn/statistics/k-means/)\n",
    "\n",
    "## **Tarefa**\n",
    "\n",
    "[Experimente diferentes m√©todos de clustering](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/assignment.md)\n",
    "\n",
    "## AGRADECIMENTOS A:\n",
    "\n",
    "[Jen Looper](https://www.twitter.com/jenlooper) por criar a vers√£o original em Python deste m√≥dulo ‚ô•Ô∏è\n",
    "\n",
    "[`Allison Horst`](https://twitter.com/allison_horst/) por criar as ilustra√ß√µes incr√≠veis que tornam o R mais acolhedor e envolvente. Encontre mais ilustra√ß√µes na sua [galeria](https://www.google.com/url?q=https://github.com/allisonhorst/stats-illustrations&sa=D&source=editors&ust=1626380772530000&usg=AOvVaw3zcfyCizFQZpkSLzxiiQEM).\n",
    "\n",
    "Boas aprendizagens,\n",
    "\n",
    "[Eric](https://twitter.com/ericntay), Embaixador Estudante Gold da Microsoft Learn.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/r_learners_sm.jpeg\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Arte por @allison_horst</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.\n"
   ]
  }
 ]
}