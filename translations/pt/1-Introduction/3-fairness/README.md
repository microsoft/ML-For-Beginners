<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T08:43:41+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "pt"
}
-->
# Construir solu√ß√µes de Machine Learning com IA respons√°vel

![Resumo da IA respons√°vel em Machine Learning em um sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

## Introdu√ß√£o

Neste curr√≠culo, come√ßar√° a descobrir como o machine learning pode impactar e j√° est√° a impactar as nossas vidas di√°rias. Mesmo agora, sistemas e modelos est√£o envolvidos em tarefas de tomada de decis√£o di√°ria, como diagn√≥sticos de sa√∫de, aprova√ß√µes de empr√©stimos ou dete√ß√£o de fraudes. Por isso, √© importante que esses modelos funcionem bem para fornecer resultados confi√°veis. Assim como qualquer aplica√ß√£o de software, os sistemas de IA podem falhar em atender √†s expectativas ou gerar resultados indesej√°veis. √â por isso que √© essencial compreender e explicar o comportamento de um modelo de IA.

Imagine o que pode acontecer quando os dados que utiliza para construir esses modelos n√£o incluem certos grupos demogr√°ficos, como ra√ßa, g√©nero, vis√£o pol√≠tica, religi√£o, ou representam esses grupos de forma desproporcional. E se a sa√≠da do modelo for interpretada de forma a favorecer um grupo demogr√°fico? Qual √© a consequ√™ncia para a aplica√ß√£o? Al√©m disso, o que acontece quando o modelo gera um resultado adverso e prejudica as pessoas? Quem √© respons√°vel pelo comportamento dos sistemas de IA? Estas s√£o algumas quest√µes que exploraremos neste curr√≠culo.

Nesta li√ß√£o, ir√°:

- Aumentar a sua consci√™ncia sobre a import√¢ncia da equidade no machine learning e os danos relacionados com a falta de equidade.
- Familiarizar-se com a pr√°tica de explorar outliers e cen√°rios incomuns para garantir fiabilidade e seguran√ßa.
- Compreender a necessidade de capacitar todos ao projetar sistemas inclusivos.
- Explorar como √© vital proteger a privacidade e a seguran√ßa dos dados e das pessoas.
- Perceber a import√¢ncia de uma abordagem de "caixa de vidro" para explicar o comportamento dos modelos de IA.
- Ter em mente como a responsabilidade √© essencial para construir confian√ßa nos sistemas de IA.

## Pr√©-requisito

Como pr√©-requisito, fa√ßa o percurso de aprendizagem "Princ√≠pios de IA Respons√°vel" e assista ao v√≠deo abaixo sobre o tema:

Saiba mais sobre IA Respons√°vel seguindo este [Percurso de Aprendizagem](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Abordagem da Microsoft para IA Respons√°vel](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Abordagem da Microsoft para IA Respons√°vel")

> üé• Clique na imagem acima para assistir ao v√≠deo: Abordagem da Microsoft para IA Respons√°vel

## Equidade

Os sistemas de IA devem tratar todos de forma justa e evitar afetar grupos semelhantes de pessoas de maneiras diferentes. Por exemplo, quando os sistemas de IA fornecem orienta√ß√µes sobre tratamentos m√©dicos, aplica√ß√µes de empr√©stimos ou emprego, devem fazer as mesmas recomenda√ß√µes para todos com sintomas, circunst√¢ncias financeiras ou qualifica√ß√µes profissionais semelhantes. Cada um de n√≥s, como seres humanos, carrega preconceitos herdados que afetam as nossas decis√µes e a√ß√µes. Esses preconceitos podem ser evidentes nos dados que usamos para treinar sistemas de IA. Tal manipula√ß√£o pode, por vezes, ocorrer de forma n√£o intencional. Muitas vezes, √© dif√≠cil perceber conscientemente quando est√° a introduzir preconceitos nos dados.

**"Injusti√ßa"** abrange impactos negativos, ou "danos", para um grupo de pessoas, como aqueles definidos em termos de ra√ßa, g√©nero, idade ou defici√™ncia. Os principais danos relacionados com a equidade podem ser classificados como:

- **Aloca√ß√£o**, se, por exemplo, um g√©nero ou etnia for favorecido em detrimento de outro.
- **Qualidade do servi√ßo**. Se treinar os dados para um cen√°rio espec√≠fico, mas a realidade for muito mais complexa, isso leva a um servi√ßo de desempenho inferior. Por exemplo, um dispensador de sab√£o que n√£o consegue detetar pessoas com pele escura. [Refer√™ncia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigra√ß√£o**. Criticar ou rotular algo ou algu√©m de forma injusta. Por exemplo, uma tecnologia de rotulagem de imagens que, de forma infame, rotulou imagens de pessoas de pele escura como gorilas.
- **Sobre ou sub-representa√ß√£o**. A ideia de que um determinado grupo n√£o √© visto numa certa profiss√£o, e qualquer servi√ßo ou fun√ß√£o que continue a promover isso est√° a contribuir para o dano.
- **Estereotipagem**. Associar um grupo a atributos pr√©-definidos. Por exemplo, um sistema de tradu√ß√£o entre ingl√™s e turco pode apresentar imprecis√µes devido a palavras com associa√ß√µes estereotipadas de g√©nero.

![tradu√ß√£o para turco](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> tradu√ß√£o para turco

![tradu√ß√£o de volta para ingl√™s](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> tradu√ß√£o de volta para ingl√™s

Ao projetar e testar sistemas de IA, precisamos garantir que a IA seja justa e n√£o programada para tomar decis√µes enviesadas ou discriminat√≥rias, que tamb√©m s√£o proibidas para os seres humanos. Garantir a equidade na IA e no machine learning continua a ser um desafio sociot√©cnico complexo.

### Fiabilidade e seguran√ßa

Para construir confian√ßa, os sistemas de IA precisam ser fi√°veis, seguros e consistentes em condi√ß√µes normais e inesperadas. √â importante saber como os sistemas de IA se comportar√£o numa variedade de situa√ß√µes, especialmente quando se trata de outliers. Ao construir solu√ß√µes de IA, √© necess√°rio focar-se substancialmente em como lidar com uma ampla variedade de circunst√¢ncias que as solu√ß√µes de IA podem encontrar. Por exemplo, um carro aut√≥nomo precisa de colocar a seguran√ßa das pessoas como prioridade m√°xima. Como resultado, a IA que alimenta o carro precisa de considerar todos os cen√°rios poss√≠veis que o carro pode encontrar, como noite, tempestades, nevascas, crian√ßas a atravessar a rua, animais de estima√ß√£o, obras na estrada, etc. O qu√£o bem um sistema de IA consegue lidar com uma ampla gama de condi√ß√µes de forma fi√°vel e segura reflete o n√≠vel de antecipa√ß√£o que o cientista de dados ou desenvolvedor de IA considerou durante o design ou teste do sistema.

> [üé• Clique aqui para um v√≠deo: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusividade

Os sistemas de IA devem ser projetados para envolver e capacitar todos. Ao projetar e implementar sistemas de IA, cientistas de dados e desenvolvedores de IA identificam e abordam potenciais barreiras no sistema que poderiam, de forma n√£o intencional, excluir pessoas. Por exemplo, existem 1 bili√£o de pessoas com defici√™ncia em todo o mundo. Com o avan√ßo da IA, elas podem aceder a uma ampla gama de informa√ß√µes e oportunidades mais facilmente nas suas vidas di√°rias. Ao abordar essas barreiras, criam-se oportunidades para inovar e desenvolver produtos de IA com melhores experi√™ncias que beneficiam todos.

> [üé• Clique aqui para um v√≠deo: inclusividade na IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Seguran√ßa e privacidade

Os sistemas de IA devem ser seguros e respeitar a privacidade das pessoas. As pessoas t√™m menos confian√ßa em sistemas que colocam a sua privacidade, informa√ß√µes ou vidas em risco. Ao treinar modelos de machine learning, confiamos nos dados para produzir os melhores resultados. Ao faz√™-lo, a origem e a integridade dos dados devem ser consideradas. Por exemplo, os dados foram submetidos por utilizadores ou estavam dispon√≠veis publicamente? Al√©m disso, ao trabalhar com os dados, √© crucial desenvolver sistemas de IA que possam proteger informa√ß√µes confidenciais e resistir a ataques. √Ä medida que a IA se torna mais prevalente, proteger a privacidade e garantir a seguran√ßa de informa√ß√µes pessoais e empresariais importantes est√° a tornar-se mais cr√≠tico e complexo. Quest√µes de privacidade e seguran√ßa de dados requerem aten√ß√£o especial para a IA, pois o acesso aos dados √© essencial para que os sistemas de IA fa√ßam previs√µes e decis√µes precisas e informadas sobre as pessoas.

> [üé• Clique aqui para um v√≠deo: seguran√ßa na IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como ind√∫stria, fizemos avan√ßos significativos em privacidade e seguran√ßa, impulsionados significativamente por regulamenta√ß√µes como o RGPD (Regulamento Geral de Prote√ß√£o de Dados).
- No entanto, com os sistemas de IA, devemos reconhecer a tens√£o entre a necessidade de mais dados pessoais para tornar os sistemas mais personalizados e eficazes ‚Äì e a privacidade.
- Assim como com o nascimento de computadores conectados √† internet, tamb√©m estamos a assistir a um grande aumento no n√∫mero de problemas de seguran√ßa relacionados com a IA.
- Ao mesmo tempo, vimos a IA ser usada para melhorar a seguran√ßa. Por exemplo, a maioria dos scanners antiv√≠rus modernos √© alimentada por heur√≠sticas de IA.
- Precisamos garantir que os nossos processos de ci√™ncia de dados se harmonizem com as pr√°ticas mais recentes de privacidade e seguran√ßa.

### Transpar√™ncia

Os sistemas de IA devem ser compreens√≠veis. Uma parte crucial da transpar√™ncia √© explicar o comportamento dos sistemas de IA e os seus componentes. Melhorar a compreens√£o dos sistemas de IA exige que as partes interessadas compreendam como e por que funcionam, para que possam identificar potenciais problemas de desempenho, preocupa√ß√µes com seguran√ßa e privacidade, preconceitos, pr√°ticas de exclus√£o ou resultados indesejados. Tamb√©m acreditamos que aqueles que usam sistemas de IA devem ser honestos e claros sobre quando, por que e como escolhem implement√°-los, bem como as limita√ß√µes dos sistemas que utilizam. Por exemplo, se um banco usa um sistema de IA para apoiar as suas decis√µes de concess√£o de cr√©dito, √© importante examinar os resultados e entender quais dados influenciam as recomenda√ß√µes do sistema. Os governos est√£o a come√ßar a regulamentar a IA em v√°rios setores, por isso, cientistas de dados e organiza√ß√µes devem explicar se um sistema de IA atende aos requisitos regulamentares, especialmente quando h√° um resultado indesej√°vel.

> [üé• Clique aqui para um v√≠deo: transpar√™ncia na IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como os sistemas de IA s√£o t√£o complexos, √© dif√≠cil entender como funcionam e interpretar os resultados.
- Essa falta de compreens√£o afeta a forma como esses sistemas s√£o geridos, operacionalizados e documentados.
- Mais importante ainda, essa falta de compreens√£o afeta as decis√µes tomadas com base nos resultados produzidos por esses sistemas.

### Responsabilidade

As pessoas que projetam e implementam sistemas de IA devem ser respons√°veis pelo funcionamento dos seus sistemas. A necessidade de responsabilidade √© particularmente crucial com tecnologias sens√≠veis, como o reconhecimento facial. Recentemente, tem havido uma crescente procura por tecnologia de reconhecimento facial, especialmente por parte de organiza√ß√µes de aplica√ß√£o da lei que veem o potencial da tecnologia em usos como encontrar crian√ßas desaparecidas. No entanto, essas tecnologias podem ser usadas por um governo para colocar em risco as liberdades fundamentais dos seus cidad√£os, por exemplo, ao permitir a vigil√¢ncia cont√≠nua de indiv√≠duos espec√≠ficos. Por isso, cientistas de dados e organiza√ß√µes precisam ser respons√°veis pelo impacto dos seus sistemas de IA em indiv√≠duos ou na sociedade.

[![Investigador l√≠der em IA alerta para vigil√¢ncia em massa atrav√©s do reconhecimento facial](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Abordagem da Microsoft para IA Respons√°vel")

> üé• Clique na imagem acima para assistir ao v√≠deo: Alertas sobre Vigil√¢ncia em Massa atrav√©s do Reconhecimento Facial

Em √∫ltima an√°lise, uma das maiores quest√µes para a nossa gera√ß√£o, como a primeira gera√ß√£o a trazer a IA para a sociedade, √© como garantir que os computadores permane√ßam respons√°veis perante as pessoas e como garantir que as pessoas que projetam computadores sejam respons√°veis perante todos os outros.

## Avalia√ß√£o de impacto

Antes de treinar um modelo de machine learning, √© importante realizar uma avalia√ß√£o de impacto para entender o prop√≥sito do sistema de IA; qual √© o uso pretendido; onde ser√° implementado; e quem interagir√° com o sistema. Estas avalia√ß√µes s√£o √∫teis para os revisores ou testadores avaliarem o sistema e saberem quais fatores considerar ao identificar potenciais riscos e consequ√™ncias esperadas.

As seguintes √°reas devem ser focadas ao realizar uma avalia√ß√£o de impacto:

* **Impacto adverso nos indiv√≠duos**. Estar ciente de quaisquer restri√ß√µes ou requisitos, usos n√£o suportados ou quaisquer limita√ß√µes conhecidas que prejudiquem o desempenho do sistema √© vital para garantir que o sistema n√£o seja usado de forma a causar danos aos indiv√≠duos.
* **Requisitos de dados**. Compreender como e onde o sistema usar√° os dados permite que os revisores explorem quaisquer requisitos de dados que precisem de ser considerados (por exemplo, regulamenta√ß√µes de dados como RGPD ou HIPAA). Al√©m disso, examine se a origem ou a quantidade de dados √© substancial para o treino.
* **Resumo do impacto**. Re√∫na uma lista de potenciais danos que possam surgir do uso do sistema. Ao longo do ciclo de vida do ML, reveja se os problemas identificados foram mitigados ou resolvidos.
* **Objetivos aplic√°veis** para cada um dos seis princ√≠pios fundamentais. Avalie se os objetivos de cada princ√≠pio foram cumpridos e se existem lacunas.

## Depura√ß√£o com IA respons√°vel

Semelhante √† depura√ß√£o de uma aplica√ß√£o de software, a depura√ß√£o de um sistema de IA √© um processo necess√°rio para identificar e resolver problemas no sistema. Existem muitos fatores que podem afetar o desempenho de um modelo ou a sua responsabilidade. A maioria das m√©tricas tradicionais de desempenho de modelos s√£o agregados quantitativos do desempenho do modelo, o que n√£o √© suficiente para analisar como um modelo viola os princ√≠pios de IA respons√°vel. Al√©m disso, um modelo de machine learning √© uma "caixa preta", o que dificulta entender o que motiva os seus resultados ou fornecer explica√ß√µes quando comete um erro. Mais tarde neste curso, aprenderemos a usar o painel de IA Respons√°vel para ajudar a depurar sistemas de IA. O painel fornece uma ferramenta hol√≠stica para cientistas de dados e desenvolvedores de IA realizarem:

* **An√°lise de erros**. Para identificar a distribui√ß√£o de erros do modelo que pode afetar a equidade ou fiabilidade do sistema.
* **Vis√£o geral do modelo**. Para descobrir onde existem disparidades no desempenho do modelo em diferentes coortes de dados.
* **An√°lise de dados**. Para compreender a distribui√ß√£o dos dados e identificar potenciais preconceitos nos dados que possam levar a problemas de equidade, inclusividade e fiabilidade.
* **Interpretabilidade do modelo**. Para entender o que afeta ou influencia as previs√µes do modelo. Isso ajuda a explicar o comportamento do modelo, o que √© importante para transpar√™ncia e responsabilidade.

## üöÄ Desafio

Para evitar que danos sejam introduzidos desde o in√≠cio, devemos:

- ter diversidade de origens e perspetivas entre as pessoas que trabalham nos sistemas
- investir em conjuntos de dados que reflitam a diversidade da nossa sociedade
- desenvolver melhores m√©todos ao longo do ciclo de vida do machine learning para detetar e corrigir problemas de IA respons√°vel quando eles ocorrerem

Pense em cen√°rios da vida real onde a falta de confian√ßa num modelo √© evidente na constru√ß√£o e utiliza√ß√£o do modelo. O que mais dever√≠amos considerar?

## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Revis√£o e Estudo Individual

Nesta li√ß√£o, aprendeu alguns conceitos b√°sicos sobre equidade e falta de equidade no machine learning.
Assista a este workshop para aprofundar os t√≥picos:

- Em busca de IA respons√°vel: Aplicando princ√≠pios na pr√°tica por Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

[![Responsible AI Toolbox: Uma framework de c√≥digo aberto para construir IA respons√°vel](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Uma framework de c√≥digo aberto para construir IA respons√°vel")

> üé• Clique na imagem acima para assistir ao v√≠deo: RAI Toolbox: Uma framework de c√≥digo aberto para construir IA respons√°vel por Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

Al√©m disso, leia:

- Centro de recursos de IA respons√°vel da Microsoft: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de pesquisa FATE da Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Reposit√≥rio GitHub do Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Leia sobre as ferramentas do Azure Machine Learning para garantir equidade:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Tarefa

[Explore o RAI Toolbox](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original no seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas resultantes do uso desta tradu√ß√£o.