<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T01:09:32+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "el"
}
-->
# Εισαγωγή στη Μάθηση Ενίσχυσης και Q-Learning

![Περίληψη της μάθησης ενίσχυσης στη μηχανική μάθηση σε ένα σκίτσο](../../../../sketchnotes/ml-reinforcement.png)
> Σκίτσο από [Tomomi Imura](https://www.twitter.com/girlie_mac)

Η μάθηση ενίσχυσης περιλαμβάνει τρεις σημαντικές έννοιες: τον πράκτορα, κάποιες καταστάσεις και ένα σύνολο ενεργειών ανά κατάσταση. Εκτελώντας μια ενέργεια σε μια συγκεκριμένη κατάσταση, ο πράκτορας λαμβάνει μια ανταμοιβή. Φανταστείτε ξανά το παιχνίδι στον υπολογιστή Super Mario. Είστε ο Mario, βρίσκεστε σε ένα επίπεδο του παιχνιδιού, δίπλα σε μια άκρη γκρεμού. Πάνω σας υπάρχει ένα νόμισμα. Εσείς, ως Mario, σε ένα επίπεδο του παιχνιδιού, σε μια συγκεκριμένη θέση... αυτή είναι η κατάστασή σας. Αν κάνετε ένα βήμα προς τα δεξιά (μια ενέργεια), θα πέσετε από την άκρη και θα λάβετε χαμηλή αριθμητική βαθμολογία. Ωστόσο, αν πατήσετε το κουμπί άλματος, θα κερδίσετε έναν πόντο και θα παραμείνετε ζωντανοί. Αυτό είναι ένα θετικό αποτέλεσμα και θα πρέπει να σας απονείμει μια θετική αριθμητική βαθμολογία.

Χρησιμοποιώντας τη μάθηση ενίσχυσης και έναν προσομοιωτή (το παιχνίδι), μπορείτε να μάθετε πώς να παίζετε το παιχνίδι για να μεγιστοποιήσετε την ανταμοιβή, δηλαδή να παραμείνετε ζωντανοί και να συγκεντρώσετε όσο το δυνατόν περισσότερους πόντους.

[![Εισαγωγή στη Μάθηση Ενίσχυσης](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 🎥 Κάντε κλικ στην εικόνα παραπάνω για να ακούσετε τον Dmitry να συζητά για τη Μάθηση Ενίσχυσης

## [Κουίζ πριν το μάθημα](https://ff-quizzes.netlify.app/en/ml/)

## Προαπαιτούμενα και Ρύθμιση

Σε αυτό το μάθημα, θα πειραματιστούμε με κώδικα σε Python. Θα πρέπει να μπορείτε να εκτελέσετε τον κώδικα του Jupyter Notebook από αυτό το μάθημα, είτε στον υπολογιστή σας είτε κάπου στο cloud.

Μπορείτε να ανοίξετε [το notebook του μαθήματος](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) και να ακολουθήσετε το μάθημα για να το δημιουργήσετε.

> **Σημείωση:** Εάν ανοίγετε αυτόν τον κώδικα από το cloud, θα πρέπει επίσης να κατεβάσετε το αρχείο [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), το οποίο χρησιμοποιείται στον κώδικα του notebook. Προσθέστε το στον ίδιο κατάλογο με το notebook.

## Εισαγωγή

Σε αυτό το μάθημα, θα εξερευνήσουμε τον κόσμο του **[Πέτρου και του Λύκου](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, εμπνευσμένο από ένα μουσικό παραμύθι του Ρώσου συνθέτη [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Θα χρησιμοποιήσουμε τη **Μάθηση Ενίσχυσης** για να αφήσουμε τον Πέτρο να εξερευνήσει το περιβάλλον του, να συλλέξει νόστιμα μήλα και να αποφύγει τη συνάντηση με τον λύκο.

Η **Μάθηση Ενίσχυσης** (RL) είναι μια τεχνική μάθησης που μας επιτρέπει να μάθουμε τη βέλτιστη συμπεριφορά ενός **πράκτορα** σε κάποιο **περιβάλλον** μέσω πολλών πειραμάτων. Ένας πράκτορας σε αυτό το περιβάλλον πρέπει να έχει κάποιο **στόχο**, ο οποίος ορίζεται από μια **συνάρτηση ανταμοιβής**.

## Το περιβάλλον

Για απλότητα, ας θεωρήσουμε ότι ο κόσμος του Πέτρου είναι ένας τετράγωνος πίνακας μεγέθους `width` x `height`, όπως αυτός:

![Περιβάλλον του Πέτρου](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Κάθε κελί σε αυτόν τον πίνακα μπορεί να είναι:

* **έδαφος**, πάνω στο οποίο ο Πέτρος και άλλα πλάσματα μπορούν να περπατήσουν.
* **νερό**, πάνω στο οποίο προφανώς δεν μπορείτε να περπατήσετε.
* **δέντρο** ή **γρασίδι**, ένα μέρος όπου μπορείτε να ξεκουραστείτε.
* **μήλο**, που αντιπροσωπεύει κάτι που ο Πέτρος θα χαρεί να βρει για να τραφεί.
* **λύκος**, που είναι επικίνδυνος και πρέπει να αποφεύγεται.

Υπάρχει ένα ξεχωριστό Python module, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), το οποίο περιέχει τον κώδικα για να δουλέψουμε με αυτό το περιβάλλον. Επειδή αυτός ο κώδικας δεν είναι σημαντικός για την κατανόηση των εννοιών μας, θα εισάγουμε το module και θα το χρησιμοποιήσουμε για να δημιουργήσουμε τον δείγμα πίνακα (code block 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Αυτός ο κώδικας θα πρέπει να εκτυπώσει μια εικόνα του περιβάλλοντος παρόμοια με την παραπάνω.

## Ενέργειες και πολιτική

Στο παράδειγμά μας, ο στόχος του Πέτρου θα είναι να βρει ένα μήλο, ενώ θα αποφεύγει τον λύκο και άλλα εμπόδια. Για να το κάνει αυτό, μπορεί ουσιαστικά να περπατήσει γύρω μέχρι να βρει ένα μήλο.

Επομένως, σε οποιαδήποτε θέση, μπορεί να επιλέξει μία από τις ακόλουθες ενέργειες: πάνω, κάτω, αριστερά και δεξιά.

Θα ορίσουμε αυτές τις ενέργειες ως ένα λεξικό και θα τις αντιστοιχίσουμε σε ζεύγη αντίστοιχων αλλαγών συντεταγμένων. Για παράδειγμα, η κίνηση προς τα δεξιά (`R`) θα αντιστοιχεί σε ένα ζεύγος `(1,0)`. (code block 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Συνοψίζοντας, η στρατηγική και ο στόχος αυτού του σεναρίου είναι οι εξής:

- **Η στρατηγική** του πράκτορά μας (Πέτρος) ορίζεται από τη λεγόμενη **πολιτική**. Μια πολιτική είναι μια συνάρτηση που επιστρέφει την ενέργεια σε οποιαδήποτε δεδομένη κατάσταση. Στην περίπτωσή μας, η κατάσταση του προβλήματος αντιπροσωπεύεται από τον πίνακα, συμπεριλαμβανομένης της τρέχουσας θέσης του παίκτη.

- **Ο στόχος** της μάθησης ενίσχυσης είναι να μάθουμε τελικά μια καλή πολιτική που θα μας επιτρέψει να λύσουμε το πρόβλημα αποτελεσματικά. Ωστόσο, ως βάση, ας θεωρήσουμε την απλούστερη πολιτική που ονομάζεται **τυχαία περιπλάνηση**.

## Τυχαία περιπλάνηση

Ας λύσουμε πρώτα το πρόβλημά μας υλοποιώντας μια στρατηγική τυχαίας περιπλάνησης. Με την τυχαία περιπλάνηση, θα επιλέγουμε τυχαία την επόμενη ενέργεια από τις επιτρεπόμενες ενέργειες, μέχρι να φτάσουμε στο μήλο (code block 3).

1. Υλοποιήστε την τυχαία περιπλάνηση με τον παρακάτω κώδικα:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Η κλήση στη `walk` θα πρέπει να επιστρέψει το μήκος της αντίστοιχης διαδρομής, το οποίο μπορεί να διαφέρει από τη μία εκτέλεση στην άλλη.

1. Εκτελέστε το πείραμα περιπλάνησης αρκετές φορές (π.χ. 100) και εκτυπώστε τα αποτελέσματα (code block 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Σημειώστε ότι το μέσο μήκος μιας διαδρομής είναι περίπου 30-40 βήματα, που είναι αρκετά μεγάλο, δεδομένου ότι η μέση απόσταση από το πλησιέστερο μήλο είναι περίπου 5-6 βήματα.

    Μπορείτε επίσης να δείτε πώς φαίνεται η κίνηση του Πέτρου κατά την τυχαία περιπλάνηση:

    ![Τυχαία Περιπλάνηση του Πέτρου](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Συνάρτηση ανταμοιβής

Για να κάνουμε την πολιτική μας πιο έξυπνη, πρέπει να κατανοήσουμε ποιες κινήσεις είναι "καλύτερες" από άλλες. Για να το κάνουμε αυτό, πρέπει να ορίσουμε τον στόχο μας.

Ο στόχος μπορεί να οριστεί με όρους μιας **συνάρτησης ανταμοιβής**, η οποία θα επιστρέφει κάποια τιμή βαθμολογίας για κάθε κατάσταση. Όσο μεγαλύτερος ο αριθμός, τόσο καλύτερη η συνάρτηση ανταμοιβής. (code block 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Ένα ενδιαφέρον πράγμα σχετικά με τις συναρτήσεις ανταμοιβής είναι ότι στις περισσότερες περιπτώσεις, *λαμβάνουμε ουσιαστική ανταμοιβή μόνο στο τέλος του παιχνιδιού*. Αυτό σημαίνει ότι ο αλγόριθμός μας πρέπει με κάποιο τρόπο να θυμάται "καλές" κινήσεις που οδηγούν σε θετική ανταμοιβή στο τέλος και να αυξάνει τη σημασία τους. Παρομοίως, όλες οι κινήσεις που οδηγούν σε κακά αποτελέσματα πρέπει να αποθαρρύνονται.

## Q-Learning

Ο αλγόριθμος που θα συζητήσουμε εδώ ονομάζεται **Q-Learning**. Σε αυτόν τον αλγόριθμο, η πολιτική ορίζεται από μια συνάρτηση (ή μια δομή δεδομένων) που ονομάζεται **Q-Table**. Καταγράφει την "καλοσύνη" κάθε ενέργειας σε μια δεδομένη κατάσταση.

Ονομάζεται Q-Table επειδή είναι συχνά βολικό να την αναπαραστήσουμε ως πίνακα ή πολυδιάστατο array. Δεδομένου ότι ο πίνακάς μας έχει διαστάσεις `width` x `height`, μπορούμε να αναπαραστήσουμε την Q-Table χρησιμοποιώντας ένα numpy array με σχήμα `width` x `height` x `len(actions)`: (code block 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Παρατηρήστε ότι αρχικοποιούμε όλες τις τιμές της Q-Table με ίση τιμή, στην περίπτωσή μας - 0.25. Αυτό αντιστοιχεί στην πολιτική "τυχαίας περιπλάνησης", επειδή όλες οι κινήσεις σε κάθε κατάσταση είναι εξίσου καλές. Μπορούμε να περάσουμε την Q-Table στη συνάρτηση `plot` για να οπτικοποιήσουμε τον πίνακα στον πίνακα: `m.plot(Q)`.

![Περιβάλλον του Πέτρου](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

Στο κέντρο κάθε κελιού υπάρχει ένα "βέλος" που υποδεικνύει την προτιμώμενη κατεύθυνση κίνησης. Επειδή όλες οι κατευθύνσεις είναι ίσες, εμφανίζεται μια κουκκίδα.

Τώρα πρέπει να εκτελέσουμε την προσομοίωση, να εξερευνήσουμε το περιβάλλον μας και να μάθουμε μια καλύτερη κατανομή τιμών Q-Table, η οποία θα μας επιτρέψει να βρούμε τον δρόμο προς το μήλο πολύ πιο γρήγορα.

## Ουσία του Q-Learning: Εξίσωση Bellman

Μόλις αρχίσουμε να κινούμαστε, κάθε ενέργεια θα έχει μια αντίστοιχη ανταμοιβή, δηλαδή θεωρητικά μπορούμε να επιλέξουμε την επόμενη ενέργεια με βάση την υψηλότερη άμεση ανταμοιβή. Ωστόσο, στις περισσότερες καταστάσεις, η κίνηση δεν θα επιτύχει τον στόχο μας να φτάσουμε στο μήλο, και έτσι δεν μπορούμε να αποφασίσουμε άμεσα ποια κατεύθυνση είναι καλύτερη.

> Θυμηθείτε ότι δεν έχει σημασία το άμεσο αποτέλεσμα, αλλά μάλλον το τελικό αποτέλεσμα, το οποίο θα επιτύχουμε στο τέλος της προσομοίωσης.

Για να λάβουμε υπόψη αυτήν την καθυστερημένη ανταμοιβή, πρέπει να χρησιμοποιήσουμε τις αρχές του **[δυναμικού προγραμματισμού](https://en.wikipedia.org/wiki/Dynamic_programming)**, που μας επιτρέπουν να σκεφτούμε το πρόβλημά μας αναδρομικά.

Ας υποθέσουμε ότι βρισκόμαστε τώρα στην κατάσταση *s*, και θέλουμε να μετακινηθούμε στην επόμενη κατάσταση *s'*. Κάνοντας αυτό, θα λάβουμε την άμεση ανταμοιβή *r(s,a)*, που ορίζεται από τη συνάρτηση ανταμοιβής, συν κάποια μελλοντική ανταμοιβή. Αν υποθέσουμε ότι η Q-Table μας αντικατοπτρίζει σωστά την "ελκυστικότητα" κάθε ενέργειας, τότε στην κατάσταση *s'* θα επιλέξουμε μια ενέργεια *a* που αντιστοιχεί στη μέγιστη τιμή του *Q(s',a')*. Έτσι, η καλύτερη δυνατή μελλοντική ανταμοιβή που θα μπορούσαμε να λάβουμε στην κατάσταση *s* θα οριστεί ως `max`

## Έλεγχος της πολιτικής

Δεδομένου ότι ο Πίνακας Q (Q-Table) καταγράφει την "ελκυστικότητα" κάθε ενέργειας σε κάθε κατάσταση, είναι αρκετά εύκολο να τον χρησιμοποιήσουμε για να ορίσουμε την αποδοτική πλοήγηση στον κόσμο μας. Στην πιο απλή περίπτωση, μπορούμε να επιλέξουμε την ενέργεια που αντιστοιχεί στη μεγαλύτερη τιμή του Πίνακα Q: (κώδικας 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Αν δοκιμάσετε τον παραπάνω κώδικα αρκετές φορές, μπορεί να παρατηρήσετε ότι κάποιες φορές "κολλάει" και χρειάζεται να πατήσετε το κουμπί STOP στο notebook για να τον διακόψετε. Αυτό συμβαίνει επειδή μπορεί να υπάρχουν καταστάσεις όπου δύο καταστάσεις "δείχνουν" η μία την άλλη με βάση τη βέλτιστη τιμή Q, οπότε ο πράκτορας καταλήγει να κινείται μεταξύ αυτών των καταστάσεων επ' αόριστον.

## 🚀Πρόκληση

> **Εργασία 1:** Τροποποιήστε τη συνάρτηση `walk` ώστε να περιορίσετε το μέγιστο μήκος της διαδρομής σε έναν συγκεκριμένο αριθμό βημάτων (π.χ., 100) και παρατηρήστε τον παραπάνω κώδικα να επιστρέφει αυτή την τιμή κατά διαστήματα.

> **Εργασία 2:** Τροποποιήστε τη συνάρτηση `walk` ώστε να μην επιστρέφει σε μέρη που έχει ήδη επισκεφθεί. Αυτό θα αποτρέψει τη `walk` από το να επαναλαμβάνει βρόχους, ωστόσο, ο πράκτορας μπορεί ακόμα να "παγιδευτεί" σε μια τοποθεσία από την οποία δεν μπορεί να διαφύγει.

## Πλοήγηση

Μια καλύτερη πολιτική πλοήγησης θα ήταν αυτή που χρησιμοποιήσαμε κατά την εκπαίδευση, η οποία συνδυάζει εκμετάλλευση και εξερεύνηση. Σε αυτή την πολιτική, θα επιλέγουμε κάθε ενέργεια με μια συγκεκριμένη πιθανότητα, ανάλογη με τις τιμές στον Πίνακα Q. Αυτή η στρατηγική μπορεί ακόμα να οδηγήσει τον πράκτορα να επιστρέψει σε μια θέση που έχει ήδη εξερευνήσει, αλλά, όπως μπορείτε να δείτε από τον παρακάτω κώδικα, οδηγεί σε πολύ μικρότερο μέσο μήκος διαδρομής προς την επιθυμητή τοποθεσία (θυμηθείτε ότι το `print_statistics` εκτελεί τη προσομοίωση 100 φορές): (κώδικας 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Μετά την εκτέλεση αυτού του κώδικα, θα πρέπει να παρατηρήσετε ένα πολύ μικρότερο μέσο μήκος διαδρομής από πριν, στην περιοχή των 3-6.

## Διερεύνηση της διαδικασίας μάθησης

Όπως αναφέραμε, η διαδικασία μάθησης είναι μια ισορροπία μεταξύ εξερεύνησης και εκμετάλλευσης της αποκτηθείσας γνώσης για τη δομή του χώρου προβλήματος. Έχουμε δει ότι τα αποτελέσματα της μάθησης (η ικανότητα να βοηθήσουμε έναν πράκτορα να βρει μια σύντομη διαδρομή προς τον στόχο) έχουν βελτιωθεί, αλλά είναι επίσης ενδιαφέρον να παρατηρήσουμε πώς συμπεριφέρεται το μέσο μήκος διαδρομής κατά τη διάρκεια της διαδικασίας μάθησης:

Οι παρατηρήσεις συνοψίζονται ως εξής:

- **Το μέσο μήκος διαδρομής αυξάνεται**. Αυτό που βλέπουμε εδώ είναι ότι αρχικά, το μέσο μήκος διαδρομής αυξάνεται. Αυτό πιθανώς οφείλεται στο γεγονός ότι όταν δεν γνωρίζουμε τίποτα για το περιβάλλον, είναι πιθανό να παγιδευτούμε σε κακές καταστάσεις, όπως νερό ή λύκους. Καθώς μαθαίνουμε περισσότερα και αρχίζουμε να χρησιμοποιούμε αυτή τη γνώση, μπορούμε να εξερευνούμε το περιβάλλον για περισσότερο χρόνο, αλλά ακόμα δεν γνωρίζουμε καλά πού βρίσκονται τα μήλα.

- **Το μήκος διαδρομής μειώνεται καθώς μαθαίνουμε περισσότερα**. Όταν μάθουμε αρκετά, γίνεται πιο εύκολο για τον πράκτορα να πετύχει τον στόχο, και το μήκος της διαδρομής αρχίζει να μειώνεται. Ωστόσο, παραμένουμε ανοιχτοί στην εξερεύνηση, οπότε συχνά αποκλίνουμε από την καλύτερη διαδρομή και εξερευνούμε νέες επιλογές, κάνοντας τη διαδρομή μεγαλύτερη από την ιδανική.

- **Το μήκος αυξάνεται απότομα**. Αυτό που παρατηρούμε επίσης στο γράφημα είναι ότι σε κάποιο σημείο, το μήκος αυξάνεται απότομα. Αυτό υποδεικνύει τη στοχαστική φύση της διαδικασίας και ότι μπορούμε σε κάποιο σημείο να "χαλάσουμε" τους συντελεστές του Πίνακα Q αντικαθιστώντας τους με νέες τιμές. Αυτό ιδανικά θα πρέπει να ελαχιστοποιηθεί μειώνοντας τον ρυθμό μάθησης (για παράδειγμα, προς το τέλος της εκπαίδευσης, προσαρμόζουμε τις τιμές του Πίνακα Q μόνο κατά μια μικρή τιμή).

Συνολικά, είναι σημαντικό να θυμόμαστε ότι η επιτυχία και η ποιότητα της διαδικασίας μάθησης εξαρτώνται σημαντικά από παραμέτρους, όπως ο ρυθμός μάθησης, η μείωση του ρυθμού μάθησης και ο συντελεστής έκπτωσης. Αυτές συχνά ονομάζονται **υπερπαράμετροι**, για να διακριθούν από τις **παραμέτρους**, τις οποίες βελτιστοποιούμε κατά την εκπαίδευση (για παράδειγμα, τους συντελεστές του Πίνακα Q). Η διαδικασία εύρεσης των καλύτερων τιμών υπερπαραμέτρων ονομάζεται **βελτιστοποίηση υπερπαραμέτρων** και αξίζει ξεχωριστή ανάλυση.

## [Κουίζ μετά τη διάλεξη](https://ff-quizzes.netlify.app/en/ml/)

## Ανάθεση 
[Ένας Πιο Ρεαλιστικός Κόσμος](assignment.md)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν σφάλματα ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.