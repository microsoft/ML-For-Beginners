<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T01:04:12+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "el"
}
-->
# Εισαγωγή στη μάθηση ενίσχυσης

Η μάθηση ενίσχυσης, RL, θεωρείται ένα από τα βασικά παραδείγματα μηχανικής μάθησης, δίπλα στη μάθηση με επίβλεψη και τη μάθηση χωρίς επίβλεψη. Η RL αφορά τις αποφάσεις: τη λήψη σωστών αποφάσεων ή τουλάχιστον τη μάθηση από αυτές.

Φανταστείτε ότι έχετε ένα προσομοιωμένο περιβάλλον, όπως η χρηματιστηριακή αγορά. Τι συμβαίνει αν επιβάλλετε έναν συγκεκριμένο κανονισμό; Έχει θετική ή αρνητική επίδραση; Αν συμβεί κάτι αρνητικό, πρέπει να λάβετε αυτήν την _αρνητική ενίσχυση_, να μάθετε από αυτήν και να αλλάξετε πορεία. Αν το αποτέλεσμα είναι θετικό, πρέπει να χτίσετε πάνω σε αυτήν την _θετική ενίσχυση_.

![ο Πέτρος και ο λύκος](../../../8-Reinforcement/images/peter.png)

> Ο Πέτρος και οι φίλοι του πρέπει να ξεφύγουν από τον πεινασμένο λύκο! Εικόνα από [Jen Looper](https://twitter.com/jenlooper)

## Τοπικό θέμα: Ο Πέτρος και ο Λύκος (Ρωσία)

[Ο Πέτρος και ο Λύκος](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) είναι ένα μουσικό παραμύθι γραμμένο από τον Ρώσο συνθέτη [Σεργκέι Προκόφιεφ](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Είναι η ιστορία του νεαρού πρωτοπόρου Πέτρου, που θαρραλέα βγαίνει από το σπίτι του στο ξέφωτο του δάσους για να κυνηγήσει τον λύκο. Σε αυτήν την ενότητα, θα εκπαιδεύσουμε αλγόριθμους μηχανικής μάθησης που θα βοηθήσουν τον Πέτρο:

- **Να εξερευνήσει** την γύρω περιοχή και να δημιουργήσει έναν βέλτιστο χάρτη πλοήγησης
- **Να μάθει** πώς να χρησιμοποιεί ένα skateboard και να ισορροπεί πάνω σε αυτό, ώστε να κινείται πιο γρήγορα.

[![Ο Πέτρος και ο Λύκος](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 🎥 Κάντε κλικ στην εικόνα παραπάνω για να ακούσετε τον Πέτρο και τον Λύκο από τον Προκόφιεφ

## Μάθηση ενίσχυσης

Στις προηγούμενες ενότητες, είδατε δύο παραδείγματα προβλημάτων μηχανικής μάθησης:

- **Με επίβλεψη**, όπου έχουμε σύνολα δεδομένων που προτείνουν παραδείγματα λύσεων για το πρόβλημα που θέλουμε να λύσουμε. Η [Ταξινόμηση](../4-Classification/README.md) και η [Παλινδρόμηση](../2-Regression/README.md) είναι εργασίες μάθησης με επίβλεψη.
- **Χωρίς επίβλεψη**, όπου δεν έχουμε δεδομένα εκπαίδευσης με ετικέτες. Το κύριο παράδειγμα μάθησης χωρίς επίβλεψη είναι η [Ομαδοποίηση](../5-Clustering/README.md).

Σε αυτήν την ενότητα, θα σας παρουσιάσουμε έναν νέο τύπο προβλήματος μάθησης που δεν απαιτεί δεδομένα εκπαίδευσης με ετικέτες. Υπάρχουν διάφοροι τύποι τέτοιων προβλημάτων:

- **[Ημι-επιβλεπόμενη μάθηση](https://wikipedia.org/wiki/Semi-supervised_learning)**, όπου έχουμε πολλά δεδομένα χωρίς ετικέτες που μπορούν να χρησιμοποιηθούν για την προκαταρκτική εκπαίδευση του μοντέλου.
- **[Μάθηση ενίσχυσης](https://wikipedia.org/wiki/Reinforcement_learning)**, στην οποία ένας πράκτορας μαθαίνει πώς να συμπεριφέρεται πραγματοποιώντας πειράματα σε ένα προσομοιωμένο περιβάλλον.

### Παράδειγμα - παιχνίδι υπολογιστή

Ας υποθέσουμε ότι θέλετε να διδάξετε έναν υπολογιστή να παίζει ένα παιχνίδι, όπως σκάκι ή [Super Mario](https://wikipedia.org/wiki/Super_Mario). Για να παίξει ο υπολογιστής ένα παιχνίδι, πρέπει να προβλέψει ποια κίνηση να κάνει σε κάθε κατάσταση του παιχνιδιού. Ενώ αυτό μπορεί να φαίνεται σαν πρόβλημα ταξινόμησης, δεν είναι - επειδή δεν έχουμε ένα σύνολο δεδομένων με καταστάσεις και αντίστοιχες ενέργειες. Παρόλο που μπορεί να έχουμε δεδομένα όπως υπάρχοντες αγώνες σκακιού ή εγγραφές παικτών που παίζουν Super Mario, είναι πιθανό αυτά τα δεδομένα να μην καλύπτουν επαρκώς μεγάλο αριθμό πιθανών καταστάσεων.

Αντί να αναζητούμε υπάρχοντα δεδομένα παιχνιδιού, η **Μάθηση Ενίσχυσης** (RL) βασίζεται στην ιδέα του *να κάνουμε τον υπολογιστή να παίξει* πολλές φορές και να παρατηρήσουμε το αποτέλεσμα. Έτσι, για να εφαρμόσουμε τη Μάθηση Ενίσχυσης, χρειαζόμαστε δύο πράγματα:

- **Ένα περιβάλλον** και **έναν προσομοιωτή** που μας επιτρέπουν να παίξουμε ένα παιχνίδι πολλές φορές. Αυτός ο προσομοιωτής θα ορίζει όλους τους κανόνες του παιχνιδιού καθώς και τις πιθανές καταστάσεις και ενέργειες.

- **Μια συνάρτηση ανταμοιβής**, που θα μας λέει πόσο καλά τα πήγαμε κατά τη διάρκεια κάθε κίνησης ή παιχνιδιού.

Η κύρια διαφορά μεταξύ άλλων τύπων μηχανικής μάθησης και RL είναι ότι στη RL συνήθως δεν γνωρίζουμε αν κερδίζουμε ή χάνουμε μέχρι να τελειώσει το παιχνίδι. Έτσι, δεν μπορούμε να πούμε αν μια συγκεκριμένη κίνηση από μόνη της είναι καλή ή όχι - λαμβάνουμε ανταμοιβή μόνο στο τέλος του παιχνιδιού. Και ο στόχος μας είναι να σχεδιάσουμε αλγόριθμους που θα μας επιτρέψουν να εκπαιδεύσουμε ένα μοντέλο υπό αβέβαιες συνθήκες. Θα μάθουμε για έναν αλγόριθμο RL που ονομάζεται **Q-learning**.

## Μαθήματα

1. [Εισαγωγή στη μάθηση ενίσχυσης και το Q-Learning](1-QLearning/README.md)
2. [Χρήση ενός προσομοιωμένου περιβάλλοντος γυμναστικής](2-Gym/README.md)

## Πιστώσεις

Το "Εισαγωγή στη Μάθηση Ενίσχυσης" γράφτηκε με ♥️ από τον [Dmitry Soshnikov](http://soshnikov.com)

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.