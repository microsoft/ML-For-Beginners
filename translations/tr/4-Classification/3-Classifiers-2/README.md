<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49047911108adc49d605cddfb455749c",
  "translation_date": "2025-09-06T08:00:12+00:00",
  "source_file": "4-Classification/3-Classifiers-2/README.md",
  "language_code": "tr"
}
-->
# Mutfak SÄ±nÄ±flandÄ±rÄ±cÄ±larÄ± 2

Bu ikinci sÄ±nÄ±flandÄ±rma dersinde, sayÄ±sal verileri sÄ±nÄ±flandÄ±rmanÄ±n daha fazla yolunu keÅŸfedeceksiniz. AyrÄ±ca, bir sÄ±nÄ±flandÄ±rÄ±cÄ±yÄ± diÄŸerine tercih etmenin sonuÃ§larÄ±nÄ± Ã¶ÄŸreneceksiniz.

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ml/)

### Ã–n KoÅŸul

Ã–nceki dersleri tamamladÄ±ÄŸÄ±nÄ±zÄ± ve bu 4 derslik klasÃ¶rÃ¼n kÃ¶k dizininde `data` klasÃ¶rÃ¼nÃ¼zde _cleaned_cuisines.csv_ adlÄ± temizlenmiÅŸ bir veri kÃ¼mesine sahip olduÄŸunuzu varsayÄ±yoruz.

### HazÄ±rlÄ±k

_Notebook.ipynb_ dosyanÄ±z temizlenmiÅŸ veri kÃ¼mesiyle yÃ¼klendi ve model oluÅŸturma sÃ¼recine hazÄ±r olacak ÅŸekilde X ve y veri Ã§erÃ§evelerine bÃ¶lÃ¼ndÃ¼.

## Bir sÄ±nÄ±flandÄ±rma haritasÄ±

Daha Ã¶nce, Microsoft'un hile sayfasÄ±nÄ± kullanarak verileri sÄ±nÄ±flandÄ±rÄ±rken sahip olduÄŸunuz Ã§eÅŸitli seÃ§enekleri Ã¶ÄŸrenmiÅŸtiniz. Scikit-learn, sÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±nÄ±zÄ± (diÄŸer bir deyiÅŸle tahmin ediciler) daraltmanÄ±za yardÄ±mcÄ± olabilecek benzer, ancak daha ayrÄ±ntÄ±lÄ± bir hile sayfasÄ± sunar:

![Scikit-learn'den ML HaritasÄ±](../../../../4-Classification/3-Classifiers-2/images/map.png)
> Ä°pucu: [bu haritayÄ± Ã§evrimiÃ§i ziyaret edin](https://scikit-learn.org/stable/tutorial/machine_learning_map/) ve belgeleri okumak iÃ§in yol boyunca tÄ±klayÄ±n.

### Plan

Bu harita, verilerinizi net bir ÅŸekilde anladÄ±ÄŸÄ±nÄ±zda Ã§ok yardÄ±mcÄ± olur, Ã§Ã¼nkÃ¼ yollarÄ±nda 'yÃ¼rÃ¼yerek' bir karara varabilirsiniz:

- 50'den fazla Ã¶rneÄŸimiz var
- Bir kategori tahmin etmek istiyoruz
- EtiketlenmiÅŸ verilerimiz var
- 100.000'den az Ã¶rneÄŸimiz var
- âœ¨ Linear SVC seÃ§ebiliriz
- Bu iÅŸe yaramazsa, sayÄ±sal verilerimiz olduÄŸu iÃ§in
    - âœ¨ KNeighbors Classifier deneyebiliriz
      - Bu iÅŸe yaramazsa, âœ¨ SVC ve âœ¨ Ensemble Classifiers deneyin

Bu, takip edilmesi Ã§ok faydalÄ± bir yol.

## AlÄ±ÅŸtÄ±rma - verileri bÃ¶lÃ¼n

Bu yolu takip ederek, kullanmak iÃ§in bazÄ± kÃ¼tÃ¼phaneleri iÃ§e aktarmalÄ±yÄ±z.

1. Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±n:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. EÄŸitim ve test verilerinizi bÃ¶lÃ¼n:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±

Support-Vector Clustering (SVC), ML tekniklerinin Support-Vector Machines ailesinin bir alt dalÄ±dÄ±r (aÅŸaÄŸÄ±da bunlar hakkÄ±nda daha fazla bilgi edinin). Bu yÃ¶ntemde, etiketleri nasÄ±l kÃ¼melendireceÄŸinize karar vermek iÃ§in bir 'kernel' seÃ§ebilirsiniz. 'C' parametresi, parametrelerin etkisini dÃ¼zenleyen 'dÃ¼zenleme' anlamÄ±na gelir. Kernel, [birkaÃ§ seÃ§enekten](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) biri olabilir; burada, Linear SVC'den yararlanmak iÃ§in 'linear' olarak ayarlÄ±yoruz. OlasÄ±lÄ±k varsayÄ±lan olarak 'false'dur; burada olasÄ±lÄ±k tahminleri toplamak iÃ§in 'true' olarak ayarlÄ±yoruz. Rastgele durumu '0' olarak ayarlÄ±yoruz, bÃ¶ylece veriler karÄ±ÅŸtÄ±rÄ±larak olasÄ±lÄ±klar elde ediliyor.

### AlÄ±ÅŸtÄ±rma - bir Linear SVC uygulayÄ±n

Bir sÄ±nÄ±flandÄ±rÄ±cÄ± dizisi oluÅŸturarak baÅŸlayÄ±n. Test ettikÃ§e bu diziye kademeli olarak ekleme yapacaksÄ±nÄ±z.

1. Linear SVC ile baÅŸlayÄ±n:

    ```python
    C = 10
    # Create different classifiers.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Modelinizi Linear SVC kullanarak eÄŸitin ve bir rapor yazdÄ±rÄ±n:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    SonuÃ§ oldukÃ§a iyi:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±

K-Neighbors, hem denetimli hem de denetimsiz Ã¶ÄŸrenme iÃ§in kullanÄ±labilen ML yÃ¶ntemlerinin "komÅŸular" ailesinin bir parÃ§asÄ±dÄ±r. Bu yÃ¶ntemde, Ã¶nceden tanÄ±mlanmÄ±ÅŸ bir nokta sayÄ±sÄ± oluÅŸturulur ve veriler bu noktalarÄ±n etrafÄ±nda toplanÄ±r, bÃ¶ylece veriler iÃ§in genelleÅŸtirilmiÅŸ etiketler tahmin edilebilir.

### AlÄ±ÅŸtÄ±rma - K-Neighbors sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± uygulayÄ±n

Ã–nceki sÄ±nÄ±flandÄ±rÄ±cÄ± iyiydi ve verilerle iyi Ã§alÄ±ÅŸtÄ±, ancak belki daha iyi bir doÄŸruluk elde edebiliriz. Bir K-Neighbors sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± deneyin.

1. SÄ±nÄ±flandÄ±rÄ±cÄ± dizinize bir satÄ±r ekleyin (Linear SVC Ã¶ÄŸesinden sonra bir virgÃ¼l ekleyin):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    SonuÃ§ biraz daha kÃ¶tÃ¼:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    âœ… [K-Neighbors hakkÄ±nda bilgi edinin](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector Classifier

Support-Vector sÄ±nÄ±flandÄ±rÄ±cÄ±larÄ±, sÄ±nÄ±flandÄ±rma ve regresyon gÃ¶revleri iÃ§in kullanÄ±lan ML yÃ¶ntemlerinin [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) ailesinin bir parÃ§asÄ±dÄ±r. SVM'ler, "eÄŸitim Ã¶rneklerini iki kategori arasÄ±ndaki mesafeyi en Ã¼st dÃ¼zeye Ã§Ä±karmak iÃ§in uzaydaki noktalara eÅŸler." Sonraki veriler bu uzaya eÅŸlenir, bÃ¶ylece kategorileri tahmin edilebilir.

### AlÄ±ÅŸtÄ±rma - bir Support Vector Classifier uygulayÄ±n

Biraz daha iyi doÄŸruluk elde etmek iÃ§in bir Support Vector Classifier deneyelim.

1. K-Neighbors Ã¶ÄŸesinden sonra bir virgÃ¼l ekleyin ve ardÄ±ndan bu satÄ±rÄ± ekleyin:

    ```python
    'SVC': SVC(),
    ```

    SonuÃ§ oldukÃ§a iyi!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    âœ… [Support-Vectors hakkÄ±nda bilgi edinin](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble Classifiers

Ã–nceki test oldukÃ§a iyi olmasÄ±na raÄŸmen, yolun sonuna kadar gidelim. BazÄ± 'Ensemble Classifiers' deneyelim, Ã¶zellikle Random Forest ve AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

SonuÃ§ Ã¶zellikle Random Forest iÃ§in Ã§ok iyi:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

âœ… [Ensemble Classifiers hakkÄ±nda bilgi edinin](https://scikit-learn.org/stable/modules/ensemble.html)

Bu Makine Ã–ÄŸrenimi yÃ¶ntemi, modelin kalitesini artÄ±rmak iÃ§in birkaÃ§ temel tahmin edicinin tahminlerini birleÅŸtirir. Ã–rneÄŸimizde, Random Trees ve AdaBoost kullandÄ±k.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), bir 'karar aÄŸaÃ§larÄ±' 'ormanÄ±' oluÅŸturan ve aÅŸÄ±rÄ± uyumu Ã¶nlemek iÃ§in rastgelelik ekleyen bir ortalama yÃ¶ntemi. N_estimators parametresi, aÄŸaÃ§ sayÄ±sÄ±nÄ± belirler.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), bir veri kÃ¼mesine bir sÄ±nÄ±flandÄ±rÄ±cÄ± uyarlar ve ardÄ±ndan aynÄ± veri kÃ¼mesine bu sÄ±nÄ±flandÄ±rÄ±cÄ±nÄ±n kopyalarÄ±nÄ± uyarlar. YanlÄ±ÅŸ sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ Ã¶ÄŸelerin aÄŸÄ±rlÄ±klarÄ±na odaklanÄ±r ve bir sonraki sÄ±nÄ±flandÄ±rÄ±cÄ±yÄ± dÃ¼zeltmek iÃ§in uyumu ayarlar.

---

## ğŸš€Meydan Okuma

Bu tekniklerin her birinin ayarlayabileceÄŸiniz Ã§ok sayÄ±da parametresi vardÄ±r. Her birinin varsayÄ±lan parametrelerini araÅŸtÄ±rÄ±n ve bu parametreleri ayarlamanÄ±n modelin kalitesi iÃ§in ne anlama geleceÄŸini dÃ¼ÅŸÃ¼nÃ¼n.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ml/)

## GÃ¶zden GeÃ§irme ve Kendi Kendine Ã‡alÄ±ÅŸma

Bu derslerde Ã§ok fazla terim var, bu yÃ¼zden [bu listeyi](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) gÃ¶zden geÃ§irmek iÃ§in bir dakikanÄ±zÄ± ayÄ±rÄ±n!

## Ã–dev 

[Parametrelerle Oynama](assignment.md)

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dilindeki hali, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.