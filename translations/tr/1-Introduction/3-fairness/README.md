# Sorumlu AI ile Makine Ã–ÄŸrenimi Ã‡Ã¶zÃ¼mleri OluÅŸturma
 
![Makine Ã–ÄŸreniminde sorumlu AI'nÄ±n bir Ã¶zet sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.tr.png)
> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Ders Ã–ncesi Quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)
 
## GiriÅŸ

Bu mÃ¼fredatta, makine Ã¶ÄŸreniminin gÃ¼nlÃ¼k hayatÄ±mÄ±zÄ± nasÄ±l etkileyebileceÄŸini ve etkilediÄŸini keÅŸfetmeye baÅŸlayacaksÄ±nÄ±z. Åu anda bile, saÄŸlÄ±k teÅŸhisleri, kredi onaylarÄ± veya dolandÄ±rÄ±cÄ±lÄ±ÄŸÄ± tespit etme gibi gÃ¼nlÃ¼k karar verme gÃ¶revlerinde sistemler ve modeller yer alÄ±yor. Bu nedenle, bu modellerin gÃ¼venilir sonuÃ§lar saÄŸlamak iÃ§in iyi Ã§alÄ±ÅŸmasÄ± Ã¶nemlidir. Herhangi bir yazÄ±lÄ±m uygulamasÄ± gibi, AI sistemleri de beklentileri karÅŸÄ±lamayacak veya istenmeyen sonuÃ§lar doÄŸuracaktÄ±r. Bu yÃ¼zden bir AI modelinin davranÄ±ÅŸÄ±nÄ± anlamak ve aÃ§Ä±klayabilmek Ã§ok Ã¶nemlidir.

Bu modelleri oluÅŸturmak iÃ§in kullandÄ±ÄŸÄ±nÄ±z veriler belirli demografik gruplardan yoksunsa, Ã¶rneÄŸin Ä±rk, cinsiyet, siyasi gÃ¶rÃ¼ÅŸ, din veya bu demografik gruplarÄ± orantÄ±sÄ±z bir ÅŸekilde temsil ediyorsa ne olabilir? Modelin Ã§Ä±ktÄ±sÄ± bazÄ± demografik gruplarÄ± kayÄ±racak ÅŸekilde yorumlandÄ±ÄŸÄ±nda ne olur? Uygulama iÃ§in sonuÃ§larÄ± nedir? AyrÄ±ca, modelin olumsuz bir sonucu olduÄŸunda ve insanlara zarar verdiÄŸinde ne olur? AI sistemlerinin davranÄ±ÅŸÄ±ndan kim sorumludur? Bu mÃ¼fredatta bu sorularÄ± keÅŸfedeceÄŸiz.

Bu derste:

- Makine Ã¶ÄŸreniminde adaletin Ã¶nemi ve adaletle ilgili zararlar konusunda farkÄ±ndalÄ±k kazanacaksÄ±nÄ±z.
- GÃ¼venilirlik ve gÃ¼venliÄŸi saÄŸlamak iÃ§in aykÄ±rÄ± durumlarÄ± ve olaÄŸandÄ±ÅŸÄ± senaryolarÄ± keÅŸfetme pratiÄŸine aÅŸina olacaksÄ±nÄ±z.
- Herkesi gÃ¼Ã§lendirmek iÃ§in kapsayÄ±cÄ± sistemler tasarlama ihtiyacÄ±nÄ± anlayacaksÄ±nÄ±z.
- Verilerin ve insanlarÄ±n gizliliÄŸini ve gÃ¼venliÄŸini korumanÄ±n ne kadar Ã¶nemli olduÄŸunu keÅŸfedeceksiniz.
- AI modellerinin davranÄ±ÅŸÄ±nÄ± aÃ§Ä±klamak iÃ§in ÅŸeffaf bir yaklaÅŸÄ±mÄ±n Ã¶nemini gÃ¶receksiniz.
- AI sistemlerine gÃ¼ven inÅŸa etmek iÃ§in hesap verebilirliÄŸin ne kadar Ã¶nemli olduÄŸunun farkÄ±nda olacaksÄ±nÄ±z.

## Ã–nkoÅŸul

Ã–nkoÅŸul olarak, "Sorumlu AI Ä°lkeleri" Ã¶ÄŸrenme yolunu tamamlayÄ±n ve aÅŸaÄŸÄ±daki videoyu izleyin:

Sorumlu AI hakkÄ±nda daha fazla bilgi edinmek iÃ§in bu [Ã–ÄŸrenme Yolu](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott) baÄŸlantÄ±sÄ±nÄ± takip edin.

[![Microsoft'un Sorumlu AI YaklaÅŸÄ±mÄ±](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsoft'un Sorumlu AI YaklaÅŸÄ±mÄ±")

> ğŸ¥ YukarÄ±daki resme tÄ±klayarak video izleyin: Microsoft'un Sorumlu AI YaklaÅŸÄ±mÄ±

## Adalet

AI sistemleri herkese adil davranmalÄ± ve benzer gruplardaki insanlarÄ± farklÄ± ÅŸekillerde etkilemekten kaÃ§Ä±nmalÄ±dÄ±r. Ã–rneÄŸin, AI sistemleri tÄ±bbi tedavi, kredi baÅŸvurularÄ± veya iÅŸe alÄ±m konusunda rehberlik saÄŸladÄ±ÄŸÄ±nda, benzer semptomlara, mali durumlara veya mesleki niteliklere sahip herkese aynÄ± Ã¶nerileri yapmalÄ±dÄ±r. Hepimiz insan olarak, kararlarÄ±mÄ±zÄ± ve eylemlerimizi etkileyen miras alÄ±nmÄ±ÅŸ Ã¶nyargÄ±lar taÅŸÄ±rÄ±z. Bu Ã¶nyargÄ±lar, AI sistemlerini eÄŸitmek iÃ§in kullandÄ±ÄŸÄ±mÄ±z verilerde de ortaya Ã§Ä±kabilir. Bu tÃ¼r manipÃ¼lasyonlar bazen istemeden olabilir. Verilerde Ã¶nyargÄ± yaratÄ±rken bunu bilinÃ§li olarak fark etmek genellikle zordur.

**â€œAdaletsizlikâ€**, Ä±rk, cinsiyet, yaÅŸ veya engellilik durumu gibi bir grup insan iÃ§in olumsuz etkileri veya â€œzararlarÄ±â€ kapsar. BaÅŸlÄ±ca adaletle ilgili zararlar ÅŸu ÅŸekilde sÄ±nÄ±flandÄ±rÄ±labilir:

- **Tahsis**, Ã¶rneÄŸin bir cinsiyet veya etnisitenin diÄŸerine gÃ¶re kayÄ±rÄ±lmasÄ±.
- **Hizmet kalitesi**. Verileri belirli bir senaryo iÃ§in eÄŸitmek, ancak gerÃ§ekte Ã§ok daha karmaÅŸÄ±k olmasÄ±, kÃ¶tÃ¼ performans gÃ¶steren bir hizmete yol aÃ§ar. Ã–rneÄŸin, koyu tenli insanlarÄ± algÄ±layamayan bir el sabunu daÄŸÄ±tÄ±cÄ±sÄ±. [Referans](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **KÃ¼Ã§Ã¼k dÃ¼ÅŸÃ¼rme**. Bir ÅŸeyi veya birini haksÄ±z yere eleÅŸtirme ve etiketleme. Ã–rneÄŸin, bir gÃ¶rÃ¼ntÃ¼ etiketleme teknolojisi, koyu tenli insanlarÄ±n gÃ¶rÃ¼ntÃ¼lerini goril olarak yanlÄ±ÅŸ etiketlemiÅŸtir.
- **AÅŸÄ±rÄ± veya yetersiz temsil**. Belirli bir grubun belirli bir meslekte gÃ¶rÃ¼lmediÄŸi ve bu durumu teÅŸvik eden herhangi bir hizmet veya iÅŸlevin zarara katkÄ±da bulunduÄŸu fikri.
- **StereotipleÅŸtirme**. Belirli bir grubu Ã¶nceden belirlenmiÅŸ Ã¶zelliklerle iliÅŸkilendirme. Ã–rneÄŸin, Ä°ngilizce ve TÃ¼rkÃ§e arasÄ±nda Ã§eviri yapan bir dil Ã§eviri sistemi, cinsiyetle iliÅŸkilendirilen kelimeler nedeniyle hatalar yapabilir.

![TÃ¼rkÃ§eye Ã§eviri](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.tr.png)
> TÃ¼rkÃ§eye Ã§eviri

![Ä°ngilizceye geri Ã§eviri](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.tr.png)
> Ä°ngilizceye geri Ã§eviri

AI sistemleri tasarlarken ve test ederken, AI'nÄ±n adil olduÄŸundan ve Ã¶nyargÄ±lÄ± veya ayrÄ±mcÄ± kararlar vermeye programlanmadÄ±ÄŸÄ±ndan emin olmalÄ±yÄ±z, ki bu kararlarÄ± insanlar da vermemelidir. AI ve makine Ã¶ÄŸreniminde adaleti saÄŸlamak karmaÅŸÄ±k bir sosyoteknik zorluktur.

### GÃ¼venilirlik ve gÃ¼venlik

GÃ¼ven inÅŸa etmek iÃ§in, AI sistemlerinin gÃ¼venilir, gÃ¼venli ve normal ve beklenmedik koÅŸullar altÄ±nda tutarlÄ± olmasÄ± gerekir. AI sistemlerinin Ã§eÅŸitli durumlarda nasÄ±l davranacaÄŸÄ±nÄ± bilmek Ã¶nemlidir, Ã¶zellikle de aykÄ±rÄ± durumlarda. AI Ã§Ã¶zÃ¼mleri oluÅŸtururken, AI Ã§Ã¶zÃ¼mlerinin karÅŸÄ±laÅŸacaÄŸÄ± geniÅŸ bir yelpazedeki durumlarÄ± nasÄ±l ele alacaÄŸÄ±na odaklanmak gerekir. Ã–rneÄŸin, kendi kendine giden bir araba, insanlarÄ±n gÃ¼venliÄŸini en Ã¼st dÃ¼zeyde tutmalÄ±dÄ±r. SonuÃ§ olarak, arabayÄ± yÃ¶nlendiren AI, gece, fÄ±rtÄ±nalar veya kar fÄ±rtÄ±nalarÄ±, sokakta koÅŸan Ã§ocuklar, evcil hayvanlar, yol Ã§alÄ±ÅŸmalarÄ± gibi arabanÄ±n karÅŸÄ±laÅŸabileceÄŸi tÃ¼m olasÄ± senaryolarÄ± dikkate almalÄ±dÄ±r. Bir AI sisteminin Ã§eÅŸitli koÅŸullarÄ± gÃ¼venilir ve gÃ¼venli bir ÅŸekilde nasÄ±l ele alabileceÄŸi, veri bilimci veya AI geliÅŸtiricisinin sistemin tasarÄ±mÄ± veya test edilmesi sÄ±rasÄ±nda ne kadar Ã¶ngÃ¶rÃ¼lÃ¼ olduÄŸunu yansÄ±tÄ±r.

> [ğŸ¥ Video iÃ§in buraya tÄ±klayÄ±n: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### KapsayÄ±cÄ±lÄ±k

AI sistemleri herkesin katÄ±lÄ±mÄ±nÄ± saÄŸlamalÄ± ve gÃ¼Ã§lendirmelidir. AI sistemlerini tasarlarken ve uygularken veri bilimciler ve AI geliÅŸtiriciler, sistemi istemeden dÄ±ÅŸlayabilecek potansiyel engelleri belirler ve ele alÄ±r. Ã–rneÄŸin, dÃ¼nya genelinde 1 milyar engelli insan var. AI'nÄ±n ilerlemesiyle, gÃ¼nlÃ¼k yaÅŸamlarÄ±nda geniÅŸ bir bilgi ve fÄ±rsat yelpazesine daha kolay eriÅŸebilirler. Engelleri ele alarak, herkesin yararÄ±na daha iyi deneyimler sunan AI Ã¼rÃ¼nlerini yenilik yapmak ve geliÅŸtirmek iÃ§in fÄ±rsatlar yaratÄ±r.

> [ğŸ¥ Video iÃ§in buraya tÄ±klayÄ±n: AI'da kapsayÄ±cÄ±lÄ±k](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### GÃ¼venlik ve gizlilik

AI sistemleri gÃ¼venli olmalÄ± ve insanlarÄ±n gizliliÄŸine saygÄ± gÃ¶stermelidir. Gizliliklerini, bilgilerini veya hayatlarÄ±nÄ± riske atan sistemlere insanlar daha az gÃ¼venir. Makine Ã¶ÄŸrenimi modellerini eÄŸitirken, en iyi sonuÃ§larÄ± elde etmek iÃ§in verilere gÃ¼veniriz. Bunu yaparken, verilerin kaynaÄŸÄ± ve bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ dikkate alÄ±nmalÄ±dÄ±r. Ã–rneÄŸin, veriler kullanÄ±cÄ± tarafÄ±ndan mÄ± gÃ¶nderildi yoksa kamuya aÃ§Ä±k mÄ±ydÄ±? SonrasÄ±nda, verilerle Ã§alÄ±ÅŸÄ±rken, gizli bilgileri koruyabilen ve saldÄ±rÄ±lara karÅŸÄ± direnÃ§li AI sistemleri geliÅŸtirmek Ã¶nemlidir. AI daha yaygÄ±n hale geldikÃ§e, gizliliÄŸi korumak ve Ã¶nemli kiÅŸisel ve ticari bilgileri gÃ¼vence altÄ±na almak daha kritik ve karmaÅŸÄ±k hale geliyor. AI iÃ§in gizlilik ve veri gÃ¼venliÄŸi sorunlarÄ±, veriye eriÅŸimin AI sistemlerinin insanlar hakkÄ±nda doÄŸru ve bilgilendirilmiÅŸ tahminler ve kararlar vermesi iÃ§in gerekli olmasÄ± nedeniyle Ã¶zellikle dikkat gerektirir.

> [ğŸ¥ Video iÃ§in buraya tÄ±klayÄ±n: AI'da gÃ¼venlik](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- EndÃ¼stri olarak, GDPR (Genel Veri Koruma YÃ¶netmeliÄŸi) gibi dÃ¼zenlemelerle bÃ¼yÃ¼k Ã¶lÃ§Ã¼de ilerlemeler kaydettik.
- Ancak AI sistemleriyle, sistemleri daha kiÅŸisel ve etkili hale getirmek iÃ§in daha fazla kiÅŸisel verilere ihtiyaÃ§ duyma ile gizlilik arasÄ±ndaki gerilimi kabul etmeliyiz.
- Ä°nternetle baÄŸlantÄ±lÄ± bilgisayarlarÄ±n doÄŸuÅŸunda olduÄŸu gibi, AI ile ilgili gÃ¼venlik sorunlarÄ±nÄ±n sayÄ±sÄ±nda bÃ¼yÃ¼k bir artÄ±ÅŸ gÃ¶rÃ¼yoruz.
- AynÄ± zamanda, AI'nÄ±n gÃ¼venliÄŸi artÄ±rmak iÃ§in kullanÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼k. Ã–rneÄŸin, Ã§oÄŸu modern antivirÃ¼s tarayÄ±cÄ±larÄ± bugÃ¼n AI heuristikleri tarafÄ±ndan yÃ¶nlendirilmektedir.
- Veri Bilimi sÃ¼reÃ§lerimizin en son gizlilik ve gÃ¼venlik uygulamalarÄ±yla uyumlu olmasÄ±nÄ± saÄŸlamalÄ±yÄ±z.

### ÅeffaflÄ±k
AI sistemleri anlaÅŸÄ±labilir olmalÄ±dÄ±r. ÅeffaflÄ±ÄŸÄ±n Ã¶nemli bir parÃ§asÄ±, AI sistemlerinin ve bileÅŸenlerinin davranÄ±ÅŸÄ±nÄ± aÃ§Ä±klamaktÄ±r. AI sistemlerinin anlaÅŸÄ±lmasÄ±nÄ± iyileÅŸtirmek, paydaÅŸlarÄ±n nasÄ±l ve neden Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamalarÄ±nÄ± gerektirir, bÃ¶ylece potansiyel performans sorunlarÄ±nÄ±, gÃ¼venlik ve gizlilik endiÅŸelerini, Ã¶nyargÄ±larÄ±, dÄ±ÅŸlayÄ±cÄ± uygulamalarÄ± veya istenmeyen sonuÃ§larÄ± belirleyebilirler. AI sistemlerini kullananlarÄ±n, ne zaman, neden ve nasÄ±l kullandÄ±klarÄ±nÄ± ve sistemlerinin sÄ±nÄ±rlamalarÄ±nÄ± aÃ§Ä±kÃ§a belirtmeleri gerektiÄŸine inanÄ±yoruz. Ã–rneÄŸin, bir banka tÃ¼ketici kredi kararlarÄ±nÄ± desteklemek iÃ§in bir AI sistemi kullanÄ±yorsa, sonuÃ§larÄ± incelemek ve sistemin Ã¶nerilerini hangi verilerin etkilediÄŸini anlamak Ã¶nemlidir. HÃ¼kÃ¼metler, AI'yÄ± endÃ¼striler arasÄ±nda dÃ¼zenlemeye baÅŸlÄ±yor, bu nedenle veri bilimciler ve kuruluÅŸlar, AI sisteminin dÃ¼zenleyici gereksinimleri karÅŸÄ±layÄ±p karÅŸÄ±lamadÄ±ÄŸÄ±nÄ±, Ã¶zellikle istenmeyen bir sonuÃ§ olduÄŸunda aÃ§Ä±klamalÄ±dÄ±r.

> [ğŸ¥ Video iÃ§in buraya tÄ±klayÄ±n: AI'da ÅŸeffaflÄ±k](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- AI sistemleri Ã§ok karmaÅŸÄ±k olduÄŸu iÃ§in nasÄ±l Ã§alÄ±ÅŸtÄ±klarÄ±nÄ± ve sonuÃ§larÄ± nasÄ±l yorumladÄ±klarÄ±nÄ± anlamak zordur.
- Bu anlayÄ±ÅŸ eksikliÄŸi, bu sistemlerin nasÄ±l yÃ¶netildiÄŸini, iÅŸletildiÄŸini ve belgelenmesini etkiler.
- Daha da Ã¶nemlisi, bu anlayÄ±ÅŸ eksikliÄŸi, bu sistemlerin Ã¼rettiÄŸi sonuÃ§larÄ± kullanarak yapÄ±lan kararlarÄ± etkiler.

### Hesap Verebilirlik 

AI sistemlerini tasarlayan ve uygulayan kiÅŸiler, sistemlerinin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan sorumlu olmalÄ±dÄ±r. Hesap verebilirlik ihtiyacÄ±, Ã¶zellikle yÃ¼z tanÄ±ma gibi hassas kullanÄ±m teknolojileri iÃ§in Ã§ok Ã¶nemlidir. Son zamanlarda, yÃ¼z tanÄ±ma teknolojisine olan talep artÄ±yor, Ã¶zellikle kayÄ±p Ã§ocuklarÄ± bulmak gibi kullanÄ±mlarda teknolojinin potansiyelini gÃ¶ren kolluk kuvvetleri tarafÄ±ndan. Ancak, bu teknolojiler, Ã¶rneÄŸin belirli bireylerin sÃ¼rekli izlenmesini saÄŸlayarak vatandaÅŸlarÄ±n temel Ã¶zgÃ¼rlÃ¼klerini riske atmak iÃ§in bir hÃ¼kÃ¼met tarafÄ±ndan kullanÄ±labilir. Bu nedenle, veri bilimciler ve kuruluÅŸlar, AI sistemlerinin bireyleri veya toplumu nasÄ±l etkilediÄŸinden sorumlu olmalÄ±dÄ±r.

[![Ã–nde Gelen AI AraÅŸtÄ±rmacÄ±sÄ± YÃ¼z TanÄ±ma Yoluyla Kitle GÃ¶zetiminden UyarÄ±yor](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.tr.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsoft'un Sorumlu AI YaklaÅŸÄ±mÄ±")

> ğŸ¥ YukarÄ±daki resme tÄ±klayarak video izleyin: YÃ¼z TanÄ±ma Yoluyla Kitle GÃ¶zetimi UyarÄ±larÄ±

SonuÃ§ta, toplumda AI'yÄ± tanÄ±tan ilk nesil olarak, bilgisayarlarÄ±n insanlara hesap verebilir kalmasÄ±nÄ± nasÄ±l saÄŸlayacaÄŸÄ±mÄ±z ve bilgisayarlarÄ± tasarlayan insanlarÄ±n diÄŸer herkese hesap verebilir kalmasÄ±nÄ± nasÄ±l saÄŸlayacaÄŸÄ±mÄ±z, neslimizin en bÃ¼yÃ¼k sorularÄ±ndan biridir.

## Etki DeÄŸerlendirmesi 

Bir makine Ã¶ÄŸrenimi modelini eÄŸitmeden Ã¶nce, AI sisteminin amacÄ±nÄ±, beklenen kullanÄ±mÄ±nÄ±, nerede konuÅŸlandÄ±rÄ±lacaÄŸÄ±nÄ± ve sistemle kimlerin etkileÅŸime gireceÄŸini anlamak iÃ§in bir etki deÄŸerlendirmesi yapmak Ã¶nemlidir. Bu, sistemi deÄŸerlendiren gÃ¶zden geÃ§irenler veya test ediciler iÃ§in potansiyel riskleri ve beklenen sonuÃ§larÄ± belirlerken dikkate alÄ±nmasÄ± gereken faktÃ¶rleri bilmeleri aÃ§Ä±sÄ±ndan yararlÄ±dÄ±r.

Etki deÄŸerlendirmesi yaparken odaklanÄ±lmasÄ± gereken alanlar ÅŸunlardÄ±r:

* **Bireyler Ã¼zerinde olumsuz etki**. Sistem performansÄ±nÄ± engelleyen herhangi bir kÄ±sÄ±tlama veya gereksinim, desteklenmeyen kullanÄ±m veya bilinen sÄ±nÄ±rlamalarÄ±n farkÄ±nda olmak, sistemin bireylere zarar verebilecek ÅŸekilde kullanÄ±lmamasÄ±nÄ± saÄŸlamak iÃ§in hayati Ã¶neme sahiptir.
* **Veri gereksinimleri**. Sistemin verileri nasÄ±l ve nerede kullanacaÄŸÄ±nÄ± anlamak, gÃ¶zden geÃ§irenlerin dikkate almasÄ± gereken veri gereksinimlerini (Ã¶rneÄŸin, GDPR veya HIPPA veri dÃ¼zenlemeleri) araÅŸtÄ±rmalarÄ±nÄ± saÄŸlar. AyrÄ±ca, verinin kaynaÄŸÄ± veya miktarÄ±nÄ±n eÄŸitime yeterli olup olmadÄ±ÄŸÄ±nÄ± inceleyin.
* **Etki Ã¶zeti**. Sistemin kullanÄ±mÄ±ndan kaynaklanabilecek potansiyel zararlarÄ±n bir listesini toplayÄ±n. ML yaÅŸam dÃ¶ngÃ¼sÃ¼ boyunca, belirlenen sorunlarÄ±n hafifletilip hafifletilmediÄŸini veya ele alÄ±nÄ±p alÄ±nmadÄ±ÄŸÄ±nÄ± gÃ¶zden geÃ§irin.
* AltÄ± temel ilkenin her biri iÃ§in **uygulanabilir hedefler**. Her ilkenin hedeflerinin karÅŸÄ±lanÄ±p karÅŸÄ±lanmadÄ±ÄŸÄ±nÄ± ve herhangi bir boÅŸluk olup olmadÄ±ÄŸÄ±nÄ± deÄŸerlendirin.

## Sorumlu AI ile Hata AyÄ±klama  

Bir yazÄ±lÄ±m uygulamasÄ±nda hata ayÄ±klama gibi, bir AI sisteminde hata ayÄ±klamak da sistemdeki sorunlarÄ± belirleme ve Ã§Ã¶zme sÃ¼recidir. Bir modelin beklenildiÄŸi gibi veya sorumlu bir ÅŸekilde performans gÃ¶stermemesine etki eden birÃ§ok faktÃ¶r vardÄ±r. Ã‡oÄŸu geleneksel model performans metriÄŸi, bir modelin performansÄ±nÄ±n nicel toplamlarÄ±dÄ±r ve sorumlu AI ilkelerini nasÄ±l ihlal ettiÄŸini analiz etmek iÃ§in yeterli deÄŸildir. AyrÄ±ca, bir makine Ã¶ÄŸrenimi modeli, sonuÃ§larÄ±nÄ± neyin yÃ¶nlendirdiÄŸini anlamayÄ± veya hata yaptÄ±ÄŸÄ±nda aÃ§Ä±klama yapmayÄ± zorlaÅŸtÄ±ran bir kara kutudur. Bu kursun ilerleyen bÃ¶lÃ¼mlerinde, AI sistemlerinde hata ayÄ±klamaya yardÄ±mcÄ± olmak iÃ§in Sorumlu AI panosunu nasÄ±l kullanacaÄŸÄ±mÄ±zÄ± Ã¶ÄŸreneceÄŸiz. Pano, veri bilimciler ve AI geliÅŸtiricilerinin ÅŸu iÅŸlemleri yapmalarÄ± iÃ§in kapsamlÄ± bir araÃ§ saÄŸlar:

* **Hata analizi**. Sistemin adaletini veya gÃ¼venilirliÄŸini etkileyebilecek modelin hata daÄŸÄ±lÄ±mÄ±nÄ± belirlemek.
* **Model genel gÃ¶rÃ¼nÃ¼mÃ¼**. Modelin performansÄ±nda veri gruplarÄ± arasÄ±nda farklÄ±lÄ±klar olup olmadÄ±ÄŸÄ±nÄ± keÅŸfetmek.
* **Veri analizi**. Veri daÄŸÄ±lÄ±mÄ±nÄ± anlamak ve adalet, kapsayÄ±cÄ±lÄ±k ve gÃ¼venilirlik sorunlarÄ±na yol aÃ§abilecek potansiyel Ã¶nyargÄ±larÄ± belirlemek.
* **Model yorumlanabilirliÄŸi**. Modelin tahminlerini neyin etkilediÄŸini veya yÃ¶nlendirdiÄŸini anlamak. Bu, modelin davranÄ±ÅŸÄ±nÄ± aÃ§Ä±klamak iÃ§in Ã¶nemlidir ve ÅŸeffaflÄ±k ve hesap verebilirlik iÃ§in kritiktir.

## ğŸš€ Meydan Okuma 
 
ZararlarÄ±n baÅŸtan itibaren ortaya Ã§Ä±kmasÄ±nÄ± Ã¶nlemek iÃ§in ÅŸunlarÄ± yapmalÄ±yÄ±z:

- Sistemler Ã¼zerinde Ã§alÄ±ÅŸan insanlarÄ±n farklÄ± geÃ§miÅŸlere ve perspektiflere sahip olmasÄ±nÄ± saÄŸlamak
- Toplumumuzun Ã§eÅŸitliliÄŸini yansÄ±tan veri setlerine yatÄ±rÄ±m yapmak
- Makine Ã¶ÄŸrenimi yaÅŸam dÃ¶ngÃ¼sÃ¼ boyunca sorumlu AI'yÄ± tespit etmek ve dÃ¼zeltmek iÃ§in daha iyi yÃ¶ntemler geliÅŸtirmek

Model oluÅŸturma ve kullanÄ±mÄ±nda bir modelin gÃ¼venilmezliÄŸinin belirgin olduÄŸu gerÃ§ek hayat senaryolarÄ±nÄ± dÃ¼ÅŸÃ¼nÃ¼n. BaÅŸka neleri gÃ¶z Ã¶nÃ¼nde bulundurmalÄ±yÄ±z?

## [Ders SonrasÄ± Quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Ä°nceleme ve Kendi Kendine Ã‡alÄ±ÅŸma 
 
Bu derste, makine Ã¶ÄŸreniminde adalet ve adaletsizlik kavramlarÄ±nÄ±n bazÄ± temel bilgilerini Ã¶ÄŸrendiniz.
 
Konulara daha derinlemesine dalmak iÃ§in bu atÃ¶lyeyi izleyin:

- Sorumlu AI PeÅŸinde: Besmira Nushi, Mehrnoosh Sameki ve Amit Sharma tarafÄ±ndan ilkeleri pratiÄŸe dÃ¶kmek

[![Sorumlu AI AraÃ§ Kutusu: Sorumlu AI oluÅŸturmak iÃ§in aÃ§Ä±k kaynaklÄ± bir Ã§erÃ§eve](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "Sorumlu AI AraÃ§ Kutusu: Sorumlu AI oluÅŸturmak iÃ§in aÃ§Ä±k kaynaklÄ± bir Ã§erÃ§eve")

> ğŸ¥ YukarÄ±daki resme tÄ±klayarak video izleyin: Sorumlu AI AraÃ§ Kutusu: Sorumlu

**Feragatname**:
Bu belge, makine tabanlÄ± AI Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belgenin kendi dilindeki hali yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi tavsiye edilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlamadan sorumlu deÄŸiliz.