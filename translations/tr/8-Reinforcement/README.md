<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-06T08:02:46+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "tr"
}
-->
# PekiÅŸtirmeli Ã–ÄŸrenmeye GiriÅŸ

PekiÅŸtirmeli Ã¶ÄŸrenme, RL, denetimli Ã¶ÄŸrenme ve denetimsiz Ã¶ÄŸrenmenin yanÄ±nda temel makine Ã¶ÄŸrenimi paradigmalarÄ±ndan biri olarak gÃ¶rÃ¼lÃ¼r. RL tamamen kararlarla ilgilidir: doÄŸru kararlar vermek veya en azÄ±ndan onlardan Ã¶ÄŸrenmek.

Hayal edin ki borsa gibi simÃ¼le edilmiÅŸ bir ortamÄ±nÄ±z var. Belirli bir dÃ¼zenleme uygularsanÄ±z ne olur? Bunun olumlu mu yoksa olumsuz bir etkisi mi olur? EÄŸer olumsuz bir ÅŸey olursa, bu _olumsuz pekiÅŸtirmeyi_ almanÄ±z, bundan Ã¶ÄŸrenmeniz ve rotanÄ±zÄ± deÄŸiÅŸtirmeniz gerekir. EÄŸer olumlu bir sonuÃ§ olursa, bu _olumlu pekiÅŸtirme_ Ã¼zerine inÅŸa etmeniz gerekir.

![peter ve kurt](../../../8-Reinforcement/images/peter.png)

> Peter ve arkadaÅŸlarÄ± aÃ§ kurtlardan kaÃ§mak zorunda! GÃ¶rsel: [Jen Looper](https://twitter.com/jenlooper)

## BÃ¶lgesel Konu: Peter ve Kurt (Rusya)

[Peter ve Kurt](https://en.wikipedia.org/wiki/Peter_and_the_Wolf), Rus besteci [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev) tarafÄ±ndan yazÄ±lmÄ±ÅŸ bir mÃ¼zikal masaldÄ±r. Bu hikaye, genÃ§ Ã¶ncÃ¼ Peter'Ä±n cesurca evinden Ã§Ä±kÄ±p kurtu kovalamak iÃ§in orman aÃ§Ä±klÄ±ÄŸÄ±na gitmesini anlatÄ±r. Bu bÃ¶lÃ¼mde, Peter'a yardÄ±mcÄ± olacak makine Ã¶ÄŸrenimi algoritmalarÄ±nÄ± eÄŸiteceÄŸiz:

- **KeÅŸfetmek**: Ã‡evredeki alanÄ± keÅŸfetmek ve en uygun navigasyon haritasÄ±nÄ± oluÅŸturmak.
- **Ã–ÄŸrenmek**: Daha hÄ±zlÄ± hareket edebilmek iÃ§in kaykay kullanmayÄ± ve dengede durmayÄ± Ã¶ÄŸrenmek.

[![Peter ve Kurt](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> ğŸ¥ Prokofiev'in Peter ve Kurt eserini dinlemek iÃ§in yukarÄ±daki gÃ¶rsele tÄ±klayÄ±n.

## PekiÅŸtirmeli Ã–ÄŸrenme

Ã–nceki bÃ¶lÃ¼mlerde, iki tÃ¼r makine Ã¶ÄŸrenimi problemini gÃ¶rdÃ¼nÃ¼z:

- **Denetimli**, burada Ã§Ã¶zmek istediÄŸimiz probleme Ã¶rnek Ã§Ã¶zÃ¼mler sunan veri setlerimiz var. [SÄ±nÄ±flandÄ±rma](../4-Classification/README.md) ve [regresyon](../2-Regression/README.md) denetimli Ã¶ÄŸrenme gÃ¶revleridir.
- **Denetimsiz**, burada etiketlenmiÅŸ eÄŸitim verilerimiz yok. Denetimsiz Ã¶ÄŸrenmenin ana Ã¶rneÄŸi [KÃ¼meleme](../5-Clustering/README.md)'dir.

Bu bÃ¶lÃ¼mde, etiketlenmiÅŸ eÄŸitim verisi gerektirmeyen yeni bir Ã¶ÄŸrenme problem tÃ¼rÃ¼yle tanÄ±ÅŸacaksÄ±nÄ±z. Bu tÃ¼r problemlerin birkaÃ§ Ã§eÅŸidi vardÄ±r:

- **[YarÄ± denetimli Ã¶ÄŸrenme](https://wikipedia.org/wiki/Semi-supervised_learning)**, burada modeli Ã¶nceden eÄŸitmek iÃ§in kullanÄ±labilecek Ã§ok miktarda etiketlenmemiÅŸ veri bulunur.
- **[PekiÅŸtirmeli Ã¶ÄŸrenme](https://wikipedia.org/wiki/Reinforcement_learning)**, burada bir ajan, bazÄ± simÃ¼le edilmiÅŸ ortamlarda deneyler yaparak nasÄ±l davranmasÄ± gerektiÄŸini Ã¶ÄŸrenir.

### Ã–rnek - Bilgisayar Oyunu

Diyelim ki bir bilgisayara bir oyun oynamayÄ± Ã¶ÄŸretmek istiyorsunuz, Ã¶rneÄŸin satranÃ§ veya [Super Mario](https://wikipedia.org/wiki/Super_Mario). BilgisayarÄ±n bir oyun oynamasÄ± iÃ§in, her oyun durumunda hangi hamleyi yapacaÄŸÄ±nÄ± tahmin etmesi gerekir. Bu bir sÄ±nÄ±flandÄ±rma problemi gibi gÃ¶rÃ¼nebilir, ancak Ã¶yle deÄŸildir - Ã§Ã¼nkÃ¼ elimizde durumlar ve karÅŸÄ±lÄ±k gelen eylemlerle ilgili bir veri seti yoktur. SatranÃ§ maÃ§larÄ± veya oyuncularÄ±n Super Mario oynarkenki kayÄ±tlarÄ± gibi bazÄ± verilerimiz olabilir, ancak bu veriler muhtemelen yeterince geniÅŸ bir durum yelpazesini kapsamayacaktÄ±r.

Mevcut oyun verilerini aramak yerine, **PekiÅŸtirmeli Ã–ÄŸrenme** (RL), bilgisayarÄ± *birÃ§ok kez oyun oynatmak* ve sonucu gÃ¶zlemlemek fikrine dayanÄ±r. Bu nedenle, PekiÅŸtirmeli Ã–ÄŸrenmeyi uygulamak iÃ§in iki ÅŸeye ihtiyacÄ±mÄ±z var:

- **Bir ortam** ve **bir simÃ¼latÃ¶r**, bu simÃ¼latÃ¶r bize oyunu birÃ§ok kez oynama imkanÄ± saÄŸlar. Bu simÃ¼latÃ¶r, tÃ¼m oyun kurallarÄ±nÄ±, olasÄ± durumlarÄ± ve eylemleri tanÄ±mlar.

- **Bir Ã¶dÃ¼l fonksiyonu**, bu fonksiyon her hamle veya oyun sÄ±rasÄ±nda ne kadar iyi performans gÃ¶sterdiÄŸimizi bize sÃ¶yler.

DiÄŸer makine Ã¶ÄŸrenimi tÃ¼rleri ile RL arasÄ±ndaki temel fark, RL'de genellikle oyunu bitirene kadar kazanÄ±p kazanmadÄ±ÄŸÄ±mÄ±zÄ± bilmememizdir. Bu nedenle, belirli bir hamlenin tek baÅŸÄ±na iyi olup olmadÄ±ÄŸÄ±nÄ± sÃ¶yleyemeyiz - Ã¶dÃ¼lÃ¼ yalnÄ±zca oyunun sonunda alÄ±rÄ±z. AmacÄ±mÄ±z, belirsiz koÅŸullar altÄ±nda bir modeli eÄŸitmemizi saÄŸlayacak algoritmalar tasarlamaktÄ±r. **Q-learning** adlÄ± bir RL algoritmasÄ±nÄ± Ã¶ÄŸreneceÄŸiz.

## Dersler

1. [PekiÅŸtirmeli Ã¶ÄŸrenmeye ve Q-Learning'e giriÅŸ](1-QLearning/README.md)
2. [Bir gym simÃ¼lasyon ortamÄ± kullanma](2-Gym/README.md)

## KatkÄ±da Bulunanlar

"PekiÅŸtirmeli Ã–ÄŸrenmeye GiriÅŸ" [Dmitry Soshnikov](http://soshnikov.com) tarafÄ±ndan â™¥ï¸ ile yazÄ±lmÄ±ÅŸtÄ±r.

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.