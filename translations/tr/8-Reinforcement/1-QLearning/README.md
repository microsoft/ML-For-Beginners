<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-06T08:03:43+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "tr"
}
-->
# PekiÅŸtirmeli Ã–ÄŸrenme ve Q-Ã–ÄŸrenme'ye GiriÅŸ

![Makine Ã¶ÄŸreniminde pekiÅŸtirme Ã¶zetini iÃ§eren bir sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote: [Tomomi Imura](https://www.twitter.com/girlie_mac)

PekiÅŸtirmeli Ã¶ÄŸrenme Ã¼Ã§ Ã¶nemli kavram iÃ§erir: ajan, bazÄ± durumlar ve her durum iÃ§in bir dizi eylem. Belirli bir durumda bir eylem gerÃ§ekleÅŸtirerek, ajana bir Ã¶dÃ¼l verilir. Yine Super Mario bilgisayar oyununu hayal edin. Siz Mario'sunuz, bir oyun seviyesindesiniz ve uÃ§urumun kenarÄ±nda duruyorsunuz. ÃœstÃ¼nÃ¼zde bir altÄ±n var. Mario olarak, bir oyun seviyesinde, belirli bir pozisyonda olmanÄ±z... bu sizin durumunuzdur. SaÄŸa bir adÄ±m atmak (bir eylem) sizi uÃ§urumdan aÅŸaÄŸÄ±ya gÃ¶tÃ¼rÃ¼r ve bu size dÃ¼ÅŸÃ¼k bir sayÄ±sal puan verir. Ancak zÄ±plama tuÅŸuna basmak size bir puan kazandÄ±rÄ±r ve hayatta kalÄ±rsÄ±nÄ±z. Bu olumlu bir sonuÃ§tur ve size olumlu bir sayÄ±sal puan kazandÄ±rmalÄ±dÄ±r.

PekiÅŸtirmeli Ã¶ÄŸrenme ve bir simÃ¼latÃ¶r (oyun) kullanarak, hayatta kalmayÄ± ve mÃ¼mkÃ¼n olduÄŸunca Ã§ok puan toplamayÄ± hedefleyerek oyunu nasÄ±l oynayacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸrenebilirsiniz.

[![PekiÅŸtirmeli Ã–ÄŸrenmeye GiriÅŸ](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ YukarÄ±daki gÃ¶rsele tÄ±klayarak Dmitry'nin PekiÅŸtirmeli Ã–ÄŸrenme hakkÄ±ndaki konuÅŸmasÄ±nÄ± dinleyin

## [Ders Ã–ncesi Test](https://ff-quizzes.netlify.app/en/ml/)

## Ã–n KoÅŸullar ve Kurulum

Bu derste, Python'da bazÄ± kodlarla denemeler yapacaÄŸÄ±z. Bu dersteki Jupyter Notebook kodunu bilgisayarÄ±nÄ±zda veya bulutta Ã§alÄ±ÅŸtÄ±rabilmelisiniz.

[Ders not defterini](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) aÃ§abilir ve bu dersi adÄ±m adÄ±m takip edebilirsiniz.

> **Not:** Bu kodu buluttan aÃ§Ä±yorsanÄ±z, not defteri kodunda kullanÄ±lan [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) dosyasÄ±nÄ± da almanÄ±z gerekir. Bu dosyayÄ± not defteriyle aynÄ± dizine ekleyin.

## GiriÅŸ

Bu derste, Rus besteci [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev) tarafÄ±ndan yazÄ±lmÄ±ÅŸ bir mÃ¼zikal masaldan esinlenilen **[Peter ve Kurt](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)** dÃ¼nyasÄ±nÄ± keÅŸfedeceÄŸiz. Peter'Ä±n Ã§evresini keÅŸfetmesine, lezzetli elmalar toplamasÄ±na ve kurtla karÅŸÄ±laÅŸmaktan kaÃ§Ä±nmasÄ±na olanak tanÄ±mak iÃ§in **PekiÅŸtirmeli Ã–ÄŸrenme** kullanacaÄŸÄ±z.

**PekiÅŸtirmeli Ã–ÄŸrenme** (RL), bir **ajan**Ä±n bir **Ã§evrede** optimal davranÄ±ÅŸÄ±nÄ± Ã¶ÄŸrenmesine olanak tanÄ±yan bir Ã¶ÄŸrenme tekniÄŸidir. Bu Ã§evredeki bir ajanÄ±n bir **hedefi** olmalÄ±dÄ±r ve bu hedef bir **Ã¶dÃ¼l fonksiyonu** ile tanÄ±mlanÄ±r.

## Ã‡evre

Basitlik aÃ§Ä±sÄ±ndan, Peter'Ä±n dÃ¼nyasÄ±nÄ± `geniÅŸlik` x `yÃ¼kseklik` boyutlarÄ±nda bir kare tahta olarak dÃ¼ÅŸÃ¼nelim:

![Peter'Ä±n Ã‡evresi](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Bu tahtadaki her hÃ¼cre ÅŸu ÅŸekilde olabilir:

* **zemin**, Peter ve diÄŸer canlÄ±larÄ±n Ã¼zerinde yÃ¼rÃ¼yebileceÄŸi yer.
* **su**, Ã¼zerinde yÃ¼rÃ¼yemeyeceÄŸiniz yer.
* bir **aÄŸaÃ§** veya **Ã§imen**, dinlenebileceÄŸiniz bir yer.
* bir **elma**, Peter'Ä±n kendini beslemek iÃ§in bulmaktan memnun olacaÄŸÄ± bir ÅŸey.
* bir **kurt**, tehlikeli ve kaÃ§Ä±nÄ±lmasÄ± gereken bir ÅŸey.

Bu Ã§evreyle Ã§alÄ±ÅŸmak iÃ§in kod iÃ§eren ayrÄ± bir Python modÃ¼lÃ¼ olan [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) bulunmaktadÄ±r. Bu kod, kavramlarÄ±mÄ±zÄ± anlamak iÃ§in Ã¶nemli olmadÄ±ÄŸÄ±ndan, modÃ¼lÃ¼ iÃ§e aktaracaÄŸÄ±z ve Ã¶rnek tahtayÄ± oluÅŸturmak iÃ§in kullanacaÄŸÄ±z (kod bloÄŸu 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Bu kod, yukarÄ±daki resme benzer bir Ã§evre gÃ¶rÃ¼ntÃ¼sÃ¼ yazdÄ±rmalÄ±dÄ±r.

## Eylemler ve Politika

Ã–rneÄŸimizde, Peter'Ä±n hedefi bir elma bulmak, kurt ve diÄŸer engellerden kaÃ§Ä±nmak olacaktÄ±r. Bunu yapmak iÃ§in, elmayÄ± bulana kadar etrafta dolaÅŸabilir.

Bu nedenle, herhangi bir pozisyonda yukarÄ±, aÅŸaÄŸÄ±, sola ve saÄŸa hareket etmek gibi eylemlerden birini seÃ§ebilir.

Bu eylemleri bir sÃ¶zlÃ¼k olarak tanÄ±mlayacaÄŸÄ±z ve bunlarÄ± ilgili koordinat deÄŸiÅŸiklikleriyle eÅŸleyeceÄŸiz. Ã–rneÄŸin, saÄŸa hareket etmek (`R`) `(1,0)` Ã§iftine karÅŸÄ±lÄ±k gelir. (kod bloÄŸu 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Bu senaryonun stratejisi ve hedefi ÅŸu ÅŸekilde Ã¶zetlenebilir:

- **Strateji**, ajanÄ±mÄ±zÄ±n (Peter) stratejisi, **politika** olarak adlandÄ±rÄ±lan bir fonksiyonla tanÄ±mlanÄ±r. Politika, herhangi bir durumda eylemi dÃ¶ndÃ¼ren bir fonksiyondur. Bizim durumumuzda, problemin durumu, oyuncunun mevcut pozisyonu dahil olmak Ã¼zere tahtayla temsil edilir.

- **Hedef**, pekiÅŸtirmeli Ã¶ÄŸrenmenin nihai hedefi, problemi verimli bir ÅŸekilde Ã§Ã¶zmemizi saÄŸlayacak iyi bir politika Ã¶ÄŸrenmektir. Ancak, temel olarak, **rastgele yÃ¼rÃ¼yÃ¼ÅŸ** adÄ± verilen en basit politikayÄ± ele alalÄ±m.

## Rastgele YÃ¼rÃ¼yÃ¼ÅŸ

Ã–ncelikle rastgele yÃ¼rÃ¼yÃ¼ÅŸ stratejisini uygulayarak problemimizi Ã§Ã¶zelim. Rastgele yÃ¼rÃ¼yÃ¼ÅŸte, elmaya ulaÅŸana kadar izin verilen eylemlerden birini rastgele seÃ§eriz (kod bloÄŸu 3).

1. AÅŸaÄŸÄ±daki kodla rastgele yÃ¼rÃ¼yÃ¼ÅŸÃ¼ uygulayÄ±n:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    `walk` Ã§aÄŸrÄ±sÄ±, bir Ã§alÄ±ÅŸtÄ±rmadan diÄŸerine deÄŸiÅŸebilen ilgili yolun uzunluÄŸunu dÃ¶ndÃ¼rmelidir.

1. YÃ¼rÃ¼yÃ¼ÅŸ deneyini birkaÃ§ kez (Ã¶rneÄŸin, 100 kez) Ã§alÄ±ÅŸtÄ±rÄ±n ve elde edilen istatistikleri yazdÄ±rÄ±n (kod bloÄŸu 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Ortalama yol uzunluÄŸunun yaklaÅŸÄ±k 30-40 adÄ±m olduÄŸunu unutmayÄ±n, bu oldukÃ§a fazladÄ±r, Ã§Ã¼nkÃ¼ en yakÄ±n elmaya olan ortalama mesafe yaklaÅŸÄ±k 5-6 adÄ±mdÄ±r.

    AyrÄ±ca Peter'Ä±n rastgele yÃ¼rÃ¼yÃ¼ÅŸ sÄ±rasÄ±nda hareketinin nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ gÃ¶rebilirsiniz:

    ![Peter'Ä±n Rastgele YÃ¼rÃ¼yÃ¼ÅŸÃ¼](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Ã–dÃ¼l Fonksiyonu

PolitikamÄ±zÄ± daha akÄ±llÄ± hale getirmek iÃ§in hangi hareketlerin diÄŸerlerinden "daha iyi" olduÄŸunu anlamamÄ±z gerekir. Bunu yapmak iÃ§in hedefimizi tanÄ±mlamamÄ±z gerekir.

Hedef, her durum iÃ§in bir puan deÄŸeri dÃ¶ndÃ¼ren bir **Ã¶dÃ¼l fonksiyonu** ile tanÄ±mlanabilir. SayÄ± ne kadar yÃ¼ksekse, Ã¶dÃ¼l fonksiyonu o kadar iyidir. (kod bloÄŸu 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Ã–dÃ¼l fonksiyonlarÄ±yla ilgili ilginÃ§ bir ÅŸey, Ã§oÄŸu durumda *oyunun sonunda yalnÄ±zca Ã¶nemli bir Ã¶dÃ¼l verilmesidir*. Bu, algoritmamÄ±zÄ±n olumlu bir Ã¶dÃ¼le yol aÃ§an "iyi" adÄ±mlarÄ± bir ÅŸekilde hatÄ±rlamasÄ± ve Ã¶nemlerini artÄ±rmasÄ± gerektiÄŸi anlamÄ±na gelir. Benzer ÅŸekilde, kÃ¶tÃ¼ sonuÃ§lara yol aÃ§an tÃ¼m hareketler caydÄ±rÄ±lmalÄ±dÄ±r.

## Q-Ã–ÄŸrenme

Burada tartÄ±ÅŸacaÄŸÄ±mÄ±z algoritma **Q-Ã–ÄŸrenme** olarak adlandÄ±rÄ±lÄ±r. Bu algoritmada politika, bir **Q-Tablosu** adÄ± verilen bir fonksiyon (veya veri yapÄ±sÄ±) ile tanÄ±mlanÄ±r. Bu tablo, belirli bir durumdaki her bir eylemin "iyiliÄŸini" kaydeder.

Q-Tablosu olarak adlandÄ±rÄ±lÄ±r Ã§Ã¼nkÃ¼ genellikle bir tablo veya Ã§ok boyutlu bir dizi olarak temsil etmek uygundur. TahtamÄ±zÄ±n boyutlarÄ± `geniÅŸlik` x `yÃ¼kseklik` olduÄŸundan, Q-Tablosunu `geniÅŸlik` x `yÃ¼kseklik` x `len(actions)` ÅŸeklinde bir numpy dizisi kullanarak temsil edebiliriz: (kod bloÄŸu 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Q-Tablosunun tÃ¼m deÄŸerlerini eÅŸit bir deÄŸerle, bizim durumumuzda - 0.25 ile baÅŸlatÄ±yoruz. Bu, her durumda tÃ¼m hareketlerin eÅŸit derecede iyi olduÄŸu "rastgele yÃ¼rÃ¼yÃ¼ÅŸ" politikasÄ±na karÅŸÄ±lÄ±k gelir. Q-Tablosunu tahtada gÃ¶rselleÅŸtirmek iÃ§in `plot` fonksiyonuna geÃ§irebiliriz: `m.plot(Q)`.

![Peter'Ä±n Ã‡evresi](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

Her hÃ¼crenin ortasÄ±nda, hareket yÃ¶nÃ¼nÃ¼ gÃ¶steren bir "ok" vardÄ±r. TÃ¼m yÃ¶nler eÅŸit olduÄŸundan, bir nokta gÃ¶sterilir.

Åimdi simÃ¼lasyonu Ã§alÄ±ÅŸtÄ±rmamÄ±z, Ã§evremizi keÅŸfetmemiz ve Q-Tablosu deÄŸerlerinin daha iyi bir daÄŸÄ±lÄ±mÄ±nÄ± Ã¶ÄŸrenmemiz gerekiyor, bu da elmaya giden yolu Ã§ok daha hÄ±zlÄ± bulmamÄ±zÄ± saÄŸlayacaktÄ±r.

## Q-Ã–ÄŸrenmenin Ã–zeti: Bellman Denklemi

Hareket etmeye baÅŸladÄ±ÄŸÄ±mÄ±zda, her eylemin karÅŸÄ±lÄ±k gelen bir Ã¶dÃ¼lÃ¼ olacaktÄ±r, yani teorik olarak en yÃ¼ksek anlÄ±k Ã¶dÃ¼le gÃ¶re bir sonraki eylemi seÃ§ebiliriz. Ancak, Ã§oÄŸu durumda, hareket hedefimize ulaÅŸmamÄ±zÄ± saÄŸlamayacak ve bu nedenle hangi yÃ¶nÃ¼n daha iyi olduÄŸunu hemen karar veremeyiz.

> Ã–nemli olan anlÄ±k sonuÃ§ deÄŸil, simÃ¼lasyonun sonunda elde edeceÄŸimiz nihai sonuÃ§tur.

Bu gecikmiÅŸ Ã¶dÃ¼lÃ¼ hesaba katmak iÃ§in, problemimizi Ã¶zyinelemeli olarak dÃ¼ÅŸÃ¼nmemize olanak tanÄ±yan **[dinamik programlama](https://en.wikipedia.org/wiki/Dynamic_programming)** ilkelerini kullanmamÄ±z gerekir.

Åimdi *s* durumundayÄ±z ve bir sonraki *s'* durumuna geÃ§mek istiyoruz. Bunu yaparak, Ã¶dÃ¼l fonksiyonu tarafÄ±ndan tanÄ±mlanan *r(s,a)* anlÄ±k Ã¶dÃ¼lÃ¼nÃ¼ ve gelecekteki bir Ã¶dÃ¼lÃ¼ alacaÄŸÄ±z. EÄŸer Q-Tablomuzun her eylemin "Ã§ekiciliÄŸini" doÄŸru bir ÅŸekilde yansÄ±ttÄ±ÄŸÄ±nÄ± varsayarsak, *s'* durumunda *Q(s',a')*'nin maksimum deÄŸerine karÅŸÄ±lÄ±k gelen bir eylem *a* seÃ§eceÄŸiz. BÃ¶ylece, *s* durumunda alabileceÄŸimiz en iyi olasÄ± gelecekteki Ã¶dÃ¼l ÅŸu ÅŸekilde tanÄ±mlanacaktÄ±r: `max`

## PolitikayÄ± Kontrol Etme

Q-Tablosu, her durumdaki her bir eylemin "Ã§ekiciliÄŸini" listelediÄŸi iÃ§in, dÃ¼nyamÄ±zda verimli bir gezinmeyi tanÄ±mlamak iÃ§in kullanÄ±lmasÄ± oldukÃ§a kolaydÄ±r. En basit durumda, en yÃ¼ksek Q-Tablo deÄŸerine karÅŸÄ±lÄ±k gelen eylemi seÃ§ebiliriz: (kod bloÄŸu 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> YukarÄ±daki kodu birkaÃ§ kez denerseniz, bazen "takÄ±ldÄ±ÄŸÄ±nÄ±" fark edebilirsiniz ve bunu durdurmak iÃ§in not defterindeki STOP dÃ¼ÄŸmesine basmanÄ±z gerekir. Bu, iki durumun optimal Q-DeÄŸeri aÃ§Ä±sÄ±ndan birbirine "iÅŸaret ettiÄŸi" durumlarda meydana gelir; bu durumda ajan, bu durumlar arasÄ±nda sonsuz bir ÅŸekilde hareket eder.

## ğŸš€Meydan Okuma

> **GÃ¶rev 1:** `walk` fonksiyonunu, yolun maksimum uzunluÄŸunu belirli bir adÄ±m sayÄ±sÄ±yla (Ã¶rneÄŸin, 100) sÄ±nÄ±rlayacak ÅŸekilde deÄŸiÅŸtirin ve yukarÄ±daki kodun bu deÄŸeri zaman zaman dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼nÃ¼ gÃ¶zlemleyin.

> **GÃ¶rev 2:** `walk` fonksiyonunu, daha Ã¶nce bulunduÄŸu yerlere geri dÃ¶nmemesini saÄŸlayacak ÅŸekilde deÄŸiÅŸtirin. Bu, `walk`'un dÃ¶ngÃ¼ye girmesini engelleyecektir, ancak ajan yine de kaÃ§amayacaÄŸÄ± bir konumda "sÄ±kÄ±ÅŸÄ±p" kalabilir.

## Gezinme

Daha iyi bir gezinme politikasÄ±, eÄŸitim sÄ±rasÄ±nda kullandÄ±ÄŸÄ±mÄ±z ve sÃ¶mÃ¼rÃ¼ ile keÅŸfi birleÅŸtiren politika olacaktÄ±r. Bu politikada, her bir eylemi belirli bir olasÄ±lÄ±kla, Q-Tablosundaki deÄŸerlere orantÄ±lÄ± olarak seÃ§iyoruz. Bu strateji, ajanÄ±n daha Ã¶nce keÅŸfettiÄŸi bir konuma geri dÃ¶nmesine neden olabilir, ancak aÅŸaÄŸÄ±daki koddan gÃ¶rebileceÄŸiniz gibi, istenen konuma Ã§ok kÄ±sa bir ortalama yol ile sonuÃ§lanÄ±r (unutmayÄ±n ki `print_statistics` simÃ¼lasyonu 100 kez Ã§alÄ±ÅŸtÄ±rÄ±r): (kod bloÄŸu 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Bu kodu Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, Ã¶nceki ortalama yol uzunluÄŸundan Ã§ok daha kÃ¼Ã§Ã¼k bir deÄŸer elde etmelisiniz, genellikle 3-6 aralÄ±ÄŸÄ±nda.

## Ã–ÄŸrenme SÃ¼recini Ä°nceleme

BahsettiÄŸimiz gibi, Ã¶ÄŸrenme sÃ¼reci, problem alanÄ±nÄ±n yapÄ±sÄ± hakkÄ±nda kazanÄ±lan bilginin keÅŸfi ve kullanÄ±mÄ± arasÄ±nda bir dengedir. Ã–ÄŸrenme sonuÃ§larÄ±nÄ±n (ajanÄ±n hedefe kÄ±sa bir yol bulmasÄ±na yardÄ±mcÄ± olma yeteneÄŸi) iyileÅŸtiÄŸini gÃ¶rdÃ¼k, ancak Ã¶ÄŸrenme sÃ¼reci sÄ±rasÄ±nda ortalama yol uzunluÄŸunun nasÄ±l davrandÄ±ÄŸÄ±nÄ± gÃ¶zlemlemek de ilginÃ§tir:

## Ã–ÄŸrenilenler ÅŸu ÅŸekilde Ã¶zetlenebilir:

- **Ortalama yol uzunluÄŸu artar**. Burada gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z ÅŸey, baÅŸlangÄ±Ã§ta ortalama yol uzunluÄŸunun arttÄ±ÄŸÄ±dÄ±r. Bu muhtemelen, Ã§evre hakkÄ±nda hiÃ§bir ÅŸey bilmediÄŸimizde kÃ¶tÃ¼ durumlarda, su veya kurt gibi, sÄ±kÄ±ÅŸma olasÄ±lÄ±ÄŸÄ±mÄ±zÄ±n yÃ¼ksek olmasÄ±ndan kaynaklanmaktadÄ±r. Daha fazla bilgi edindikÃ§e ve bu bilgiyi kullanmaya baÅŸladÄ±kÃ§a, Ã§evreyi daha uzun sÃ¼re keÅŸfedebiliriz, ancak elmalarÄ±n nerede olduÄŸunu hala Ã§ok iyi bilmiyoruz.

- **Yol uzunluÄŸu, daha fazla Ã¶ÄŸrendikÃ§e azalÄ±r**. Yeterince Ã¶ÄŸrendikten sonra, ajanÄ±n hedefe ulaÅŸmasÄ± daha kolay hale gelir ve yol uzunluÄŸu azalmaya baÅŸlar. Ancak, hala keÅŸfe aÃ§Ä±k olduÄŸumuz iÃ§in, genellikle en iyi yoldan saparÄ±z ve yeni seÃ§enekleri keÅŸfederek yolu optimalden daha uzun hale getiririz.

- **Uzunluk aniden artar**. Grafikte ayrÄ±ca uzunluÄŸun bir noktada aniden arttÄ±ÄŸÄ±nÄ± gÃ¶zlemliyoruz. Bu, sÃ¼recin stokastik doÄŸasÄ±nÄ± ve Q-Tablo katsayÄ±larÄ±nÄ± yeni deÄŸerlerle Ã¼zerine yazmak suretiyle "bozabileceÄŸimizi" gÃ¶sterir. Bu, ideal olarak Ã¶ÄŸrenme oranÄ±nÄ± azaltarak (Ã¶rneÄŸin, eÄŸitimin sonlarÄ±na doÄŸru Q-Tablo deÄŸerlerini yalnÄ±zca kÃ¼Ã§Ã¼k bir deÄŸerle ayarlayarak) en aza indirgenmelidir.

Genel olarak, Ã¶ÄŸrenme sÃ¼recinin baÅŸarÄ±sÄ± ve kalitesi, Ã¶ÄŸrenme oranÄ±, Ã¶ÄŸrenme oranÄ± azalmasÄ± ve indirim faktÃ¶rÃ¼ gibi parametrelere Ã¶nemli Ã¶lÃ§Ã¼de baÄŸlÄ±dÄ±r. Bunlar genellikle **hiperparametreler** olarak adlandÄ±rÄ±lÄ±r, Ã§Ã¼nkÃ¼ eÄŸitim sÄ±rasÄ±nda optimize ettiÄŸimiz **parametrelerden** (Ã¶rneÄŸin, Q-Tablo katsayÄ±larÄ±) farklÄ±dÄ±rlar. En iyi hiperparametre deÄŸerlerini bulma sÃ¼recine **hiperparametre optimizasyonu** denir ve bu ayrÄ± bir konuyu hak eder.

## [Ders SonrasÄ± Test](https://ff-quizzes.netlify.app/en/ml/)

## Ã–dev 
[Daha GerÃ§ekÃ§i Bir DÃ¼nya](assignment.md)

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±k iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalar iÃ§in sorumluluk kabul etmiyoruz.