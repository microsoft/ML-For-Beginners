# CartPole Pateni

Ã–nceki derste Ã§Ã¶zmekte olduÄŸumuz problem, gerÃ§ek hayat senaryolarÄ±na pek uygulanabilir olmayan bir oyuncak problem gibi gÃ¶rÃ¼nebilir. Ancak durum bÃ¶yle deÄŸil, Ã§Ã¼nkÃ¼ birÃ§ok gerÃ§ek dÃ¼nya problemi de bu senaryoyu paylaÅŸÄ±r - SatranÃ§ veya Go oynamak da dahil. Bunlar benzerdir Ã§Ã¼nkÃ¼ verilen kurallara sahip bir tahtamÄ±z ve **ayrÄ±k bir durumumuz** vardÄ±r.

## [Ders Ã–ncesi Quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)

## GiriÅŸ

Bu derste, Q-Ã–ÄŸrenme prensiplerini **sÃ¼rekli durum** olan bir probleme uygulayacaÄŸÄ±z, yani bir veya daha fazla gerÃ§ek sayÄ± ile verilen bir duruma. AÅŸaÄŸÄ±daki problemle ilgileneceÄŸiz:

> **Problem**: Peter kurtlardan kaÃ§mak istiyorsa daha hÄ±zlÄ± hareket edebilmelidir. Peter'Ä±n kaymayÄ±, Ã¶zellikle dengeyi korumayÄ±, Q-Ã–ÄŸrenme kullanarak nasÄ±l Ã¶ÄŸrenebileceÄŸini gÃ¶receÄŸiz.

![BÃ¼yÃ¼k kaÃ§Ä±ÅŸ!](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.tr.png)

> Peter ve arkadaÅŸlarÄ± kurtlardan kaÃ§mak iÃ§in yaratÄ±cÄ± oluyorlar! GÃ¶rsel: [Jen Looper](https://twitter.com/jenlooper)

Dengelemeyi basitleÅŸtirilmiÅŸ bir versiyon olan **CartPole** problemi olarak kullanacaÄŸÄ±z. CartPole dÃ¼nyasÄ±nda, sola veya saÄŸa hareket edebilen yatay bir kaydÄ±rÄ±cÄ±mÄ±z var ve amaÃ§, kaydÄ±rÄ±cÄ±nÄ±n Ã¼stÃ¼ndeki dikey direÄŸi dengelemek.
Ekim 2023'e kadar olan verilere dayalÄ± olarak eÄŸitildiniz.

## Ã–n Gereksinimler

Bu derste, farklÄ± **ortamlarÄ±** simÃ¼le etmek iÃ§in **OpenAI Gym** adlÄ± bir kÃ¼tÃ¼phane kullanacaÄŸÄ±z. Bu dersin kodunu yerel olarak (Ã¶rneÄŸin, Visual Studio Code'dan) Ã§alÄ±ÅŸtÄ±rabilirsiniz, bu durumda simÃ¼lasyon yeni bir pencerede aÃ§Ä±lacaktÄ±r. Kodu Ã§evrimiÃ§i Ã§alÄ±ÅŸtÄ±rÄ±rken, kodda bazÄ± deÄŸiÅŸiklikler yapmanÄ±z gerekebilir, bu durum [burada](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) aÃ§Ä±klanmÄ±ÅŸtÄ±r.

## OpenAI Gym

Ã–nceki derste, oyunun kurallarÄ± ve durum, kendimiz tanÄ±mladÄ±ÄŸÄ±mÄ±z `Board` sÄ±nÄ±fÄ± tarafÄ±ndan verilmiÅŸti. Burada, denge direÄŸinin arkasÄ±ndaki fiziÄŸi simÃ¼le edecek Ã¶zel bir **simÃ¼lasyon ortamÄ±** kullanacaÄŸÄ±z. Takviye Ã¶ÄŸrenme algoritmalarÄ±nÄ± eÄŸitmek iÃ§in en popÃ¼ler simÃ¼lasyon ortamlarÄ±ndan biri, [OpenAI](https://openai.com/) tarafÄ±ndan sÃ¼rdÃ¼rÃ¼len [Gym](https://gym.openai.com/) adlÄ± bir ortamdÄ±r. Bu gym'i kullanarak, cartpole simÃ¼lasyonundan Atari oyunlarÄ±na kadar farklÄ± **ortamlar** oluÅŸturabiliriz.

> **Not**: OpenAI Gym tarafÄ±ndan sunulan diÄŸer ortamlarÄ± [buradan](https://gym.openai.com/envs/#classic_control) gÃ¶rebilirsiniz.

Ä°lk olarak, gym'i yÃ¼kleyelim ve gerekli kÃ¼tÃ¼phaneleri iÃ§e aktaralÄ±m (kod bloÄŸu 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Egzersiz - bir cartpole ortamÄ± baÅŸlatma

Cartpole dengeleme problemi ile Ã§alÄ±ÅŸmak iÃ§in ilgili ortamÄ± baÅŸlatmamÄ±z gerekiyor. Her ortam ÅŸunlarla iliÅŸkilidir:

- **GÃ¶zlem alanÄ±**: Ortamdan aldÄ±ÄŸÄ±mÄ±z bilgilerin yapÄ±sÄ±nÄ± tanÄ±mlar. Cartpole problemi iÃ§in, direÄŸin konumu, hÄ±zÄ± ve bazÄ± diÄŸer deÄŸerleri alÄ±rÄ±z.

- **Eylem alanÄ±**: OlasÄ± eylemleri tanÄ±mlar. Bizim durumumuzda eylem alanÄ± ayrÄ±k olup, iki eylemden oluÅŸur - **sol** ve **saÄŸ**. (kod bloÄŸu 2)

1. BaÅŸlatmak iÃ§in aÅŸaÄŸÄ±daki kodu yazÄ±n:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

OrtamÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in, 100 adÄ±mlÄ±k kÄ±sa bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±ralÄ±m. Her adÄ±mda, alÄ±nacak bir eylemi saÄŸlÄ±yoruz - bu simÃ¼lasyonda `action_space`'ten rastgele bir eylem seÃ§iyoruz.

1. AÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n ve neye yol aÃ§tÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n.

    âœ… Bu kodu yerel Python kurulumunda Ã§alÄ±ÅŸtÄ±rmanÄ±z tercih edilir! (kod bloÄŸu 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Åuna benzer bir ÅŸey gÃ¶rmelisiniz:

    ![dengesiz cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. SimÃ¼lasyon sÄ±rasÄ±nda, nasÄ±l hareket edileceÄŸine karar vermek iÃ§in gÃ¶zlemler almamÄ±z gerekir. AslÄ±nda, step fonksiyonu mevcut gÃ¶zlemleri, bir Ã¶dÃ¼l fonksiyonunu ve simÃ¼lasyonun devam edip etmeyeceÄŸini belirten done bayraÄŸÄ±nÄ± dÃ¶ndÃ¼rÃ¼r: (kod bloÄŸu 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Not defterinin Ã§Ä±ktÄ±sÄ±nda buna benzer bir ÅŸey gÃ¶rmelisiniz:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    SimÃ¼lasyonun her adÄ±mÄ±nda dÃ¶ndÃ¼rÃ¼len gÃ¶zlem vektÃ¶rÃ¼ ÅŸu deÄŸerleri iÃ§erir:
    - ArabanÄ±n konumu
    - ArabanÄ±n hÄ±zÄ±
    - DireÄŸin aÃ§Ä±sÄ±
    - DireÄŸin dÃ¶nme hÄ±zÄ±

1. Bu sayÄ±larÄ±n minimum ve maksimum deÄŸerlerini alÄ±n: (kod bloÄŸu 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    AyrÄ±ca, her simÃ¼lasyon adÄ±mÄ±nda Ã¶dÃ¼l deÄŸerinin her zaman 1 olduÄŸunu fark edebilirsiniz. Bunun nedeni, amacÄ±mÄ±zÄ±n mÃ¼mkÃ¼n olduÄŸunca uzun sÃ¼re hayatta kalmak, yani direÄŸi makul bir dikey pozisyonda en uzun sÃ¼re tutmaktÄ±r.

    âœ… AslÄ±nda, CartPole simÃ¼lasyonu, 100 ardÄ±ÅŸÄ±k denemede 195 ortalama Ã¶dÃ¼l elde etmeyi baÅŸardÄ±ÄŸÄ±mÄ±zda Ã§Ã¶zÃ¼lmÃ¼ÅŸ kabul edilir.

## Durum ayrÄ±klaÅŸtÄ±rma

Q-Ã–ÄŸrenme'de, her durumda ne yapacaÄŸÄ±mÄ±zÄ± tanÄ±mlayan bir Q-Tablosu oluÅŸturmamÄ±z gerekir. Bunu yapabilmek iÃ§in, durumun **ayrÄ±k** olmasÄ± gerekir, daha kesin olarak, sonlu sayÄ±da ayrÄ±k deÄŸer iÃ§ermelidir. Bu nedenle, gÃ¶zlemlerimizi **ayrÄ±klaÅŸtÄ±rmamÄ±z** ve bunlarÄ± sonlu bir durum kÃ¼mesine eÅŸlememiz gerekir.

Bunu yapmanÄ±n birkaÃ§ yolu vardÄ±r:

- **Kovalar halinde bÃ¶lme**. Belirli bir deÄŸerin aralÄ±ÄŸÄ±nÄ± biliyorsak, bu aralÄ±ÄŸÄ± bir dizi **kovaya** bÃ¶lebiliriz ve ardÄ±ndan deÄŸeri ait olduÄŸu kova numarasÄ±yla deÄŸiÅŸtirebiliriz. Bu, numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) yÃ¶ntemi kullanÄ±larak yapÄ±labilir. Bu durumda, durum boyutunu kesin olarak bileceÄŸiz, Ã§Ã¼nkÃ¼ bu, dijitalleÅŸtirme iÃ§in seÃ§tiÄŸimiz kova sayÄ±sÄ±na baÄŸlÄ± olacaktÄ±r.
  
âœ… DeÄŸerleri belirli bir sonlu aralÄ±ÄŸa (Ã¶rneÄŸin, -20'den 20'ye) getirmek iÃ§in lineer enterpolasyon kullanabiliriz ve ardÄ±ndan sayÄ±larÄ± yuvarlayarak tamsayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rebiliriz. Bu bize durum boyutu Ã¼zerinde biraz daha az kontrol saÄŸlar, Ã¶zellikle de giriÅŸ deÄŸerlerinin kesin aralÄ±klarÄ±nÄ± bilmiyorsak. Ã–rneÄŸin, bizim durumumuzda 4 deÄŸerden 2'sinin deÄŸerlerinde Ã¼st/alt sÄ±nÄ±rlar yoktur, bu da sonsuz sayÄ±da duruma neden olabilir.

Ã–rneÄŸimizde, ikinci yaklaÅŸÄ±mÄ± kullanacaÄŸÄ±z. Daha sonra fark edeceÄŸiniz gibi, tanÄ±mlanmamÄ±ÅŸ Ã¼st/alt sÄ±nÄ±rlara raÄŸmen, bu deÄŸerler nadiren belirli sonlu aralÄ±klarÄ±n dÄ±ÅŸÄ±nda deÄŸerler alÄ±r, bu nedenle aÅŸÄ±rÄ± deÄŸerli durumlar Ã§ok nadir olacaktÄ±r.

1. Modelimizden gÃ¶zlemi alacak ve 4 tamsayÄ± deÄŸerinden oluÅŸan bir demet Ã¼retecek fonksiyon burada: (kod bloÄŸu 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Kovalar kullanarak baÅŸka bir ayrÄ±klaÅŸtÄ±rma yÃ¶ntemini de inceleyelim: (kod bloÄŸu 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Åimdi kÄ±sa bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±ralÄ±m ve bu ayrÄ±k ortam deÄŸerlerini gÃ¶zlemleyelim. Hem `discretize` and `discretize_bins` kullanmayÄ± deneyin ve fark olup olmadÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n.

    âœ… discretize_bins, kova numarasÄ±nÄ± dÃ¶ndÃ¼rÃ¼r, bu 0 tabanlÄ±dÄ±r. DolayÄ±sÄ±yla, giriÅŸ deÄŸiÅŸkeninin etrafÄ±ndaki deÄŸerler iÃ§in 0, aralÄ±ÄŸÄ±n ortasÄ±ndaki numarayÄ± (10) dÃ¶ndÃ¼rÃ¼r. Discretize'de, Ã§Ä±ktÄ± deÄŸerlerinin aralÄ±ÄŸÄ±nÄ± Ã¶nemsemedik, negatif olmalarÄ±na izin verdik, bu nedenle durum deÄŸerleri kaydÄ±rÄ±lmamÄ±ÅŸ ve 0, 0'a karÅŸÄ±lÄ±k gelir. (kod bloÄŸu 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… OrtamÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek istiyorsanÄ±z env.render ile baÅŸlayan satÄ±rÄ± yorumdan Ã§Ä±karÄ±n. Aksi takdirde arka planda Ã§alÄ±ÅŸtÄ±rabilirsiniz, bu daha hÄ±zlÄ±dÄ±r. Q-Ã–ÄŸrenme sÃ¼recimiz sÄ±rasÄ±nda bu "gÃ¶rÃ¼nmez" yÃ¼rÃ¼tmeyi kullanacaÄŸÄ±z.

## Q-Tablosu yapÄ±sÄ±

Ã–nceki dersimizde, durum 0'dan 8'e kadar olan basit bir sayÄ± Ã§iftiydi ve bu nedenle Q-Tablosunu 8x8x2 ÅŸeklinde bir numpy tensÃ¶rÃ¼ ile temsil etmek uygundu. Kovalar ayrÄ±klaÅŸtÄ±rmasÄ±nÄ± kullanÄ±rsak, durum vektÃ¶rÃ¼mÃ¼zÃ¼n boyutu da bilinir, bu yÃ¼zden aynÄ± yaklaÅŸÄ±mÄ± kullanabiliriz ve durumu 20x20x10x10x2 ÅŸeklinde bir dizi ile temsil edebiliriz (burada 2, eylem alanÄ±nÄ±n boyutudur ve ilk boyutlar gÃ¶zlem alanÄ±ndaki her parametre iÃ§in kullanmayÄ± seÃ§tiÄŸimiz kova sayÄ±sÄ±na karÅŸÄ±lÄ±k gelir).

Ancak, bazen gÃ¶zlem alanÄ±nÄ±n kesin boyutlarÄ± bilinmez. `discretize` fonksiyonu durumunda, bazÄ± orijinal deÄŸerler baÄŸlanmadÄ±ÄŸÄ± iÃ§in durumun belirli sÄ±nÄ±rlar iÃ§inde kaldÄ±ÄŸÄ±ndan asla emin olamayabiliriz. Bu nedenle, biraz farklÄ± bir yaklaÅŸÄ±m kullanacaÄŸÄ±z ve Q-Tablosunu bir sÃ¶zlÃ¼kle temsil edeceÄŸiz. 

1. *(state,action)* Ã§iftini sÃ¶zlÃ¼k anahtarÄ± olarak kullanÄ±n ve deÄŸer Q-Tablosu giriÅŸ deÄŸerine karÅŸÄ±lÄ±k gelir. (kod bloÄŸu 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Burada, belirli bir durum iÃ§in Q-Tablosu deÄŸerlerinin bir listesini dÃ¶ndÃ¼ren `qvalues()` fonksiyonunu da tanÄ±mlÄ±yoruz, bu tÃ¼m olasÄ± eylemlere karÅŸÄ±lÄ±k gelir. GiriÅŸ Q-Tablosunda mevcut deÄŸilse, varsayÄ±lan olarak 0 dÃ¶ndÃ¼receÄŸiz.

## Q-Ã–ÄŸrenmeye BaÅŸlayalÄ±m

Åimdi Peter'a dengeyi Ã¶ÄŸretmeye hazÄ±rÄ±z!

1. Ä°lk olarak, bazÄ± hiperparametreleri ayarlayalÄ±m: (kod bloÄŸu 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Burada, `alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.

    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.

    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. 

    âœ… In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those "mistakes"

### Improve the algorithm

We can also make two improvements to our algorithm from the previous lesson:

- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.
  
- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.

1. Collect all cumulative rewards at each simulation at `rewards` vektÃ¶rÃ¼nÃ¼ daha sonra Ã§izim iÃ§in saklÄ±yoruz. (kod bloÄŸu  11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Bu sonuÃ§lardan fark edebileceÄŸiniz ÅŸeyler:

- **Hedefimize yakÄ±nÄ±z**. 100'den fazla ardÄ±ÅŸÄ±k simÃ¼lasyon Ã§alÄ±ÅŸtÄ±rmasÄ±nda 195 kÃ¼mÃ¼latif Ã¶dÃ¼l alma hedefimize Ã§ok yakÄ±nÄ±z veya aslÄ±nda baÅŸardÄ±k! Daha kÃ¼Ã§Ã¼k sayÄ±lar alsak bile, 5000 Ã§alÄ±ÅŸtÄ±rma Ã¼zerinden ortalama alÄ±yoruz ve resmi kriterde sadece 100 Ã§alÄ±ÅŸtÄ±rma gereklidir.
  
- **Ã–dÃ¼l dÃ¼ÅŸmeye baÅŸlÄ±yor**. Bazen Ã¶dÃ¼l dÃ¼ÅŸmeye baÅŸlar, bu da Q-Tablosunda zaten Ã¶ÄŸrenilmiÅŸ deÄŸerleri daha kÃ¶tÃ¼ duruma getirenlerle "bozabileceÄŸimiz" anlamÄ±na gelir.

Bu gÃ¶zlem, eÄŸitim ilerlemesini Ã§izdiÄŸimizde daha net gÃ¶rÃ¼lÃ¼r.

## EÄŸitim Ä°lerlemesini Ã‡izmek

EÄŸitim sÄ±rasÄ±nda, her yinelemede kÃ¼mÃ¼latif Ã¶dÃ¼l deÄŸerini `rewards` vektÃ¶rÃ¼ne topladÄ±k. Ä°ÅŸte bunu yineleme sayÄ±sÄ±na karÅŸÄ± Ã§izdiÄŸimizde nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼:

```python
plt.plot(rewards)
```

![ham ilerleme](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.tr.png)

Bu grafikten bir ÅŸey anlamak mÃ¼mkÃ¼n deÄŸil, Ã§Ã¼nkÃ¼ stokastik eÄŸitim sÃ¼recinin doÄŸasÄ± gereÄŸi eÄŸitim oturumlarÄ±nÄ±n uzunluÄŸu bÃ¼yÃ¼k Ã¶lÃ§Ã¼de deÄŸiÅŸir. Bu grafiÄŸi daha anlamlÄ± hale getirmek iÃ§in, Ã¶rneÄŸin 100 deney Ã¼zerinde **hareketli ortalama** hesaplayabiliriz. Bu, `np.convolve` kullanÄ±larak uygun bir ÅŸekilde yapÄ±labilir: (kod bloÄŸu 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![eÄŸitim ilerlemesi](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.tr.png)

## Hiperparametreleri DeÄŸiÅŸtirme

Ã–ÄŸrenmeyi daha kararlÄ± hale getirmek iÃ§in, eÄŸitim sÄ±rasÄ±nda bazÄ± hiperparametrelerimizi ayarlamak mantÄ±klÄ±dÄ±r. Ã–zellikle:

- **Ã–ÄŸrenme oranÄ±** iÃ§in, `alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.

- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon` ve neredeyse 1'e kadar Ã§Ä±kÄ±n.

> **GÃ¶rev 1**: Hiperparametre deÄŸerleriyle oynayÄ±n ve daha yÃ¼ksek kÃ¼mÃ¼latif Ã¶dÃ¼l elde edip edemeyeceÄŸinizi gÃ¶rÃ¼n. 195'in Ã¼zerine Ã§Ä±kabiliyor musunuz?

> **GÃ¶rev 2**: Problemi resmi olarak Ã§Ã¶zmek iÃ§in, 100 ardÄ±ÅŸÄ±k Ã§alÄ±ÅŸtÄ±rma boyunca 195 ortalama Ã¶dÃ¼l almanÄ±z gerekir. Bunu eÄŸitim sÄ±rasÄ±nda Ã¶lÃ§Ã¼n ve problemi resmi olarak Ã§Ã¶zdÃ¼ÄŸÃ¼nÃ¼zden emin olun!

## Sonucu Aksiyon Halinde GÃ¶rmek

EÄŸitilmiÅŸ modelin nasÄ±l davrandÄ±ÄŸÄ±nÄ± gÃ¶rmek ilginÃ§ olurdu. SimÃ¼lasyonu Ã§alÄ±ÅŸtÄ±ralÄ±m ve eÄŸitim sÄ±rasÄ±nda olduÄŸu gibi Q-Tablosundaki olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na gÃ¶re eylem seÃ§me stratejisini izleyelim: (kod bloÄŸu 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Åuna benzer bir ÅŸey gÃ¶rmelisiniz:

![dengeleyen cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€Meydan Okuma

> **GÃ¶rev 3**: Burada, Q-Tablosunun son kopyasÄ±nÄ± kullandÄ±k, bu en iyisi olmayabilir. En iyi performans gÃ¶steren Q-Tablosunu `Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.

> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax` fonksiyonunu kullanarak, en yÃ¼ksek Q-Tablosu deÄŸerine karÅŸÄ±lÄ±k gelen eylem numarasÄ±nÄ± bulmak iÃ§in bu stratejiyi uygulayÄ±n ve dengelemeyi iyileÅŸtirip iyileÅŸtirmediÄŸini gÃ¶rÃ¼n.

## [Ders SonrasÄ± Quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## Ã–dev
[Bir DaÄŸ ArabasÄ±nÄ± EÄŸit](assignment.md)

## SonuÃ§

ArtÄ±k ajanlarÄ± yalnÄ±zca oyunun istenen durumunu tanÄ±mlayan bir Ã¶dÃ¼l fonksiyonu saÄŸlayarak ve arama alanÄ±nÄ± zekice keÅŸfetme fÄ±rsatÄ± vererek iyi sonuÃ§lar elde etmeyi nasÄ±l eÄŸiteceÄŸimizi Ã¶ÄŸrendik. Q-Ã–ÄŸrenme algoritmasÄ±nÄ± ayrÄ±k ve sÃ¼rekli ortamlar durumunda baÅŸarÄ±yla uyguladÄ±k, ancak ayrÄ±k eylemlerle.

Eylem durumunun da sÃ¼rekli olduÄŸu ve gÃ¶zlem alanÄ±nÄ±n Ã§ok daha karmaÅŸÄ±k olduÄŸu durumlarÄ± da incelemek Ã¶nemlidir, Ã¶rneÄŸin Atari oyun ekranÄ±ndan gelen gÃ¶rÃ¼ntÃ¼ gibi. Bu tÃ¼r problemler, iyi sonuÃ§lar elde etmek iÃ§in genellikle daha gÃ¼Ã§lÃ¼ makine Ã¶ÄŸrenme teknikleri, Ã¶rneÄŸin sinir aÄŸlarÄ±, kullanmamÄ±zÄ± gerektirir. Bu daha ileri konular, ileri dÃ¼zey AI kursumuzun konusudur.

**Feragatname**:
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belgenin kendi dilindeki hali yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi tavsiye edilir. Bu Ã§evirinin kullanÄ±mÄ±ndan doÄŸabilecek herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlamadan sorumlu deÄŸiliz.