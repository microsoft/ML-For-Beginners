<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-06T08:04:44+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "tr"
}
-->
## Ã–n KoÅŸullar

Bu derste, farklÄ± **ortamlarÄ±** simÃ¼le etmek iÃ§in **OpenAI Gym** adlÄ± bir kÃ¼tÃ¼phane kullanacaÄŸÄ±z. Bu dersin kodunu yerel olarak (Ã¶rneÄŸin, Visual Studio Code'dan) Ã§alÄ±ÅŸtÄ±rabilirsiniz; bu durumda simÃ¼lasyon yeni bir pencerede aÃ§Ä±lacaktÄ±r. Kodu Ã§evrimiÃ§i Ã§alÄ±ÅŸtÄ±rÄ±rken, [burada](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) aÃ§Ä±klandÄ±ÄŸÄ± gibi kodda bazÄ± deÄŸiÅŸiklikler yapmanÄ±z gerekebilir.

## OpenAI Gym

Ã–nceki derste, oyunun kurallarÄ± ve durum, kendimiz tanÄ±mladÄ±ÄŸÄ±mÄ±z `Board` sÄ±nÄ±fÄ± tarafÄ±ndan belirlenmiÅŸti. Burada, dengeleyen direÄŸin fiziÄŸini simÃ¼le edecek Ã¶zel bir **simÃ¼lasyon ortamÄ±** kullanacaÄŸÄ±z. Takviye Ã¶ÄŸrenme algoritmalarÄ±nÄ± eÄŸitmek iÃ§in en popÃ¼ler simÃ¼lasyon ortamlarÄ±ndan biri, [Gym](https://gym.openai.com/) olarak adlandÄ±rÄ±lÄ±r ve [OpenAI](https://openai.com/) tarafÄ±ndan geliÅŸtirilmiÅŸtir. Bu gym'i kullanarak, bir cartpole simÃ¼lasyonundan Atari oyunlarÄ±na kadar farklÄ± **ortamlar** oluÅŸturabiliriz.

> **Not**: OpenAI Gym tarafÄ±ndan sunulan diÄŸer ortamlarÄ± [buradan](https://gym.openai.com/envs/#classic_control) gÃ¶rebilirsiniz.

Ã–ncelikle gym'i yÃ¼kleyelim ve gerekli kÃ¼tÃ¼phaneleri iÃ§e aktaralÄ±m (kod bloÄŸu 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## AlÄ±ÅŸtÄ±rma - bir cartpole ortamÄ± baÅŸlatma

Cartpole dengeleme problemiyle Ã§alÄ±ÅŸmak iÃ§in ilgili ortamÄ± baÅŸlatmamÄ±z gerekiyor. Her ortam ÅŸu Ã¶zelliklere sahiptir:

- **GÃ¶zlem alanÄ±**: Ortamdan aldÄ±ÄŸÄ±mÄ±z bilgilerin yapÄ±sÄ±nÄ± tanÄ±mlar. Cartpole probleminde, direÄŸin pozisyonu, hÄ±zÄ± ve diÄŸer bazÄ± deÄŸerleri alÄ±rÄ±z.

- **Eylem alanÄ±**: OlasÄ± eylemleri tanÄ±mlar. Bizim durumumuzda eylem alanÄ± ayrÄ±k olup iki eylemden oluÅŸur - **sol** ve **saÄŸ**. (kod bloÄŸu 2)

1. BaÅŸlatmak iÃ§in ÅŸu kodu yazÄ±n:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

OrtamÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in 100 adÄ±mlÄ±k kÄ±sa bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±ralÄ±m. Her adÄ±mda, yapÄ±lacak bir eylemi saÄŸlÄ±yoruz - bu simÃ¼lasyonda `action_space`'den rastgele bir eylem seÃ§iyoruz.

1. AÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n ve ne sonuÃ§ verdiÄŸini gÃ¶rÃ¼n.

    âœ… Bu kodu yerel bir Python kurulumunda Ã§alÄ±ÅŸtÄ±rmanÄ±z Ã¶nerilir! (kod bloÄŸu 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Åuna benzer bir ÅŸey gÃ¶rmelisiniz:

    ![dengelemeyen cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. SimÃ¼lasyon sÄ±rasÄ±nda, nasÄ±l hareket edeceÄŸimize karar vermek iÃ§in gÃ¶zlemler almamÄ±z gerekiyor. AslÄ±nda, step fonksiyonu mevcut gÃ¶zlemleri, bir Ã¶dÃ¼l fonksiyonunu ve simÃ¼lasyona devam etmenin mantÄ±klÄ± olup olmadÄ±ÄŸÄ±nÄ± gÃ¶steren bir done bayraÄŸÄ±nÄ± dÃ¶ndÃ¼rÃ¼r: (kod bloÄŸu 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Notebook Ã§Ä±ktÄ±sÄ±nda buna benzer bir ÅŸey gÃ¶receksiniz:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    SimÃ¼lasyonun her adÄ±mÄ±nda dÃ¶ndÃ¼rÃ¼len gÃ¶zlem vektÃ¶rÃ¼ ÅŸu deÄŸerleri iÃ§erir:
    - KartÄ±n pozisyonu
    - KartÄ±n hÄ±zÄ±
    - DireÄŸin aÃ§Ä±sÄ±
    - DireÄŸin dÃ¶nÃ¼ÅŸ hÄ±zÄ±

1. Bu sayÄ±larÄ±n minimum ve maksimum deÄŸerlerini alÄ±n: (kod bloÄŸu 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    AyrÄ±ca, her simÃ¼lasyon adÄ±mÄ±nda Ã¶dÃ¼l deÄŸerinin her zaman 1 olduÄŸunu fark edebilirsiniz. Bunun nedeni, amacÄ±mÄ±zÄ±n mÃ¼mkÃ¼n olduÄŸunca uzun sÃ¼re hayatta kalmak, yani direÄŸi makul bir dikey pozisyonda en uzun sÃ¼re tutmak olmasÄ±dÄ±r.

    âœ… AslÄ±nda, CartPole simÃ¼lasyonu, 100 ardÄ±ÅŸÄ±k deneme boyunca ortalama 195 Ã¶dÃ¼l almayÄ± baÅŸardÄ±ÄŸÄ±mÄ±zda Ã§Ã¶zÃ¼lmÃ¼ÅŸ kabul edilir.

## Durum AyrÄ±klaÅŸtÄ±rma

Q-Ã–ÄŸrenme'de, her durumda ne yapÄ±lacaÄŸÄ±nÄ± tanÄ±mlayan bir Q-Tablosu oluÅŸturmamÄ±z gerekiyor. Bunu yapabilmek iÃ§in, durumun **ayrÄ±k** olmasÄ±, daha doÄŸrusu, sonlu sayÄ±da ayrÄ±k deÄŸer iÃ§ermesi gerekir. Bu nedenle, gÃ¶zlemlerimizi bir ÅŸekilde **ayrÄ±klaÅŸtÄ±rmamÄ±z**, onlarÄ± sonlu bir durum kÃ¼mesine eÅŸlememiz gerekiyor.

Bunu yapmanÄ±n birkaÃ§ yolu vardÄ±r:

- **AralÄ±klara bÃ¶lmek**. Belirli bir deÄŸerin aralÄ±ÄŸÄ±nÄ± biliyorsak, bu aralÄ±ÄŸÄ± bir dizi **aralÄ±ÄŸa** bÃ¶lebilir ve ardÄ±ndan deÄŸeri ait olduÄŸu aralÄ±k numarasÄ±yla deÄŸiÅŸtirebiliriz. Bu, numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) yÃ¶ntemi kullanÄ±larak yapÄ±labilir. Bu durumda, durum boyutunu kesin olarak bileceÄŸiz, Ã§Ã¼nkÃ¼ dijitalleÅŸtirme iÃ§in seÃ§tiÄŸimiz aralÄ±k sayÄ±sÄ±na baÄŸlÄ± olacaktÄ±r.
  
âœ… DeÄŸerleri belirli bir sonlu aralÄ±ÄŸa (Ã¶rneÄŸin, -20 ile 20 arasÄ±nda) getirmek iÃ§in doÄŸrusal interpolasyon kullanabilir ve ardÄ±ndan sayÄ±larÄ± yuvarlayarak tam sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rebiliriz. Bu, durum boyutunun bÃ¼yÃ¼klÃ¼ÄŸÃ¼ Ã¼zerinde biraz daha az kontrol saÄŸlar, Ã¶zellikle de giriÅŸ deÄŸerlerinin kesin aralÄ±klarÄ±nÄ± bilmiyorsak. Ã–rneÄŸin, bizim durumumuzda 4 deÄŸerden 2'sinin Ã¼st/alt sÄ±nÄ±rlarÄ± yoktur, bu da sonsuz sayÄ±da duruma yol aÃ§abilir.

Bizim Ã¶rneÄŸimizde ikinci yaklaÅŸÄ±mÄ± kullanacaÄŸÄ±z. Daha sonra fark edeceÄŸiniz gibi, tanÄ±msÄ±z Ã¼st/alt sÄ±nÄ±rlara raÄŸmen, bu deÄŸerler nadiren belirli sonlu aralÄ±klarÄ±n dÄ±ÅŸÄ±na Ã§Ä±kar, bu nedenle aÅŸÄ±rÄ± deÄŸerlerle durumlar Ã§ok nadir olacaktÄ±r.

1. Ä°ÅŸte modelimizden gÃ¶zlemi alacak ve 4 tam sayÄ± deÄŸerinden oluÅŸan bir tuple Ã¼retecek fonksiyon: (kod bloÄŸu 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. AralÄ±klar kullanarak baÅŸka bir ayrÄ±klaÅŸtÄ±rma yÃ¶ntemini de keÅŸfedelim: (kod bloÄŸu 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Åimdi kÄ±sa bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±ralÄ±m ve bu ayrÄ±k ortam deÄŸerlerini gÃ¶zlemleyelim. `discretize` ve `discretize_bins` yÃ¶ntemlerini denemekten Ã§ekinmeyin ve aralarÄ±nda bir fark olup olmadÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼n.

    âœ… discretize_bins, 0 tabanlÄ± olan aralÄ±k numarasÄ±nÄ± dÃ¶ndÃ¼rÃ¼r. Bu nedenle, giriÅŸ deÄŸiÅŸkeninin 0 civarÄ±ndaki deÄŸerleri iÃ§in aralÄ±ÄŸÄ±n ortasÄ±ndaki numarayÄ± (10) dÃ¶ndÃ¼rÃ¼r. Discretize'de, Ã§Ä±ktÄ± deÄŸerlerinin aralÄ±ÄŸÄ±yla ilgilenmedik, onlarÄ±n negatif olmasÄ±na izin verdik, bu nedenle durum deÄŸerleri kaydÄ±rÄ±lmadÄ± ve 0, 0'a karÅŸÄ±lÄ±k gelir. (kod bloÄŸu 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… OrtamÄ±n nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek istiyorsanÄ±z, env.render ile baÅŸlayan satÄ±rÄ± yorumdan Ã§Ä±karabilirsiniz. Aksi takdirde, bunu arka planda Ã§alÄ±ÅŸtÄ±rabilirsiniz, bu daha hÄ±zlÄ±dÄ±r. Q-Ã–ÄŸrenme sÃ¼recimiz sÄ±rasÄ±nda bu "gÃ¶rÃ¼nmez" yÃ¼rÃ¼tmeyi kullanacaÄŸÄ±z.

## Q-Tablosu YapÄ±sÄ±

Ã–nceki dersimizde, durum 0'dan 8'e kadar olan basit bir sayÄ± Ã§iftiydi ve bu nedenle Q-Tablosunu 8x8x2 ÅŸekline sahip bir numpy tensÃ¶rÃ¼ ile temsil etmek uygundu. AralÄ±k ayrÄ±klaÅŸtÄ±rmasÄ±nÄ± kullanÄ±rsak, durum vektÃ¶rÃ¼mÃ¼zÃ¼n boyutu da bilinir, bu nedenle aynÄ± yaklaÅŸÄ±mÄ± kullanabilir ve durumu 20x20x10x10x2 ÅŸeklinde bir dizi ile temsil edebiliriz (burada 2, eylem alanÄ±nÄ±n boyutudur ve ilk boyutlar gÃ¶zlem alanÄ±ndaki her parametre iÃ§in seÃ§tiÄŸimiz aralÄ±k sayÄ±sÄ±na karÅŸÄ±lÄ±k gelir).

Ancak, bazen gÃ¶zlem alanÄ±nÄ±n kesin boyutlarÄ± bilinmez. `discretize` fonksiyonu durumunda, durumumuzun belirli sÄ±nÄ±rlar iÃ§inde kalacaÄŸÄ±ndan asla emin olamayabiliriz, Ã§Ã¼nkÃ¼ bazÄ± orijinal deÄŸerler sÄ±nÄ±rlÄ± deÄŸildir. Bu nedenle, biraz farklÄ± bir yaklaÅŸÄ±m kullanacaÄŸÄ±z ve Q-Tablosunu bir sÃ¶zlÃ¼kle temsil edeceÄŸiz.

1. *(state,action)* Ã§iftini sÃ¶zlÃ¼k anahtarÄ± olarak kullanÄ±n ve deÄŸer Q-Tablosu giriÅŸ deÄŸerine karÅŸÄ±lÄ±k gelir. (kod bloÄŸu 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Burada ayrÄ±ca, bir duruma karÅŸÄ±lÄ±k gelen tÃ¼m olasÄ± eylemler iÃ§in Q-Tablosu deÄŸerlerinin bir listesini dÃ¶ndÃ¼ren `qvalues()` fonksiyonunu tanÄ±mlÄ±yoruz. GiriÅŸ Q-Tablosunda mevcut deÄŸilse, varsayÄ±lan olarak 0 dÃ¶ndÃ¼receÄŸiz.

## Q-Ã–ÄŸrenmeye BaÅŸlayalÄ±m

Åimdi Peter'a dengeyi Ã¶ÄŸretmeye hazÄ±rÄ±z!

1. Ä°lk olarak, bazÄ± hiperparametreler belirleyelim: (kod bloÄŸu 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Burada, `alpha`, Q-Tablosunun mevcut deÄŸerlerini her adÄ±mda ne Ã¶lÃ§Ã¼de ayarlamamÄ±z gerektiÄŸini tanÄ±mlayan **Ã¶ÄŸrenme oranÄ±dÄ±r**. Ã–nceki derste 1 ile baÅŸladÄ±k ve ardÄ±ndan `alpha` deÄŸerlerini eÄŸitim sÄ±rasÄ±nda daha dÃ¼ÅŸÃ¼k deÄŸerlere dÃ¼ÅŸÃ¼rdÃ¼k. Bu Ã¶rnekte basitlik adÄ±na sabit tutacaÄŸÄ±z ve daha sonra `alpha` deÄŸerlerini ayarlamayÄ± deneyebilirsiniz.

    `gamma`, gelecekteki Ã¶dÃ¼lÃ¼ mevcut Ã¶dÃ¼le gÃ¶re ne Ã¶lÃ§Ã¼de Ã¶nceliklendirmemiz gerektiÄŸini gÃ¶steren **indirim faktÃ¶rÃ¼dÃ¼r**.

    `epsilon`, keÅŸif/istismar faktÃ¶rÃ¼dÃ¼r ve keÅŸfi mi yoksa istismarÄ± mÄ± tercih etmemiz gerektiÄŸini belirler. AlgoritmamÄ±zda, `epsilon` yÃ¼zdesinde bir sonraki eylemi Q-Tablosu deÄŸerlerine gÃ¶re seÃ§eceÄŸiz ve kalan durumlarda rastgele bir eylem gerÃ§ekleÅŸtireceÄŸiz. Bu, daha Ã¶nce hiÃ§ gÃ¶rmediÄŸimiz arama alanÄ± bÃ¶lgelerini keÅŸfetmemizi saÄŸlayacaktÄ±r.

    âœ… Denge aÃ§Ä±sÄ±ndan - rastgele bir eylem seÃ§mek (keÅŸif), yanlÄ±ÅŸ yÃ¶nde rastgele bir darbe gibi davranÄ±r ve direk bu "hatalardan" dengeyi nasÄ±l kurtaracaÄŸÄ±nÄ± Ã¶ÄŸrenmek zorunda kalÄ±r.

### AlgoritmayÄ± GeliÅŸtirme

Ã–nceki dersten algoritmamÄ±za iki iyileÅŸtirme yapabiliriz:

- **Ortalama kÃ¼mÃ¼latif Ã¶dÃ¼lÃ¼ hesaplama**, bir dizi simÃ¼lasyon boyunca. Ä°lerlemeyi her 5000 iterasyonda yazdÄ±racaÄŸÄ±z ve kÃ¼mÃ¼latif Ã¶dÃ¼lÃ¼mÃ¼zÃ¼ bu sÃ¼re boyunca ortalama alacaÄŸÄ±z. Bu, 195 puandan fazla alÄ±rsak - problemi Ã§Ã¶zmÃ¼ÅŸ kabul edebiliriz, hatta gereken kaliteden daha yÃ¼ksek bir ÅŸekilde.

- **Maksimum ortalama kÃ¼mÃ¼latif sonucu hesaplama**, `Qmax`, ve bu sonuca karÅŸÄ±lÄ±k gelen Q-Tablosunu saklayacaÄŸÄ±z. EÄŸitimi Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zda, bazen ortalama kÃ¼mÃ¼latif sonucun dÃ¼ÅŸmeye baÅŸladÄ±ÄŸÄ±nÄ± fark edeceksiniz ve bu durumda, Q-Tablosunda durumu daha kÃ¶tÃ¼ hale getiren deÄŸerlerle zaten Ã¶ÄŸrenilmiÅŸ deÄŸerleri "bozabiliriz".

1. Her simÃ¼lasyondaki kÃ¼mÃ¼latif Ã¶dÃ¼lleri `rewards` vektÃ¶rÃ¼nde toplayÄ±n ve daha sonra grafik Ã§izmek iÃ§in kullanÄ±n. (kod bloÄŸu 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Bu sonuÃ§lardan fark edebileceÄŸiniz ÅŸeyler:

- **Hedefimize Ã§ok yakÄ±nÄ±z**. 100+ ardÄ±ÅŸÄ±k simÃ¼lasyon Ã§alÄ±ÅŸtÄ±rmasÄ± boyunca 195 kÃ¼mÃ¼latif Ã¶dÃ¼l alma hedefimize Ã§ok yakÄ±nÄ±z veya aslÄ±nda bunu baÅŸarmÄ±ÅŸ olabiliriz! Daha kÃ¼Ã§Ã¼k sayÄ±lar alsak bile, hala bilmiyoruz, Ã§Ã¼nkÃ¼ 5000 Ã§alÄ±ÅŸtÄ±rma boyunca ortalama alÄ±yoruz ve resmi kriterde yalnÄ±zca 100 Ã§alÄ±ÅŸtÄ±rma gerekiyor.

- **Ã–dÃ¼l dÃ¼ÅŸmeye baÅŸlÄ±yor**. Bazen Ã¶dÃ¼l dÃ¼ÅŸmeye baÅŸlar, bu da Q-Tablosunda zaten Ã¶ÄŸrenilmiÅŸ deÄŸerleri daha kÃ¶tÃ¼ hale getiren deÄŸerlerle "bozabileceÄŸimiz" anlamÄ±na gelir.

Bu gÃ¶zlem, eÄŸitim ilerlemesini grafikle gÃ¶sterdiÄŸimizde daha net bir ÅŸekilde gÃ¶rÃ¼lebilir.

## EÄŸitim Ä°lerlemesini Grafikle GÃ¶sterme

EÄŸitim sÄ±rasÄ±nda, her iterasyondaki kÃ¼mÃ¼latif Ã¶dÃ¼l deÄŸerini `rewards` vektÃ¶rÃ¼ne topladÄ±k. Ä°ÅŸte bunu iterasyon numarasÄ±na karÅŸÄ± grafikle gÃ¶sterdiÄŸimizde nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼:

```python
plt.plot(rewards)
```

![ham ilerleme](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Bu grafikten bir ÅŸey sÃ¶ylemek mÃ¼mkÃ¼n deÄŸil, Ã§Ã¼nkÃ¼ stokastik eÄŸitim sÃ¼recinin doÄŸasÄ± gereÄŸi eÄŸitim oturumlarÄ±nÄ±n uzunluÄŸu bÃ¼yÃ¼k Ã¶lÃ§Ã¼de deÄŸiÅŸiyor. Bu grafiÄŸi daha anlamlÄ± hale getirmek iÃ§in, bir dizi deney boyunca **hareketli ortalama** hesaplayabiliriz, Ã¶rneÄŸin 100. Bu, `np.convolve` kullanÄ±larak kolayca yapÄ±labilir: (kod bloÄŸu 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![eÄŸitim ilerlemesi](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Hiperparametreleri DeÄŸiÅŸtirme

Ã–ÄŸrenmeyi daha istikrarlÄ± hale getirmek iÃ§in, eÄŸitim sÄ±rasÄ±nda bazÄ± hiperparametrelerimizi ayarlamak mantÄ±klÄ± olabilir. Ã–zellikle:

- **Ã–ÄŸrenme oranÄ±** iÃ§in, `alpha`, 1'e yakÄ±n deÄŸerlerle baÅŸlayabilir ve ardÄ±ndan bu parametreyi dÃ¼ÅŸÃ¼rmeye devam edebiliriz. Zamanla, Q-Tablosunda iyi olasÄ±lÄ±k deÄŸerleri elde edeceÄŸiz ve bu nedenle onlarÄ± hafifÃ§e ayarlamalÄ±, tamamen yeni deÄŸerlerle Ã¼zerine yazmamalÄ±yÄ±z.

- **Epsilon'u artÄ±rma**. `epsilon`'u yavaÅŸÃ§a artÄ±rmak isteyebiliriz, bÃ¶ylece daha az keÅŸif yapÄ±p daha fazla istismar yapabiliriz. Muhtemelen daha dÃ¼ÅŸÃ¼k bir `epsilon` deÄŸeriyle baÅŸlamak ve neredeyse 1'e kadar Ã§Ä±karmak mantÄ±klÄ± olacaktÄ±r.
> **GÃ¶rev 1**: Hiperparametre deÄŸerleriyle oynayÄ±n ve daha yÃ¼ksek toplam Ã¶dÃ¼l elde edip edemeyeceÄŸinizi gÃ¶rÃ¼n. 195'in Ã¼zerine Ã§Ä±kabiliyor musunuz?
> **GÃ¶rev 2**: Problemi resmi olarak Ã§Ã¶zmek iÃ§in, 100 ardÄ±ÅŸÄ±k Ã§alÄ±ÅŸmada 195 ortalama Ã¶dÃ¼l almanÄ±z gerekiyor. Bunu eÄŸitim sÄ±rasÄ±nda Ã¶lÃ§Ã¼n ve problemi resmi olarak Ã§Ã¶zdÃ¼ÄŸÃ¼nÃ¼zden emin olun!

## Sonucu eylemde gÃ¶rmek

EÄŸitilmiÅŸ modelin nasÄ±l davrandÄ±ÄŸÄ±nÄ± gÃ¶rmek ilginÃ§ olurdu. SimÃ¼lasyonu Ã§alÄ±ÅŸtÄ±ralÄ±m ve eÄŸitim sÄ±rasÄ±nda olduÄŸu gibi aynÄ± eylem seÃ§me stratejisini izleyelim, Q-Tablosundaki olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na gÃ¶re Ã¶rnekleme yapalÄ±m: (kod bloÄŸu 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Åuna benzer bir ÅŸey gÃ¶rmelisiniz:

![dengeleyen bir cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€Meydan Okuma

> **GÃ¶rev 3**: Burada, Q-Tablosunun son kopyasÄ±nÄ± kullanÄ±yorduk, ancak bu en iyi olanÄ± olmayabilir. UnutmayÄ±n ki en iyi performans gÃ¶steren Q-Tablosunu `Qbest` deÄŸiÅŸkenine kaydettik! `Qbest`'i `Q` Ã¼zerine kopyalayarak en iyi performans gÃ¶steren Q-Tablosuyla aynÄ± Ã¶rneÄŸi deneyin ve farkÄ± fark edip etmediÄŸinizi gÃ¶rÃ¼n.

> **GÃ¶rev 4**: Burada her adÄ±mda en iyi eylemi seÃ§miyorduk, bunun yerine ilgili olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±yla Ã¶rnekleme yapÄ±yorduk. Her zaman en iyi eylemi, en yÃ¼ksek Q-Tablosu deÄŸerine sahip olanÄ± seÃ§mek daha mantÄ±klÄ± olur mu? Bu, en yÃ¼ksek Q-Tablosu deÄŸerine karÅŸÄ±lÄ±k gelen eylem numarasÄ±nÄ± bulmak iÃ§in `np.argmax` fonksiyonu kullanÄ±larak yapÄ±labilir. Bu stratejiyi uygulayÄ±n ve dengelemeyi iyileÅŸtirip iyileÅŸtirmediÄŸini gÃ¶rÃ¼n.

## [Ders sonrasÄ± sÄ±nav](https://ff-quizzes.netlify.app/en/ml/)

## Ã–dev
[Bir Mountain Car EÄŸitin](assignment.md)

## SonuÃ§

ArtÄ±k ajanlarÄ±, oyunun istenen durumunu tanÄ±mlayan bir Ã¶dÃ¼l fonksiyonu saÄŸlayarak ve arama alanÄ±nÄ± akÄ±llÄ±ca keÅŸfetme fÄ±rsatÄ± vererek iyi sonuÃ§lar elde etmeleri iÃ§in nasÄ±l eÄŸiteceÄŸimizi Ã¶ÄŸrendik. Q-Ã–ÄŸrenme algoritmasÄ±nÄ±, ayrÄ±k ve sÃ¼rekli ortamlar durumunda, ancak ayrÄ±k eylemlerle baÅŸarÄ±yla uyguladÄ±k.

Eylem durumunun da sÃ¼rekli olduÄŸu ve gÃ¶zlem alanÄ±nÄ±n Ã§ok daha karmaÅŸÄ±k olduÄŸu, Ã¶rneÄŸin Atari oyun ekranÄ±ndan bir gÃ¶rÃ¼ntÃ¼ gibi durumlarÄ± incelemek de Ã¶nemlidir. Bu tÃ¼r problemler iÃ§in genellikle iyi sonuÃ§lar elde etmek amacÄ±yla daha gÃ¼Ã§lÃ¼ makine Ã¶ÄŸrenimi tekniklerini, Ã¶rneÄŸin sinir aÄŸlarÄ±nÄ± kullanmamÄ±z gerekir. Bu daha ileri dÃ¼zey konular, yaklaÅŸan daha geliÅŸmiÅŸ AI kursumuzun konusudur.

---

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluÄŸu saÄŸlamak iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§eviriler hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebilir. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalardan sorumlu deÄŸiliz.