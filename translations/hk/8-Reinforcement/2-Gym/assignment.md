<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-03T18:44:21+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "hk"
}
-->
# 訓練山地車

[OpenAI Gym](http://gym.openai.com) 的設計使得所有環境都提供相同的 API —— 即相同的方法 `reset`、`step` 和 `render`，以及相同的 **動作空間** 和 **觀察空間** 抽象。因此，應該可以用最少的代碼更改將相同的強化學習算法適配到不同的環境中。

## 山地車環境

[山地車環境](https://gym.openai.com/envs/MountainCar-v0/) 包含一輛被困在山谷中的車：

目標是通過以下其中一個動作，在每一步中讓車輛成功逃出山谷並捕獲旗幟：

| 值   | 意義             |
|------|------------------|
| 0    | 向左加速         |
| 1    | 不加速           |
| 2    | 向右加速         |

然而，這個問題的主要難點在於車輛的引擎不夠強大，無法一次性爬上山頂。因此，唯一成功的方法是來回駕駛以積累動量。

觀察空間僅包含兩個值：

| 編號 | 觀察項目         | 最小值 | 最大值 |
|------|------------------|--------|--------|
|  0   | 車輛位置         | -1.2   | 0.6    |
|  1   | 車輛速度         | -0.07  | 0.07   |

山地車的獎勵系統相當棘手：

 * 如果代理到達山頂的旗幟位置（位置 = 0.5），則獲得 0 的獎勵。
 * 如果代理的位置小於 0.5，則獲得 -1 的獎勵。

當車輛位置超過 0.5 或劇集長度超過 200 時，劇集終止。

## 指引

將我們的強化學習算法適配到山地車問題中。從現有的 [notebook.ipynb](notebook.ipynb) 代碼開始，替換新的環境，修改狀態離散化函數，並嘗試以最少的代碼修改使現有算法進行訓練。通過調整超參數來優化結果。

> **注意**: 可能需要調整超參數以使算法收斂。

## 評分標準

| 標準     | 優秀表現                                                                 | 合格表現                                                                 | 需要改進                                                                 |
|----------|------------------------------------------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|
|          | 成功從 CartPole 示例中適配 Q-Learning 算法，並以最少的代碼修改解決捕獲旗幟的問題，且能在 200 步內完成。 | 從網絡上採用新的 Q-Learning 算法，但有良好的文檔記錄；或採用現有算法但未達到預期結果。 | 未能成功採用任何算法，但在解決方案上有重大進展（例如實現狀態離散化、Q-Table 數據結構等）。 |

---

**免責聲明**：  
本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始語言的文件應被視為權威來源。對於重要資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。