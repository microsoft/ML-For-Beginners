<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-09-03T18:37:36+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "hk"
}
-->
# 一個更真實的世界

在我們的情境中，Peter 幾乎可以不感到疲倦或飢餓地四處移動。在一個更真實的世界中，他需要時不時坐下休息，並且需要進食。讓我們通過實現以下規則，使這個世界更加真實：

1. 每次從一個地方移動到另一個地方，Peter 會失去**能量**並增加一些**疲勞**。
2. Peter 可以通過吃蘋果來獲得更多能量。
3. Peter 可以通過在樹下或草地上休息來消除疲勞（即走到有樹或草的棋盤位置 - 綠色區域）。
4. Peter 需要找到並擊殺狼。
5. 為了擊殺狼，Peter 需要達到一定的能量和疲勞水平，否則他會輸掉戰鬥。

## 指引

使用原始的 [notebook.ipynb](notebook.ipynb) 筆記本作為解決方案的起點。

根據遊戲規則修改上述的獎勵函數，運行強化學習算法以學習贏得遊戲的最佳策略，並比較隨機行走與您的算法在贏得和輸掉遊戲次數上的結果。

> **Note**: 在您的新世界中，狀態更加複雜，除了人類的位置之外，還包括疲勞和能量水平。您可以選擇將狀態表示為一個元組 (Board, energy, fatigue)，或者為狀態定義一個類（您可能還希望從 `Board` 派生），甚至修改原始的 `Board` 類，位於 [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py)。

在您的解決方案中，請保留負責隨機行走策略的代碼，並在最後比較您的算法與隨機行走的結果。

> **Note**: 您可能需要調整超參數以使其正常工作，尤其是訓練的迭代次數。由於遊戲的成功（擊敗狼）是一個罕見事件，您可以預期需要更長的訓練時間。

## 評分標準

| 評分標準 | 優秀                                                                                                                                                                                                 | 合格                                                                                                                                                                                | 需要改進                                                                                                                          |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
|          | 提供了一個筆記本，其中定義了新世界規則、Q-Learning 算法以及一些文字解釋。Q-Learning 能夠顯著改善與隨機行走相比的結果。                                                                 | 提供了筆記本，實現了 Q-Learning 並改善了與隨機行走相比的結果，但改善不顯著；或者筆記本文檔不完整，代碼結構不佳。                                                                 | 嘗試重新定義世界規則，但 Q-Learning 算法無法正常工作，或者獎勵函數未完全定義。                                                                 |

---

**免責聲明**：  
本文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始語言的文件應被視為權威來源。對於重要資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋概不負責。