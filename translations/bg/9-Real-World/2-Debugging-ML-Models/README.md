<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T00:14:20+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "bg"
}
-->
# Постскриптум: Дебъгване на модели в машинното обучение с помощта на компоненти от таблото за отговорен AI

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ml/)

## Въведение

Машинното обучение оказва влияние върху ежедневния ни живот. AI навлиза в някои от най-важните системи, които ни засягат като индивиди и като общество – от здравеопазване, финанси, образование до заетост. Например, системи и модели участват в ежедневни задачи за вземане на решения, като диагностика в здравеопазването или откриване на измами. В резултат на това напредъкът в AI, заедно с ускореното му приемане, се среща с променящи се обществени очаквания и нарастваща регулация. Постоянно виждаме области, в които AI системите не оправдават очакванията, излагат нови предизвикателства, а правителствата започват да регулират AI решенията. Затова е важно тези модели да бъдат анализирани, за да предоставят справедливи, надеждни, приобщаващи, прозрачни и отговорни резултати за всички.

В този курс ще разгледаме практически инструменти, които могат да се използват за оценка дали даден модел има проблеми, свързани с отговорния AI. Традиционните техники за дебъгване на машинното обучение обикновено се основават на количествени изчисления, като агрегирана точност или средна загуба на грешка. Представете си какво може да се случи, ако данните, които използвате за изграждане на тези модели, липсват определени демографски характеристики, като раса, пол, политически възгледи, религия, или непропорционално представят такива демографски характеристики. Ами ако изходът на модела се интерпретира така, че да благоприятства определена демографска група? Това може да доведе до свръх или недостатъчно представяне на тези чувствителни характеристики, което да доведе до проблеми със справедливостта, приобщаването или надеждността на модела. Друг фактор е, че моделите за машинно обучение се считат за "черни кутии", което затруднява разбирането и обяснението на това, което движи прогнозите на модела. Всички тези предизвикателства са проблеми, пред които се изправят специалистите по данни и разработчиците на AI, когато нямат подходящи инструменти за дебъгване и оценка на справедливостта или надеждността на модела.

В този урок ще научите как да дебъгвате вашите модели, използвайки:

- **Анализ на грешки**: идентифициране на области в разпределението на данните, където моделът има високи нива на грешки.
- **Общ преглед на модела**: извършване на сравнителен анализ между различни групи данни, за да се открият несъответствия в метриките за производителност на модела.
- **Анализ на данни**: изследване на области, където може да има свръх или недостатъчно представяне на данни, което може да наклони модела да благоприятства една демографска група пред друга.
- **Важност на характеристиките**: разбиране кои характеристики движат прогнозите на модела на глобално или локално ниво.

## Предварителни знания

Като предварително условие, моля, прегледайте [Инструменти за отговорен AI за разработчици](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif за инструменти за отговорен AI](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Анализ на грешки

Традиционните метрики за производителност на моделите, използвани за измерване на точността, обикновено са изчисления, базирани на правилни срещу неправилни прогнози. Например, определянето, че моделът е точен в 89% от случаите със загуба на грешка от 0.001, може да се счита за добро представяне. Грешките обаче често не са разпределени равномерно в основния набор от данни. Може да получите резултат за точност на модела от 89%, но да откриете, че има различни области в данните, за които моделът се проваля в 42% от случаите. Последиците от тези модели на провал с определени групи данни могат да доведат до проблеми със справедливостта или надеждността. Затова е важно да се разберат областите, в които моделът се представя добре или не. Областите в данните, където има голям брой неточности в модела, могат да се окажат важни демографски данни.

![Анализ и дебъгване на грешки в модела](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

Компонентът за анализ на грешки в таблото за отговорен AI показва как провалите на модела са разпределени в различни групи с визуализация на дърво. Това е полезно за идентифициране на характеристики или области, където има висока степен на грешки в набора от данни. Като видите откъде идват повечето неточности на модела, можете да започнете да изследвате основната причина. Можете също така да създавате групи данни за анализ. Тези групи данни помагат в процеса на дебъгване, за да се определи защо представянето на модела е добро в една група, но грешно в друга.

![Анализ на грешки](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

Визуалните индикатори на картата на дървото помагат за по-бързо локализиране на проблемните области. Например, колкото по-тъмно червен е цветът на възел в дървото, толкова по-висока е степента на грешка.

Топлинната карта е друга функционалност за визуализация, която потребителите могат да използват за изследване на степента на грешка, използвайки една или две характеристики, за да намерят фактор, допринасящ за грешките на модела в целия набор от данни или групи.

![Топлинна карта за анализ на грешки](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Използвайте анализ на грешки, когато трябва:

* Да получите дълбоко разбиране за това как провалите на модела са разпределени в набора от данни и в няколко входни и характеристични измерения.
* Да разбиете агрегирани метрики за производителност, за да откриете автоматично грешни групи и да информирате за целенасочени стъпки за смекчаване.

## Общ преглед на модела

Оценяването на производителността на модел за машинно обучение изисква цялостно разбиране на неговото поведение. Това може да се постигне чрез преглед на повече от една метрика, като степен на грешка, точност, припомняне, прецизност или MAE (средна абсолютна грешка), за да се открият несъответствия между метриките за производителност. Една метрика за производителност може да изглежда добре, но неточности могат да бъдат разкрити в друга метрика. Освен това, сравняването на метриките за несъответствия в целия набор от данни или групи помага да се осветят областите, в които моделът се представя добре или не. Това е особено важно за наблюдение на представянето на модела сред чувствителни срещу нечувствителни характеристики (например раса, пол или възраст на пациента), за да се разкрие потенциална несправедливост в модела. Например, откриването, че моделът е по-неточен в група, която има чувствителни характеристики, може да разкрие потенциална несправедливост.

Компонентът "Общ преглед на модела" в таблото за отговорен AI помага не само за анализиране на метриките за производителност на представянето на данните в група, но и дава на потребителите възможност да сравняват поведението на модела между различни групи.

![Групи от данни - общ преглед на модела в таблото за отговорен AI](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

Функционалността за анализ, базирана на характеристики, позволява на потребителите да стеснят подгрупи от данни в рамките на определена характеристика, за да идентифицират аномалии на по-гранулирано ниво. Например, таблото има вградена интелигентност за автоматично генериране на групи за избрана от потребителя характеристика (например *"time_in_hospital < 3"* или *"time_in_hospital >= 7"*). Това позволява на потребителя да изолира определена характеристика от по-голяма група данни, за да види дали тя е ключов фактор за неточните резултати на модела.

![Групи от характеристики - общ преглед на модела в таблото за отговорен AI](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

Компонентът "Общ преглед на модела" поддържа два класа метрики за несъответствия:

**Несъответствие в производителността на модела**: Тези метрики изчисляват несъответствието (разликата) в стойностите на избраната метрика за производителност между подгрупи от данни. Ето няколко примера:

* Несъответствие в степента на точност
* Несъответствие в степента на грешка
* Несъответствие в прецизността
* Несъответствие в припомнянето
* Несъответствие в средната абсолютна грешка (MAE)

**Несъответствие в степента на селекция**: Тази метрика съдържа разликата в степента на селекция (благоприятна прогноза) между подгрупи. Пример за това е несъответствието в степента на одобрение на заеми. Степента на селекция означава частта от точките от данни във всяка класа, класифицирани като 1 (в бинарна класификация) или разпределението на прогнозните стойности (в регресия).

## Анализ на данни

> "Ако измъчвате данните достатъчно дълго, те ще признаят всичко" - Роналд Коуз

Това изказване звучи крайно, но е вярно, че данните могат да бъдат манипулирани, за да подкрепят всяко заключение. Такава манипулация понякога може да се случи неволно. Като хора, всички имаме предразсъдъци и често е трудно да осъзнаем кога въвеждаме предразсъдъци в данните. Гарантирането на справедливост в AI и машинното обучение остава сложен предизвикателство.

Данните са голямо сляпо петно за традиционните метрики за производителност на моделите. Може да имате високи резултати за точност, но това не винаги отразява основните предразсъдъци в набора от данни. Например, ако набор от данни за служители показва, че 27% от жените заемат изпълнителни позиции в компания, а 73% от мъжете са на същото ниво, AI модел за рекламиране на работни места, обучен на тези данни, може да насочи предимно мъжка аудитория за старши позиции. Този дисбаланс в данните накланя прогнозата на модела да благоприятства един пол. Това разкрива проблем със справедливостта, където има полова предразсъдък в AI модела.

Компонентът "Анализ на данни" в таблото за отговорен AI помага да се идентифицират области, където има свръх или недостатъчно представяне в набора от данни. Той помага на потребителите да диагностицират основната причина за грешки и проблеми със справедливостта, въведени от дисбаланси в данните или липса на представяне на определена група данни. Това дава на потребителите възможност да визуализират набори от данни въз основа на прогнозирани и действителни резултати, групи с грешки и специфични характеристики. Понякога откриването на недостатъчно представена група данни може също така да разкрие, че моделът не се учи добре, което води до високи неточности. Модел с предразсъдъци в данните не само е проблем със справедливостта, но показва, че моделът не е приобщаващ или надежден.

![Компонент за анализ на данни в таблото за отговорен AI](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Използвайте анализ на данни, когато трябва:

* Да изследвате статистиката на вашия набор от данни, като избирате различни филтри, за да разделите данните в различни измерения (известни също като групи).
* Да разберете разпределението на вашия набор от данни между различни групи и характеристики.
* Да определите дали вашите открития, свързани със справедливостта, анализа на грешки и причинността (получени от други компоненти на таблото), са резултат от разпределението на вашия набор от данни.
* Да решите в кои области да съберете повече данни, за да смекчите грешки, произтичащи от проблеми с представянето, шум в етикетите, шум в характеристиките, предразсъдъци в етикетите и подобни фактори.

## Интерпретируемост на модела

Моделите за машинно обучение обикновено са "черни кутии". Разбирането кои ключови характеристики на данните движат прогнозата на модела може да бъде предизвикателство. Важно е да се осигури прозрачност относно причините, поради които моделът прави определена прогноза. Например, ако AI система прогнозира, че диабетен пациент е изложен на риск от повторно приемане в болница в рамките на по-малко от 30 дни, тя трябва да може да предостави подкрепящи данни, които са довели до тази прогноза. Наличието на подкрепящи данни носи прозрачност, която помага на клиницистите или болниците да вземат добре информирани решения. Освен това, възможността да се обясни защо моделът е направил прогноза за индивидуален пациент осигурява отговорност спрямо здравните регулации. Когато използвате модели за машинно обучение по начини, които засягат живота на хората, е от съществено значение да разберете и обясните какво влияе върху поведението на модела. Интерпретируемостта и обяснимостта на модела помагат да се отговори на въпроси в сценарии като:

* Дебъгване на модела: Защо моделът ми направи тази грешка? Как мога да го подобря?
* Сътрудничество между хора и AI: Как мога да разбера и да се доверя на решенията на модела?
* Регулаторно съответствие: Дали моделът ми отговаря на законовите изисквания?

Компонентът "Важност на характеристиките" в таблото за отговорен AI помага да се дебъгва и да се получи цялостно разбиране за това как моделът прави прогнози. Това е полезен инструмент за професионалисти в машинното обучение и вземащи решения, за да обяснят и покажат доказателства за характеристиките, които влияят върху поведението на модела за регулаторно съответствие. Потребителите могат да изследват както глобални, така и локални обяснения, за да валидират кои характеристики движат прогнозата на модела. Глобалните обяснения изброяват основните характеристики, които са повлияли на общата прогноза на модела. Локалните обяснения показват кои характеристики са довели до прогноза на модела за индивидуален случай. Възможността за оценка на локални обяснения също е полезна при дебъгване или одит на конкретен случай, за да се разбере и интерпретира защо моделът е направил точна или неточна прогноза.

![Компонент за важност на характеристиките в таблото за отговорен AI](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Глобални обяснения: Например, кои характеристики влияят върху общото поведение на модела за повторно приемане в болница на диабетни пациенти?
* Локални обяснения: Например, защо диаб
- **Прекомерно или недостатъчно представяне**. Идеята е, че определена група не е представена в дадена професия, и всяка услуга или функция, която продължава да насърчава това, допринася за вреда.

### Табло за отговорен AI в Azure

[Табло за отговорен AI в Azure](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) е изградено върху инструменти с отворен код, разработени от водещи академични институции и организации, включително Microsoft. Те са от съществено значение за специалистите по данни и разработчиците на AI, за да разбират по-добре поведението на моделите, да откриват и смекчават нежелани проблеми в AI моделите.

- Научете как да използвате различните компоненти, като разгледате [документацията за таблото за отговорен AI.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Разгледайте някои [примерни ноутбуци за таблото за отговорен AI](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks), за да дебъгвате по-отговорни AI сценарии в Azure Machine Learning.

---
## 🚀 Предизвикателство

За да предотвратим въвеждането на статистически или данни пристрастия от самото начало, трябва:

- да имаме разнообразие от произходи и перспективи сред хората, работещи върху системите
- да инвестираме в набори от данни, които отразяват разнообразието на нашето общество
- да разработим по-добри методи за откриване и коригиране на пристрастия, когато те се появят

Помислете за реални сценарии, в които несправедливостта е очевидна при изграждането и използването на модели. Какво друго трябва да вземем предвид?

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ml/)
## Преглед и самостоятелно обучение

В този урок научихте някои от практическите инструменти за интегриране на отговорен AI в машинното обучение.

Гледайте този уъркшоп, за да се задълбочите в темите:

- Табло за отговорен AI: Единно решение за оперативизиране на отговорен AI на практика от Бесмира Нуши и Мехрнуш Самеки

[![Табло за отговорен AI: Единно решение за оперативизиране на отговорен AI на практика](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Табло за отговорен AI: Единно решение за оперативизиране на отговорен AI на практика")

> 🎥 Кликнете върху изображението по-горе за видео: Табло за отговорен AI: Единно решение за оперативизиране на отговорен AI на практика от Бесмира Нуши и Мехрнуш Самеки

Прегледайте следните материали, за да научите повече за отговорния AI и как да изграждате по-надеждни модели:

- Инструменти на Microsoft за отговорен AI за дебъгване на ML модели: [Ресурси за инструменти за отговорен AI](https://aka.ms/rai-dashboard)

- Разгледайте комплекта инструменти за отговорен AI: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Център за ресурси за отговорен AI на Microsoft: [Ресурси за отговорен AI – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Изследователска група FATE на Microsoft: [FATE: Справедливост, отчетност, прозрачност и етика в AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Задача

[Разгледайте таблото за отговорен AI](assignment.md)

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.