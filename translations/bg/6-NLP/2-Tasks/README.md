<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5f3cb462e3122e1afe7ab0050ccf2bd3",
  "translation_date": "2025-09-05T01:20:43+00:00",
  "source_file": "6-NLP/2-Tasks/README.md",
  "language_code": "bg"
}
-->
# Често срещани задачи и техники в обработката на естествен език

За повечето задачи, свързани с *обработката на естествен език*, текстът, който трябва да бъде обработен, трябва да бъде разделен, анализиран и резултатите съхранени или сравнени с правила и набори от данни. Тези задачи позволяват на програмиста да извлече _значението_, _намерението_ или само _честотата_ на термините и думите в текста.

## [Тест преди лекцията](https://ff-quizzes.netlify.app/en/ml/)

Нека разгледаме често използваните техники за обработка на текст. В комбинация с машинно обучение, тези техники ви помагат да анализирате големи количества текст ефективно. Преди да приложите ML към тези задачи, обаче, нека разберем проблемите, с които се сблъсква специалистът по NLP.

## Често срещани задачи в NLP

Има различни начини за анализиране на текста, върху който работите. Има задачи, които можете да изпълните, и чрез тях можете да разберете текста и да направите изводи. Обикновено тези задачи се изпълняват в определена последователност.

### Токенизация

Вероятно първото нещо, което повечето NLP алгоритми трябва да направят, е да разделят текста на токени или думи. Макар това да звучи просто, вземането предвид на пунктуацията и различните езикови разделители за думи и изречения може да го направи сложно. Може да се наложи да използвате различни методи за определяне на границите.

![tokenization](../../../../6-NLP/2-Tasks/images/tokenization.png)
> Токенизация на изречение от **Гордост и предразсъдъци**. Инфографика от [Jen Looper](https://twitter.com/jenlooper)

### Вграждания

[Вграждания на думи](https://wikipedia.org/wiki/Word_embedding) са начин за числено преобразуване на текстовите данни. Вгражданията се правят така, че думи с подобно значение или думи, използвани заедно, да се групират.

![word embeddings](../../../../6-NLP/2-Tasks/images/embedding.png)
> "Имам най-голямо уважение към вашите нерви, те са мои стари приятели." - Вграждания на думи за изречение от **Гордост и предразсъдъци**. Инфографика от [Jen Looper](https://twitter.com/jenlooper)

✅ Опитайте [този интересен инструмент](https://projector.tensorflow.org/), за да експериментирате с вграждания на думи. Кликването върху една дума показва групи от подобни думи: 'toy' се групира с 'disney', 'lego', 'playstation' и 'console'.

### Парсинг и маркиране на части на речта

Всяка дума, която е токенизирана, може да бъде маркирана като част на речта - съществително, глагол или прилагателно. Изречението `бързата червена лисица скочи над мързеливото кафяво куче` може да бъде маркирано като лисица = съществително, скочи = глагол.

![parsing](../../../../6-NLP/2-Tasks/images/parse.png)

> Парсинг на изречение от **Гордост и предразсъдъци**. Инфографика от [Jen Looper](https://twitter.com/jenlooper)

Парсингът разпознава кои думи са свързани една с друга в изречението - например `бързата червена лисица скочи` е последователност от прилагателно-съществително-глагол, която е отделна от `мързеливото кафяво куче`.

### Честота на думи и фрази

Полезна процедура при анализиране на голям текстов корпус е изграждането на речник на всяка дума или фраза от интерес и колко често се появява. Фразата `бързата червена лисица скочи над мързеливото кафяво куче` има честота на думата "the" от 2.

Нека разгледаме примерен текст, в който броим честотата на думите. Стихотворението "The Winners" на Ръдиард Киплинг съдържа следния стих:

```output
What the moral? Who rides may read.
When the night is thick and the tracks are blind
A friend at a pinch is a friend, indeed,
But a fool to wait for the laggard behind.
Down to Gehenna or up to the Throne,
He travels the fastest who travels alone.
```

Тъй като честотата на фразите може да бъде чувствителна или нечувствителна към регистъра, фразата `a friend` има честота 2, `the` има честота 6, а `travels` е 2.

### N-грамове

Текстът може да бъде разделен на последователности от думи с определена дължина: една дума (униграм), две думи (биграм), три думи (триграм) или произволен брой думи (n-грам).

Например, `бързата червена лисица скочи над мързеливото кафяво куче` с n-грам стойност 2 произвежда следните n-грамове:

1. бързата червена  
2. червена лисица  
3. лисица скочи  
4. скочи над  
5. над мързеливото  
6. мързеливото кафяво  
7. кафяво куче  

Може да е по-лесно да го визуализирате като плъзгащ прозорец върху изречението. Ето го за n-грамове от 3 думи, n-грамът е удебелен във всяко изречение:

1.   <u>**бързата червена лисица**</u> скочи над мързеливото кафяво куче  
2.   бързата **<u>червена лисица скочи</u>** над мързеливото кафяво куче  
3.   бързата червена **<u>лисица скочи над</u>** мързеливото кафяво куче  
4.   бързата червена лисица **<u>скочи над мързеливото</u>** кафяво куче  
5.   бързата червена лисица скочи **<u>над мързеливото кафяво</u>** куче  
6.   бързата червена лисица скочи над <u>**мързеливото кафяво куче**</u>  

![n-grams sliding window](../../../../6-NLP/2-Tasks/images/n-grams.gif)

> N-грам стойност 3: Инфографика от [Jen Looper](https://twitter.com/jenlooper)

### Извличане на съществителни фрази

В повечето изречения има съществително, което е подлог или допълнение на изречението. В английския език то често може да бъде идентифицирано като предшествано от 'a', 'an' или 'the'. Идентифицирането на подлога или допълнението на изречението чрез 'извличане на съществителната фраза' е често срещана задача в NLP, когато се опитвате да разберете значението на изречението.

✅ В изречението "Не мога да определя часа, мястото, погледа или думите, които положиха основите. Това беше твърде отдавна. Бях в средата, преди да разбера, че съм започнал.", можете ли да идентифицирате съществителните фрази?

В изречението `бързата червена лисица скочи над мързеливото кафяво куче` има 2 съществителни фрази: **бързата червена лисица** и **мързеливото кафяво куче**.

### Анализ на настроението

Изречение или текст може да бъде анализирано за настроение, или колко *положително* или *отрицателно* е то. Настроението се измерва чрез *полярност* и *обективност/субективност*. Полярността се измерва от -1.0 до 1.0 (отрицателно до положително), а обективността от 0.0 до 1.0 (най-обективно до най-субективно).

✅ По-късно ще научите, че има различни начини за определяне на настроението чрез машинно обучение, но един от тях е да имате списък с думи и фрази, които са категоризирани като положителни или отрицателни от човешки експерт, и да приложите този модел към текста, за да изчислите полярност. Можете ли да видите как това би работило в някои случаи и по-малко добре в други?

### Инфлексия

Инфлексията ви позволява да вземете дума и да получите нейния единствено или множествено число.

### Лематизация

*Лема* е коренът или основната дума за набор от думи, например *летя*, *лети*, *летене* имат лема на глагола *летя*.

Съществуват и полезни бази данни за изследователите в NLP, като например:

### WordNet

[WordNet](https://wordnet.princeton.edu/) е база данни от думи, синоними, антоними и много други детайли за всяка дума на много различни езици. Тя е изключително полезна при опити за изграждане на преводи, проверка на правописа или езикови инструменти от всякакъв тип.

## Библиотеки за NLP

За щастие, не е нужно да изграждате всички тези техники сами, тъй като има отлични Python библиотеки, които правят обработката на естествен език много по-достъпна за разработчици, които не са специализирани в NLP или машинно обучение. В следващите уроци ще има повече примери за тях, но тук ще научите някои полезни примери, които ще ви помогнат със следващата задача.

### Упражнение - използване на библиотеката `TextBlob`

Нека използваме библиотека, наречена TextBlob, тъй като тя съдържа полезни API за справяне с тези типове задачи. TextBlob "стои на гигантските рамене на [NLTK](https://nltk.org) и [pattern](https://github.com/clips/pattern), и работи добре с двете." Тя има значително количество ML, вградено в своя API.

> Забележка: Полезен [Ръководство за бърз старт](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) е налично за TextBlob и се препоръчва за опитни Python разработчици.

Когато се опитвате да идентифицирате *съществителни фрази*, TextBlob предлага няколко опции за екстрактори, които да намерят съществителни фрази.

1. Разгледайте `ConllExtractor`.

    ```python
    from textblob import TextBlob
    from textblob.np_extractors import ConllExtractor
    # import and create a Conll extractor to use later 
    extractor = ConllExtractor()
    
    # later when you need a noun phrase extractor:
    user_input = input("> ")
    user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified
    np = user_input_blob.noun_phrases                                    
    ```

    > Какво се случва тук? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) е "Екстрактор на съществителни фрази, който използва chunk парсинг, обучен с корпуса ConLL-2000." ConLL-2000 се отнася до Конференцията за компютърно обучение на естествен език през 2000 г. Всяка година конференцията провежда работилница за справяне с труден NLP проблем, а през 2000 г. това беше chunking на съществителни. Моделът беше обучен върху Wall Street Journal, с "секции 15-18 като тренировъчни данни (211727 токена) и секция 20 като тестови данни (47377 токена)". Можете да разгледате използваните процедури [тук](https://www.clips.uantwerpen.be/conll2000/chunking/) и [резултатите](https://ifarm.nl/erikt/research/np-chunking.html).

### Предизвикателство - подобряване на вашия бот с NLP

В предишния урок създадохте много прост Q&A бот. Сега ще направите Марвин малко по-съпричастен, като анализирате вашия вход за настроение и отпечатате отговор, който съответства на настроението. Също така ще трябва да идентифицирате `noun_phrase` и да зададете въпрос за нея.

Вашите стъпки при изграждането на по-добър разговорен бот:

1. Отпечатайте инструкции, които съветват потребителя как да взаимодейства с бота.
2. Започнете цикъл:
   1. Приемете вход от потребителя.
   2. Ако потребителят поиска изход, излезте.
   3. Обработете входа на потребителя и определете подходящ отговор за настроението.
   4. Ако се открие съществителна фраза в настроението, направете я множествено число и поискайте повече информация за тази тема.
   5. Отпечатайте отговор.
3. Върнете се към стъпка 2.

Ето кодовият фрагмент за определяне на настроение с TextBlob. Забележете, че има само четири *градиента* на отговор за настроение (можете да добавите повече, ако желаете):

```python
if user_input_blob.polarity <= -0.5:
  response = "Oh dear, that sounds bad. "
elif user_input_blob.polarity <= 0:
  response = "Hmm, that's not great. "
elif user_input_blob.polarity <= 0.5:
  response = "Well, that sounds positive. "
elif user_input_blob.polarity <= 1:
  response = "Wow, that sounds great. "
```

Ето примерен изход, който да ви насочи (входът на потребителя е на редовете, започващи с >):

```output
Hello, I am Marvin, the friendly robot.
You can end this conversation at any time by typing 'bye'
After typing each answer, press 'enter'
How are you today?
> I am ok
Well, that sounds positive. Can you tell me more?
> I went for a walk and saw a lovely cat
Well, that sounds positive. Can you tell me more about lovely cats?
> cats are the best. But I also have a cool dog
Wow, that sounds great. Can you tell me more about cool dogs?
> I have an old hounddog but he is sick
Hmm, that's not great. Can you tell me more about old hounddogs?
> bye
It was nice talking to you, goodbye!
```

Едно възможно решение на задачата е [тук](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/2-Tasks/solution/bot.py)

✅ Проверка на знанията

1. Смятате ли, че съпричастните отговори биха "заблудили" някого да мисли, че ботът всъщност го разбира?
2. Прави ли идентифицирането на съществителната фраза бота по-убедителен?
3. Защо извличането на "съществителна фраза" от изречение е полезно?

---

Изпълнете бота от предишната проверка на знанията и го тествайте на приятел. Може ли да ги заблуди? Можете ли да направите бота си по-убедителен?

## 🚀Предизвикателство

Вземете задача от предишната проверка на знанията и опитайте да я изпълните. Тествайте бота на приятел. Може ли да ги заблуди? Можете ли да направите бота си по-убедителен?

## [Тест след лекцията](https://ff-quizzes.netlify.app/en/ml/)

## Преглед и самостоятелно обучение

В следващите няколко урока ще научите повече за анализа на настроението. Изследвайте тази интересна техника в статии като тези на [KDNuggets](https://www.kdnuggets.com/tag/nlp)

## Задание

[Накарайте бота да отговаря](assignment.md)

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.