<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-05T01:18:12+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "bg"
}
-->
# Обучение на Mountain Car

[OpenAI Gym](http://gym.openai.com) е проектиран така, че всички среди предоставят един и същ API - т.е. същите методи `reset`, `step` и `render`, както и същите абстракции за **пространство на действията** и **пространство на наблюденията**. Следователно, би трябвало да е възможно да се адаптират едни и същи алгоритми за обучение чрез подсилване към различни среди с минимални промени в кода.

## Среда Mountain Car

[Средата Mountain Car](https://gym.openai.com/envs/MountainCar-v0/) съдържа кола, която е заседнала в долина:

Целта е да излезете от долината и да достигнете до флага, като извършвате едно от следните действия на всяка стъпка:

| Стойност | Значение |
|---|---|
| 0 | Ускорение наляво |
| 1 | Без ускорение |
| 2 | Ускорение надясно |

Основният трик в този проблем е, че двигателят на колата не е достатъчно мощен, за да изкачи планината с едно преминаване. Затова единственият начин за успех е да се движите напред-назад, за да натрупате инерция.

Пространството на наблюденията се състои само от две стойности:

| № | Наблюдение  | Мин | Макс |
|-----|--------------|-----|-----|
|  0  | Позиция на колата | -1.2| 0.6 |
|  1  | Скорост на колата | -0.07 | 0.07 |

Системата за награди в Mountain Car е доста сложна:

 * Награда от 0 се присъжда, ако агентът достигне флага (позиция = 0.5) на върха на планината.
 * Награда от -1 се присъжда, ако позицията на агента е по-малка от 0.5.

Епизодът приключва, ако позицията на колата е повече от 0.5 или дължината на епизода надвишава 200.
## Инструкции

Адаптирайте нашия алгоритъм за обучение чрез подсилване, за да решите проблема с Mountain Car. Започнете с наличния код в [notebook.ipynb](../../../../8-Reinforcement/2-Gym/notebook.ipynb), заменете средата, променете функциите за дискретизация на състоянията и се опитайте да накарате съществуващия алгоритъм да се обучава с минимални промени в кода. Оптимизирайте резултата, като коригирате хиперпараметрите.

> **Забележка**: Вероятно ще е необходимо коригиране на хиперпараметрите, за да се постигне сближаване на алгоритъма.
## Критерии за оценка

| Критерии | Отлично | Адекватно | Нуждае се от подобрение |
| -------- | --------- | -------- | ----------------- |
|          | Алгоритъмът Q-Learning е успешно адаптиран от примера с CartPole, с минимални промени в кода, и е способен да реши проблема с достигането на флага за по-малко от 200 стъпки. | Нов алгоритъм Q-Learning е взет от интернет, но е добре документиран; или съществуващият алгоритъм е адаптиран, но не достига желаните резултати. | Студентът не е успял успешно да адаптира никакъв алгоритъм, но е направил значителни стъпки към решението (реализирал е дискретизация на състоянията, структура на данни за Q-таблицата и др.) |

---

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за недоразумения или погрешни интерпретации, произтичащи от използването на този превод.