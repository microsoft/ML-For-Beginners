<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T21:39:15+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "no"
}
-->
# Bygge maskinl√¶ringsl√∏sninger med ansvarlig AI

![Oppsummering av ansvarlig AI i maskinl√¶ring i en sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote av [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Introduksjon

I dette kurset vil du begynne √• utforske hvordan maskinl√¶ring p√•virker v√•re daglige liv. Allerede n√• er systemer og modeller involvert i beslutningsprosesser som helsevesendiagnoser, l√•nes√∏knader eller oppdagelse av svindel. Derfor er det viktig at disse modellene fungerer godt og gir p√•litelige resultater. Akkurat som med andre programvareapplikasjoner, kan AI-systemer feile eller gi u√∏nskede utfall. Derfor er det avgj√∏rende √• kunne forst√• og forklare oppf√∏rselen til en AI-modell.

Tenk p√• hva som kan skje hvis dataene du bruker til √• bygge disse modellene mangler visse demografiske grupper, som rase, kj√∏nn, politisk syn eller religion, eller hvis de overrepresenterer visse grupper. Hva skjer hvis modellens resultater tolkes til √• favorisere en bestemt gruppe? Hva er konsekvensene for applikasjonen? Og hva skjer hvis modellen gir et skadelig utfall? Hvem er ansvarlig for oppf√∏rselen til AI-systemet? Dette er noen av sp√∏rsm√•lene vi vil utforske i dette kurset.

I denne leksjonen vil du:

- √òke bevisstheten om viktigheten av rettferdighet i maskinl√¶ring og skader relatert til urettferdighet.
- Bli kjent med praksisen med √• utforske avvik og uvanlige scenarier for √• sikre p√•litelighet og sikkerhet.
- Forst√• behovet for √• styrke alle ved √• designe inkluderende systemer.
- Utforske hvor viktig det er √• beskytte personvern og sikkerhet for data og mennesker.
- Se viktigheten av en "glassboks"-tiln√¶rming for √• forklare oppf√∏rselen til AI-modeller.
- V√¶re oppmerksom p√• hvordan ansvarlighet er essensielt for √• bygge tillit til AI-systemer.

## Forutsetninger

Som en forutsetning, vennligst ta "Ansvarlige AI-prinsipper" l√¶ringsstien og se videoen nedenfor om emnet:

L√¶r mer om ansvarlig AI ved √• f√∏lge denne [l√¶ringsstien](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Microsofts tiln√¶rming til ansvarlig AI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsofts tiln√¶rming til ansvarlig AI")

> üé• Klikk p√• bildet over for en video: Microsofts tiln√¶rming til ansvarlig AI

## Rettferdighet

AI-systemer b√∏r behandle alle rettferdig og unng√• √• p√•virke lignende grupper p√• ulike m√•ter. For eksempel, n√•r AI-systemer gir veiledning om medisinsk behandling, l√•nes√∏knader eller ansettelser, b√∏r de gi samme anbefalinger til alle med lignende symptomer, √∏konomiske forhold eller kvalifikasjoner. Vi mennesker b√¶rer med oss arvede skjevheter som p√•virker v√•re beslutninger og handlinger. Disse skjevhetene kan ogs√• v√¶re til stede i dataene vi bruker til √• trene AI-systemer. Slike skjevheter kan noen ganger oppst√• utilsiktet. Det er ofte vanskelig √• v√¶re bevisst p√• n√•r man introduserer skjevheter i data.

**"Urettferdighet"** omfatter negative konsekvenser, eller "skader", for en gruppe mennesker, som definert av rase, kj√∏nn, alder eller funksjonshemming. De viktigste skadene relatert til rettferdighet kan klassifiseres som:

- **Allokering**, hvis for eksempel et kj√∏nn eller en etnisitet favoriseres over en annen.
- **Kvalitet p√• tjenesten**. Hvis du trener data for ett spesifikt scenario, men virkeligheten er mye mer kompleks, kan det f√∏re til d√•rlig ytelse. For eksempel en s√•pedispenser som ikke klarte √• registrere personer med m√∏rk hud. [Referanse](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Nedvurdering**. √Ö urettferdig kritisere eller merke noe eller noen. For eksempel ble en bildemerkingsteknologi beryktet for √• feilmerke bilder av m√∏rkhudede mennesker som gorillaer.
- **Over- eller underrepresentasjon**. Ideen om at en bestemt gruppe ikke er synlig i et bestemt yrke, og enhver tjeneste eller funksjon som fortsetter √• fremme dette, bidrar til skade.
- **Stereotypisering**. √Ö assosiere en gitt gruppe med forh√•ndsbestemte egenskaper. For eksempel kan et spr√•koversettingssystem mellom engelsk og tyrkisk ha un√∏yaktigheter p√• grunn av ord med stereotypiske kj√∏nnsassosiasjoner.

![oversettelse til tyrkisk](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> oversettelse til tyrkisk

![oversettelse tilbake til engelsk](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> oversettelse tilbake til engelsk

N√•r vi designer og tester AI-systemer, m√• vi s√∏rge for at AI er rettferdig og ikke programmert til √• ta skjeve eller diskriminerende beslutninger, noe som ogs√• er forbudt for mennesker. √Ö garantere rettferdighet i AI og maskinl√¶ring forblir en kompleks sosioteknisk utfordring.

### P√•litelighet og sikkerhet

For √• bygge tillit m√• AI-systemer v√¶re p√•litelige, sikre og konsistente under normale og uventede forhold. Det er viktig √• vite hvordan AI-systemer vil oppf√∏re seg i ulike situasjoner, spesielt n√•r det gjelder avvik. N√•r vi bygger AI-l√∏sninger, m√• vi fokusere mye p√• hvordan vi h√•ndterer et bredt spekter av omstendigheter som AI-l√∏sningene kan m√∏te. For eksempel m√• en selvkj√∏rende bil prioritere menneskers sikkerhet. Derfor m√• AI som driver bilen, ta hensyn til alle mulige scenarier bilen kan m√∏te, som natt, tordenv√¶r eller sn√∏stormer, barn som l√∏per over gaten, kj√¶ledyr, veiarbeid osv. Hvor godt et AI-system kan h√•ndtere et bredt spekter av forhold p√•litelig og sikkert, reflekterer niv√•et av forberedelse dataforskeren eller AI-utvikleren har vurdert under design eller testing av systemet.

> [üé• Klikk her for en video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inkludering

AI-systemer b√∏r designes for √• engasjere og styrke alle. N√•r dataforskere og AI-utviklere designer og implementerer AI-systemer, identifiserer og adresserer de potensielle barrierer i systemet som utilsiktet kan ekskludere mennesker. For eksempel finnes det 1 milliard mennesker med funksjonshemminger over hele verden. Med fremskritt innen AI kan de lettere f√• tilgang til et bredt spekter av informasjon og muligheter i hverdagen. Ved √• adressere barrierene skapes det muligheter for √• innovere og utvikle AI-produkter med bedre opplevelser som gagner alle.

> [üé• Klikk her for en video: inkludering i AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sikkerhet og personvern

AI-systemer b√∏r v√¶re trygge og respektere folks personvern. Folk har mindre tillit til systemer som setter deres personvern, informasjon eller liv i fare. N√•r vi trener maskinl√¶ringsmodeller, er vi avhengige av data for √• oppn√• de beste resultatene. I denne prosessen m√• vi vurdere opprinnelsen til dataene og deres integritet. For eksempel, ble dataene sendt inn av brukere eller var de offentlig tilgjengelige? Videre, mens vi arbeider med dataene, er det avgj√∏rende √• utvikle AI-systemer som kan beskytte konfidensiell informasjon og motst√• angrep. Etter hvert som AI blir mer utbredt, blir det stadig viktigere og mer komplekst √• beskytte personvern og sikre viktig personlig og forretningsmessig informasjon. Personvern og datasikkerhet krever spesielt n√∏ye oppmerksomhet for AI, fordi tilgang til data er essensielt for at AI-systemer skal kunne gi n√∏yaktige og informerte prediksjoner og beslutninger om mennesker.

> [üé• Klikk her for en video: sikkerhet i AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Som bransje har vi gjort betydelige fremskritt innen personvern og sikkerhet, drevet i stor grad av reguleringer som GDPR (General Data Protection Regulation).
- Likevel m√• vi med AI-systemer erkjenne spenningen mellom behovet for mer personlige data for √• gj√∏re systemer mer personlige og effektive ‚Äì og personvern.
- Akkurat som med fremveksten av tilkoblede datamaskiner via internett, ser vi ogs√• en stor √∏kning i antall sikkerhetsproblemer relatert til AI.
- Samtidig har vi sett AI bli brukt til √• forbedre sikkerhet. For eksempel drives de fleste moderne antivirusprogrammer i dag av AI-heuristikk.
- Vi m√• s√∏rge for at v√•re dataforskningsprosesser harmonerer med de nyeste praksisene for personvern og sikkerhet.

### √Öpenhet

AI-systemer b√∏r v√¶re forst√•elige. En viktig del av √•penhet er √• forklare oppf√∏rselen til AI-systemer og deres komponenter. Forbedring av forst√•elsen av AI-systemer krever at interessenter forst√•r hvordan og hvorfor de fungerer, slik at de kan identifisere potensielle ytelsesproblemer, sikkerhets- og personvernhensyn, skjevheter, ekskluderende praksis eller utilsiktede utfall. Vi mener ogs√• at de som bruker AI-systemer, b√∏r v√¶re √¶rlige og √•pne om n√•r, hvorfor og hvordan de velger √• bruke dem, samt begrensningene til systemene de bruker. For eksempel, hvis en bank bruker et AI-system for √• st√∏tte sine utl√•nsbeslutninger, er det viktig √• unders√∏ke resultatene og forst√• hvilke data som p√•virker systemets anbefalinger. Regjeringer begynner √• regulere AI p√• tvers av bransjer, s√• dataforskere og organisasjoner m√• forklare om et AI-system oppfyller regulatoriske krav, spesielt n√•r det oppst√•r et u√∏nsket utfall.

> [üé• Klikk her for en video: √•penhet i AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Fordi AI-systemer er s√• komplekse, er det vanskelig √• forst√• hvordan de fungerer og tolke resultatene.
- Denne mangelen p√• forst√•else p√•virker hvordan disse systemene administreres, operasjonaliseres og dokumenteres.
- Enda viktigere p√•virker denne mangelen p√• forst√•else beslutningene som tas basert p√• resultatene disse systemene produserer.

### Ansvarlighet

De som designer og implementerer AI-systemer, m√• v√¶re ansvarlige for hvordan systemene deres fungerer. Behovet for ansvarlighet er spesielt viktig for sensitive teknologier som ansiktsgjenkjenning. Nylig har det v√¶rt en √∏kende ettersp√∏rsel etter ansiktsgjenkjenningsteknologi, spesielt fra rettsh√•ndhevelsesorganisasjoner som ser potensialet i teknologien for bruk som √• finne savnede barn. Imidlertid kan disse teknologiene potensielt brukes av en regjering til √• sette borgernes grunnleggende friheter i fare, for eksempel ved √• muliggj√∏re kontinuerlig overv√•king av spesifikke individer. Derfor m√• dataforskere og organisasjoner v√¶re ansvarlige for hvordan deres AI-system p√•virker enkeltpersoner eller samfunnet.

[![Ledende AI-forsker advarer om masseoverv√•king gjennom ansiktsgjenkjenning](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsofts tiln√¶rming til ansvarlig AI")

> üé• Klikk p√• bildet over for en video: Advarsler om masseoverv√•king gjennom ansiktsgjenkjenning

Til syvende og sist er et av de st√∏rste sp√∏rsm√•lene for v√•r generasjon, som den f√∏rste generasjonen som bringer AI til samfunnet, hvordan vi kan sikre at datamaskiner forblir ansvarlige overfor mennesker, og hvordan vi kan sikre at de som designer datamaskiner, forblir ansvarlige overfor alle andre.

## Konsekvensvurdering

F√∏r du trener en maskinl√¶ringsmodell, er det viktig √• gjennomf√∏re en konsekvensvurdering for √• forst√• form√•let med AI-systemet; hva den tiltenkte bruken er; hvor det vil bli implementert; og hvem som vil samhandle med systemet. Dette er nyttig for vurderere eller testere som evaluerer systemet for √• vite hvilke faktorer de skal ta hensyn til n√•r de identifiserer potensielle risikoer og forventede konsekvenser.

F√∏lgende er fokusomr√•der n√•r du gjennomf√∏rer en konsekvensvurdering:

* **Negative konsekvenser for enkeltpersoner**. √Ö v√¶re klar over eventuelle begrensninger eller krav, ikke-st√∏ttet bruk eller kjente begrensninger som hindrer systemets ytelse, er avgj√∏rende for √• sikre at systemet ikke brukes p√• en m√•te som kan skade enkeltpersoner.
* **Datakrav**. √Ö forst√• hvordan og hvor systemet vil bruke data, gj√∏r det mulig for vurderere √• utforske eventuelle datakrav du m√• v√¶re oppmerksom p√• (f.eks. GDPR eller HIPAA-reguleringer). I tillegg b√∏r du unders√∏ke om kilden eller mengden av data er tilstrekkelig for trening.
* **Oppsummering av konsekvenser**. Samle en liste over potensielle skader som kan oppst√• ved bruk av systemet. Gjennom hele ML-livssyklusen b√∏r du vurdere om de identifiserte problemene er adressert eller redusert.
* **Gjeldende m√•l** for hver av de seks kjerneprinsippene. Vurder om m√•lene fra hvert prinsipp er oppfylt, og om det er noen mangler.

## Feils√∏king med ansvarlig AI

Akkurat som med feils√∏king av en programvareapplikasjon, er feils√∏king av et AI-system en n√∏dvendig prosess for √• identifisere og l√∏se problemer i systemet. Det er mange faktorer som kan p√•virke at en modell ikke presterer som forventet eller ansvarlig. De fleste tradisjonelle modellytelsesm√•linger er kvantitative aggregater av en modells ytelse, som ikke er tilstrekkelige for √• analysere hvordan en modell bryter med prinsippene for ansvarlig AI. Videre er en maskinl√¶ringsmodell en "svart boks" som gj√∏r det vanskelig √• forst√• hva som driver dens utfall eller forklare hvorfor den gj√∏r feil. Senere i dette kurset vil vi l√¶re hvordan vi bruker Responsible AI-dashbordet for √• hjelpe med feils√∏king av AI-systemer. Dashbordet gir et helhetlig verkt√∏y for dataforskere og AI-utviklere til √• utf√∏re:

* **Feilanalyse**. For √• identifisere feilfordelingen i modellen som kan p√•virke systemets rettferdighet eller p√•litelighet.
* **Modelloversikt**. For √• oppdage hvor det er ulikheter i modellens ytelse p√• tvers av datakohorter.
* **Dataanalyse**. For √• forst√• datadistribusjonen og identifisere potensielle skjevheter i dataene som kan f√∏re til problemer med rettferdighet, inkludering og p√•litelighet.
* **Modellfortolkning**. For √• forst√• hva som p√•virker eller driver modellens prediksjoner. Dette hjelper med √• forklare modellens oppf√∏rsel, som er viktig for √•penhet og ansvarlighet.

## üöÄ Utfordring

For √• forhindre at skader introduseres i utgangspunktet, b√∏r vi:

- ha et mangfold av bakgrunner og perspektiver blant de som jobber med systemer
- investere i datasett som reflekterer mangfoldet i samfunnet v√•rt
- utvikle bedre metoder gjennom hele maskinl√¶ringslivssyklusen for √• oppdage og korrigere ansvarlig AI n√•r det oppst√•r

Tenk p√• virkelige scenarier der en modells up√•litelighet er tydelig i modellbygging og bruk. Hva annet b√∏r vi vurdere?

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Gjennomgang og selvstudium

I denne leksjonen har du l√¶rt noen grunnleggende konsepter om rettferdighet og urettferdighet i maskinl√¶ring.
Se denne workshopen for √• fordype deg i temaene:

- P√• jakt etter ansvarlig AI: Fra prinsipper til praksis av Besmira Nushi, Mehrnoosh Sameki og Amit Sharma

[![Responsible AI Toolbox: En √•pen kildekode-rammeverk for √• bygge ansvarlig AI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: En √•pen kildekode-rammeverk for √• bygge ansvarlig AI")


> üé• Klikk p√• bildet over for en video: RAI Toolbox: En √•pen kildekode-rammeverk for √• bygge ansvarlig AI av Besmira Nushi, Mehrnoosh Sameki og Amit Sharma

Les ogs√•:

- Microsofts RAI ressursenter: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Microsofts FATE forskningsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox:

- [Responsible AI Toolbox GitHub repository](https://github.com/microsoft/responsible-ai-toolbox)

Les om Azure Machine Learning sine verkt√∏y for √• sikre rettferdighet:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## Oppgave

[Utforsk RAI Toolbox](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.