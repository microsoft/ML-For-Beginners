<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "730225ea274c9174fe688b21d421539d",
  "translation_date": "2025-09-05T21:27:24+00:00",
  "source_file": "5-Clustering/1-Visualize/README.md",
  "language_code": "no"
}
-->
# Introduksjon til klynging

Klynging er en type [Usupervisert lÃ¦ring](https://wikipedia.org/wiki/Unsupervised_learning) som forutsetter at et datasett er umerket eller at dets input ikke er koblet til forhÃ¥ndsdefinerte output. Det bruker ulike algoritmer for Ã¥ sortere gjennom umerket data og gi grupperinger basert pÃ¥ mÃ¸nstre det oppdager i dataen.

[![No One Like You av PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM "No One Like You av PSquare")

> ðŸŽ¥ Klikk pÃ¥ bildet over for en video. Mens du studerer maskinlÃ¦ring med klynging, nyt noen nigerianske Dance Hall-lÃ¥ter â€“ dette er en hÃ¸yt rangert sang fra 2014 av PSquare.

## [Quiz fÃ¸r forelesning](https://ff-quizzes.netlify.app/en/ml/)

### Introduksjon

[Klynging](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) er svÃ¦rt nyttig for datautforskning. La oss se om det kan hjelpe med Ã¥ oppdage trender og mÃ¸nstre i hvordan nigerianske publikum konsumerer musikk.

âœ… Ta et Ã¸yeblikk til Ã¥ tenke pÃ¥ bruksomrÃ¥dene for klynging. I hverdagen skjer klynging nÃ¥r du har en haug med klesvask og mÃ¥ sortere familiemedlemmenes klÃ¦r ðŸ§¦ðŸ‘•ðŸ‘–ðŸ©². I dataanalyse skjer klynging nÃ¥r man prÃ¸ver Ã¥ analysere en brukers preferanser eller bestemme egenskapene til et umerket datasett. Klynging hjelper pÃ¥ en mÃ¥te med Ã¥ skape orden i kaos, som en sokkeskuff.

[![Introduksjon til ML](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds "Introduksjon til klynging")

> ðŸŽ¥ Klikk pÃ¥ bildet over for en video: MITs John Guttag introduserer klynging

I en profesjonell setting kan klynging brukes til Ã¥ bestemme ting som markedssegmentering, for eksempel Ã¥ finne ut hvilke aldersgrupper som kjÃ¸per hvilke varer. Et annet bruksomrÃ¥de kan vÃ¦re Ã¥ oppdage avvik, kanskje for Ã¥ oppdage svindel i et datasett med kredittkorttransaksjoner. Eller du kan bruke klynging til Ã¥ identifisere svulster i en samling medisinske skanninger.

âœ… Tenk et Ã¸yeblikk pÃ¥ hvordan du kan ha stÃ¸tt pÃ¥ klynging 'i det virkelige liv', i en bank-, e-handels- eller forretningssetting.

> ðŸŽ“ Interessant nok oppsto klyngeanalyse innenfor feltene antropologi og psykologi pÃ¥ 1930-tallet. Kan du forestille deg hvordan det kan ha blitt brukt?

Alternativt kan du bruke det til Ã¥ gruppere sÃ¸keresultater â€“ for eksempel etter shoppinglenker, bilder eller anmeldelser. Klynging er nyttig nÃ¥r du har et stort datasett som du vil redusere og utfÃ¸re mer detaljert analyse pÃ¥, sÃ¥ teknikken kan brukes til Ã¥ lÃ¦re om data fÃ¸r andre modeller bygges.

âœ… NÃ¥r dataen din er organisert i klynger, tildeler du den en klynge-ID, og denne teknikken kan vÃ¦re nyttig for Ã¥ bevare et datasets personvern; du kan i stedet referere til et datapunkt med klynge-ID-en, i stedet for med mer avslÃ¸rende identifiserbare data. Kan du tenke pÃ¥ andre grunner til hvorfor du ville referere til en klynge-ID i stedet for andre elementer i klyngen for Ã¥ identifisere den?

Fordyp deg i klyngingsteknikker i denne [Learn-modulen](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott)

## Komme i gang med klynging

[Scikit-learn tilbyr et stort utvalg](https://scikit-learn.org/stable/modules/clustering.html) av metoder for Ã¥ utfÃ¸re klynging. Typen du velger vil avhenge av bruksomrÃ¥det ditt. IfÃ¸lge dokumentasjonen har hver metode ulike fordeler. Her er en forenklet tabell over metodene som stÃ¸ttes av Scikit-learn og deres passende bruksomrÃ¥der:

| Metodenavn                  | BruksomrÃ¥de                                                             |
| :--------------------------- | :---------------------------------------------------------------------- |
| K-Means                      | generell bruk, induktiv                                                 |
| Affinity propagation         | mange, ujevne klynger, induktiv                                         |
| Mean-shift                   | mange, ujevne klynger, induktiv                                         |
| Spectral clustering          | fÃ¥, jevne klynger, transduktiv                                          |
| Ward hierarchical clustering | mange, begrensede klynger, transduktiv                                  |
| Agglomerative clustering     | mange, begrensede, ikke-Euklidiske avstander, transduktiv               |
| DBSCAN                       | ikke-flat geometri, ujevne klynger, transduktiv                         |
| OPTICS                       | ikke-flat geometri, ujevne klynger med variabel tetthet, transduktiv    |
| Gaussian mixtures            | flat geometri, induktiv                                                 |
| BIRCH                        | stort datasett med uteliggere, induktiv                                 |

> ðŸŽ“ Hvordan vi lager klynger har mye Ã¥ gjÃ¸re med hvordan vi samler datapunktene i grupper. La oss pakke ut litt vokabular:
>
> ðŸŽ“ ['Transduktiv' vs. 'induktiv'](https://wikipedia.org/wiki/Transduction_(machine_learning))
> 
> Transduktiv inferens er avledet fra observerte treningscaser som kartlegges til spesifikke testcaser. Induktiv inferens er avledet fra treningscaser som kartlegges til generelle regler som deretter brukes pÃ¥ testcaser.
> 
> Et eksempel: Tenk deg at du har et datasett som bare delvis er merket. Noen ting er 'plater', noen 'CD-er', og noen er blanke. Din oppgave er Ã¥ gi etiketter til de blanke. Hvis du velger en induktiv tilnÃ¦rming, vil du trene en modell som ser etter 'plater' og 'CD-er', og bruke disse etikettene pÃ¥ din umerkede data. Denne tilnÃ¦rmingen vil ha problemer med Ã¥ klassifisere ting som faktisk er 'kassetter'. En transduktiv tilnÃ¦rming, derimot, hÃ¥ndterer denne ukjente dataen mer effektivt ved Ã¥ jobbe for Ã¥ gruppere lignende elementer sammen og deretter bruke en etikett pÃ¥ en gruppe. I dette tilfellet kan klynger reflektere 'runde musikalske ting' og 'firkantede musikalske ting'.
> 
> ðŸŽ“ ['Ikke-flat' vs. 'flat' geometri](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)
> 
> Avledet fra matematisk terminologi, refererer ikke-flat vs. flat geometri til mÃ¥lingen av avstander mellom punkter ved enten 'flat' ([Euklidisk](https://wikipedia.org/wiki/Euclidean_geometry)) eller 'ikke-flat' (ikke-Euklidisk) geometriske metoder.
>
>'Flat' i denne sammenhengen refererer til Euklidisk geometri (deler av dette lÃ¦res som 'plan' geometri), og ikke-flat refererer til ikke-Euklidisk geometri. Hva har geometri med maskinlÃ¦ring Ã¥ gjÃ¸re? Vel, som to felt som er forankret i matematikk, mÃ¥ det vÃ¦re en felles mÃ¥te Ã¥ mÃ¥le avstander mellom punkter i klynger, og det kan gjÃ¸res pÃ¥ en 'flat' eller 'ikke-flat' mÃ¥te, avhengig av dataens natur. [Euklidiske avstander](https://wikipedia.org/wiki/Euclidean_distance) mÃ¥les som lengden pÃ¥ en linjesegment mellom to punkter. [Ikke-Euklidiske avstander](https://wikipedia.org/wiki/Non-Euclidean_geometry) mÃ¥les langs en kurve. Hvis dataen din, visualisert, ser ut til Ã¥ ikke eksistere pÃ¥ et plan, kan det hende du mÃ¥ bruke en spesialisert algoritme for Ã¥ hÃ¥ndtere det.
>
![Flat vs Ikke-flat Geometri Infografikk](../../../../5-Clustering/1-Visualize/images/flat-nonflat.png)
> Infografikk av [Dasani Madipalli](https://twitter.com/dasani_decoded)
> 
> ðŸŽ“ ['Avstander'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)
> 
> Klynger defineres av deres avstandsmatrise, f.eks. avstandene mellom punkter. Denne avstanden kan mÃ¥les pÃ¥ flere mÃ¥ter. Euklidiske klynger defineres av gjennomsnittet av punktverdiene, og inneholder et 'sentroid' eller midtpunkt. Avstander mÃ¥les dermed ved avstanden til dette sentroidet. Ikke-Euklidiske avstander refererer til 'clustroids', punktet nÃ¦rmest andre punkter. Clustroids kan pÃ¥ sin side defineres pÃ¥ ulike mÃ¥ter.
> 
> ðŸŽ“ ['Begrenset'](https://wikipedia.org/wiki/Constrained_clustering)
> 
> [Begrenset klynging](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduserer 'semi-supervisert' lÃ¦ring i denne usuperviserte metoden. Forholdene mellom punkter flagges som 'kan ikke kobles' eller 'mÃ¥ kobles', slik at noen regler tvinges pÃ¥ datasettet.
>
>Et eksempel: Hvis en algoritme slippes lÃ¸s pÃ¥ en batch med umerket eller semi-merket data, kan klyngene den produserer vÃ¦re av dÃ¥rlig kvalitet. I eksemplet ovenfor kan klyngene gruppere 'runde musikalske ting' og 'firkantede musikalske ting' og 'trekantede ting' og 'kjeks'. Hvis algoritmen fÃ¥r noen begrensninger, eller regler Ã¥ fÃ¸lge ("elementet mÃ¥ vÃ¦re laget av plast", "elementet mÃ¥ kunne produsere musikk"), kan dette hjelpe med Ã¥ 'begrense' algoritmen til Ã¥ ta bedre valg.
> 
> ðŸŽ“ 'Tetthet'
> 
> Data som er 'stÃ¸yete' anses Ã¥ vÃ¦re 'tett'. Avstandene mellom punkter i hver av klyngene kan vise seg, ved undersÃ¸kelse, Ã¥ vÃ¦re mer eller mindre tette, eller 'trange', og dermed mÃ¥ denne dataen analyseres med den passende klyngemetoden. [Denne artikkelen](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) demonstrerer forskjellen mellom Ã¥ bruke K-Means klynging vs. HDBSCAN-algoritmer for Ã¥ utforske et stÃ¸yete datasett med ujevn klyngetetthet.

## Klyngealgoritmer

Det finnes over 100 klyngealgoritmer, og deres bruk avhenger av naturen til dataen som skal analyseres. La oss diskutere noen av de viktigste:

- **Hierarkisk klynging**. Hvis et objekt klassifiseres basert pÃ¥ dets nÃ¦rhet til et nÃ¦rliggende objekt, snarere enn til et som er lenger unna, dannes klynger basert pÃ¥ avstanden mellom medlemmene. Scikit-learns agglomerative klynging er hierarkisk.

   ![Hierarkisk klynging Infografikk](../../../../5-Clustering/1-Visualize/images/hierarchical.png)
   > Infografikk av [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Sentroid klynging**. Denne populÃ¦re algoritmen krever valg av 'k', eller antall klynger som skal dannes, hvoretter algoritmen bestemmer midtpunktet for en klynge og samler data rundt dette punktet. [K-means klynging](https://wikipedia.org/wiki/K-means_clustering) er en populÃ¦r versjon av sentroid klynging. Midtpunktet bestemmes av nÃ¦rmeste gjennomsnitt, derav navnet. Den kvadrerte avstanden fra klyngen minimeres.

   ![Sentroid klynging Infografikk](../../../../5-Clustering/1-Visualize/images/centroid.png)
   > Infografikk av [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Distribusjonsbasert klynging**. Basert pÃ¥ statistisk modellering, fokuserer distribusjonsbasert klynging pÃ¥ Ã¥ bestemme sannsynligheten for at et datapunkt tilhÃ¸rer en klynge, og tilordner det deretter. Gaussian mixture-metoder tilhÃ¸rer denne typen.

- **Tetthetsbasert klynging**. Datapunkter tilordnes klynger basert pÃ¥ deres tetthet, eller deres gruppering rundt hverandre. Datapunkter langt fra gruppen anses som uteliggere eller stÃ¸y. DBSCAN, Mean-shift og OPTICS tilhÃ¸rer denne typen klynging.

- **Grid-basert klynging**. For multidimensjonale datasett opprettes et rutenett, og dataen deles mellom rutenettets celler, og skaper dermed klynger.

## Ã˜velse â€“ klyng dataen din

Klynging som teknikk stÃ¸ttes sterkt av god visualisering, sÃ¥ la oss komme i gang med Ã¥ visualisere musikkdataen vÃ¥r. Denne Ã¸velsen vil hjelpe oss med Ã¥ avgjÃ¸re hvilken av klyngemetodene vi mest effektivt bÃ¸r bruke for naturen til denne dataen.

1. Ã…pne filen [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) i denne mappen.

1. Importer pakken `Seaborn` for god visualisering av data.

    ```python
    !pip install seaborn
    ```

1. Legg til musikkdataen fra [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Last opp en dataframe med noen data om sangene. GjÃ¸r deg klar til Ã¥ utforske denne dataen ved Ã¥ importere bibliotekene og skrive ut dataen:

    ```python
    import matplotlib.pyplot as plt
    import pandas as pd
    
    df = pd.read_csv("../data/nigerian-songs.csv")
    df.head()
    ```

    Sjekk de fÃ¸rste linjene med data:

    |     | navn                     | album                        | artist              | artist_top_genre | release_date | lengde | popularitet | dansbarhet | akustisitet | energi | instrumentalisme | livlighet | lydstyrke | talbarhet | tempo   | taktart         |
    | --- | ------------------------ | ---------------------------- | ------------------- | ---------------- | ------------ | ------ | ---------- | ---------- | ------------ | ------ | ---------------- | -------- | -------- | --------- | ------- | -------------- |
    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | alternativ r&b   | 2019         | 144000 | 48         | 0.666      | 0.851        | 0.42   | 0.534            | 0.11     | -6.699   | 0.0829    | 133.015 | 5              |
    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop          | 2020         | 89488  | 30         | 0.71       | 0.0822       | 0.683  | 0.000169         | 0.101    | -5.64    | 0.36      | 129.993 | 3              |
| 2   | LITT!                    | LITT!                        | AYLÃ˜                | indie r&b        | 2018         | 207758 | 40         | 0.836        | 0.272        | 0.564  | 0.000537         | 0.11     | -7.127   | 0.0424      | 130.005 | 4              |
| 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop     | 2019         | 175135 | 14         | 0.894        | 0.798        | 0.611  | 0.000187         | 0.0964   | -4.961   | 0.113       | 111.087 | 4              |
| 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop          | 2018         | 152049 | 25         | 0.702        | 0.116        | 0.833  | 0.91             | 0.348    | -6.044   | 0.0447      | 105.115 | 4              |

1. FÃ¥ litt informasjon om dataframe, ved Ã¥ kalle `info()`:

    ```python
    df.info()
    ```

   Utdata ser slik ut:

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 530 entries, 0 to 529
    Data columns (total 16 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   name              530 non-null    object 
     1   album             530 non-null    object 
     2   artist            530 non-null    object 
     3   artist_top_genre  530 non-null    object 
     4   release_date      530 non-null    int64  
     5   length            530 non-null    int64  
     6   popularity        530 non-null    int64  
     7   danceability      530 non-null    float64
     8   acousticness      530 non-null    float64
     9   energy            530 non-null    float64
     10  instrumentalness  530 non-null    float64
     11  liveness          530 non-null    float64
     12  loudness          530 non-null    float64
     13  speechiness       530 non-null    float64
     14  tempo             530 non-null    float64
     15  time_signature    530 non-null    int64  
    dtypes: float64(8), int64(4), object(4)
    memory usage: 66.4+ KB
    ```

1. Dobbeltsjekk for nullverdier, ved Ã¥ kalle `isnull()` og verifisere at summen er 0:

    ```python
    df.isnull().sum()
    ```

    Ser bra ut:

    ```output
    name                0
    album               0
    artist              0
    artist_top_genre    0
    release_date        0
    length              0
    popularity          0
    danceability        0
    acousticness        0
    energy              0
    instrumentalness    0
    liveness            0
    loudness            0
    speechiness         0
    tempo               0
    time_signature      0
    dtype: int64
    ```

1. Beskriv dataen:

    ```python
    df.describe()
    ```

    |       | release_date | length      | popularity | danceability | acousticness | energy   | instrumentalness | liveness | loudness  | speechiness | tempo      | time_signature |
    | ----- | ------------ | ----------- | ---------- | ------------ | ------------ | -------- | ---------------- | -------- | --------- | ----------- | ---------- | -------------- |
    | count | 530          | 530         | 530        | 530          | 530          | 530      | 530              | 530      | 530       | 530         | 530        | 530            |
    | mean  | 2015.390566  | 222298.1698 | 17.507547  | 0.741619     | 0.265412     | 0.760623 | 0.016305         | 0.147308 | -4.953011 | 0.130748    | 116.487864 | 3.986792       |
    | std   | 3.131688     | 39696.82226 | 18.992212  | 0.117522     | 0.208342     | 0.148533 | 0.090321         | 0.123588 | 2.464186  | 0.092939    | 23.518601  | 0.333701       |
    | min   | 1998         | 89488       | 0          | 0.255        | 0.000665     | 0.111    | 0                | 0.0283   | -19.362   | 0.0278      | 61.695     | 3              |
    | 25%   | 2014         | 199305      | 0          | 0.681        | 0.089525     | 0.669    | 0                | 0.07565  | -6.29875  | 0.0591      | 102.96125  | 4              |
    | 50%   | 2016         | 218509      | 13         | 0.761        | 0.2205       | 0.7845   | 0.000004         | 0.1035   | -4.5585   | 0.09795     | 112.7145   | 4              |
    | 75%   | 2017         | 242098.5    | 31         | 0.8295       | 0.403        | 0.87575  | 0.000234         | 0.164    | -3.331    | 0.177       | 125.03925  | 4              |
    | max   | 2020         | 511738      | 73         | 0.966        | 0.954        | 0.995    | 0.91             | 0.811    | 0.582     | 0.514       | 206.007    | 5              |

> ðŸ¤” Hvis vi jobber med clustering, en usupervisert metode som ikke krever merket data, hvorfor viser vi denne dataen med etiketter? I utforskningsfasen av dataen er de nyttige, men de er ikke nÃ¸dvendige for at clustering-algoritmer skal fungere. Du kan like gjerne fjerne kolonneoverskriftene og referere til dataen med kolonnenummer.

Se pÃ¥ de generelle verdiene i dataen. Merk at popularitet kan vÃ¦re '0', som viser sanger som ikke har noen rangering. La oss fjerne disse snart.

1. Bruk et stolpediagram for Ã¥ finne de mest populÃ¦re sjangrene:

    ```python
    import seaborn as sns
    
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top[:5].index,y=top[:5].values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

    ![mest populÃ¦re](../../../../5-Clustering/1-Visualize/images/popular.png)

âœ… Hvis du vil se flere toppverdier, endre toppen `[:5]` til en stÃ¸rre verdi, eller fjern den for Ã¥ se alt.

Merk, nÃ¥r toppsjangeren er beskrevet som 'Missing', betyr det at Spotify ikke klassifiserte den, sÃ¥ la oss fjerne den.

1. Fjern manglende data ved Ã¥ filtrere det ut

    ```python
    df = df[df['artist_top_genre'] != 'Missing']
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top.index,y=top.values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

    NÃ¥ sjekk sjangrene pÃ¥ nytt:

    ![mest populÃ¦re](../../../../5-Clustering/1-Visualize/images/all-genres.png)

1. De tre toppsjangrene dominerer datasetet. La oss konsentrere oss om `afro dancehall`, `afropop` og `nigerian pop`, og i tillegg filtrere datasetet for Ã¥ fjerne alt med en popularitetsverdi pÃ¥ 0 (som betyr at det ikke ble klassifisert med en popularitet i datasetet og kan betraktes som stÃ¸y for vÃ¥re formÃ¥l):

    ```python
    df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
    df = df[(df['popularity'] > 0)]
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top.index,y=top.values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

1. GjÃ¸r en rask test for Ã¥ se om dataen korrelerer pÃ¥ noen spesielt sterk mÃ¥te:

    ```python
    corrmat = df.corr(numeric_only=True)
    f, ax = plt.subplots(figsize=(12, 9))
    sns.heatmap(corrmat, vmax=.8, square=True)
    ```

    ![korrelasjoner](../../../../5-Clustering/1-Visualize/images/correlation.png)

    Den eneste sterke korrelasjonen er mellom `energy` og `loudness`, noe som ikke er sÃ¥ overraskende, gitt at hÃ¸y musikk vanligvis er ganske energisk. Ellers er korrelasjonene relativt svake. Det vil vÃ¦re interessant Ã¥ se hva en clustering-algoritme kan gjÃ¸re med denne dataen.

    > ðŸŽ“ Merk at korrelasjon ikke innebÃ¦rer Ã¥rsakssammenheng! Vi har bevis pÃ¥ korrelasjon, men ingen bevis pÃ¥ Ã¥rsakssammenheng. En [morsom nettside](https://tylervigen.com/spurious-correlations) har noen visuelle eksempler som understreker dette poenget.

Er det noen konvergens i dette datasetet rundt en sangs opplevde popularitet og dansbarhet? En FacetGrid viser at det er konsentriske sirkler som stemmer overens, uavhengig av sjanger. Kan det vÃ¦re at nigerianske smaker konvergerer pÃ¥ et visst nivÃ¥ av dansbarhet for denne sjangeren?  

âœ… PrÃ¸v forskjellige datapunkter (energy, loudness, speechiness) og flere eller forskjellige musikksjangre. Hva kan du oppdage? Ta en titt pÃ¥ `df.describe()`-tabellen for Ã¥ se den generelle spredningen av datapunktene.

### Ã˜velse - datafordeling

Er disse tre sjangrene betydelig forskjellige i oppfatningen av deres dansbarhet, basert pÃ¥ deres popularitet?

1. UndersÃ¸k datafordelingen for vÃ¥re tre toppsjangre for popularitet og dansbarhet langs en gitt x- og y-akse.

    ```python
    sns.set_theme(style="ticks")
    
    g = sns.jointplot(
        data=df,
        x="popularity", y="danceability", hue="artist_top_genre",
        kind="kde",
    )
    ```

    Du kan oppdage konsentriske sirkler rundt et generelt konvergenspunkt, som viser fordelingen av punkter.

    > ðŸŽ“ Merk at dette eksemplet bruker en KDE (Kernel Density Estimate)-graf som representerer dataen ved hjelp av en kontinuerlig sannsynlighetstetthetskurve. Dette lar oss tolke data nÃ¥r vi jobber med flere fordelinger.

    Generelt sett er de tre sjangrene lÃ¸st tilpasset nÃ¥r det gjelder deres popularitet og dansbarhet. Ã… bestemme klynger i denne lÃ¸st tilpassede dataen vil vÃ¦re en utfordring:

    ![fordeling](../../../../5-Clustering/1-Visualize/images/distribution.png)

1. Lag et spredningsdiagram:

    ```python
    sns.FacetGrid(df, hue="artist_top_genre", height=5) \
       .map(plt.scatter, "popularity", "danceability") \
       .add_legend()
    ```

    Et spredningsdiagram av de samme aksene viser et lignende mÃ¸nster av konvergens

    ![Facetgrid](../../../../5-Clustering/1-Visualize/images/facetgrid.png)

Generelt, for clustering, kan du bruke spredningsdiagrammer for Ã¥ vise klynger av data, sÃ¥ det Ã¥ mestre denne typen visualisering er veldig nyttig. I neste leksjon vil vi ta denne filtrerte dataen og bruke k-means clustering for Ã¥ oppdage grupper i denne dataen som ser ut til Ã¥ overlappe pÃ¥ interessante mÃ¥ter.

---

## ðŸš€Utfordring

Som forberedelse til neste leksjon, lag et diagram over de forskjellige clustering-algoritmene du kan oppdage og bruke i et produksjonsmiljÃ¸. Hvilke typer problemer prÃ¸ver clustering Ã¥ adressere?

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Gjennomgang & Selvstudium

FÃ¸r du bruker clustering-algoritmer, som vi har lÃ¦rt, er det en god idÃ© Ã¥ forstÃ¥ naturen til datasetet ditt. Les mer om dette emnet [her](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)

[Denne nyttige artikkelen](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) gir deg en oversikt over hvordan forskjellige clustering-algoritmer oppfÃ¸rer seg, gitt forskjellige dataformer.

## Oppgave

[UndersÃ¸k andre visualiseringer for clustering](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber nÃ¸yaktighet, vennligst vÃ¦r oppmerksom pÃ¥ at automatiske oversettelser kan inneholde feil eller unÃ¸yaktigheter. Det originale dokumentet pÃ¥ sitt opprinnelige sprÃ¥k bÃ¸r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforstÃ¥elser eller feiltolkninger som oppstÃ¥r ved bruk av denne oversettelsen.