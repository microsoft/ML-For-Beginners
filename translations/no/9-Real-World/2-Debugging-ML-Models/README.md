<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T21:36:12+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "no"
}
-->
# Postscript: Modellfeils√∏king i maskinl√¶ring ved bruk av komponenter fra Responsible AI-dashboardet

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Introduksjon

Maskinl√¶ring p√•virker v√•re daglige liv. AI finner veien inn i noen av de viktigste systemene som ber√∏rer oss som individer og samfunnet v√•rt, fra helsevesen, finans, utdanning og arbeidsliv. For eksempel er systemer og modeller involvert i daglige beslutningsoppgaver, som helsediagnoser eller deteksjon av svindel. Som en konsekvens blir fremskrittene innen AI og den akselererte adopsjonen m√∏tt med stadig utviklende samfunnsforventninger og √∏kende regulering. Vi ser stadig omr√•der der AI-systemer ikke lever opp til forventningene; de avdekker nye utfordringer; og myndigheter begynner √• regulere AI-l√∏sninger. Derfor er det viktig at disse modellene analyseres for √• sikre rettferdige, p√•litelige, inkluderende, transparente og ansvarlige resultater for alle.

I dette kurset skal vi se p√• praktiske verkt√∏y som kan brukes til √• vurdere om en modell har problemer knyttet til ansvarlig AI. Tradisjonelle feils√∏kingsmetoder for maskinl√¶ring er ofte basert p√• kvantitative beregninger som aggregert n√∏yaktighet eller gjennomsnittlig feilrate. Tenk deg hva som kan skje n√•r dataene du bruker til √• bygge disse modellene mangler visse demografiske grupper, som rase, kj√∏nn, politisk syn, religion, eller er uforholdsmessig representert. Hva med n√•r modellens output tolkes til √• favorisere en demografisk gruppe? Dette kan f√∏re til over- eller underrepresentasjon av sensitive egenskaper, noe som resulterer i rettferdighets-, inkluderings- eller p√•litelighetsproblemer fra modellen. En annen faktor er at maskinl√¶ringsmodeller ofte anses som "black boxes", noe som gj√∏r det vanskelig √• forst√• og forklare hva som driver modellens prediksjoner. Alle disse er utfordringer dataforskere og AI-utviklere st√•r overfor n√•r de ikke har tilstrekkelige verkt√∏y for √• feils√∏ke og vurdere rettferdigheten eller p√•liteligheten til en modell.

I denne leksjonen vil du l√¶re om feils√∏king av modeller ved bruk av:

- **Feilanalyse**: Identifisere hvor i datadistribusjonen modellen har h√∏ye feilrater.
- **Modelloversikt**: Utf√∏re sammenlignende analyser p√• tvers av ulike datakohorter for √• avdekke forskjeller i modellens ytelsesmetrikker.
- **Dataanalyse**: Unders√∏ke hvor det kan v√¶re over- eller underrepresentasjon i dataene som kan skjevfordele modellen til √• favorisere √©n demografisk gruppe fremfor en annen.
- **Funksjonsviktighet**: Forst√• hvilke funksjoner som driver modellens prediksjoner p√• globalt eller lokalt niv√•.

## Forutsetninger

Som en forutsetning, vennligst gjennomg√• [Responsible AI tools for developers](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif om Responsible AI Tools](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Feilanalyse

Tradisjonelle ytelsesmetrikker for modeller som brukes til √• m√•le n√∏yaktighet, er ofte beregninger basert p√• korrekte vs feilaktige prediksjoner. For eksempel kan det √• fastsl√• at en modell er n√∏yaktig 89 % av tiden med en feilrate p√• 0,001 anses som god ytelse. Feilene er imidlertid ofte ikke jevnt fordelt i det underliggende datasettet. Du kan f√• en modelln√∏yaktighet p√• 89 %, men oppdage at det er ulike omr√•der i dataene der modellen feiler 42 % av tiden. Konsekvensen av disse feilene i visse datagrupper kan f√∏re til rettferdighets- eller p√•litelighetsproblemer. Det er avgj√∏rende √• forst√• omr√•dene der modellen presterer godt eller ikke. Dataomr√•dene med h√∏y feilrate kan vise seg √• v√¶re viktige demografiske grupper.

![Analyser og feils√∏k modellfeil](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

Feilanalysemodulen p√• RAI-dashboardet illustrerer hvordan modellfeil er fordelt p√• ulike kohorter med en trevisualisering. Dette er nyttig for √• identifisere funksjoner eller omr√•der der det er h√∏y feilrate i datasettet ditt. Ved √• se hvor de fleste av modellens feil oppst√•r, kan du begynne √• unders√∏ke √•rsaken. Du kan ogs√• opprette datakohorter for √• utf√∏re analyser. Disse datakohortene hjelper i feils√∏kingsprosessen med √• avgj√∏re hvorfor modellens ytelse er god i √©n kohort, men feilaktig i en annen.

![Feilanalyse](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

De visuelle indikatorene p√• trevisualiseringen hjelper deg med √• lokalisere problemomr√•dene raskere. For eksempel, jo m√∏rkere r√∏dfargen p√• en tre-node, desto h√∏yere er feilraten.

Varmekart er en annen visualiseringsfunksjonalitet som brukere kan bruke til √• unders√∏ke feilraten ved hjelp av √©n eller to funksjoner for √• finne bidragsytere til modellfeil p√• tvers av hele datasettet eller kohorter.

![Feilanalyse Varmekart](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Bruk feilanalyse n√•r du trenger √•:

* F√• en dyp forst√•else av hvordan modellfeil er fordelt p√• et datasett og p√• flere input- og funksjonsdimensjoner.
* Bryte ned de samlede ytelsesmetrikker for automatisk √• oppdage feilaktige kohorter og informere om m√•lrettede tiltak for √• redusere feil.

## Modelloversikt

Evaluering av ytelsen til en maskinl√¶ringsmodell krever en helhetlig forst√•else av dens oppf√∏rsel. Dette kan oppn√•s ved √• gjennomg√• mer enn √©n metrikk, som feilrate, n√∏yaktighet, tilbakekalling, presisjon eller MAE (Mean Absolute Error), for √• finne forskjeller blant ytelsesmetrikker. √ân ytelsesmetrikk kan se bra ut, men un√∏yaktigheter kan avdekkes i en annen metrikk. I tillegg hjelper sammenligning av metrikker for forskjeller p√• tvers av hele datasettet eller kohorter med √• belyse hvor modellen presterer godt eller ikke. Dette er spesielt viktig for √• se modellens ytelse blant sensitive vs insensitive funksjoner (f.eks. pasientens rase, kj√∏nn eller alder) for √• avdekke potensiell urettferdighet modellen kan ha. For eksempel kan det √• oppdage at modellen er mer feilaktig i en kohort med sensitive funksjoner avsl√∏re potensiell urettferdighet.

Modelloversiktsmodulen p√• RAI-dashboardet hjelper ikke bare med √• analysere ytelsesmetrikker for datarepresentasjon i en kohort, men gir brukere muligheten til √• sammenligne modellens oppf√∏rsel p√• tvers av ulike kohorter.

![Datasettkohorter - modelloversikt i RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

Modulens funksjonsbaserte analysefunksjonalitet lar brukere snevre inn datasubgrupper innenfor en bestemt funksjon for √• identifisere avvik p√• et detaljert niv√•. For eksempel har dashboardet innebygd intelligens for automatisk √• generere kohorter for en bruker-valgt funksjon (f.eks. *"time_in_hospital < 3"* eller *"time_in_hospital >= 7"*). Dette gj√∏r det mulig for en bruker √• isolere en bestemt funksjon fra en st√∏rre datagruppe for √• se om den er en n√∏kkelp√•virker for modellens feilaktige resultater.

![Funksjonskohorter - modelloversikt i RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

Modelloversiktsmodulen st√∏tter to klasser av forskjellsmetrikker:

**Forskjeller i modellens ytelse**: Disse metrikker beregner forskjellen i verdiene til den valgte ytelsesmetrikk p√• tvers av undergrupper av data. Her er noen eksempler:

* Forskjell i n√∏yaktighetsrate
* Forskjell i feilrate
* Forskjell i presisjon
* Forskjell i tilbakekalling
* Forskjell i gjennomsnittlig absolutt feil (MAE)

**Forskjeller i utvalgsrate**: Denne metrikk inneholder forskjellen i utvalgsrate (gunstig prediksjon) blant undergrupper. Et eksempel p√• dette er forskjellen i l√•negodkjenningsrater. Utvalgsrate betyr andelen datapunkter i hver klasse klassifisert som 1 (i bin√¶r klassifisering) eller distribusjonen av prediksjonsverdier (i regresjon).

## Dataanalyse

> "Hvis du torturerer dataene lenge nok, vil de tilst√• hva som helst" - Ronald Coase

Denne uttalelsen h√∏res ekstrem ut, men det er sant at data kan manipuleres for √• st√∏tte enhver konklusjon. Slik manipulasjon kan noen ganger skje utilsiktet. Som mennesker har vi alle bias, og det er ofte vanskelig √• bevisst vite n√•r vi introduserer bias i dataene. √Ö garantere rettferdighet i AI og maskinl√¶ring forblir en kompleks utfordring.

Data er et stort blindpunkt for tradisjonelle modellytelsesmetrikker. Du kan ha h√∏ye n√∏yaktighetsscorer, men dette reflekterer ikke alltid den underliggende databiasen som kan v√¶re i datasettet ditt. For eksempel, hvis et datasett med ansatte har 27 % kvinner i lederstillinger i et selskap og 73 % menn p√• samme niv√•, kan en AI-modell for jobbannonsering som er trent p√• disse dataene, m√•lrette seg mot en overvekt av mannlige kandidater for seniorstillinger. Denne ubalansen i dataene skjevfordelte modellens prediksjon til √• favorisere ett kj√∏nn. Dette avsl√∏rer et rettferdighetsproblem der det er kj√∏nnsbias i AI-modellen.

Dataanalysemodulen p√• RAI-dashboardet hjelper med √• identifisere omr√•der der det er over- og underrepresentasjon i datasettet. Den hjelper brukere med √• diagnostisere √•rsaken til feil og rettferdighetsproblemer som introduseres fra dataubalanser eller mangel p√• representasjon av en bestemt datagruppe. Dette gir brukere muligheten til √• visualisere datasett basert p√• predikerte og faktiske resultater, feilgrupper og spesifikke funksjoner. Noen ganger kan det √• oppdage en underrepresentert datagruppe ogs√• avdekke at modellen ikke l√¶rer godt, og dermed har h√∏ye un√∏yaktigheter. En modell med databias er ikke bare et rettferdighetsproblem, men viser ogs√• at modellen ikke er inkluderende eller p√•litelig.

![Dataanalysemodulen p√• RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Bruk dataanalyse n√•r du trenger √•:

* Utforske statistikken i datasettet ditt ved √• velge ulike filtre for √• dele opp dataene i ulike dimensjoner (ogs√• kjent som kohorter).
* Forst√• distribusjonen av datasettet ditt p√• tvers av ulike kohorter og funksjonsgrupper.
* Bestemme om funnene dine relatert til rettferdighet, feilanalyse og √•rsakssammenhenger (avledet fra andre dashboardmoduler) er et resultat av distribusjonen i datasettet ditt.
* Avgj√∏re hvilke omr√•der du b√∏r samle inn mer data for √• redusere feil som kommer fra representasjonsproblemer, st√∏y i etiketter, st√∏y i funksjoner, etikettbias og lignende faktorer.

## Modellfortolkning

Maskinl√¶ringsmodeller har en tendens til √• v√¶re "black boxes". √Ö forst√• hvilke n√∏kkelfunksjoner i dataene som driver en modells prediksjon kan v√¶re utfordrende. Det er viktig √• gi transparens om hvorfor en modell gj√∏r en bestemt prediksjon. For eksempel, hvis et AI-system forutsier at en diabetiker er i fare for √• bli innlagt p√• sykehus igjen innen 30 dager, b√∏r det kunne gi st√∏ttende data som ledet til denne prediksjonen. √Ö ha st√∏ttende dataindikatorer gir transparens som hjelper klinikere eller sykehus med √• ta velinformerte beslutninger. I tillegg gj√∏r det √• kunne forklare hvorfor en modell gjorde en prediksjon for en individuell pasient det mulig √• oppfylle ansvarlighet med helsereguleringer. N√•r du bruker maskinl√¶ringsmodeller p√• m√•ter som p√•virker menneskers liv, er det avgj√∏rende √• forst√• og forklare hva som p√•virker modellens oppf√∏rsel. Modellforklarbarhet og fortolkning hjelper med √• svare p√• sp√∏rsm√•l i scenarier som:

* Modellfeils√∏king: Hvorfor gjorde modellen min denne feilen? Hvordan kan jeg forbedre modellen min?
* Menneske-AI-samarbeid: Hvordan kan jeg forst√• og stole p√• modellens beslutninger?
* Regulatorisk samsvar: Oppfyller modellen min juridiske krav?

Funksjonsviktighetsmodulen p√• RAI-dashboardet hjelper deg med √• feils√∏ke og f√• en omfattende forst√•else av hvordan en modell gj√∏r prediksjoner. Det er ogs√• et nyttig verkt√∏y for maskinl√¶ringsprofesjonelle og beslutningstakere for √• forklare og vise bevis p√• funksjoner som p√•virker modellens oppf√∏rsel for regulatorisk samsvar. Videre kan brukere utforske b√•de globale og lokale forklaringer for √• validere hvilke funksjoner som driver modellens prediksjon. Globale forklaringer viser de viktigste funksjonene som p√•virket modellens samlede prediksjon. Lokale forklaringer viser hvilke funksjoner som ledet til modellens prediksjon for en individuell sak. Muligheten til √• evaluere lokale forklaringer er ogs√• nyttig i feils√∏king eller revisjon av en spesifikk sak for bedre √• forst√• og tolke hvorfor en modell gjorde en korrekt eller feilaktig prediksjon.

![Funksjonsviktighetsmodulen p√• RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Globale forklaringer: For eksempel, hvilke funksjoner p√•virker den generelle oppf√∏rselen til en diabetesmodell for sykehusinnleggelse?
* Lokale forklaringer: For eksempel, hvorfor ble en diabetiker over 60 √•r med tidligere sykehusinnleggelser forutsagt √• bli innlagt eller ikke innlagt igjen innen 30 dager?

I feils√∏kingsprosessen med √• unders√∏ke modellens ytelse p√• tvers av ulike kohorter, viser Funksjonsviktighet hvilken grad av p√•virkning en funksjon har p√• tvers av kohorter. Det hjelper med √• avdekke avvik n√•r man sammenligner niv√•et av innflytelse funksjonen har p√• √• drive modellens feilaktige prediksjoner. Funksjonsviktighetsmodulen kan vise hvilke verdier i en funksjon som positivt eller negativt p√•virket modellens resultat. For eksempel, hvis en modell gjorde en feilaktig prediksjon, gir modulen deg muligheten til √• bore ned og identifisere hvilke funksjoner eller funksjonsverdier som drev prediksjonen. Dette detaljniv√•et hjelper ikke bare med feils√∏king, men gir transparens og ansvarlighet i revisjonssituasjoner. Til slutt kan modulen hjelpe deg med √• identifisere rettferdighetsproblemer. For √• illustrere, hvis en sensitiv funksjon som etnisitet eller kj√∏nn har stor innflytelse p√• modellens prediksjon, kan dette v√¶re et tegn p√• rase- eller kj√∏nnsbias i modellen.

![Funksjonsviktighet](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Bruk fortolkning n√•r du trenger √•:

* Bestemme hvor p√•litelige prediksjonene til AI-systemet ditt er ved √• forst√• hvilke funksjoner som er viktigst for prediksjonene.
* Tiln√¶rme deg feils√∏kingen av modellen din ved f√∏rst √• forst√• den og identifisere om modellen bruker sunne funksjoner eller bare falske korrelasjoner.
* Avdekke potensielle kilder til urettferdighet ved √• forst√• om modellen baserer prediksjoner p√• sensitive funksjoner eller p√• funksjoner som er sterkt korrelert med dem.
* Bygge brukertillit til modellens beslutninger ved √• generere lokale forklaringer for √• illustrere resultatene.
* Fullf√∏re en regulatorisk revisjon av et AI-system for √• validere modeller og overv√•ke effekten av modellens beslutninger p√• mennesker.

## Konklusjon

Alle komponentene i RAI-dashboardet er praktiske verkt√∏y som hjelper deg med √• bygge maskinl√¶ringsmodeller som er mindre skadelige og mer p√•litelige for samfunnet. Det forbedrer forebyggingen av trusler mot menneskerettigheter; diskriminering eller ekskludering av visse grupper fra livsmuligheter; og risikoen for fysisk eller psykisk skade. Det hjelper ogs√• med √• bygge tillit til modellens beslutninger ved √• generere lokale forklaringer for √• illustrere resultatene. Noen av de potensielle skadene kan klassifiseres som:

- **Allokering**, hvis for eksempel ett kj√∏nn eller en etnisitet favoriseres over en annen.
- **Tjenestekvalitet**. Hvis du trener dataene for ett spesifikt scenario, men virkeligheten er mye mer kompleks, f√∏rer det til en d√•rlig fungerende tjeneste.
- **Stereotypisering**. √Ö assosiere en gitt gruppe med forh√•ndsbestemte attributter.
- **Nedvurdering**. √Ö urettferdig kritisere og merke noe eller noen.
- **Over- eller underrepresentasjon**. Tanken er at en bestemt gruppe ikke er synlig i en viss yrke, og enhver tjeneste eller funksjon som fortsetter √• fremme dette bidrar til skade.

### Azure RAI-dashboard

[Azure RAI-dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) er bygget p√• √•pen kildekodeverkt√∏y utviklet av ledende akademiske institusjoner og organisasjoner, inkludert Microsoft, som er avgj√∏rende for datasientister og AI-utviklere for bedre √• forst√• modellatferd, oppdage og redusere u√∏nskede problemer fra AI-modeller.

- L√¶r hvordan du bruker de forskjellige komponentene ved √• sjekke ut RAI-dashboard [dokumentasjonen.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Sjekk ut noen RAI-dashboard [eksempelskript](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) for √• feils√∏ke mer ansvarlige AI-scenarier i Azure Machine Learning.

---
## üöÄ Utfordring

For √• forhindre at statistiske eller datamessige skjevheter introduseres i utgangspunktet, b√∏r vi:

- ha en mangfoldig bakgrunn og ulike perspektiver blant de som jobber med systemene
- investere i datasett som reflekterer mangfoldet i samfunnet v√•rt
- utvikle bedre metoder for √• oppdage og korrigere skjevheter n√•r de oppst√•r

Tenk p√• virkelige situasjoner der urettferdighet er tydelig i modellbygging og bruk. Hva annet b√∏r vi ta hensyn til?

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)
## Gjennomgang og selvstudium

I denne leksjonen har du l√¶rt noen av de praktiske verkt√∏yene for √• integrere ansvarlig AI i maskinl√¶ring.

Se denne workshoppen for √• dykke dypere inn i temaene:

- Responsible AI Dashboard: En helhetlig l√∏sning for √• operationalisere RAI i praksis av Besmira Nushi og Mehrnoosh Sameki

[![Responsible AI Dashboard: En helhetlig l√∏sning for √• operationalisere RAI i praksis](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: En helhetlig l√∏sning for √• operationalisere RAI i praksis")

> üé• Klikk p√• bildet over for en video: Responsible AI Dashboard: En helhetlig l√∏sning for √• operationalisere RAI i praksis av Besmira Nushi og Mehrnoosh Sameki

Referer til f√∏lgende materialer for √• l√¶re mer om ansvarlig AI og hvordan man bygger mer p√•litelige modeller:

- Microsofts RAI-dashboardverkt√∏y for feils√∏king av ML-modeller: [Ressurser for ansvarlige AI-verkt√∏y](https://aka.ms/rai-dashboard)

- Utforsk Responsible AI-verkt√∏ysettet: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsofts RAI-ressurssenter: [Ressurser for ansvarlig AI ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE-forskningsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Oppgave

[Utforsk RAI-dashboard](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.