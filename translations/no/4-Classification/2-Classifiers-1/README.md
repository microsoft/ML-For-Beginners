<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T21:50:40+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "no"
}
-->
# Klassifisering av matretter 1

I denne leksjonen skal du bruke datasettet du lagret fra forrige leksjon, som inneholder balanserte og rene data om matretter.

Du vil bruke dette datasettet med ulike klassifiseringsmetoder for √• _forutsi en nasjonal matrett basert p√• en gruppe ingredienser_. Mens du gj√∏r dette, vil du l√¶re mer om hvordan algoritmer kan brukes til klassifiseringsoppgaver.

## [Quiz f√∏r leksjonen](https://ff-quizzes.netlify.app/en/ml/)
# Forberedelse

Forutsatt at du fullf√∏rte [Leksjon 1](../1-Introduction/README.md), s√∏rg for at en _cleaned_cuisines.csv_-fil finnes i rotmappen `/data` for disse fire leksjonene.

## √òvelse - forutsi en nasjonal matrett

1. Arbeid i denne leksjonens _notebook.ipynb_-mappe, og importer filen sammen med Pandas-biblioteket:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Dataen ser slik ut:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Importer n√• flere biblioteker:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Del X- og y-koordinatene inn i to dataframes for trening. `cuisine` kan v√¶re etikett-datasettet:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Det vil se slik ut:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. Fjern kolonnen `Unnamed: 0` og kolonnen `cuisine` ved √• bruke `drop()`. Lagre resten av dataene som trenbare funksjoner:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    Funksjonene dine ser slik ut:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

N√• er du klar til √• trene modellen din!

## Velge klassifiseringsmetode

N√• som dataene dine er rene og klare for trening, m√• du bestemme hvilken algoritme du skal bruke til oppgaven.

Scikit-learn grupperer klassifisering under Supervised Learning, og i den kategorien finner du mange m√•ter √• klassifisere p√•. [Utvalget](https://scikit-learn.org/stable/supervised_learning.html) kan virke overveldende ved f√∏rste √∏yekast. F√∏lgende metoder inkluderer klassifiseringsteknikker:

- Line√¶re modeller
- Support Vector Machines
- Stokastisk gradientnedstigning
- N√¶rmeste naboer
- Gaussiske prosesser
- Beslutningstr√¶r
- Ensemble-metoder (voting Classifier)
- Multiklasse- og multioutput-algoritmer (multiklasse- og multilabel-klassifisering, multiklasse-multioutput-klassifisering)

> Du kan ogs√• bruke [nevrale nettverk til √• klassifisere data](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), men det er utenfor omfanget av denne leksjonen.

### Hvilken klassifiseringsmetode skal du velge?

S√•, hvilken klassifiseringsmetode b√∏r du velge? Ofte kan det v√¶re nyttig √• teste flere metoder og se etter gode resultater. Scikit-learn tilbyr en [side-ved-side-sammenligning](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) p√• et opprettet datasett, som sammenligner KNeighbors, SVC p√• to m√•ter, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB og QuadraticDiscriminationAnalysis, og viser resultatene visuelt:

![sammenligning av klassifiseringsmetoder](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Diagrammer generert fra Scikit-learns dokumentasjon

> AutoML l√∏ser dette problemet elegant ved √• kj√∏re disse sammenligningene i skyen, slik at du kan velge den beste algoritmen for dataene dine. Pr√∏v det [her](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### En bedre tiln√¶rming

En bedre tiln√¶rming enn √• gjette vilk√•rlig er √• f√∏lge ideene i dette nedlastbare [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). Her oppdager vi at vi har noen valg for v√•rt multiklasse-problem:

![jukselapp for multiklasse-problemer](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> En del av Microsofts Algorithm Cheat Sheet, som beskriver alternativer for multiklasse-klassifisering

‚úÖ Last ned denne jukselappen, skriv den ut, og heng den p√• veggen!

### Resonnement

La oss se om vi kan resonnere oss frem til ulike tiln√¶rminger gitt de begrensningene vi har:

- **Nevrale nettverk er for tunge**. Gitt v√•rt rene, men minimale datasett, og det faktum at vi kj√∏rer trening lokalt via notebooks, er nevrale nettverk for tunge for denne oppgaven.
- **Ingen to-klasse klassifiserer**. Vi bruker ikke en to-klasse klassifiserer, s√• det utelukker one-vs-all.
- **Beslutningstre eller logistisk regresjon kan fungere**. Et beslutningstre kan fungere, eller logistisk regresjon for multiklasse-data.
- **Multiklasse Boosted Decision Trees l√∏ser et annet problem**. Multiklasse Boosted Decision Trees er mest egnet for ikke-parametriske oppgaver, f.eks. oppgaver designet for √• bygge rangeringer, s√• det er ikke nyttig for oss.

### Bruke Scikit-learn 

Vi skal bruke Scikit-learn til √• analysere dataene v√•re. Det finnes imidlertid mange m√•ter √• bruke logistisk regresjon i Scikit-learn. Ta en titt p√• [parametrene du kan sende](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

I hovedsak er det to viktige parametere - `multi_class` og `solver` - som vi m√• spesifisere n√•r vi ber Scikit-learn om √• utf√∏re en logistisk regresjon. `multi_class`-verdien gir en viss oppf√∏rsel. Verdien av solver angir hvilken algoritme som skal brukes. Ikke alle solvers kan kombineres med alle `multi_class`-verdier.

If√∏lge dokumentasjonen, i multiklasse-tilfellet, treningsalgoritmen:

- **Bruker one-vs-rest (OvR)-skjemaet**, hvis `multi_class`-alternativet er satt til `ovr`
- **Bruker kryssentropitap**, hvis `multi_class`-alternativet er satt til `multinomial`. (For √∏yeblikket st√∏ttes `multinomial`-alternativet kun av solverne ‚Äòlbfgs‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô og ‚Äònewton-cg‚Äô.)

> üéì 'Skjemaet' her kan enten v√¶re 'ovr' (one-vs-rest) eller 'multinomial'. Siden logistisk regresjon egentlig er designet for √• st√∏tte bin√¶r klassifisering, lar disse skjemaene den h√•ndtere multiklasse-klassifiseringsoppgaver bedre. [kilde](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> üéì 'Solver' er definert som "algoritmen som skal brukes i optimaliseringsproblemet". [kilde](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn tilbyr denne tabellen for √• forklare hvordan solvers h√•ndterer ulike utfordringer presentert av forskjellige typer datastrukturer:

![solvers](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## √òvelse - del dataene

Vi kan fokusere p√• logistisk regresjon for v√•r f√∏rste treningsrunde siden du nylig l√¶rte om dette i en tidligere leksjon.
Del dataene dine inn i trenings- og testgrupper ved √• bruke `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## √òvelse - bruk logistisk regresjon

Siden du bruker multiklasse-tilfellet, m√• du velge hvilket _skjema_ du skal bruke og hvilken _solver_ du skal sette. Bruk LogisticRegression med en multiklasse-innstilling og **liblinear** solver for √• trene.

1. Opprett en logistisk regresjon med multi_class satt til `ovr` og solver satt til `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    ‚úÖ Pr√∏v en annen solver som `lbfgs`, som ofte er satt som standard
> Merk, bruk Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html)-funksjonen for √• flate ut dataene dine n√•r det er n√∏dvendig.
N√∏yaktigheten er god p√• over **80%**!

1. Du kan se denne modellen i aksjon ved √• teste √©n rad med data (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Resultatet skrives ut:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   ‚úÖ Pr√∏v et annet radnummer og sjekk resultatene

1. For √• g√• dypere, kan du sjekke n√∏yaktigheten til denne prediksjonen:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Resultatet skrives ut - indisk mat er modellens beste gjetning, med h√∏y sannsynlighet:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    ‚úÖ Kan du forklare hvorfor modellen er ganske sikker p√• at dette er indisk mat?

1. F√• mer detaljert informasjon ved √• skrive ut en klassifikasjonsrapport, slik du gjorde i regresjonsleksjonene:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | presisjon | recall | f1-score | st√∏tte |
    | ------------ | --------- | ------ | -------- | ------ |
    | chinese      | 0.73      | 0.71   | 0.72     | 229    |
    | indian       | 0.91      | 0.93   | 0.92     | 254    |
    | japanese     | 0.70      | 0.75   | 0.72     | 220    |
    | korean       | 0.86      | 0.76   | 0.81     | 242    |
    | thai         | 0.79      | 0.85   | 0.82     | 254    |
    | n√∏yaktighet  | 0.80      | 1199   |          |        |
    | makro snitt  | 0.80      | 0.80   | 0.80     | 1199   |
    | vektet snitt | 0.80      | 0.80   | 0.80     | 1199   |

## üöÄUtfordring

I denne leksjonen brukte du de rensede dataene dine til √• bygge en maskinl√¶ringsmodell som kan forutsi en nasjonal matrett basert p√• en rekke ingredienser. Ta deg tid til √• lese gjennom de mange alternativene Scikit-learn tilbyr for √• klassifisere data. G√• dypere inn i konseptet 'solver' for √• forst√• hva som skjer i bakgrunnen.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Gjennomgang & Selvstudium

Grav litt dypere i matematikken bak logistisk regresjon i [denne leksjonen](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## Oppgave 

[Studer solverne](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.