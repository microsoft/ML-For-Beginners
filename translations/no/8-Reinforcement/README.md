<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T22:01:42+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "no"
}
-->
# Introduksjon til forsterkende l칝ring

Forsterkende l칝ring, RL, regnes som en av de grunnleggende paradigmer innen maskinl칝ring, ved siden av veiledet l칝ring og uveiledet l칝ring. RL handler om beslutninger: 친 ta riktige beslutninger eller i det minste l칝re av dem.

Tenk deg at du har et simulert milj칮, som aksjemarkedet. Hva skjer hvis du innf칮rer en gitt regulering? Har det en positiv eller negativ effekt? Hvis noe negativt skjer, m친 du ta denne _negative forsterkningen_, l칝re av den og endre kurs. Hvis det er et positivt utfall, m친 du bygge videre p친 den _positive forsterkningen_.

![Peter og ulven](../../../8-Reinforcement/images/peter.png)

> Peter og vennene hans m친 unnslippe den sultne ulven! Bilde av [Jen Looper](https://twitter.com/jenlooper)

## Regionalt tema: Peter og ulven (Russland)

[Peter og ulven](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) er et musikalsk eventyr skrevet av den russiske komponisten [Sergej Prokofjev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Det er en historie om den unge pioneren Peter, som modig g친r ut av huset sitt til skogkanten for 친 jage ulven. I denne delen skal vi trene maskinl칝ringsalgoritmer som vil hjelpe Peter:

- **Utforske** omr친det rundt og bygge et optimalt navigasjonskart.
- **L칝re** 친 bruke et skateboard og balansere p친 det, for 친 bevege seg raskere rundt.

[![Peter og ulven](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 游꿘 Klikk p친 bildet over for 친 h칮re Peter og ulven av Prokofjev

## Forsterkende l칝ring

I tidligere deler har du sett to eksempler p친 maskinl칝ringsproblemer:

- **Veiledet**, der vi har datasett som foresl친r eksempler p친 l칮sninger til problemet vi 칮nsker 친 l칮se. [Klassifisering](../4-Classification/README.md) og [regresjon](../2-Regression/README.md) er oppgaver innen veiledet l칝ring.
- **Uveiledet**, der vi ikke har merkede treningsdata. Hovedeksempelet p친 uveiledet l칝ring er [Clustering](../5-Clustering/README.md).

I denne delen vil vi introdusere deg for en ny type l칝ringsproblem som ikke krever merkede treningsdata. Det finnes flere typer slike problemer:

- **[Semi-veiledet l칝ring](https://wikipedia.org/wiki/Semi-supervised_learning)**, der vi har mye umerkede data som kan brukes til 친 forh친ndstrene modellen.
- **[Forsterkende l칝ring](https://wikipedia.org/wiki/Reinforcement_learning)**, der en agent l칝rer hvordan den skal oppf칮re seg ved 친 utf칮re eksperimenter i et simulert milj칮.

### Eksempel - dataspill

Anta at du vil l칝re en datamaskin 친 spille et spill, som sjakk eller [Super Mario](https://wikipedia.org/wiki/Super_Mario). For at datamaskinen skal spille et spill, m친 den forutsi hvilken handling den skal ta i hver spilltilstand. Selv om dette kan virke som et klassifiseringsproblem, er det ikke det - fordi vi ikke har et datasett med tilstander og tilsvarende handlinger. Selv om vi kanskje har noen data, som eksisterende sjakkpartier eller opptak av spillere som spiller Super Mario, er det sannsynlig at disse dataene ikke dekker et stort nok antall mulige tilstander.

I stedet for 친 lete etter eksisterende spilldata, er **Forsterkende l칝ring** (RL) basert p친 ideen om *친 f친 datamaskinen til 친 spille* mange ganger og observere resultatet. For 친 bruke forsterkende l칝ring trenger vi to ting:

- **Et milj칮** og **en simulator** som lar oss spille et spill mange ganger. Denne simulatoren vil definere alle spillregler samt mulige tilstander og handlinger.

- **En bel칮nningsfunksjon**, som forteller oss hvor godt vi gjorde det under hver handling eller spill.

Den st칮rste forskjellen mellom andre typer maskinl칝ring og RL er at i RL vet vi vanligvis ikke om vi vinner eller taper f칮r vi er ferdige med spillet. Dermed kan vi ikke si om en bestemt handling alene er god eller ikke - vi mottar bare en bel칮nning ved slutten av spillet. M친let v친rt er 친 designe algoritmer som lar oss trene en modell under usikre forhold. Vi skal l칝re om en RL-algoritme kalt **Q-l칝ring**.

## Leksjoner

1. [Introduksjon til forsterkende l칝ring og Q-l칝ring](1-QLearning/README.md)
2. [Bruke et gymsimuleringsmilj칮](2-Gym/README.md)

## Kreditering

"Introduksjon til forsterkende l칝ring" ble skrevet med 鮫봺잺 av [Dmitry Soshnikov](http://soshnikov.com)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.