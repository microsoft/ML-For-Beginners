<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T22:09:29+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "no"
}
-->
## Forutsetninger

I denne leksjonen skal vi bruke et bibliotek kalt **OpenAI Gym** for √• simulere ulike **milj√∏er**. Du kan kj√∏re koden fra denne leksjonen lokalt (f.eks. fra Visual Studio Code), i s√• fall vil simuleringen √•pne seg i et nytt vindu. N√•r du kj√∏rer koden online, kan det v√¶re n√∏dvendig √• gj√∏re noen justeringer i koden, som beskrevet [her](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

I den forrige leksjonen ble reglene for spillet og tilstanden definert av klassen `Board`, som vi laget selv. Her skal vi bruke et spesielt **simuleringsmilj√∏** som simulerer fysikken bak den balanserende stangen. Et av de mest popul√¶re simuleringsmilj√∏ene for trening av forsterkningsl√¶ringsalgoritmer kalles [Gym](https://gym.openai.com/), som vedlikeholdes av [OpenAI](https://openai.com/). Ved √• bruke dette gymmet kan vi lage ulike **milj√∏er**, fra en CartPole-simulering til Atari-spill.

> **Merk**: Du kan se andre tilgjengelige milj√∏er fra OpenAI Gym [her](https://gym.openai.com/envs/#classic_control).

F√∏rst, la oss installere gym og importere n√∏dvendige biblioteker (kodeblokk 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Oppgave - initialiser et CartPole-milj√∏

For √• jobbe med et CartPole-balanseringsproblem m√• vi initialisere det tilsvarende milj√∏et. Hvert milj√∏ er knyttet til:

- **Observasjonsrom** som definerer strukturen til informasjonen vi mottar fra milj√∏et. For CartPole-problemet mottar vi posisjonen til stangen, hastighet og noen andre verdier.

- **Handlingsrom** som definerer mulige handlinger. I v√•rt tilfelle er handlingsrommet diskret og best√•r av to handlinger - **venstre** og **h√∏yre**. (kodeblokk 2)

1. For √• initialisere, skriv f√∏lgende kode:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

For √• se hvordan milj√∏et fungerer, la oss kj√∏re en kort simulering i 100 steg. Ved hvert steg gir vi en av handlingene som skal utf√∏res - i denne simuleringen velger vi bare tilfeldig en handling fra `action_space`.

1. Kj√∏r koden nedenfor og se hva det f√∏rer til.

    ‚úÖ Husk at det er foretrukket √• kj√∏re denne koden p√• en lokal Python-installasjon! (kodeblokk 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Du b√∏r se noe som ligner p√• dette bildet:

    ![ikke-balanserende CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Under simuleringen m√• vi f√• observasjoner for √• avgj√∏re hvordan vi skal handle. Faktisk returnerer step-funksjonen n√•v√¶rende observasjoner, en bel√∏nningsfunksjon og en flagg som indikerer om det gir mening √• fortsette simuleringen eller ikke: (kodeblokk 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Du vil ende opp med √• se noe som dette i notebook-utgangen:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Observasjonsvektoren som returneres ved hvert steg av simuleringen inneholder f√∏lgende verdier:
    - Posisjon til vognen
    - Hastighet til vognen
    - Vinkel til stangen
    - Rotasjonshastighet til stangen

1. Finn minimums- og maksimumsverdien av disse tallene: (kodeblokk 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Du vil ogs√• legge merke til at bel√∏nningsverdien ved hvert simuleringssteg alltid er 1. Dette er fordi m√•let v√•rt er √• overleve s√• lenge som mulig, dvs. holde stangen i en rimelig vertikal posisjon i lengst mulig tid.

    ‚úÖ Faktisk anses CartPole-simuleringen som l√∏st hvis vi klarer √• oppn√• en gjennomsnittlig bel√∏nning p√• 195 over 100 p√•f√∏lgende fors√∏k.

## Diskretisering av tilstand

I Q-L√¶ring m√• vi bygge en Q-Tabell som definerer hva vi skal gj√∏re i hver tilstand. For √• kunne gj√∏re dette m√• tilstanden v√¶re **diskret**, mer presist, den b√∏r inneholde et begrenset antall diskrete verdier. Dermed m√• vi p√• en eller annen m√•te **diskretisere** observasjonene v√•re, og kartlegge dem til et begrenset sett med tilstander.

Det finnes noen m√•ter vi kan gj√∏re dette p√•:

- **Del inn i intervaller**. Hvis vi kjenner intervallet til en viss verdi, kan vi dele dette intervallet inn i et antall **intervaller**, og deretter erstatte verdien med nummeret p√• intervallet den tilh√∏rer. Dette kan gj√∏res ved hjelp av numpy-metoden [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). I dette tilfellet vil vi n√∏yaktig vite st√∏rrelsen p√• tilstanden, fordi den vil avhenge av antall intervaller vi velger for digitalisering.

‚úÖ Vi kan bruke line√¶r interpolasjon for √• bringe verdier til et begrenset intervall (for eksempel fra -20 til 20), og deretter konvertere tallene til heltall ved √• runde dem av. Dette gir oss litt mindre kontroll over st√∏rrelsen p√• tilstanden, spesielt hvis vi ikke kjenner de n√∏yaktige grensene for inngangsverdiene. For eksempel, i v√•rt tilfelle har 2 av 4 verdier ikke √∏vre/nedre grenser for verdiene sine, noe som kan resultere i et uendelig antall tilstander.

I v√•rt eksempel vil vi g√• for den andre tiln√¶rmingen. Som du kanskje legger merke til senere, til tross for udefinerte √∏vre/nedre grenser, tar disse verdiene sjelden verdier utenfor visse begrensede intervaller, og dermed vil tilstander med ekstreme verdier v√¶re sv√¶rt sjeldne.

1. Her er funksjonen som vil ta observasjonen fra modellen v√•r og produsere en tuple med 4 heltallsverdier: (kodeblokk 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. La oss ogs√• utforske en annen diskretiseringsmetode ved bruk av intervaller: (kodeblokk 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. La oss n√• kj√∏re en kort simulering og observere disse diskrete milj√∏verdiene. Pr√∏v gjerne b√•de `discretize` og `discretize_bins` og se om det er noen forskjell.

    ‚úÖ `discretize_bins` returnerer nummeret p√• intervallet, som er 0-basert. Dermed returnerer den for verdier av inngangsvariabelen rundt 0 nummeret fra midten av intervallet (10). I `discretize` brydde vi oss ikke om omr√•det for utgangsverdiene, og tillot dem √• v√¶re negative, slik at tilstandsverdiene ikke er forskj√∏vet, og 0 tilsvarer 0. (kodeblokk 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Fjern kommentaren p√• linjen som starter med `env.render` hvis du vil se hvordan milj√∏et utf√∏res. Ellers kan du utf√∏re det i bakgrunnen, som er raskere. Vi vil bruke denne "usynlige" utf√∏relsen under v√•r Q-L√¶ringsprosess.

## Strukturen til Q-Tabellen

I v√•r forrige leksjon var tilstanden et enkelt par med tall fra 0 til 8, og dermed var det praktisk √• representere Q-Tabellen med en numpy-tensor med formen 8x8x2. Hvis vi bruker intervall-diskretisering, er st√∏rrelsen p√• tilstandsvektoren v√•r ogs√• kjent, s√• vi kan bruke samme tiln√¶rming og representere tilstanden med en array med formen 20x20x10x10x2 (her er 2 dimensjonen til handlingsrommet, og de f√∏rste dimensjonene tilsvarer antall intervaller vi har valgt √• bruke for hver av parameterne i observasjonsrommet).

Imidlertid er det noen ganger ikke kjent de n√∏yaktige dimensjonene til observasjonsrommet. I tilfelle av funksjonen `discretize`, kan vi aldri v√¶re sikre p√• at tilstanden v√•r holder seg innenfor visse grenser, fordi noen av de opprinnelige verdiene ikke er begrenset. Dermed vil vi bruke en litt annen tiln√¶rming og representere Q-Tabellen med en ordbok.

1. Bruk paret *(state,action)* som n√∏kkelen i ordboken, og verdien vil tilsvare verdien i Q-Tabellen. (kodeblokk 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Her definerer vi ogs√• en funksjon `qvalues()`, som returnerer en liste med verdier fra Q-Tabellen for en gitt tilstand som tilsvarer alle mulige handlinger. Hvis oppf√∏ringen ikke er til stede i Q-Tabellen, vil vi returnere 0 som standard.

## La oss starte Q-L√¶ring

N√• er vi klare til √• l√¶re Peter √• balansere!

1. F√∏rst, la oss sette noen hyperparametere: (kodeblokk 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Her er `alpha` **l√¶ringsraten** som definerer i hvilken grad vi skal justere de n√•v√¶rende verdiene i Q-Tabellen ved hvert steg. I den forrige leksjonen startet vi med 1, og deretter reduserte vi `alpha` til lavere verdier under treningen. I dette eksemplet vil vi holde den konstant bare for enkelhets skyld, og du kan eksperimentere med √• justere `alpha`-verdiene senere.

    `gamma` er **diskonteringsfaktoren** som viser i hvilken grad vi skal prioritere fremtidig bel√∏nning over n√•v√¶rende bel√∏nning.

    `epsilon` er **utforsknings-/utnyttelsesfaktoren** som avgj√∏r om vi skal foretrekke utforskning fremfor utnyttelse eller omvendt. I algoritmen v√•r vil vi i `epsilon` prosent av tilfellene velge neste handling i henhold til verdiene i Q-Tabellen, og i de resterende tilfellene vil vi utf√∏re en tilfeldig handling. Dette vil tillate oss √• utforske omr√•der av s√∏keomr√•det som vi aldri har sett f√∏r.

    ‚úÖ N√•r det gjelder balansering - √• velge en tilfeldig handling (utforskning) vil fungere som et tilfeldig dytt i feil retning, og stangen m√• l√¶re seg √• gjenopprette balansen fra disse "feilene".

### Forbedre algoritmen

Vi kan ogs√• gj√∏re to forbedringer i algoritmen v√•r fra forrige leksjon:

- **Beregn gjennomsnittlig kumulativ bel√∏nning** over et antall simuleringer. Vi vil skrive ut fremgangen hver 5000 iterasjoner, og vi vil beregne gjennomsnittet av v√•r kumulative bel√∏nning over den perioden. Det betyr at hvis vi f√•r mer enn 195 poeng - kan vi anse problemet som l√∏st, med enda h√∏yere kvalitet enn n√∏dvendig.

- **Beregn maksimal gjennomsnittlig kumulativ bel√∏nning**, `Qmax`, og vi vil lagre Q-Tabellen som tilsvarer det resultatet. N√•r du kj√∏rer treningen, vil du legge merke til at noen ganger begynner den gjennomsnittlige kumulative bel√∏nningen √• synke, og vi √∏nsker √• beholde verdiene i Q-Tabellen som tilsvarer den beste modellen observert under treningen.

1. Samle alle kumulative bel√∏nninger ved hver simulering i `rewards`-vektoren for videre plotting. (kodeblokk 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Hva du kan legge merke til fra disse resultatene:

- **N√¶r m√•let v√•rt**. Vi er veldig n√¶r √• oppn√• m√•let om √• f√• 195 kumulative bel√∏nninger over 100+ p√•f√∏lgende simuleringer, eller vi kan faktisk ha oppn√•dd det! Selv om vi f√•r mindre tall, vet vi fortsatt ikke, fordi vi beregner gjennomsnittet over 5000 kj√∏ringer, og bare 100 kj√∏ringer er n√∏dvendig i de formelle kriteriene.

- **Bel√∏nningen begynner √• synke**. Noen ganger begynner bel√∏nningen √• synke, noe som betyr at vi kan "√∏delegge" allerede l√¶rte verdier i Q-Tabellen med de som gj√∏r situasjonen verre.

Denne observasjonen er mer tydelig synlig hvis vi plotter treningsfremgangen.

## Plotting av treningsfremgang

Under treningen har vi samlet den kumulative bel√∏nningsverdien ved hver iterasjon i `rewards`-vektoren. Slik ser det ut n√•r vi plotter det mot iterasjonsnummeret:

```python
plt.plot(rewards)
```

![r√• fremgang](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Fra denne grafen er det ikke mulig √• si noe, fordi p√• grunn av den stokastiske treningsprosessen varierer lengden p√• trenings√∏ktene sterkt. For √• gi mer mening til denne grafen kan vi beregne **glidende gjennomsnitt** over en serie eksperimenter, la oss si 100. Dette kan gj√∏res praktisk ved hjelp av `np.convolve`: (kodeblokk 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![treningsfremgang](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variasjon av hyperparametere

For √• gj√∏re l√¶ringen mer stabil gir det mening √• justere noen av hyperparameterne v√•re under treningen. Spesielt:

- **For l√¶ringsraten**, `alpha`, kan vi starte med verdier n√¶r 1, og deretter fortsette √• redusere parameteren. Med tiden vil vi f√• gode sannsynlighetsverdier i Q-Tabellen, og dermed b√∏r vi justere dem litt, og ikke overskrive dem helt med nye verdier.

- **√òk epsilon**. Vi kan √∏nske √• √∏ke `epsilon` sakte, for √• utforske mindre og utnytte mer. Det gir sannsynligvis mening √• starte med en lav verdi av `epsilon`, og bevege seg opp mot nesten 1.
> **Oppgave 1**: Pr√∏v deg frem med verdier for hyperparametere og se om du kan oppn√• h√∏yere kumulativ bel√∏nning. F√•r du over 195?
> **Oppgave 2**: For √• l√∏se problemet formelt, m√• du oppn√• en gjennomsnittlig bel√∏nning p√• 195 over 100 sammenhengende kj√∏ringer. M√•l dette under treningen og s√∏rg for at du har l√∏st problemet formelt!

## Se resultatet i praksis

Det kan v√¶re interessant √• faktisk se hvordan den trente modellen oppf√∏rer seg. La oss kj√∏re simuleringen og f√∏lge samme strategi for valg av handlinger som under treningen, ved √• pr√∏ve ut fra sannsynlighetsfordelingen i Q-Tabellen: (kodeblokk 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Du b√∏r se noe som dette:

![en balanserende cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄUtfordring

> **Oppgave 3**: Her brukte vi den endelige kopien av Q-Tabellen, som kanskje ikke er den beste. Husk at vi har lagret den best-presterende Q-Tabellen i variabelen `Qbest`! Pr√∏v det samme eksempelet med den best-presterende Q-Tabellen ved √• kopiere `Qbest` over til `Q` og se om du merker noen forskjell.

> **Oppgave 4**: Her valgte vi ikke den beste handlingen p√• hvert steg, men pr√∏vde heller ut fra den tilsvarende sannsynlighetsfordelingen. Ville det gi mer mening √• alltid velge den beste handlingen, med den h√∏yeste verdien i Q-Tabellen? Dette kan gj√∏res ved √• bruke funksjonen `np.argmax` for √• finne handlingsnummeret som tilsvarer den h√∏yeste verdien i Q-Tabellen. Implementer denne strategien og se om det forbedrer balansen.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Oppgave
[Tren en Mountain Car](assignment.md)

## Konklusjon

Vi har n√• l√¶rt hvordan vi kan trene agenter til √• oppn√• gode resultater bare ved √• gi dem en bel√∏nningsfunksjon som definerer √∏nsket tilstand i spillet, og ved √• gi dem muligheten til √• utforske s√∏keomr√•det intelligent. Vi har med suksess anvendt Q-Learning-algoritmen i tilfeller med diskrete og kontinuerlige milj√∏er, men med diskrete handlinger.

Det er ogs√• viktig √• studere situasjoner der handlingsrommet ogs√• er kontinuerlig, og n√•r observasjonsrommet er mye mer komplekst, som bildet fra skjermen i et Atari-spill. I slike problemer trenger vi ofte √• bruke mer kraftige maskinl√¶ringsteknikker, som nevrale nettverk, for √• oppn√• gode resultater. Disse mer avanserte temaene er emnet for v√•rt kommende mer avanserte AI-kurs.

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.