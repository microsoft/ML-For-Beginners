<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T22:05:43+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "no"
}
-->
# Introduksjon til forsterkende l칝ring og Q-L칝ring

![Oppsummering av forsterkning i maskinl칝ring i en sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote av [Tomomi Imura](https://www.twitter.com/girlie_mac)

Forsterkende l칝ring involverer tre viktige konsepter: agenten, noen tilstander, og et sett med handlinger per tilstand. Ved 친 utf칮re en handling i en spesifisert tilstand, f친r agenten en bel칮nning. Tenk igjen p친 dataspillet Super Mario. Du er Mario, du er i et spillniv친, st친ende ved kanten av en klippe. Over deg er det en mynt. Du som Mario, i et spillniv친, p친 en spesifikk posisjon ... det er din tilstand. 칀 ta et steg til h칮yre (en handling) vil f칮re deg over kanten, og det vil gi deg en lav numerisk score. Men 친 trykke p친 hopp-knappen vil gi deg et poeng og holde deg i live. Det er et positivt utfall og b칮r gi deg en positiv numerisk score.

Ved 친 bruke forsterkende l칝ring og en simulator (spillet), kan du l칝re hvordan du spiller spillet for 친 maksimere bel칮nningen, som er 친 holde seg i live og score s친 mange poeng som mulig.

[![Intro til forsterkende l칝ring](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 游꿘 Klikk p친 bildet over for 친 h칮re Dmitry diskutere forsterkende l칝ring

## [Quiz f칮r forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Forutsetninger og oppsett

I denne leksjonen skal vi eksperimentere med litt kode i Python. Du b칮r kunne kj칮re Jupyter Notebook-koden fra denne leksjonen, enten p친 din egen datamaskin eller i skyen.

Du kan 친pne [notatboken for leksjonen](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) og g친 gjennom denne leksjonen for 친 bygge.

> **Merk:** Hvis du 친pner denne koden fra skyen, m친 du ogs친 hente filen [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), som brukes i notatbok-koden. Legg den i samme katalog som notatboken.

## Introduksjon

I denne leksjonen skal vi utforske verdenen til **[Peter og ulven](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirert av et musikalsk eventyr av den russiske komponisten [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Vi skal bruke **forsterkende l칝ring** for 친 la Peter utforske sitt milj칮, samle smakfulle epler og unng친 친 m칮te ulven.

**Forsterkende l칝ring** (RL) er en l칝ringsteknikk som lar oss l칝re optimal oppf칮rsel for en **agent** i et **milj칮** ved 친 kj칮re mange eksperimenter. En agent i dette milj칮et b칮r ha et **m친l**, definert av en **bel칮nningsfunksjon**.

## Milj칮et

For enkelhets skyld, la oss anta at Peters verden er et kvadratisk brett med st칮rrelse `bredde` x `h칮yde`, som dette:

![Peters milj칮](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Hver celle i dette brettet kan enten v칝re:

* **bakke**, som Peter og andre skapninger kan g친 p친.
* **vann**, som man 친penbart ikke kan g친 p친.
* et **tre** eller **gress**, et sted hvor man kan hvile.
* et **eple**, som representerer noe Peter vil v칝re glad for 친 finne for 친 mate seg selv.
* en **ulv**, som er farlig og b칮r unng친s.

Det finnes et eget Python-modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), som inneholder koden for 친 arbeide med dette milj칮et. Fordi denne koden ikke er viktig for 친 forst친 konseptene v친re, vil vi importere modulen og bruke den til 친 lage brettet (kodeblokk 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Denne koden b칮r skrive ut et bilde av milj칮et som ligner p친 det ovenfor.

## Handlinger og strategi

I v친rt eksempel vil Peters m친l v칝re 친 finne et eple, mens han unng친r ulven og andre hindringer. For 친 gj칮re dette kan han i hovedsak g친 rundt til han finner et eple.

Derfor kan han p친 enhver posisjon velge mellom en av f칮lgende handlinger: opp, ned, venstre og h칮yre.

Vi vil definere disse handlingene som et ordbok, og knytte dem til par av tilsvarende koordinatendringer. For eksempel vil det 친 bevege seg til h칮yre (`R`) tilsvare paret `(1,0)`. (kodeblokk 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

For 친 oppsummere, strategien og m친let for dette scenariet er som f칮lger:

- **Strategien**, for v친r agent (Peter) er definert av en s친kalt **policy**. En policy er en funksjon som returnerer handlingen i en gitt tilstand. I v친rt tilfelle er tilstanden til problemet representert av brettet, inkludert spillerens n친v칝rende posisjon.

- **M친let**, med forsterkende l칝ring er 친 til slutt l칝re en god policy som lar oss l칮se problemet effektivt. Men som en baseline, la oss vurdere den enkleste policyen kalt **tilfeldig vandring**.

## Tilfeldig vandring

La oss f칮rst l칮se problemet v친rt ved 친 implementere en strategi for tilfeldig vandring. Med tilfeldig vandring vil vi tilfeldig velge neste handling fra de tillatte handlingene, til vi n친r eplet (kodeblokk 3).

1. Implementer den tilfeldige vandringen med koden nedenfor:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Kallet til `walk` b칮r returnere lengden p친 den tilsvarende stien, som kan variere fra en kj칮ring til en annen. 

1. Kj칮r vandringseksperimentet et antall ganger (si, 100), og skriv ut de resulterende statistikkene (kodeblokk 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Merk at gjennomsnittlig lengde p친 en sti er rundt 30-40 steg, noe som er ganske mye, gitt at gjennomsnittlig avstand til n칝rmeste eple er rundt 5-6 steg.

    Du kan ogs친 se hvordan Peters bevegelser ser ut under den tilfeldige vandringen:

    ![Peters tilfeldige vandring](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Bel칮nningsfunksjon

For 친 gj칮re v친r policy mer intelligent, m친 vi forst친 hvilke bevegelser som er "bedre" enn andre. For 친 gj칮re dette m친 vi definere m친let v친rt.

M친let kan defineres i form av en **bel칮nningsfunksjon**, som vil returnere en scoreverdi for hver tilstand. Jo h칮yere tallet er, jo bedre er bel칮nningsfunksjonen. (kodeblokk 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

En interessant ting med bel칮nningsfunksjoner er at i de fleste tilfeller *f친r vi bare en betydelig bel칮nning p친 slutten av spillet*. Dette betyr at algoritmen v친r p친 en eller annen m친te m친 huske "gode" steg som f칮rer til en positiv bel칮nning til slutt, og 칮ke deres betydning. P친 samme m친te b칮r alle bevegelser som f칮rer til d친rlige resultater avskrekkes.

## Q-L칝ring

En algoritme som vi skal diskutere her kalles **Q-L칝ring**. I denne algoritmen er policyen definert av en funksjon (eller en datastruktur) kalt en **Q-Tabell**. Den registrerer "godheten" til hver av handlingene i en gitt tilstand.

Den kalles en Q-Tabell fordi det ofte er praktisk 친 representere den som en tabell, eller en flerdimensjonal matrise. Siden brettet v친rt har dimensjonene `bredde` x `h칮yde`, kan vi representere Q-Tabellen ved hjelp av en numpy-matrise med formen `bredde` x `h칮yde` x `len(actions)`: (kodeblokk 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Legg merke til at vi initialiserer alle verdiene i Q-Tabellen med en lik verdi, i v친rt tilfelle - 0.25. Dette tilsvarer policyen "tilfeldig vandring", fordi alle bevegelser i hver tilstand er like gode. Vi kan sende Q-Tabellen til `plot`-funksjonen for 친 visualisere tabellen p친 brettet: `m.plot(Q)`.

![Peters milj칮](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

I midten av hver celle er det en "pil" som indikerer den foretrukne retningen for bevegelse. Siden alle retninger er like, vises en prikk.

N친 m친 vi kj칮re simuleringen, utforske milj칮et v친rt, og l칝re en bedre fordeling av Q-Tabell-verdier, som vil tillate oss 친 finne veien til eplet mye raskere.

## Essensen av Q-L칝ring: Bellman-likningen

N친r vi begynner 친 bevege oss, vil hver handling ha en tilsvarende bel칮nning, dvs. vi kan teoretisk velge neste handling basert p친 den h칮yeste umiddelbare bel칮nningen. Men i de fleste tilstander vil ikke bevegelsen oppn친 m친let v친rt om 친 n친 eplet, og dermed kan vi ikke umiddelbart avgj칮re hvilken retning som er bedre.

> Husk at det ikke er det umiddelbare resultatet som betyr noe, men heller det endelige resultatet, som vi vil oppn친 p친 slutten av simuleringen.

For 친 ta hensyn til denne forsinkede bel칮nningen, m친 vi bruke prinsippene for **[dynamisk programmering](https://en.wikipedia.org/wiki/Dynamic_programming)**, som lar oss tenke p친 problemet v친rt rekursivt.

Anta at vi n친 er i tilstanden *s*, og vi 칮nsker 친 bevege oss til neste tilstand *s'*. Ved 친 gj칮re det vil vi motta den umiddelbare bel칮nningen *r(s,a)*, definert av bel칮nningsfunksjonen, pluss en fremtidig bel칮nning. Hvis vi antar at Q-Tabellen v친r korrekt reflekterer "attraktiviteten" til hver handling, vil vi i tilstanden *s'* velge en handling *a* som tilsvarer maksimal verdi av *Q(s',a')*. Dermed vil den beste mulige fremtidige bel칮nningen vi kunne f친 i tilstanden *s* bli definert som `max`

## Sjekke policyen

Siden Q-Tabellen viser "attraktiviteten" til hver handling i hver tilstand, er det ganske enkelt 친 bruke den til 친 definere effektiv navigering i v친r verden. I det enkleste tilfellet kan vi velge handlingen som tilsvarer den h칮yeste verdien i Q-Tabellen: (kodeblokk 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Hvis du pr칮ver koden ovenfor flere ganger, kan du legge merke til at den noen ganger "henger", og du m친 trykke p친 STOP-knappen i notatboken for 친 avbryte den. Dette skjer fordi det kan oppst친 situasjoner der to tilstander "peker" p친 hverandre n친r det gjelder optimal Q-verdi, i s친 fall ender agenten opp med 친 bevege seg mellom disse tilstandene uendelig.

## 游Utfordring

> **Oppgave 1:** Endre `walk`-funksjonen slik at den begrenser maksimal lengde p친 stien til et visst antall steg (for eksempel 100), og se koden ovenfor returnere denne verdien fra tid til annen.

> **Oppgave 2:** Endre `walk`-funksjonen slik at den ikke g친r tilbake til steder den allerede har v칝rt tidligere. Dette vil forhindre at `walk` g친r i loop, men agenten kan fortsatt ende opp med 친 bli "fanget" p친 et sted den ikke kan unnslippe fra.

## Navigering

En bedre navigeringspolicy ville v칝re den vi brukte under trening, som kombinerer utnyttelse og utforskning. I denne policyen vil vi velge hver handling med en viss sannsynlighet, proporsjonal med verdiene i Q-Tabellen. Denne strategien kan fortsatt f칮re til at agenten returnerer til en posisjon den allerede har utforsket, men som du kan se fra koden nedenfor, resulterer den i en veldig kort gjennomsnittlig sti til 칮nsket lokasjon (husk at `print_statistics` kj칮rer simuleringen 100 ganger): (kodeblokk 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Etter 친 ha kj칮rt denne koden, b칮r du f친 en mye kortere gjennomsnittlig stilengde enn f칮r, i omr친det 3-6.

## Unders칮ke l칝ringsprosessen

Som vi har nevnt, er l칝ringsprosessen en balanse mellom utforskning og utnyttelse av opparbeidet kunnskap om problemrommets struktur. Vi har sett at resultatene av l칝ringen (evnen til 친 hjelpe en agent med 친 finne en kort sti til m친let) har forbedret seg, men det er ogs친 interessant 친 observere hvordan gjennomsnittlig stilengde oppf칮rer seg under l칝ringsprosessen:

## Oppsummering av l칝ringen:

- **Gjennomsnittlig stilengde 칮ker**. Det vi ser her er at i starten 칮ker gjennomsnittlig stilengde. Dette skyldes sannsynligvis at n친r vi ikke vet noe om milj칮et, er vi mer sannsynlig 친 bli fanget i d친rlige tilstander, som vann eller ulv. Etter hvert som vi l칝rer mer og begynner 친 bruke denne kunnskapen, kan vi utforske milj칮et lenger, men vi vet fortsatt ikke veldig godt hvor eplene er.

- **Stilengden avtar etter hvert som vi l칝rer mer**. N친r vi har l칝rt nok, blir det lettere for agenten 친 n친 m친let, og stilengden begynner 친 avta. Vi er imidlertid fortsatt 친pne for utforskning, s친 vi avviker ofte fra den beste stien og utforsker nye alternativer, noe som gj칮r stien lengre enn optimal.

- **Lengden 칮ker br친tt**. Det vi ogs친 observerer p친 denne grafen er at p친 et tidspunkt 칮kte lengden br친tt. Dette indikerer den stokastiske naturen til prosessen, og at vi p친 et tidspunkt kan "칮delegge" Q-Tabell-koeffisientene ved 친 overskrive dem med nye verdier. Dette b칮r ideelt sett minimeres ved 친 redusere l칝ringsraten (for eksempel mot slutten av treningen justerer vi Q-Tabell-verdiene med en liten verdi).

Alt i alt er det viktig 친 huske at suksessen og kvaliteten p친 l칝ringsprosessen avhenger betydelig av parametere, som l칝ringsrate, l칝ringsrate-avtakelse og diskonteringsfaktor. Disse kalles ofte **hyperparametere**, for 친 skille dem fra **parametere**, som vi optimaliserer under trening (for eksempel Q-Tabell-koeffisienter). Prosessen med 친 finne de beste verdiene for hyperparametere kalles **hyperparameteroptimalisering**, og det fortjener et eget tema.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Oppgave 
[En mer realistisk verden](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.