<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T21:07:11+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "no"
}
-->
# Bygg en regresjonsmodell med Scikit-learn: regresjon p√• fire m√•ter

![Infografikk for line√¶r vs. polynomisk regresjon](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografikk av [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denne leksjonen er ogs√• tilgjengelig i R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introduksjon 

S√• langt har du utforsket hva regresjon er med eksempeldata hentet fra gresskarpris-datasettet som vi skal bruke gjennom hele denne leksjonen. Du har ogs√• visualisert det ved hjelp av Matplotlib.

N√• er du klar til √• dykke dypere inn i regresjon for maskinl√¶ring. Mens visualisering hjelper deg med √• forst√• data, ligger den virkelige kraften i maskinl√¶ring i _√• trene modeller_. Modeller trenes p√• historiske data for automatisk √• fange opp datam√∏nstre, og de lar deg forutsi utfall for nye data som modellen ikke har sett f√∏r.

I denne leksjonen vil du l√¶re mer om to typer regresjon: _grunnleggende line√¶r regresjon_ og _polynomisk regresjon_, sammen med noe av matematikken som ligger til grunn for disse teknikkene. Disse modellene vil hjelpe oss med √• forutsi gresskarpriser basert p√• ulike inngangsdata. 

[![ML for nybegynnere - Forst√• line√¶r regresjon](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for nybegynnere - Forst√• line√¶r regresjon")

> üé• Klikk p√• bildet over for en kort videooversikt over line√¶r regresjon.

> Gjennom hele dette kurset antar vi minimal kunnskap om matematikk og s√∏ker √• gj√∏re det tilgjengelig for studenter fra andre felt. Se etter notater, üßÆ utrop, diagrammer og andre l√¶ringsverkt√∏y for √• hjelpe med forst√•elsen.

### Forutsetninger

Du b√∏r n√• v√¶re kjent med strukturen til gresskar-datasettet vi unders√∏ker. Du finner det forh√•ndslastet og forh√•ndsrenset i denne leksjonens _notebook.ipynb_-fil. I filen vises gresskarprisen per bushel i en ny data frame. S√∏rg for at du kan kj√∏re disse notatb√∏kene i kjerner i Visual Studio Code.

### Forberedelse

Som en p√•minnelse, du laster inn disse dataene for √• stille sp√∏rsm√•l til dem. 

- N√•r er det best √• kj√∏pe gresskar? 
- Hvilken pris kan jeg forvente for en kasse med miniatyrgresskar?
- B√∏r jeg kj√∏pe dem i halv-bushelkurver eller i 1 1/9 bushel-esker?
La oss fortsette √• grave i disse dataene.

I forrige leksjon opprettet du en Pandas data frame og fylte den med en del av det opprinnelige datasettet, og standardiserte prisen per bushel. Ved √• gj√∏re det var du imidlertid bare i stand til √• samle rundt 400 datapunkter, og kun for h√∏stm√•nedene. 

Ta en titt p√• dataene som vi har forh√•ndslastet i denne leksjonens tilh√∏rende notatbok. Dataene er forh√•ndslastet, og et f√∏rste spredningsdiagram er laget for √• vise m√•nedsdata. Kanskje vi kan f√• litt mer innsikt i dataene ved √• rense dem ytterligere.

## En line√¶r regresjonslinje

Som du l√¶rte i leksjon 1, er m√•let med en line√¶r regresjons√∏velse √• kunne tegne en linje for √•:

- **Vise variabelsammenhenger**. Vise forholdet mellom variabler
- **Gj√∏re forutsigelser**. Gj√∏re n√∏yaktige forutsigelser om hvor et nytt datapunkt vil falle i forhold til den linjen. 
 
Det er typisk for **minste kvadraters regresjon** √• tegne denne typen linje. Begrepet 'minste kvadrater' betyr at alle datapunktene rundt regresjonslinjen kvadreres og deretter summeres. Ideelt sett er denne summen s√• liten som mulig, fordi vi √∏nsker et lavt antall feil, eller `minste kvadrater`. 

Vi gj√∏r dette fordi vi √∏nsker √• modellere en linje som har minst mulig kumulativ avstand fra alle datapunktene v√•re. Vi kvadrerer ogs√• termene f√∏r vi legger dem sammen, siden vi er opptatt av st√∏rrelsen snarere enn retningen.

> **üßÆ Vis meg matematikken** 
> 
> Denne linjen, kalt _linjen for beste tilpasning_, kan uttrykkes ved [en ligning](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` er den 'forklarende variabelen'. `Y` er den 'avhengige variabelen'. Stigningen p√• linjen er `b`, og `a` er skj√¶ringspunktet med y-aksen, som refererer til verdien av `Y` n√•r `X = 0`. 
>
>![beregn stigningen](../../../../2-Regression/3-Linear/images/slope.png)
>
> F√∏rst, beregn stigningen `b`. Infografikk av [Jen Looper](https://twitter.com/jenlooper)
>
> Med andre ord, og med henvisning til det opprinnelige sp√∏rsm√•let om gresskar-dataene: "forutsi prisen p√• et gresskar per bushel etter m√•ned", ville `X` referere til prisen og `Y` til salgsdatoen. 
>
>![fullf√∏r ligningen](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Beregn verdien av Y. Hvis du betaler rundt $4, m√• det v√¶re april! Infografikk av [Jen Looper](https://twitter.com/jenlooper)
>
> Matematikk som beregner linjen m√• vise stigningen p√• linjen, som ogs√• avhenger av skj√¶ringspunktet, eller hvor `Y` er plassert n√•r `X = 0`.
>
> Du kan se metoden for beregning av disse verdiene p√• nettstedet [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Bes√∏k ogs√• [denne minste kvadraters kalkulatoren](https://www.mathsisfun.com/data/least-squares-calculator.html) for √• se hvordan tallverdiene p√•virker linjen.

## Korrelasjon

Et annet begrep √• forst√• er **korrelasjonskoeffisienten** mellom gitte X- og Y-variabler. Ved hjelp av et spredningsdiagram kan du raskt visualisere denne koeffisienten. Et diagram med datapunkter spredt i en ryddig linje har h√∏y korrelasjon, men et diagram med datapunkter spredt overalt mellom X og Y har lav korrelasjon.

En god line√¶r regresjonsmodell vil v√¶re en som har en h√∏y (n√¶rmere 1 enn 0) korrelasjonskoeffisient ved bruk av minste kvadraters regresjonsmetode med en regresjonslinje.

‚úÖ Kj√∏r notatboken som f√∏lger med denne leksjonen, og se p√• spredningsdiagrammet for m√•ned til pris. Ser dataene som knytter m√•ned til pris for gresskarsalg ut til √• ha h√∏y eller lav korrelasjon, if√∏lge din visuelle tolkning av spredningsdiagrammet? Endrer det seg hvis du bruker en mer detaljert m√•ling i stedet for `M√•ned`, for eksempel *dag i √•ret* (dvs. antall dager siden begynnelsen av √•ret)?

I koden nedenfor antar vi at vi har renset dataene og f√•tt en data frame kalt `new_pumpkins`, som ligner p√• f√∏lgende:

ID | M√•ned | DagI√Öret | Sort | By | Pakke | Lav pris | H√∏y pris | Pris
---|-------|----------|------|-----|--------|----------|----------|-----
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Koden for √• rense dataene er tilgjengelig i [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Vi har utf√∏rt de samme rensetrinnene som i forrige leksjon, og har beregnet `DagI√Öret`-kolonnen ved hjelp av f√∏lgende uttrykk: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

N√• som du har en forst√•else av matematikken bak line√¶r regresjon, la oss lage en regresjonsmodell for √• se om vi kan forutsi hvilken pakke med gresskar som vil ha de beste prisene. Noen som kj√∏per gresskar til en h√∏stfest kan ha nytte av denne informasjonen for √• optimalisere kj√∏pene sine.

## Lete etter korrelasjon

[![ML for nybegynnere - Lete etter korrelasjon: N√∏kkelen til line√¶r regresjon](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for nybegynnere - Lete etter korrelasjon: N√∏kkelen til line√¶r regresjon")

> üé• Klikk p√• bildet over for en kort videooversikt over korrelasjon.

Fra forrige leksjon har du sannsynligvis sett at gjennomsnittsprisen for ulike m√•neder ser slik ut:

<img alt="Gjennomsnittspris per m√•ned" src="../2-Data/images/barchart.png" width="50%"/>

Dette antyder at det b√∏r v√¶re en viss korrelasjon, og vi kan pr√∏ve √• trene en line√¶r regresjonsmodell for √• forutsi forholdet mellom `M√•ned` og `Pris`, eller mellom `DagI√Öret` og `Pris`. Her er spredningsdiagrammet som viser det sistnevnte forholdet:

<img alt="Spredningsdiagram av pris vs. dag i √•ret" src="images/scatter-dayofyear.png" width="50%" /> 

La oss se om det er en korrelasjon ved hjelp av `corr`-funksjonen:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Det ser ut til at korrelasjonen er ganske liten, -0.15 for `M√•ned` og -0.17 for `DagI√Öret`, men det kan v√¶re et annet viktig forhold. Det ser ut til at det er forskjellige prisgrupper som tilsvarer ulike gresskarsorter. For √• bekrefte denne hypotesen, la oss plotte hver gresskarkategori med en annen farge. Ved √• sende en `ax`-parameter til `scatter`-plottefunksjonen kan vi plotte alle punkter p√• samme graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Spredningsdiagram av pris vs. dag i √•ret" src="images/scatter-dayofyear-color.png" width="50%" /> 

V√•r unders√∏kelse antyder at sorten har st√∏rre effekt p√• den totale prisen enn selve salgsdatoen. Vi kan se dette med et stolpediagram:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Stolpediagram av pris vs. sort" src="images/price-by-variety.png" width="50%" /> 

La oss for √∏yeblikket fokusere kun p√• √©n gresskarsort, 'pie type', og se hvilken effekt datoen har p√• prisen:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Spredningsdiagram av pris vs. dag i √•ret" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Hvis vi n√• beregner korrelasjonen mellom `Pris` og `DagI√Öret` ved hjelp av `corr`-funksjonen, vil vi f√• noe som `-0.27` - noe som betyr at det gir mening √• trene en prediktiv modell.

> F√∏r du trener en line√¶r regresjonsmodell, er det viktig √• s√∏rge for at dataene v√•re er rene. Line√¶r regresjon fungerer ikke godt med manglende verdier, s√• det gir mening √• fjerne alle tomme celler:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

En annen tiln√¶rming ville v√¶re √• fylle de tomme verdiene med gjennomsnittsverdier fra den tilsvarende kolonnen.

## Enkel line√¶r regresjon

[![ML for nybegynnere - Line√¶r og polynomisk regresjon med Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for nybegynnere - Line√¶r og polynomisk regresjon med Scikit-learn")

> üé• Klikk p√• bildet over for en kort videooversikt over line√¶r og polynomisk regresjon.

For √• trene v√•r line√¶re regresjonsmodell, vil vi bruke **Scikit-learn**-biblioteket.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Vi starter med √• skille inngangsverdier (funksjoner) og forventet utgang (etikett) i separate numpy-arrays:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Merk at vi m√•tte utf√∏re `reshape` p√• inngangsdataene for at pakken for line√¶r regresjon skulle forst√• dem riktig. Line√¶r regresjon forventer et 2D-array som inngang, hvor hver rad i arrayet tilsvarer en vektor av inngangsfunksjoner. I v√•rt tilfelle, siden vi bare har √©n inngang, trenger vi et array med formen N√ó1, hvor N er datasettets st√∏rrelse.

Deretter m√• vi dele dataene inn i trenings- og testdatasett, slik at vi kan validere modellen v√•r etter trening:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Til slutt tar det bare to linjer med kode √• trene den faktiske line√¶re regresjonsmodellen. Vi definerer `LinearRegression`-objektet og tilpasser det til dataene v√•re ved hjelp av `fit`-metoden:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objektet inneholder etter `fit`-prosessen alle koeffisientene for regresjonen, som kan n√•s ved hjelp av `.coef_`-egenskapen. I v√•rt tilfelle er det bare √©n koeffisient, som b√∏r v√¶re rundt `-0.017`. Dette betyr at prisene ser ut til √• synke litt over tid, men ikke mye, rundt 2 cent per dag. Vi kan ogs√• f√• tilgang til skj√¶ringspunktet med Y-aksen ved hjelp av `lin_reg.intercept_` - det vil v√¶re rundt `21` i v√•rt tilfelle, noe som indikerer prisen ved begynnelsen av √•ret.

For √• se hvor n√∏yaktig modellen v√•r er, kan vi forutsi priser p√• et testdatasett og deretter m√•le hvor n√¶rme forutsigelsene v√•re er de forventede verdiene. Dette kan gj√∏res ved hjelp av middelkvadratfeil (MSE)-metrikken, som er gjennomsnittet av alle kvadrerte forskjeller mellom forventet og forutsagt verdi.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Feilen v√•r ser ut til √• v√¶re rundt 2 punkter, som er ~17 %. Ikke s√• bra. En annen indikator p√• modellkvalitet er **determinasjonskoeffisienten**, som kan beregnes slik:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Hvis verdien er 0, betyr det at modellen ikke tar hensyn til inputdata, og fungerer som den *d√•rligste line√¶re prediktoren*, som bare er gjennomsnittsverdien av resultatet. Verdien 1 betyr at vi kan perfekt forutsi alle forventede utfall. I v√•rt tilfelle er koeffisienten rundt 0,06, som er ganske lav.

Vi kan ogs√• plotte testdata sammen med regresjonslinjen for bedre √• se hvordan regresjonen fungerer i v√•rt tilfelle:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Line√¶r regresjon" src="images/linear-results.png" width="50%" />

## Polynomisk regresjon

En annen type line√¶r regresjon er polynomisk regresjon. Selv om det noen ganger er en line√¶r sammenheng mellom variabler - jo st√∏rre gresskaret er i volum, jo h√∏yere pris - kan det noen ganger v√¶re slik at disse sammenhengene ikke kan plottes som et plan eller en rett linje.

‚úÖ Her er [noen flere eksempler](https://online.stat.psu.edu/stat501/lesson/9/9.8) p√• data som kan bruke polynomisk regresjon.

Se en gang til p√• sammenhengen mellom dato og pris. Ser dette spredningsdiagrammet ut som det n√∏dvendigvis b√∏r analyseres med en rett linje? Kan ikke priser svinge? I dette tilfellet kan du pr√∏ve polynomisk regresjon.

‚úÖ Polynomier er matematiske uttrykk som kan best√• av √©n eller flere variabler og koeffisienter.

Polynomisk regresjon skaper en kurvet linje for bedre √• tilpasse seg ikke-line√¶re data. I v√•rt tilfelle, hvis vi inkluderer en kvadrert `DayOfYear`-variabel i inputdataene, b√∏r vi kunne tilpasse dataene v√•re med en parabolsk kurve, som vil ha et minimum p√• et bestemt punkt i l√∏pet av √•ret.

Scikit-learn inkluderer en nyttig [pipeline-API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) for √• kombinere ulike trinn i databehandlingen. En **pipeline** er en kjede av **estimators**. I v√•rt tilfelle vil vi lage en pipeline som f√∏rst legger til polynomiske funksjoner til modellen v√•r, og deretter trener regresjonen:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Ved √• bruke `PolynomialFeatures(2)` betyr det at vi vil inkludere alle andregrads polynomier fra inputdataene. I v√•rt tilfelle vil det bare bety `DayOfYear`<sup>2</sup>, men gitt to inputvariabler X og Y, vil dette legge til X<sup>2</sup>, XY og Y<sup>2</sup>. Vi kan ogs√• bruke polynomier av h√∏yere grad hvis vi √∏nsker.

Pipelines kan brukes p√• samme m√•te som det opprinnelige `LinearRegression`-objektet, dvs. vi kan `fit` pipelinen, og deretter bruke `predict` for √• f√• prediksjonsresultatene. Her er grafen som viser testdataene og tiln√¶rmingskurven:

<img alt="Polynomisk regresjon" src="images/poly-results.png" width="50%" />

Ved √• bruke polynomisk regresjon kan vi f√• litt lavere MSE og h√∏yere determinasjon, men ikke betydelig. Vi m√• ta hensyn til andre funksjoner!

> Du kan se at de laveste gresskarprisene observeres et sted rundt Halloween. Hvordan kan du forklare dette?

üéÉ Gratulerer, du har nettopp laget en modell som kan hjelpe med √• forutsi prisen p√• pai-gresskar. Du kan sannsynligvis gjenta samme prosedyre for alle gresskartyper, men det ville v√¶re tidkrevende. La oss n√• l√¶re hvordan vi kan ta gresskarsort i betraktning i modellen v√•r!

## Kategoriske funksjoner

I en ideell verden √∏nsker vi √• kunne forutsi priser for ulike gresskarsorter ved hjelp av samme modell. Imidlertid er `Variety`-kolonnen litt annerledes enn kolonner som `Month`, fordi den inneholder ikke-numeriske verdier. Slike kolonner kalles **kategoriske**.

[![ML for nybegynnere - Kategoriske funksjoner med line√¶r regresjon](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for nybegynnere - Kategoriske funksjoner med line√¶r regresjon")

> üé• Klikk p√• bildet over for en kort videooversikt om bruk av kategoriske funksjoner.

Her kan du se hvordan gjennomsnittsprisen avhenger av sort:

<img alt="Gjennomsnittspris etter sort" src="images/price-by-variety.png" width="50%" />

For √• ta sort i betraktning, m√• vi f√∏rst konvertere den til numerisk form, eller **enkode** den. Det finnes flere m√•ter vi kan gj√∏re dette p√•:

* Enkel **numerisk enkoding** vil bygge en tabell over ulike sorter, og deretter erstatte sortnavnet med en indeks i den tabellen. Dette er ikke den beste ideen for line√¶r regresjon, fordi line√¶r regresjon tar den faktiske numeriske verdien av indeksen og legger den til resultatet, multiplisert med en koeffisient. I v√•rt tilfelle er sammenhengen mellom indeksnummeret og prisen tydelig ikke-line√¶r, selv om vi s√∏rger for at indeksene er ordnet p√• en spesifikk m√•te.
* **One-hot enkoding** vil erstatte `Variety`-kolonnen med 4 forskjellige kolonner, √©n for hver sort. Hver kolonne vil inneholde `1` hvis den tilsvarende raden er av en gitt sort, og `0` ellers. Dette betyr at det vil v√¶re fire koeffisienter i line√¶r regresjon, √©n for hver gresskarsort, som er ansvarlig for "startpris" (eller rettere sagt "tilleggspris") for den spesifikke sorten.

Koden nedenfor viser hvordan vi kan one-hot enkode en sort:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

For √• trene line√¶r regresjon ved bruk av one-hot enkodet sort som input, trenger vi bare √• initialisere `X` og `y`-dataene korrekt:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Resten av koden er den samme som vi brukte ovenfor for √• trene line√¶r regresjon. Hvis du pr√∏ver det, vil du se at den gjennomsnittlige kvadratiske feilen er omtrent den samme, men vi f√•r en mye h√∏yere determinasjonskoeffisient (~77 %). For √• f√• enda mer n√∏yaktige prediksjoner kan vi ta flere kategoriske funksjoner i betraktning, samt numeriske funksjoner, som `Month` eller `DayOfYear`. For √• f√• √©n stor funksjonsmatrise kan vi bruke `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Her tar vi ogs√• hensyn til `City` og `Package`-type, som gir oss MSE 2,84 (10 %) og determinasjon 0,94!

## Alt samlet

For √• lage den beste modellen kan vi bruke kombinerte (one-hot enkodede kategoriske + numeriske) data fra eksempelet ovenfor sammen med polynomisk regresjon. Her er den komplette koden for enkelhets skyld:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Dette b√∏r gi oss den beste determinasjonskoeffisienten p√• nesten 97 %, og MSE=2,23 (~8 % prediksjonsfeil).

| Modell | MSE | Determinasjon |
|--------|-----|---------------|
| `DayOfYear` Line√¶r | 2,77 (17,2 %) | 0,07 |
| `DayOfYear` Polynomisk | 2,73 (17,0 %) | 0,08 |
| `Variety` Line√¶r | 5,24 (19,7 %) | 0,77 |
| Alle funksjoner Line√¶r | 2,84 (10,5 %) | 0,94 |
| Alle funksjoner Polynomisk | 2,23 (8,25 %) | 0,97 |

üèÜ Bra jobbet! Du har laget fire regresjonsmodeller i √©n leksjon, og forbedret modellkvaliteten til 97 %. I den siste delen om regresjon vil du l√¶re om logistisk regresjon for √• bestemme kategorier.

---
## üöÄUtfordring

Test flere forskjellige variabler i denne notatboken for √• se hvordan korrelasjon samsvarer med modellens n√∏yaktighet.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Gjennomgang og selvstudium

I denne leksjonen l√¶rte vi om line√¶r regresjon. Det finnes andre viktige typer regresjon. Les om Stepwise, Ridge, Lasso og Elasticnet-teknikker. Et godt kurs for √• l√¶re mer er [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Oppgave

[Bygg en modell](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.