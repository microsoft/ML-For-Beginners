<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T21:11:16+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "no"
}
-->
# Logistisk regresjon for √• forutsi kategorier

![Infografikk om logistisk vs. line√¶r regresjon](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denne leksjonen er tilgjengelig i R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Introduksjon

I denne siste leksjonen om regresjon, en av de grunnleggende _klassiske_ ML-teknikkene, skal vi se n√¶rmere p√• logistisk regresjon. Du kan bruke denne teknikken til √• oppdage m√∏nstre for √• forutsi bin√¶re kategorier. Er dette godteri sjokolade eller ikke? Er denne sykdommen smittsom eller ikke? Vil denne kunden velge dette produktet eller ikke?

I denne leksjonen vil du l√¶re:

- Et nytt bibliotek for datavisualisering
- Teknikker for logistisk regresjon

‚úÖ Fordyp deg i √• arbeide med denne typen regresjon i dette [Learn-modulet](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Forutsetning

Etter √• ha jobbet med gresskar-dataene, er vi n√• kjent nok med dem til √• innse at det finnes √©n bin√¶r kategori vi kan jobbe med: `Color`.

La oss bygge en logistisk regresjonsmodell for √• forutsi, gitt noen variabler, _hvilken farge et gitt gresskar sannsynligvis har_ (oransje üéÉ eller hvit üëª).

> Hvorfor snakker vi om bin√¶r klassifisering i en leksjon om regresjon? Bare av spr√•klig bekvemmelighet, siden logistisk regresjon [egentlig er en klassifiseringsmetode](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), om enn en line√¶r-basert en. L√¶r om andre m√•ter √• klassifisere data p√• i neste leksjonsgruppe.

## Definer sp√∏rsm√•let

For v√•rt form√•l vil vi uttrykke dette som en bin√¶r: 'Hvit' eller 'Ikke hvit'. Det finnes ogs√• en 'stripet' kategori i datasettet v√•rt, men det er f√• forekomster av den, s√• vi vil ikke bruke den. Den forsvinner uansett n√•r vi fjerner nullverdier fra datasettet.

> üéÉ Fun fact: Vi kaller noen ganger hvite gresskar for 'sp√∏kelsesgresskar'. De er ikke veldig lette √• skj√¶re ut, s√• de er ikke like popul√¶re som de oransje, men de ser kule ut! S√• vi kunne ogs√• formulert sp√∏rsm√•let v√•rt som: 'Sp√∏kelse' eller 'Ikke sp√∏kelse'. üëª

## Om logistisk regresjon

Logistisk regresjon skiller seg fra line√¶r regresjon, som du l√¶rte om tidligere, p√• noen viktige m√•ter.

[![ML for nybegynnere - Forst√• logistisk regresjon for maskinl√¶ringsklassifisering](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML for nybegynnere - Forst√• logistisk regresjon for maskinl√¶ringsklassifisering")

> üé• Klikk p√• bildet ovenfor for en kort videooversikt over logistisk regresjon.

### Bin√¶r klassifisering

Logistisk regresjon tilbyr ikke de samme funksjonene som line√¶r regresjon. Den f√∏rstnevnte gir en prediksjon om en bin√¶r kategori ("hvit eller ikke hvit"), mens den sistnevnte er i stand til √• forutsi kontinuerlige verdier, for eksempel gitt opprinnelsen til et gresskar og tidspunktet for innh√∏stingen, _hvor mye prisen vil stige_.

![Gresskar klassifiseringsmodell](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografikk av [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Andre klassifiseringer

Det finnes andre typer logistisk regresjon, inkludert multinomial og ordinal:

- **Multinomial**, som inneb√¶rer √• ha mer enn √©n kategori - "Oransje, Hvit og Stripet".
- **Ordinal**, som inneb√¶rer ordnede kategorier, nyttig hvis vi √∏nsket √• ordne resultatene v√•re logisk, som v√•re gresskar som er ordnet etter et begrenset antall st√∏rrelser (mini, sm, med, lg, xl, xxl).

![Multinomial vs ordinal regresjon](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Variabler trenger IKKE √• korrelere

Husker du hvordan line√¶r regresjon fungerte bedre med mer korrelerte variabler? Logistisk regresjon er det motsatte - variablene trenger ikke √• samsvare. Dette fungerer for disse dataene som har noe svake korrelasjoner.

### Du trenger mye rene data

Logistisk regresjon gir mer n√∏yaktige resultater hvis du bruker mer data; v√•rt lille datasett er ikke optimalt for denne oppgaven, s√• husk det.

[![ML for nybegynnere - Dataanalyse og forberedelse for logistisk regresjon](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML for nybegynnere - Dataanalyse og forberedelse for logistisk regresjon")

> üé• Klikk p√• bildet ovenfor for en kort videooversikt over forberedelse av data for line√¶r regresjon

‚úÖ Tenk p√• hvilke typer data som egner seg godt for logistisk regresjon

## √òvelse - rydd opp i dataene

F√∏rst, rydd opp i dataene litt, fjern nullverdier og velg bare noen av kolonnene:

1. Legg til f√∏lgende kode:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Du kan alltid ta en titt p√• din nye dataframe:

    ```python
    pumpkins.info
    ```

### Visualisering - kategorisk plott

N√• har du lastet opp [startnotatboken](../../../../2-Regression/4-Logistic/notebook.ipynb) med gresskar-data igjen og ryddet den slik at du har et datasett som inneholder noen f√• variabler, inkludert `Color`. La oss visualisere dataene i notatboken ved hjelp av et annet bibliotek: [Seaborn](https://seaborn.pydata.org/index.html), som er bygget p√• Matplotlib som vi brukte tidligere.

Seaborn tilbyr noen smarte m√•ter √• visualisere dataene dine p√•. For eksempel kan du sammenligne distribusjoner av dataene for hver `Variety` og `Color` i et kategorisk plott.

1. Lag et slikt plott ved √• bruke funksjonen `catplot`, med gresskar-dataene `pumpkins`, og spesifisere en fargekartlegging for hver gresskarkategori (oransje eller hvit):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Et rutenett med visualiserte data](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Ved √• observere dataene kan du se hvordan `Color`-dataene relaterer seg til `Variety`.

    ‚úÖ Gitt dette kategoriske plottet, hvilke interessante utforskninger kan du forestille deg?

### Datapreprosessering: funksjons- og etikettkoding
Datasettet v√•rt inneholder strengverdier for alle kolonnene. √Ö jobbe med kategoriske data er intuitivt for mennesker, men ikke for maskiner. Maskinl√¶ringsalgoritmer fungerer godt med tall. Derfor er koding et veldig viktig steg i datapreprosesseringen, siden det lar oss gj√∏re kategoriske data om til numeriske data, uten √• miste informasjon. God koding f√∏rer til √• bygge en god modell.

For funksjonskoding finnes det to hovedtyper av kodere:

1. Ordinal koder: passer godt for ordinale variabler, som er kategoriske variabler der dataene f√∏lger en logisk rekkef√∏lge, som kolonnen `Item Size` i datasettet v√•rt. Den lager en kartlegging slik at hver kategori representeres av et tall, som er rekkef√∏lgen til kategorien i kolonnen.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorisk koder: passer godt for nominelle variabler, som er kategoriske variabler der dataene ikke f√∏lger en logisk rekkef√∏lge, som alle funksjonene bortsett fra `Item Size` i datasettet v√•rt. Det er en one-hot encoding, som betyr at hver kategori representeres av en bin√¶r kolonne: den kodede variabelen er lik 1 hvis gresskaret tilh√∏rer den `Variety` og 0 ellers.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Deretter brukes `ColumnTransformer` til √• kombinere flere kodere i ett enkelt steg og anvende dem p√• de riktige kolonnene.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
For √• kode etiketten bruker vi scikit-learn-klassen `LabelEncoder`, som er en hjelpeklasse for √• normalisere etiketter slik at de bare inneholder verdier mellom 0 og n_classes-1 (her, 0 og 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
N√•r vi har kodet funksjonene og etiketten, kan vi sl√• dem sammen til en ny dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
‚úÖ Hva er fordelene med √• bruke en ordinal koder for kolonnen `Item Size`?

### Analyser forholdet mellom variabler

N√• som vi har forh√•ndsprosesserte dataene, kan vi analysere forholdet mellom funksjonene og etiketten for √• f√• en id√© om hvor godt modellen vil kunne forutsi etiketten gitt funksjonene.
Den beste m√•ten √• utf√∏re denne typen analyse p√• er √• plotte dataene. Vi bruker igjen Seaborn-funksjonen `catplot` for √• visualisere forholdet mellom `Item Size`, `Variety` og `Color` i et kategorisk plott. For √• bedre plotte dataene bruker vi den kodede kolonnen `Item Size` og den ukodede kolonnen `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![Et kategorisk plott av visualiserte data](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Bruk et swarm-plott

Siden `Color` er en bin√¶r kategori (Hvit eller Ikke), trenger den 'en [spesialisert tiln√¶rming](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) til visualisering'. Det finnes andre m√•ter √• visualisere forholdet mellom denne kategorien og andre variabler.

Du kan visualisere variabler side om side med Seaborn-plott.

1. Pr√∏v et 'swarm'-plott for √• vise distribusjonen av verdier:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Et swarm-plott av visualiserte data](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**V√¶r oppmerksom**: koden ovenfor kan generere en advarsel, siden Seaborn har problemer med √• representere s√• mange datapunkter i et swarm-plott. En mulig l√∏sning er √• redusere st√∏rrelsen p√• mark√∏ren ved √• bruke parameteren 'size'. V√¶r imidlertid oppmerksom p√• at dette p√•virker lesbarheten til plottet.

> **üßÆ Vis meg matematikken**
>
> Logistisk regresjon baserer seg p√• konseptet 'maksimal sannsynlighet' ved bruk av [sigmoid-funksjoner](https://wikipedia.org/wiki/Sigmoid_function). En 'Sigmoid-funksjon' p√• et plott ser ut som en 'S'-form. Den tar en verdi og kartlegger den til et sted mellom 0 og 1. Kurven kalles ogs√• en 'logistisk kurve'. Formelen ser slik ut:
>
> ![logistisk funksjon](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> der sigmoids midtpunkt befinner seg ved x's 0-punkt, L er kurvens maksimumsverdi, og k er kurvens bratthet. Hvis resultatet av funksjonen er mer enn 0.5, vil etiketten i sp√∏rsm√•let bli gitt klassen '1' av det bin√¶re valget. Hvis ikke, vil den bli klassifisert som '0'.

## Bygg modellen din

√Ö bygge en modell for √• finne disse bin√¶re klassifiseringene er overraskende enkelt i Scikit-learn.

[![ML for nybegynnere - Logistisk regresjon for klassifisering av data](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML for nybegynnere - Logistisk regresjon for klassifisering av data")

> üé• Klikk p√• bildet ovenfor for en kort videooversikt over √• bygge en line√¶r regresjonsmodell

1. Velg variablene du vil bruke i klassifiseringsmodellen din og del opp trenings- og testsett ved √• kalle `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. N√• kan du trene modellen din ved √• kalle `fit()` med treningsdataene dine, og skrive ut resultatet:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Ta en titt p√• modellens resultattavle. Det er ikke d√•rlig, med tanke p√• at du bare har rundt 1000 rader med data:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Bedre forst√•else via en forvirringsmatrise

Mens du kan f√• en resultattavlerapport [termer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) ved √• skrive ut elementene ovenfor, kan du kanskje forst√• modellen din bedre ved √• bruke en [forvirringsmatrise](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) for √• hjelpe oss med √• forst√• hvordan modellen presterer.

> üéì En '[forvirringsmatrise](https://wikipedia.org/wiki/Confusion_matrix)' (eller 'feilmatrise') er en tabell som uttrykker modellens sanne vs. falske positive og negative, og dermed m√•ler n√∏yaktigheten av prediksjonene.

1. For √• bruke en forvirringsmatrise, kall `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Ta en titt p√• modellens forvirringsmatrise:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

I Scikit-learn er rader (akse 0) faktiske etiketter og kolonner (akse 1) predikerte etiketter.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Hva skjer her? La oss si at modellen v√•r blir bedt om √• klassifisere gresskar mellom to bin√¶re kategorier, kategori 'hvit' og kategori 'ikke-hvit'.

- Hvis modellen din forutsier et gresskar som ikke hvitt og det faktisk tilh√∏rer kategorien 'ikke-hvit', kaller vi det en sann negativ, vist av det √∏verste venstre tallet.
- Hvis modellen din forutsier et gresskar som hvitt og det faktisk tilh√∏rer kategorien 'ikke-hvit', kaller vi det en falsk negativ, vist av det nederste venstre tallet.
- Hvis modellen din forutsier et gresskar som ikke hvitt og det faktisk tilh√∏rer kategorien 'hvit', kaller vi det en falsk positiv, vist av det √∏verste h√∏yre tallet.
- Hvis modellen din forutsier et gresskar som hvitt og det faktisk tilh√∏rer kategorien 'hvit', kaller vi det en sann positiv, vist av det nederste h√∏yre tallet.

Som du kanskje har gjettet, er det √• foretrekke √• ha et st√∏rre antall sanne positive og sanne negative og et lavere antall falske positive og falske negative, noe som inneb√¶rer at modellen presterer bedre.
Hvordan henger forvirringsmatrisen sammen med presisjon og tilbakekalling? Husk, klassifiseringsrapporten som ble skrevet ut ovenfor viste presisjon (0.85) og tilbakekalling (0.67).

Presisjon = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461

Tilbakekalling = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666

‚úÖ Sp√∏rsm√•l: If√∏lge forvirringsmatrisen, hvordan gjorde modellen det? Svar: Ikke d√•rlig; det er et godt antall sanne negative, men ogs√• noen f√• falske negative.

La oss g√• tilbake til begrepene vi s√• tidligere ved hjelp av forvirringsmatrisens kartlegging av TP/TN og FP/FN:

üéì Presisjon: TP/(TP + FP) Andelen relevante instanser blant de hentede instansene (f.eks. hvilke etiketter som ble godt merket)

üéì Tilbakekalling: TP/(TP + FN) Andelen relevante instanser som ble hentet, enten de var godt merket eller ikke

üéì f1-score: (2 * presisjon * tilbakekalling)/(presisjon + tilbakekalling) Et vektet gjennomsnitt av presisjon og tilbakekalling, der det beste er 1 og det verste er 0

üéì St√∏tte: Antall forekomster av hver etikett som ble hentet

üéì N√∏yaktighet: (TP + TN)/(TP + TN + FP + FN) Prosentandelen av etiketter som ble korrekt forutsagt for et utvalg.

üéì Makro Gjennomsnitt: Beregningen av det uvektede gjennomsnittet av metrikker for hver etikett, uten √• ta hensyn til ubalanse i etiketter.

üéì Vektet Gjennomsnitt: Beregningen av gjennomsnittet av metrikker for hver etikett, som tar hensyn til ubalanse i etiketter ved √• vekte dem etter deres st√∏tte (antall sanne instanser for hver etikett).

‚úÖ Kan du tenke deg hvilken metrikk du b√∏r f√∏lge med p√• hvis du vil at modellen din skal redusere antall falske negative?

## Visualiser ROC-kurven for denne modellen

[![ML for nybegynnere - Analyse av logistisk regresjonsytelse med ROC-kurver](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML for nybegynnere - Analyse av logistisk regresjonsytelse med ROC-kurver")


> üé• Klikk p√• bildet ovenfor for en kort videooversikt over ROC-kurver

La oss gj√∏re √©n visualisering til for √• se den s√•kalte 'ROC'-kurven:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Ved hjelp av Matplotlib, plott modellens [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) eller ROC. ROC-kurver brukes ofte for √• f√• en oversikt over utdataene fra en klassifiserer i form av sanne vs. falske positive. "ROC-kurver har vanligvis sanne positive p√• Y-aksen, og falske positive p√• X-aksen." Dermed er brattheten p√• kurven og avstanden mellom midtlinjen og kurven viktig: du vil ha en kurve som raskt g√•r opp og over linjen. I v√•rt tilfelle er det falske positive i starten, og deretter g√•r linjen opp og over riktig:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Til slutt, bruk Scikit-learns [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) for √• beregne den faktiske 'Area Under the Curve' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Resultatet er `0.9749908725812341`. Siden AUC varierer fra 0 til 1, √∏nsker du en h√∏y score, ettersom en modell som er 100 % korrekt i sine prediksjoner vil ha en AUC p√• 1; i dette tilfellet er modellen _ganske god_. 

I fremtidige leksjoner om klassifiseringer vil du l√¶re hvordan du kan iterere for √• forbedre modellens resultater. Men for n√•, gratulerer! Du har fullf√∏rt disse regresjonsleksjonene!

---
## üöÄUtfordring

Det er mye mer √• utforske n√•r det gjelder logistisk regresjon! Men den beste m√•ten √• l√¶re p√• er √• eksperimentere. Finn et datasett som egner seg til denne typen analyse og bygg en modell med det. Hva l√¶rer du? tips: pr√∏v [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) for interessante datasett.

## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ml/)

## Gjennomgang & Selvstudium

Les de f√∏rste sidene av [denne artikkelen fra Stanford](https://web.stanford.edu/~jurafsky/slp3/5.pdf) om noen praktiske bruksomr√•der for logistisk regresjon. Tenk p√• oppgaver som egner seg bedre for den ene eller den andre typen regresjonsoppgaver som vi har studert s√• langt. Hva ville fungert best?

## Oppgave 

[Pr√∏v denne regresjonen p√• nytt](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n√∏yaktighet, vennligst v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.