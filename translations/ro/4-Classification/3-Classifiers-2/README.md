<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49047911108adc49d605cddfb455749c",
  "translation_date": "2025-09-05T16:24:14+00:00",
  "source_file": "4-Classification/3-Classifiers-2/README.md",
  "language_code": "ro"
}
-->
# Clasificatori culinari 2

Ãn aceastÄƒ a doua lecÈ›ie despre clasificare, vei explora mai multe modalitÄƒÈ›i de a clasifica date numerice. De asemenea, vei Ã®nvÄƒÈ›a despre implicaÈ›iile alegerii unui clasificator Ã®n detrimentul altuia.

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ml/)

### CerinÈ›e preliminare

Presupunem cÄƒ ai finalizat lecÈ›iile anterioare È™i ai un set de date curÄƒÈ›at Ã®n folderul `data`, numit _cleaned_cuisines.csv_, Ã®n rÄƒdÄƒcina acestui folder cu 4 lecÈ›ii.

### PregÄƒtire

Am Ã®ncÄƒrcat fiÈ™ierul tÄƒu _notebook.ipynb_ cu setul de date curÄƒÈ›at È™i l-am Ã®mpÄƒrÈ›it Ã®n cadre de date X È™i y, pregÄƒtite pentru procesul de construire a modelului.

## O hartÄƒ a clasificÄƒrii

Anterior, ai Ã®nvÄƒÈ›at despre diversele opÈ›iuni pe care le ai atunci cÃ¢nd clasifici date folosind fiÈ™a de ajutor de la Microsoft. Scikit-learn oferÄƒ o fiÈ™Äƒ similarÄƒ, dar mai detaliatÄƒ, care te poate ajuta sÄƒ restrÃ¢ngi È™i mai mult alegerea estimatoarelor (un alt termen pentru clasificatori):

![Harta ML de la Scikit-learn](../../../../4-Classification/3-Classifiers-2/images/map.png)
> Sfat: [viziteazÄƒ aceastÄƒ hartÄƒ online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) È™i exploreazÄƒ cÄƒile pentru a citi documentaÈ›ia.

### Planul

AceastÄƒ hartÄƒ este foarte utilÄƒ odatÄƒ ce ai o Ã®nÈ›elegere clarÄƒ a datelor tale, deoarece poÈ›i â€parcurgeâ€ cÄƒile pentru a lua o decizie:

- Avem >50 de mostre
- Vrem sÄƒ prezicem o categorie
- Avem date etichetate
- Avem mai puÈ›in de 100K mostre
- âœ¨ Putem alege un Linear SVC
- DacÄƒ acest lucru nu funcÈ›ioneazÄƒ, deoarece avem date numerice
    - Putem Ã®ncerca un âœ¨ KNeighbors Classifier 
      - DacÄƒ nici acesta nu funcÈ›ioneazÄƒ, Ã®ncercÄƒm âœ¨ SVC È™i âœ¨ Ensemble Classifiers

Aceasta este o cale foarte utilÄƒ de urmat.

## ExerciÈ›iu - Ã®mparte datele

UrmÃ¢nd aceastÄƒ cale, ar trebui sÄƒ Ã®ncepem prin a importa cÃ¢teva biblioteci necesare.

1. ImportÄƒ bibliotecile necesare:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Ãmparte datele de antrenament È™i test:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
    ```

## Clasificator Linear SVC

Support-Vector Clustering (SVC) face parte din familia tehnicilor ML Support-Vector Machines (aflÄƒ mai multe despre acestea mai jos). Ãn aceastÄƒ metodÄƒ, poÈ›i alege un â€kernelâ€ pentru a decide cum sÄƒ grupezi etichetele. Parametrul 'C' se referÄƒ la 'regularizare', care regleazÄƒ influenÈ›a parametrilor. Kernel-ul poate fi unul dintre [mai multe](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); aici Ã®l setÄƒm la 'linear' pentru a ne asigura cÄƒ folosim Linear SVC. Probabilitatea este implicit 'false'; aici o setÄƒm la 'true' pentru a obÈ›ine estimÄƒri de probabilitate. SetÄƒm starea aleatorie la '0' pentru a amesteca datele È™i a obÈ›ine probabilitÄƒÈ›i.

### ExerciÈ›iu - aplicÄƒ un Linear SVC

Ãncepe prin a crea un array de clasificatori. Vei adÄƒuga progresiv la acest array pe mÄƒsurÄƒ ce testÄƒm.

1. Ãncepe cu un Linear SVC:

    ```python
    C = 10
    # Create different classifiers.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. AntreneazÄƒ modelul folosind Linear SVC È™i afiÈ™eazÄƒ un raport:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Rezultatul este destul de bun:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## Clasificator K-Neighbors

K-Neighbors face parte din familia â€neighborsâ€ a metodelor ML, care pot fi utilizate atÃ¢t pentru Ã®nvÄƒÈ›are supravegheatÄƒ, cÃ¢t È™i nesupravegheatÄƒ. Ãn aceastÄƒ metodÄƒ, se creeazÄƒ un numÄƒr predefinit de puncte, iar datele sunt grupate Ã®n jurul acestor puncte astfel Ã®ncÃ¢t sÄƒ se poatÄƒ prezice etichete generalizate pentru date.

### ExerciÈ›iu - aplicÄƒ clasificatorul K-Neighbors

Clasificatorul anterior a fost bun È™i a funcÈ›ionat bine cu datele, dar poate putem obÈ›ine o acurateÈ›e mai bunÄƒ. ÃncearcÄƒ un clasificator K-Neighbors.

1. AdaugÄƒ o linie Ã®n array-ul de clasificatori (adaugÄƒ o virgulÄƒ dupÄƒ elementul Linear SVC):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Rezultatul este puÈ›in mai slab:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    âœ… AflÄƒ mai multe despre [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Clasificator Support Vector

Clasificatorii Support-Vector fac parte din familia [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) a metodelor ML utilizate pentru sarcini de clasificare È™i regresie. SVM-urile â€mapeazÄƒ exemplele de antrenament Ã®n puncte din spaÈ›iuâ€ pentru a maximiza distanÈ›a dintre douÄƒ categorii. Datele ulterioare sunt mapate Ã®n acest spaÈ›iu astfel Ã®ncÃ¢t categoria lor sÄƒ poatÄƒ fi prezisÄƒ.

### ExerciÈ›iu - aplicÄƒ un Support Vector Classifier

SÄƒ Ã®ncercÄƒm sÄƒ obÈ›inem o acurateÈ›e puÈ›in mai bunÄƒ cu un Support Vector Classifier.

1. AdaugÄƒ o virgulÄƒ dupÄƒ elementul K-Neighbors, apoi adaugÄƒ aceastÄƒ linie:

    ```python
    'SVC': SVC(),
    ```

    Rezultatul este destul de bun!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    âœ… AflÄƒ mai multe despre [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Clasificatori Ensemble

SÄƒ urmÄƒm calea pÃ¢nÄƒ la capÄƒt, chiar dacÄƒ testul anterior a fost destul de bun. SÄƒ Ã®ncercÄƒm cÃ¢È›iva 'Clasificatori Ensemble', Ã®n special Random Forest È™i AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Rezultatul este foarte bun, mai ales pentru Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

âœ… AflÄƒ mai multe despre [Clasificatori Ensemble](https://scikit-learn.org/stable/modules/ensemble.html)

AceastÄƒ metodÄƒ de Ã®nvÄƒÈ›are automatÄƒ â€combinÄƒ predicÈ›iile mai multor estimatori de bazÄƒâ€ pentru a Ã®mbunÄƒtÄƒÈ›i calitatea modelului. Ãn exemplul nostru, am folosit Random Trees È™i AdaBoost. 

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), o metodÄƒ de mediere, construieÈ™te o â€pÄƒdureâ€ de â€arbori de decizieâ€ infuzaÈ›i cu aleatoriu pentru a evita supraÃ®nvÄƒÈ›area. Parametrul n_estimators este setat la numÄƒrul de arbori.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) ajusteazÄƒ un clasificator pe un set de date È™i apoi ajusteazÄƒ copii ale acelui clasificator pe acelaÈ™i set de date. Se concentreazÄƒ pe greutÄƒÈ›ile elementelor clasificate incorect È™i ajusteazÄƒ potrivirea pentru urmÄƒtorul clasificator pentru a corecta.

---

## ğŸš€Provocare

Fiecare dintre aceste tehnici are un numÄƒr mare de parametri pe care Ã®i poÈ›i ajusta. CerceteazÄƒ parametrii impliciÈ›i ai fiecÄƒrei metode È™i gÃ¢ndeÈ™te-te ce ar Ã®nsemna ajustarea acestor parametri pentru calitatea modelului.

## [Chestionar dupÄƒ lecÈ›ie](https://ff-quizzes.netlify.app/en/ml/)

## Recapitulare È™i studiu individual

ExistÄƒ mulÈ›i termeni tehnici Ã®n aceste lecÈ›ii, aÈ™a cÄƒ ia-È›i un moment pentru a revizui [aceastÄƒ listÄƒ](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) de terminologie utilÄƒ!

## TemÄƒ 

[JoacÄƒ-te cu parametrii](assignment.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa maternÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.