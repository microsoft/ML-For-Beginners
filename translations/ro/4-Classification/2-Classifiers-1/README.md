<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T16:19:16+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "ro"
}
-->
# Clasificatori de bucÄƒtÄƒrie 1

Ãn aceastÄƒ lecÈ›ie, vei folosi setul de date salvat din lecÈ›ia anterioarÄƒ, plin de date echilibrate È™i curate despre bucÄƒtÄƒrii.

Vei utiliza acest set de date cu o varietate de clasificatori pentru a _prezice o bucÄƒtÄƒrie naÈ›ionalÄƒ datÄƒ pe baza unui grup de ingrediente_. Ãn timp ce faci acest lucru, vei Ã®nvÄƒÈ›a mai multe despre unele dintre modurile Ã®n care algoritmii pot fi utilizaÈ›i pentru sarcini de clasificare.

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ml/)
# PregÄƒtire

PresupunÃ¢nd cÄƒ ai finalizat [LecÈ›ia 1](../1-Introduction/README.md), asigurÄƒ-te cÄƒ un fiÈ™ier _cleaned_cuisines.csv_ existÄƒ Ã®n folderul rÄƒdÄƒcinÄƒ `/data` pentru aceste patru lecÈ›ii.

## ExerciÈ›iu - prezicerea unei bucÄƒtÄƒrii naÈ›ionale

1. LucrÃ¢nd Ã®n folderul _notebook.ipynb_ al acestei lecÈ›ii, importÄƒ acel fiÈ™ier Ã®mpreunÄƒ cu biblioteca Pandas:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Datele aratÄƒ astfel:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Acum, importÄƒ mai multe biblioteci:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Ãmparte coordonatele X È™i y Ã®n douÄƒ cadre de date pentru antrenament. `cuisine` poate fi cadrul de date pentru etichete:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Va arÄƒta astfel:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. EliminÄƒ coloana `Unnamed: 0` È™i coloana `cuisine`, folosind `drop()`. SalveazÄƒ restul datelor ca caracteristici antrenabile:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    Caracteristicile tale aratÄƒ astfel:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Acum eÈ™ti gata sÄƒ Ã®È›i antrenezi modelul!

## Alegerea clasificatorului

Acum cÄƒ datele tale sunt curate È™i pregÄƒtite pentru antrenament, trebuie sÄƒ decizi ce algoritm sÄƒ foloseÈ™ti pentru sarcinÄƒ. 

Scikit-learn grupeazÄƒ clasificarea sub ÃnvÄƒÈ›are SupervizatÄƒ, iar Ã®n aceastÄƒ categorie vei gÄƒsi multe moduri de a clasifica. [Varietatea](https://scikit-learn.org/stable/supervised_learning.html) poate fi copleÈ™itoare la prima vedere. Metodele urmÄƒtoare includ tehnici de clasificare:

- Modele liniare
- MaÈ™ini cu vectori de suport
- Gradient descendent stochastic
- Vecini cei mai apropiaÈ›i
- Procese Gaussiene
- Arbori de decizie
- Metode de ansamblu (clasificator prin vot)
- Algoritmi multiclasÄƒ È™i multioutput (clasificare multiclasÄƒ È™i multilabel, clasificare multiclasÄƒ-multioutput)

> PoÈ›i folosi È™i [reÈ›ele neuronale pentru a clasifica date](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), dar acest subiect este Ã®n afara scopului acestei lecÈ›ii.

### Ce clasificator sÄƒ alegi?

Deci, ce clasificator ar trebui sÄƒ alegi? Deseori, testarea mai multor clasificatori È™i cÄƒutarea unui rezultat bun este o metodÄƒ de testare. Scikit-learn oferÄƒ o [comparaÈ›ie alÄƒturatÄƒ](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) pe un set de date creat, comparÃ¢nd KNeighbors, SVC Ã®n douÄƒ moduri, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB È™i QuadraticDiscriminationAnalysis, arÄƒtÃ¢nd rezultatele vizualizate: 

![comparaÈ›ie clasificatori](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Grafice generate din documentaÈ›ia Scikit-learn

> AutoML rezolvÄƒ aceastÄƒ problemÄƒ elegant prin rularea acestor comparaÈ›ii Ã®n cloud, permiÈ›Ã¢ndu-È›i sÄƒ alegi cel mai bun algoritm pentru datele tale. ÃncearcÄƒ [aici](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### O abordare mai bunÄƒ

O metodÄƒ mai bunÄƒ decÃ¢t ghicitul aleatoriu este sÄƒ urmezi ideile din acest [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott) descÄƒrcabil. Aici descoperim cÄƒ, pentru problema noastrÄƒ multiclasÄƒ, avem cÃ¢teva opÈ›iuni:

![cheatsheet pentru probleme multiclasÄƒ](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> O secÈ›iune din Algorithm Cheat Sheet de la Microsoft, detaliind opÈ›iuni de clasificare multiclasÄƒ

âœ… DescarcÄƒ acest cheat sheet, printeazÄƒ-l È™i agaÈ›Äƒ-l pe perete!

### RaÈ›ionament

SÄƒ vedem dacÄƒ putem raÈ›iona prin diferite abordÄƒri, avÃ¢nd Ã®n vedere constrÃ¢ngerile pe care le avem:

- **ReÈ›elele neuronale sunt prea grele**. AvÃ¢nd Ã®n vedere setul nostru de date curat, dar minimal, È™i faptul cÄƒ rulÄƒm antrenamentul local prin notebook-uri, reÈ›elele neuronale sunt prea complexe pentru aceastÄƒ sarcinÄƒ.
- **Nu folosim clasificatori cu douÄƒ clase**. Nu utilizÄƒm un clasificator cu douÄƒ clase, deci excludem metoda one-vs-all. 
- **Arborele de decizie sau regresia logisticÄƒ ar putea funcÈ›iona**. Un arbore de decizie ar putea funcÈ›iona, sau regresia logisticÄƒ pentru date multiclasÄƒ. 
- **Arborii de decizie boostaÈ›i multiclasÄƒ rezolvÄƒ o altÄƒ problemÄƒ**. Arborele de decizie boostat multiclasÄƒ este cel mai potrivit pentru sarcini nonparametrice, de exemplu sarcini concepute pentru a construi clasamente, deci nu este util pentru noi.

### Utilizarea Scikit-learn 

Vom folosi Scikit-learn pentru a analiza datele noastre. TotuÈ™i, existÄƒ multe moduri de a utiliza regresia logisticÄƒ Ã®n Scikit-learn. ConsultÄƒ [parametrii de transmis](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

Practic, existÄƒ doi parametri importanÈ›i - `multi_class` È™i `solver` - pe care trebuie sÄƒ Ã®i specificÄƒm atunci cÃ¢nd cerem Scikit-learn sÄƒ efectueze o regresie logisticÄƒ. Valoarea `multi_class` aplicÄƒ un anumit comportament. Valoarea solverului indicÄƒ ce algoritm sÄƒ se foloseascÄƒ. Nu toÈ›i solverii pot fi combinaÈ›i cu toate valorile `multi_class`.

Conform documentaÈ›iei, Ã®n cazul multiclasÄƒ, algoritmul de antrenament:

- **FoloseÈ™te schema one-vs-rest (OvR)**, dacÄƒ opÈ›iunea `multi_class` este setatÄƒ la `ovr`
- **FoloseÈ™te pierderea cross-entropy**, dacÄƒ opÈ›iunea `multi_class` este setatÄƒ la `multinomial`. (Ãn prezent, opÈ›iunea `multinomial` este suportatÄƒ doar de solverii â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™ È™i â€˜newton-cgâ€™.)"

> ğŸ“ 'Schema' aici poate fi 'ovr' (one-vs-rest) sau 'multinomial'. Deoarece regresia logisticÄƒ este conceputÄƒ pentru a susÈ›ine clasificarea binarÄƒ, aceste scheme Ã®i permit sÄƒ gestioneze mai bine sarcinile de clasificare multiclasÄƒ. [sursa](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> ğŸ“ 'Solverul' este definit ca "algoritmul utilizat Ã®n problema de optimizare". [sursa](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn oferÄƒ acest tabel pentru a explica modul Ã®n care solverii gestioneazÄƒ diferite provocÄƒri prezentate de diferite structuri de date:

![solvers](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## ExerciÈ›iu - Ã®mparte datele

Ne putem concentra pe regresia logisticÄƒ pentru primul nostru test de antrenament, deoarece ai Ã®nvÄƒÈ›at recent despre aceasta Ã®ntr-o lecÈ›ie anterioarÄƒ.
Ãmparte datele tale Ã®n grupuri de antrenament È™i testare, apelÃ¢nd `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## ExerciÈ›iu - aplicÄƒ regresia logisticÄƒ

Deoarece foloseÈ™ti cazul multiclasÄƒ, trebuie sÄƒ alegi ce _schemÄƒ_ sÄƒ foloseÈ™ti È™i ce _solver_ sÄƒ setezi. FoloseÈ™te LogisticRegression cu o setare multiclasÄƒ È™i solverul **liblinear** pentru antrenament.

1. CreeazÄƒ o regresie logisticÄƒ cu `multi_class` setat la `ovr` È™i solverul setat la `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    âœ… ÃncearcÄƒ un alt solver, cum ar fi `lbfgs`, care este adesea setat ca implicit
> NotÄƒ, foloseÈ™te funcÈ›ia Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) pentru a aplatiza datele tale atunci cÃ¢nd este necesar.
AcurateÈ›ea este bunÄƒ la peste **80%**!

1. PoÈ›i vedea acest model Ã®n acÈ›iune testÃ¢nd un rÃ¢nd de date (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Rezultatul este afiÈ™at:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   âœ… ÃncearcÄƒ un alt numÄƒr de rÃ¢nd È™i verificÄƒ rezultatele

1. ExplorÃ¢nd mai profund, poÈ›i verifica acurateÈ›ea acestei predicÈ›ii:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Rezultatul este afiÈ™at - bucÄƒtÄƒria indianÄƒ este cea mai probabilÄƒ, cu o bunÄƒ probabilitate:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    âœ… PoÈ›i explica de ce modelul este destul de sigur cÄƒ aceasta este o bucÄƒtÄƒrie indianÄƒ?

1. ObÈ›ine mai multe detalii prin afiÈ™area unui raport de clasificare, aÈ™a cum ai fÄƒcut Ã®n lecÈ›iile despre regresie:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | precizie | recall | f1-score | suport |
    | ------------ | -------- | ------ | -------- | ------ |
    | chinese      | 0.73     | 0.71   | 0.72     | 229    |
    | indian       | 0.91     | 0.93   | 0.92     | 254    |
    | japanese     | 0.70     | 0.75   | 0.72     | 220    |
    | korean       | 0.86     | 0.76   | 0.81     | 242    |
    | thai         | 0.79     | 0.85   | 0.82     | 254    |
    | acurateÈ›e    | 0.80     | 1199   |          |        |
    | macro avg    | 0.80     | 0.80   | 0.80     | 1199   |
    | weighted avg | 0.80     | 0.80   | 0.80     | 1199   |

## ğŸš€Provocare

Ãn aceastÄƒ lecÈ›ie, ai folosit datele curÄƒÈ›ate pentru a construi un model de Ã®nvÄƒÈ›are automatÄƒ care poate prezice o bucÄƒtÄƒrie naÈ›ionalÄƒ pe baza unei serii de ingrediente. Ia-È›i timp sÄƒ explorezi numeroasele opÈ›iuni pe care Scikit-learn le oferÄƒ pentru clasificarea datelor. ExploreazÄƒ mai profund conceptul de 'solver' pentru a Ã®nÈ›elege ce se Ã®ntÃ¢mplÄƒ Ã®n culise.

## [Chestionar post-lecÈ›ie](https://ff-quizzes.netlify.app/en/ml/)

## Recapitulare & Studiu Individual

ExploreazÄƒ puÈ›in mai mult matematica din spatele regresiei logistice Ã®n [aceastÄƒ lecÈ›ie](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## TemÄƒ 

[StudiazÄƒ solvers](assignment.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.