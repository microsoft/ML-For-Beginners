<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T16:36:06+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "ro"
}
-->
# Introducere 칥n 칥nv캒탵area prin 칥nt캒rire

칉nv캒탵area prin 칥nt캒rire, RL, este considerat캒 unul dintre paradigmele de baz캒 ale 칥nv캒탵캒rii automate, al캒turi de 칥nv캒탵area supravegheat캒 탳i cea nesupravegheat캒. RL se concentreaz캒 pe luarea deciziilor: luarea deciziilor corecte sau cel pu탵in 칥nv캒탵area din ele.

Imagineaz캒-탵i c캒 ai un mediu simulat, cum ar fi pia탵a bursier캒. Ce se 칥nt칙mpl캒 dac캒 impui o anumit캒 reglementare? Are un efect pozitiv sau negativ? Dac캒 se 칥nt칙mpl캒 ceva negativ, trebuie s캒 iei acest _칥nt캒rire negativ캒_, s캒 칥nve탵i din ea 탳i s캒 schimbi direc탵ia. Dac캒 rezultatul este pozitiv, trebuie s캒 construie탳ti pe baza acestui _칥nt캒rire pozitiv캒_.

![peter 탳i lupul](../../../8-Reinforcement/images/peter.png)

> Peter 탳i prietenii s캒i trebuie s캒 scape de lupul fl캒m칙nd! Imagine de [Jen Looper](https://twitter.com/jenlooper)

## Subiect regional: Peter 탳i Lupul (Rusia)

[Peter 탳i Lupul](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) este o poveste muzical캒 scris캒 de compozitorul rus [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Este povestea unui t칙n캒r pionier, Peter, care se aventureaz캒 curajos din casa sa spre poiana din p캒dure pentru a urm캒ri lupul. 칉n aceast캒 sec탵iune, vom antrena algoritmi de 칥nv캒탵are automat캒 care 칥l vor ajuta pe Peter:

- **S캒 exploreze** zona 칥nconjur캒toare 탳i s캒 construiasc캒 o hart캒 optim캒 de navigare
- **S캒 칥nve탵e** cum s캒 foloseasc캒 un skateboard 탳i s캒 칥탳i men탵in캒 echilibrul pe el, pentru a se deplasa mai rapid.

[![Peter 탳i Lupul](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 游꿘 Click pe imaginea de mai sus pentru a asculta Peter 탳i Lupul de Prokofiev

## 칉nv캒탵area prin 칥nt캒rire

칉n sec탵iunile anterioare, ai v캒zut dou캒 exemple de probleme de 칥nv캒탵are automat캒:

- **Supravegheat캒**, unde avem seturi de date care sugereaz캒 solu탵ii de exemplu pentru problema pe care dorim s캒 o rezolv캒m. [Clasificarea](../4-Classification/README.md) 탳i [regresia](../2-Regression/README.md) sunt sarcini de 칥nv캒탵are supravegheat캒.
- **Nesupravegheat캒**, 칥n care nu avem date de antrenament etichetate. Principalul exemplu de 칥nv캒탵are nesupravegheat캒 este [Clustering](../5-Clustering/README.md).

칉n aceast캒 sec탵iune, 칥탵i vom prezenta un nou tip de problem캒 de 칥nv캒탵are care nu necesit캒 date de antrenament etichetate. Exist캒 mai multe tipuri de astfel de probleme:

- **[칉nv캒탵are semi-supravegheat캒](https://wikipedia.org/wiki/Semi-supervised_learning)**, unde avem o cantitate mare de date neetichetate care pot fi folosite pentru pre-antrenarea modelului.
- **[칉nv캒탵are prin 칥nt캒rire](https://wikipedia.org/wiki/Reinforcement_learning)**, 칥n care un agent 칥nva탵캒 cum s캒 se comporte prin efectuarea de experimente 칥ntr-un mediu simulat.

### Exemplu - joc pe calculator

S캒 presupunem c캒 vrei s캒 칥nve탵i un calculator s캒 joace un joc, cum ar fi 탳ahul sau [Super Mario](https://wikipedia.org/wiki/Super_Mario). Pentru ca calculatorul s캒 joace un joc, trebuie s캒 prezic캒 ce mi탳care s캒 fac캒 칥n fiecare dintre st캒rile jocului. De탳i acest lucru poate p캒rea o problem캒 de clasificare, nu este - deoarece nu avem un set de date cu st캒ri 탳i ac탵iuni corespunz캒toare. De탳i putem avea unele date, cum ar fi meciuri de 탳ah existente sau 칥nregistr캒ri ale juc캒torilor care joac캒 Super Mario, este probabil ca aceste date s캒 nu acopere suficient de bine un num캒r mare de st캒ri posibile.

칉n loc s캒 c캒ut캒m date existente despre joc, **칉nv캒탵area prin 칥nt캒rire** (RL) se bazeaz캒 pe ideea de *a face calculatorul s캒 joace* de multe ori 탳i s캒 observe rezultatul. Astfel, pentru a aplica 칉nv캒탵area prin 칥nt캒rire, avem nevoie de dou캒 lucruri:

- **Un mediu** 탳i **un simulator** care ne permit s캒 juc캒m un joc de multe ori. Acest simulator ar defini toate regulile jocului, precum 탳i st캒rile 탳i ac탵iunile posibile.

- **O func탵ie de recompens캒**, care ne-ar spune c칙t de bine ne-am descurcat 칥n timpul fiec캒rei mi탳c캒ri sau joc.

Principala diferen탵캒 칥ntre alte tipuri de 칥nv캒탵are automat캒 탳i RL este c캒 칥n RL, de obicei, nu 탳tim dac캒 c칙탳tig캒m sau pierdem p칙n캒 nu termin캒m jocul. Astfel, nu putem spune dac캒 o anumit캒 mi탳care este bun캒 sau nu - primim o recompens캒 doar la sf칙r탳itul jocului. Iar scopul nostru este s캒 proiect캒m algoritmi care s캒 ne permit캒 s캒 antren캒m un model 칥n condi탵ii de incertitudine. Vom 칥nv캒탵a despre un algoritm RL numit **Q-learning**.

## Lec탵ii

1. [Introducere 칥n 칥nv캒탵area prin 칥nt캒rire 탳i Q-Learning](1-QLearning/README.md)
2. [Utilizarea unui mediu de simulare gym](2-Gym/README.md)

## Credite

"Introducere 칥n 칉nv캒탵area prin 칉nt캒rire" a fost scris캒 cu 鮫봺잺 de [Dmitry Soshnikov](http://soshnikov.com)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 re탵ine탵i c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa natal캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea profesional캒 realizat캒 de un specialist uman. Nu ne asum캒m responsabilitatea pentru eventualele ne칥n탵elegeri sau interpret캒ri gre탳ite care pot ap캒rea din utilizarea acestei traduceri.