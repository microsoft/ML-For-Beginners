<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T16:41:50+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "ro"
}
-->
# Introducere 칥n 칉nv캒탵area prin Recompens캒 탳i Q-Learning

![Rezumat al 칥nv캒탵캒rii prin recompens캒 칥n machine learning 칥ntr-un sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote de [Tomomi Imura](https://www.twitter.com/girlie_mac)

칉nv캒탵area prin recompens캒 implic캒 trei concepte importante: agentul, c칙teva st캒ri 탳i un set de ac탵iuni pentru fiecare stare. Prin executarea unei ac탵iuni 칥ntr-o stare specificat캒, agentul prime탳te o recompens캒. Imagineaz캒 din nou jocul pe calculator Super Mario. Tu e탳ti Mario, te afli 칥ntr-un nivel de joc, st칙nd l칙ng캒 marginea unei pr캒p캒stii. Deasupra ta este o moned캒. Tu, fiind Mario, 칥ntr-un nivel de joc, 칥ntr-o pozi탵ie specific캒... aceasta este starea ta. Dac캒 faci un pas spre dreapta (o ac탵iune), vei c캒dea 칥n pr캒pastie, ceea ce 칥탵i va aduce un scor numeric sc캒zut. Totu탳i, dac캒 ape탳i butonul de s캒ritur캒, vei ob탵ine un punct 탳i vei r캒m칙ne 칥n via탵캒. Acesta este un rezultat pozitiv 탳i ar trebui s캒 칥탵i aduc캒 un scor numeric pozitiv.

Folosind 칥nv캒탵area prin recompens캒 탳i un simulator (jocul), po탵i 칥nv캒탵a cum s캒 joci pentru a maximiza recompensa, adic캒 s캒 r캒m칙i 칥n via탵캒 탳i s캒 ob탵ii c칙t mai multe puncte.

[![Introducere 칥n 칉nv캒탵area prin Recompens캒](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 游꿘 Click pe imaginea de mai sus pentru a-l asculta pe Dmitry discut칙nd despre 칉nv캒탵area prin Recompens캒

## [Chestionar 칥nainte de lec탵ie](https://ff-quizzes.netlify.app/en/ml/)

## Cerin탵e preliminare 탳i configurare

칉n aceast캒 lec탵ie, vom experimenta cu ceva cod 칥n Python. Ar trebui s캒 po탵i rula codul din Jupyter Notebook din aceast캒 lec탵ie, fie pe computerul t캒u, fie undeva 칥n cloud.

Po탵i deschide [notebook-ul lec탵iei](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) 탳i s캒 parcurgi lec탵ia pentru a construi.

> **Not캒:** Dac캒 deschizi acest cod din cloud, trebuie s캒 ob탵ii 탳i fi탳ierul [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), care este utilizat 칥n codul notebook-ului. Adaug캒-l 칥n acela탳i director ca notebook-ul.

## Introducere

칉n aceast캒 lec탵ie, vom explora lumea **[Petru 탳i Lupul](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirat캒 de un basm muzical al compozitorului rus [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Vom folosi **칉nv캒탵area prin Recompens캒** pentru a-l l캒sa pe Petru s캒-탳i exploreze mediul, s캒 colecteze mere gustoase 탳i s캒 evite 칥nt칙lnirea cu lupul.

**칉nv캒탵area prin Recompens캒** (RL) este o tehnic캒 de 칥nv캒탵are care ne permite s캒 칥nv캒탵캒m un comportament optim al unui **agent** 칥ntr-un **mediu** prin efectuarea multor experimente. Un agent 칥n acest mediu ar trebui s캒 aib캒 un **scop**, definit de o **func탵ie de recompens캒**.

## Mediul

Pentru simplitate, s캒 consider캒m lumea lui Petru ca o tabl캒 p캒trat캒 de dimensiune `width` x `height`, ca aceasta:

![Mediul lui Petru](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Fiecare celul캒 din aceast캒 tabl캒 poate fi:

* **teren**, pe care Petru 탳i alte creaturi pot merge.
* **ap캒**, pe care evident nu po탵i merge.
* un **copac** sau **iarb캒**, un loc unde te po탵i odihni.
* un **m캒r**, care reprezint캒 ceva ce Petru ar fi bucuros s캒 g캒seasc캒 pentru a se hr캒ni.
* un **lup**, care este periculos 탳i ar trebui evitat.

Exist캒 un modul Python separat, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), care con탵ine codul pentru a lucra cu acest mediu. Deoarece acest cod nu este important pentru 칥n탵elegerea conceptelor noastre, vom importa modulul 탳i 칥l vom folosi pentru a crea tabla exemplu (bloc de cod 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Acest cod ar trebui s캒 imprime o imagine a mediului similar캒 cu cea de mai sus.

## Ac탵iuni 탳i politic캒

칉n exemplul nostru, scopul lui Petru ar fi s캒 g캒seasc캒 un m캒r, evit칙nd lupul 탳i alte obstacole. Pentru a face acest lucru, el poate, 칥n esen탵캒, s캒 se plimbe p칙n캒 g캒se탳te un m캒r.

Astfel, 칥n orice pozi탵ie, el poate alege 칥ntre una dintre urm캒toarele ac탵iuni: sus, jos, st칙nga 탳i dreapta.

Vom defini aceste ac탵iuni ca un dic탵ionar 탳i le vom mapa la perechi de modific캒ri corespunz캒toare ale coordonatelor. De exemplu, deplasarea spre dreapta (`R`) ar corespunde perechii `(1,0)`. (bloc de cod 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Pentru a rezuma, strategia 탳i scopul acestui scenariu sunt urm캒toarele:

- **Strategia** agentului nostru (Petru) este definit캒 de o a탳a-numit캒 **politic캒**. O politic캒 este o func탵ie care returneaz캒 ac탵iunea 칥ntr-o stare dat캒. 칉n cazul nostru, starea problemei este reprezentat캒 de tabl캒, inclusiv pozi탵ia curent캒 a juc캒torului.

- **Scopul** 칥nv캒탵캒rii prin recompens캒 este s캒 칥nv캒탵캒m 칥n cele din urm캒 o politic캒 bun캒 care ne va permite s캒 rezolv캒m problema eficient. Totu탳i, ca baz캒, s캒 consider캒m cea mai simpl캒 politic캒 numit캒 **plimbare aleatorie**.

## Plimbare aleatorie

S캒 rezolv캒m mai 칥nt칙i problema noastr캒 implement칙nd o strategie de plimbare aleatorie. Cu plimbarea aleatorie, vom alege aleator urm캒toarea ac탵iune din ac탵iunile permise, p칙n캒 ajungem la m캒r (bloc de cod 3).

1. Implementeaz캒 plimbarea aleatorie cu codul de mai jos:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Apelul la `walk` ar trebui s캒 returneze lungimea traseului corespunz캒tor, care poate varia de la o rulare la alta.

1. Ruleaz캒 experimentul de plimbare de mai multe ori (s캒 zicem, 100) 탳i imprim캒 statisticile rezultate (bloc de cod 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Observ캒 c캒 lungimea medie a unui traseu este 칥n jur de 30-40 de pa탳i, ceea ce este destul de mult, av칙nd 칥n vedere c캒 distan탵a medie p칙n캒 la cel mai apropiat m캒r este 칥n jur de 5-6 pa탳i.

    Po탵i vedea 탳i cum arat캒 mi탳carea lui Petru 칥n timpul plimb캒rii aleatorii:

    ![Plimbarea aleatorie a lui Petru](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Func탵ia de recompens캒

Pentru a face politica noastr캒 mai inteligent캒, trebuie s캒 칥n탵elegem care mi탳c캒ri sunt "mai bune" dec칙t altele. Pentru a face acest lucru, trebuie s캒 definim scopul nostru.

Scopul poate fi definit 칥n termeni de o **func탵ie de recompens캒**, care va returna o valoare de scor pentru fiecare stare. Cu c칙t num캒rul este mai mare, cu at칙t func탵ia de recompens캒 este mai bun캒. (bloc de cod 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Un lucru interesant despre func탵iile de recompens캒 este c캒, 칥n cele mai multe cazuri, *primim o recompens캒 substan탵ial캒 doar la sf칙r탳itul jocului*. Aceasta 칥nseamn캒 c캒 algoritmul nostru ar trebui cumva s캒-탳i aminteasc캒 "pa탳ii buni" care duc la o recompens캒 pozitiv캒 la final 탳i s캒 le creasc캒 importan탵a. 칉n mod similar, toate mi탳c캒rile care duc la rezultate proaste ar trebui descurajate.

## Q-Learning

Un algoritm pe care 칥l vom discuta aici se nume탳te **Q-Learning**. 칉n acest algoritm, politica este definit캒 de o func탵ie (sau o structur캒 de date) numit캒 **Q-Table**. Aceasta 칥nregistreaz캒 "calitatea" fiec캒rei ac탵iuni 칥ntr-o stare dat캒.

Se nume탳te Q-Table deoarece este adesea convenabil s캒 o reprezent캒m ca o tabel캒 sau un array multidimensional. Deoarece tabla noastr캒 are dimensiuni `width` x `height`, putem reprezenta Q-Table folosind un array numpy cu forma `width` x `height` x `len(actions)`: (bloc de cod 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Observ캒 c캒 ini탵ializ캒m toate valorile din Q-Table cu o valoare egal캒, 칥n cazul nostru - 0.25. Aceasta corespunde politicii de "plimbare aleatorie", deoarece toate mi탳c캒rile din fiecare stare sunt la fel de bune. Putem transmite Q-Table func탵iei `plot` pentru a vizualiza tabela pe tabl캒: `m.plot(Q)`.

![Mediul lui Petru](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

칉n centrul fiec캒rei celule exist캒 o "s캒geat캒" care indic캒 direc탵ia preferat캒 de mi탳care. Deoarece toate direc탵iile sunt egale, se afi탳eaz캒 un punct.

Acum trebuie s캒 rul캒m simularea, s캒 explor캒m mediul nostru 탳i s캒 칥nv캒탵캒m o distribu탵ie mai bun캒 a valorilor din Q-Table, care ne va permite s캒 g캒sim drumul c캒tre m캒r mult mai rapid.

## Esen탵a Q-Learning: Ecua탵ia Bellman

Odat캒 ce 칥ncepem s캒 ne mi탳c캒m, fiecare ac탵iune va avea o recompens캒 corespunz캒toare, adic캒 teoretic putem selecta urm캒toarea ac탵iune pe baza celei mai mari recompense imediate. Totu탳i, 칥n cele mai multe st캒ri, mi탳carea nu va atinge scopul nostru de a ajunge la m캒r 탳i, astfel, nu putem decide imediat care direc탵ie este mai bun캒.

> Aminte탳te-탵i c캒 nu rezultatul imediat conteaz캒, ci mai degrab캒 rezultatul final, pe care 칥l vom ob탵ine la sf칙r탳itul simul캒rii.

Pentru a 탵ine cont de aceast캒 recompens캒 칥nt칙rziat캒, trebuie s캒 folosim principiile **[program캒rii dinamice](https://en.wikipedia.org/wiki/Dynamic_programming)**, care ne permit s캒 g칙ndim problema noastr캒 recursiv.

S캒 presupunem c캒 acum ne afl캒m 칥n starea *s* 탳i vrem s캒 ne mut캒m 칥n urm캒toarea stare *s'*. Prin aceasta, vom primi recompensa imediat캒 *r(s,a)*, definit캒 de func탵ia de recompens캒, plus o recompens캒 viitoare. Dac캒 presupunem c캒 Q-Table reflect캒 corect "atractivitatea" fiec캒rei ac탵iuni, atunci 칥n starea *s'* vom alege o ac탵iune *a'* care corespunde valorii maxime a *Q(s',a')*. Astfel, cea mai bun캒 recompens캒 viitoare posibil캒 pe care am putea s캒 o ob탵inem 칥n starea *s* va fi definit캒 ca `max`

## Verificarea politicii

Deoarece Q-Table listeaz캒 "atractivitatea" fiec캒rei ac탵iuni 칥n fiecare stare, este destul de simplu s캒 o folosim pentru a defini navigarea eficient캒 칥n lumea noastr캒. 칉n cel mai simplu caz, putem selecta ac탵iunea corespunz캒toare valorii maxime din Q-Table: (bloc de cod 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Dac캒 칥ncerca탵i codul de mai sus de mai multe ori, este posibil s캒 observa탵i c캒 uneori "se blocheaz캒" 탳i trebuie s캒 ap캒sa탵i butonul STOP din notebook pentru a-l 칥ntrerupe. Acest lucru se 칥nt칙mpl캒 deoarece pot exista situa탵ii 칥n care dou캒 st캒ri "indic캒" una c캒tre cealalt캒 칥n termeni de valoare optim캒 Q, caz 칥n care agentul ajunge s캒 se mi탳te 칥ntre aceste st캒ri la nesf칙r탳it.

## 游Provocare

> **Sarcina 1:** Modifica탵i func탵ia `walk` pentru a limita lungimea maxim캒 a traseului la un anumit num캒r de pa탳i (de exemplu, 100) 탳i observa탵i cum codul de mai sus returneaz캒 aceast캒 valoare din c칙nd 칥n c칙nd.

> **Sarcina 2:** Modifica탵i func탵ia `walk` astfel 칥nc칙t s캒 nu se 칥ntoarc캒 칥n locurile 칥n care a fost deja anterior. Acest lucru va preveni ca `walk` s캒 intre 칥ntr-un ciclu, 칥ns캒 agentul poate ajunge totu탳i s캒 fie "blocat" 칥ntr-o loca탵ie din care nu poate sc캒pa.

## Navigare

O politic캒 de navigare mai bun캒 ar fi cea pe care am folosit-o 칥n timpul antrenamentului, care combin캒 exploatarea 탳i explorarea. 칉n aceast캒 politic캒, vom selecta fiecare ac탵iune cu o anumit캒 probabilitate, propor탵ional캒 cu valorile din Q-Table. Aceast캒 strategie poate duce totu탳i la revenirea agentului 칥ntr-o pozi탵ie pe care a explorat-o deja, dar, dup캒 cum pute탵i vedea din codul de mai jos, rezult캒 칥ntr-un traseu mediu foarte scurt c캒tre loca탵ia dorit캒 (aminti탵i-v캒 c캒 `print_statistics` ruleaz캒 simularea de 100 de ori): (bloc de cod 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Dup캒 rularea acestui cod, ar trebui s캒 ob탵ine탵i o lungime medie a traseului mult mai mic캒 dec칙t 칥nainte, 칥n intervalul 3-6.

## Investigarea procesului de 칥nv캒탵are

A탳a cum am men탵ionat, procesul de 칥nv캒탵are este un echilibru 칥ntre explorarea 탳i exploatarea cuno탳tin탵elor dob칙ndite despre structura spa탵iului problemei. Am observat c캒 rezultatele 칥nv캒탵캒rii (capacitatea de a ajuta un agent s캒 g캒seasc캒 un traseu scurt c캒tre obiectiv) s-au 칥mbun캒t캒탵it, dar este, de asemenea, interesant s캒 observ캒m cum se comport캒 lungimea medie a traseului 칥n timpul procesului de 칥nv캒탵are:

## Rezumarea 칥nv캒탵캒rii

- **Lungimea medie a traseului cre탳te**. Ce vedem aici este c캒, la 칥nceput, lungimea medie a traseului cre탳te. Acest lucru se datoreaz캒 probabil faptului c캒, atunci c칙nd nu 탳tim nimic despre mediu, avem tendin탵a s캒 fim prin탳i 칥n st캒ri nefavorabile, cum ar fi ap캒 sau lup. Pe m캒sur캒 ce 칥nv캒탵캒m mai multe 탳i 칥ncepem s캒 folosim aceste cuno탳tin탵e, putem explora mediul mai mult, dar 칥nc캒 nu 탳tim foarte bine unde sunt merele.

- **Lungimea traseului scade, pe m캒sur캒 ce 칥nv캒탵캒m mai mult**. Odat캒 ce 칥nv캒탵캒m suficient, devine mai u탳or pentru agent s캒 ating캒 obiectivul, iar lungimea traseului 칥ncepe s캒 scad캒. Totu탳i, suntem 칥nc캒 deschi탳i la explorare, a탳a c캒 deseori ne abatem de la cel mai bun traseu 탳i explor캒m op탵iuni noi, ceea ce face traseul mai lung dec칙t optimul.

- **Cre탳tere brusc캒 a lungimii**. Ce mai observ캒m pe acest grafic este c캒, la un moment dat, lungimea a crescut brusc. Acest lucru indic캒 natura stochastic캒 a procesului 탳i faptul c캒, la un moment dat, putem "strica" coeficien탵ii din Q-Table prin suprascrierea lor cu valori noi. Acest lucru ar trebui minimizat ideal prin reducerea ratei de 칥nv캒탵are (de exemplu, spre sf칙r탳itul antrenamentului, ajust캒m valorile din Q-Table doar cu o valoare mic캒).

칉n general, este important s캒 ne amintim c캒 succesul 탳i calitatea procesului de 칥nv캒탵are depind semnificativ de parametri, cum ar fi rata de 칥nv캒탵are, reducerea ratei de 칥nv캒탵are 탳i factorul de discount. Ace탳tia sunt adesea numi탵i **hiperparametri**, pentru a-i distinge de **parametri**, pe care 칥i optimiz캒m 칥n timpul antrenamentului (de exemplu, coeficien탵ii din Q-Table). Procesul de g캒sire a celor mai bune valori pentru hiperparametri se nume탳te **optimizarea hiperparametrilor** 탳i merit캒 o discu탵ie separat캒.

## [Test de verificare post-lectur캒](https://ff-quizzes.netlify.app/en/ml/)

## Tem캒 
[O lume mai realist캒](assignment.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). De탳i ne str캒duim s캒 asigur캒m acurate탵ea, v캒 rug캒m s캒 fi탵i con탳tien탵i c캒 traducerile automate pot con탵ine erori sau inexactit캒탵i. Documentul original 칥n limba sa natal캒 ar trebui considerat sursa autoritar캒. Pentru informa탵ii critice, se recomand캒 traducerea profesional캒 realizat캒 de un specialist uman. Nu ne asum캒m responsabilitatea pentru eventualele ne칥n탵elegeri sau interpret캒ri gre탳ite care pot ap캒rea din utilizarea acestei traduceri.