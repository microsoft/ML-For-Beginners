<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T16:47:06+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "ro"
}
-->
## CerinÈ›e preliminare

Ãn aceastÄƒ lecÈ›ie, vom folosi o bibliotecÄƒ numitÄƒ **OpenAI Gym** pentru a simula diferite **medii**. PoÈ›i rula codul lecÈ›iei local (de exemplu, din Visual Studio Code), caz Ã®n care simularea se va deschide Ã®ntr-o fereastrÄƒ nouÄƒ. DacÄƒ rulezi codul online, poate fi necesar sÄƒ faci unele ajustÄƒri, aÈ™a cum este descris [aici](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Ãn lecÈ›ia anterioarÄƒ, regulile jocului È™i starea erau definite de clasa `Board`, pe care am creat-o noi. Aici vom folosi un **mediu de simulare** special, care va simula fizica din spatele balansÄƒrii unui stÃ¢lp. Unul dintre cele mai populare medii de simulare pentru antrenarea algoritmilor de Ã®nvÄƒÈ›are prin Ã®ntÄƒrire se numeÈ™te [Gym](https://gym.openai.com/), care este Ã®ntreÈ›inut de [OpenAI](https://openai.com/). Folosind acest Gym, putem crea diverse **medii**, de la simulÄƒri de tip cartpole pÃ¢nÄƒ la jocuri Atari.

> **NotÄƒ**: PoÈ›i vedea alte medii disponibile Ã®n OpenAI Gym [aici](https://gym.openai.com/envs/#classic_control).

Mai Ã®ntÃ¢i, sÄƒ instalÄƒm Gym È™i sÄƒ importÄƒm bibliotecile necesare (bloc de cod 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## ExerciÈ›iu - iniÈ›ializeazÄƒ un mediu cartpole

Pentru a lucra cu problema balansÄƒrii cartpole, trebuie sÄƒ iniÈ›ializÄƒm mediul corespunzÄƒtor. Fiecare mediu este asociat cu:

- **SpaÈ›iul de observaÈ›ie**, care defineÈ™te structura informaÈ›iilor pe care le primim de la mediu. Pentru problema cartpole, primim poziÈ›ia stÃ¢lpului, viteza È™i alte valori.

- **SpaÈ›iul de acÈ›iune**, care defineÈ™te acÈ›iunile posibile. Ãn cazul nostru, spaÈ›iul de acÈ›iune este discret È™i constÄƒ din douÄƒ acÈ›iuni - **stÃ¢nga** È™i **dreapta**. (bloc de cod 2)

1. Pentru a iniÈ›ializa, tasteazÄƒ urmÄƒtorul cod:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Pentru a vedea cum funcÈ›ioneazÄƒ mediul, sÄƒ rulÄƒm o simulare scurtÄƒ de 100 de paÈ™i. La fiecare pas, oferim una dintre acÈ›iunile care trebuie luate - Ã®n aceastÄƒ simulare selectÄƒm aleatoriu o acÈ›iune din `action_space`.

1. RuleazÄƒ codul de mai jos È™i vezi ce rezultÄƒ.

    âœ… AminteÈ™te-È›i cÄƒ este preferabil sÄƒ rulezi acest cod pe o instalare localÄƒ de Python! (bloc de cod 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Ar trebui sÄƒ vezi ceva similar cu aceastÄƒ imagine:

    ![cartpole fÄƒrÄƒ balans](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Ãn timpul simulÄƒrii, trebuie sÄƒ obÈ›inem observaÈ›ii pentru a decide cum sÄƒ acÈ›ionÄƒm. De fapt, funcÈ›ia step returneazÄƒ observaÈ›iile curente, o funcÈ›ie de recompensÄƒ È™i un indicator `done` care aratÄƒ dacÄƒ are sens sÄƒ continuÄƒm simularea sau nu: (bloc de cod 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Vei vedea ceva asemÄƒnÄƒtor Ã®n output-ul notebook-ului:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Vectorul de observaÈ›ie returnat la fiecare pas al simulÄƒrii conÈ›ine urmÄƒtoarele valori:
    - PoziÈ›ia cÄƒruciorului
    - Viteza cÄƒruciorului
    - Unghiul stÃ¢lpului
    - Rata de rotaÈ›ie a stÃ¢lpului

1. ObÈ›ine valoarea minimÄƒ È™i maximÄƒ a acestor numere: (bloc de cod 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    De asemenea, vei observa cÄƒ valoarea recompensei la fiecare pas al simulÄƒrii este Ã®ntotdeauna 1. Acest lucru se datoreazÄƒ faptului cÄƒ scopul nostru este sÄƒ supravieÈ›uim cÃ¢t mai mult timp posibil, adicÄƒ sÄƒ menÈ›inem stÃ¢lpul Ã®ntr-o poziÈ›ie rezonabil verticalÄƒ pentru cea mai lungÄƒ perioadÄƒ de timp.

    âœ… De fapt, simularea CartPole este consideratÄƒ rezolvatÄƒ dacÄƒ reuÈ™im sÄƒ obÈ›inem o recompensÄƒ medie de 195 pe parcursul a 100 de Ã®ncercÄƒri consecutive.

## Discretizarea stÄƒrii

Ãn Q-Learning, trebuie sÄƒ construim un Q-Table care defineÈ™te ce sÄƒ facem Ã®n fiecare stare. Pentru a face acest lucru, starea trebuie sÄƒ fie **discretÄƒ**, mai precis, trebuie sÄƒ conÈ›inÄƒ un numÄƒr finit de valori discrete. Astfel, trebuie sÄƒ **discretizÄƒm** cumva observaÈ›iile, mapÃ¢ndu-le la un set finit de stÄƒri.

ExistÄƒ cÃ¢teva moduri Ã®n care putem face acest lucru:

- **ÃmpÄƒrÈ›irea Ã®n intervale**. DacÄƒ È™tim intervalul unei anumite valori, putem Ã®mpÄƒrÈ›i acest interval Ã®n mai multe **intervale** È™i apoi sÄƒ Ã®nlocuim valoarea cu numÄƒrul intervalului cÄƒruia Ã®i aparÈ›ine. Acest lucru poate fi realizat folosind metoda [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) din numpy. Ãn acest caz, vom È™ti exact dimensiunea stÄƒrii, deoarece va depinde de numÄƒrul de intervale pe care le selectÄƒm pentru digitalizare.

âœ… Putem folosi interpolarea liniarÄƒ pentru a aduce valorile la un interval finit (de exemplu, de la -20 la 20) È™i apoi sÄƒ convertim numerele Ã®n Ã®ntregi prin rotunjire. Acest lucru ne oferÄƒ un control mai redus asupra dimensiunii stÄƒrii, mai ales dacÄƒ nu cunoaÈ™tem intervalele exacte ale valorilor de intrare. De exemplu, Ã®n cazul nostru, 2 din cele 4 valori nu au limite superioare/inferioare, ceea ce poate duce la un numÄƒr infinit de stÄƒri.

Ãn exemplul nostru, vom folosi a doua abordare. DupÄƒ cum vei observa mai tÃ¢rziu, Ã®n ciuda lipsei limitelor superioare/inferioare, aceste valori rareori iau valori Ã®n afara unor intervale finite, astfel Ã®ncÃ¢t stÄƒrile cu valori extreme vor fi foarte rare.

1. IatÄƒ funcÈ›ia care va lua observaÈ›ia din modelul nostru È™i va produce un tuplu de 4 valori Ã®ntregi: (bloc de cod 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. SÄƒ explorÄƒm È™i o altÄƒ metodÄƒ de discretizare folosind intervale: (bloc de cod 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Acum sÄƒ rulÄƒm o simulare scurtÄƒ È™i sÄƒ observÄƒm aceste valori discrete ale mediului. Simte-te liber sÄƒ Ã®ncerci atÃ¢t `discretize`, cÃ¢t È™i `discretize_bins` È™i sÄƒ vezi dacÄƒ existÄƒ vreo diferenÈ›Äƒ.

    âœ… `discretize_bins` returneazÄƒ numÄƒrul intervalului, care Ã®ncepe de la 0. Astfel, pentru valorile variabilei de intrare Ã®n jurul valorii 0, returneazÄƒ numÄƒrul din mijlocul intervalului (10). Ãn `discretize`, nu ne-am preocupat de intervalul valorilor de ieÈ™ire, permiÈ›Ã¢ndu-le sÄƒ fie negative, astfel Ã®ncÃ¢t valorile stÄƒrii nu sunt deplasate, iar 0 corespunde lui 0. (bloc de cod 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… DebifeazÄƒ linia care Ã®ncepe cu `env.render` dacÄƒ vrei sÄƒ vezi cum se executÄƒ mediul. Ãn caz contrar, poÈ›i sÄƒ-l execuÈ›i Ã®n fundal, ceea ce este mai rapid. Vom folosi aceastÄƒ execuÈ›ie "invizibilÄƒ" Ã®n timpul procesului de Q-Learning.

## Structura Q-Table

Ãn lecÈ›ia anterioarÄƒ, starea era un simplu pereche de numere de la 0 la 8, È™i astfel era convenabil sÄƒ reprezentÄƒm Q-Table printr-un tensor numpy cu o formÄƒ de 8x8x2. DacÄƒ folosim discretizarea prin intervale, dimensiunea vectorului de stare este de asemenea cunoscutÄƒ, astfel Ã®ncÃ¢t putem folosi aceeaÈ™i abordare È™i sÄƒ reprezentÄƒm starea printr-un array cu forma 20x20x10x10x2 (aici 2 este dimensiunea spaÈ›iului de acÈ›iune, iar primele dimensiuni corespund numÄƒrului de intervale pe care le-am selectat pentru fiecare dintre parametrii din spaÈ›iul de observaÈ›ie).

TotuÈ™i, uneori dimensiunile precise ale spaÈ›iului de observaÈ›ie nu sunt cunoscute. Ãn cazul funcÈ›iei `discretize`, nu putem fi niciodatÄƒ siguri cÄƒ starea rÄƒmÃ¢ne Ã®n anumite limite, deoarece unele dintre valorile originale nu sunt limitate. Astfel, vom folosi o abordare uÈ™or diferitÄƒ È™i vom reprezenta Q-Table printr-un dicÈ›ionar.

1. FoloseÈ™te perechea *(state,action)* ca cheie a dicÈ›ionarului, iar valoarea ar corespunde valorii din Q-Table. (bloc de cod 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aici definim È™i o funcÈ›ie `qvalues()`, care returneazÄƒ o listÄƒ de valori din Q-Table pentru o stare datÄƒ, corespunzÄƒtoare tuturor acÈ›iunilor posibile. DacÄƒ intrarea nu este prezentÄƒ Ã®n Q-Table, vom returna 0 ca valoare implicitÄƒ.

## SÄƒ Ã®ncepem Q-Learning

Acum suntem gata sÄƒ-l Ã®nvÄƒÈ›Äƒm pe Peter sÄƒ-È™i menÈ›inÄƒ echilibrul!

1. Mai Ã®ntÃ¢i, sÄƒ setÄƒm cÃ¢È›iva hiperparametri: (bloc de cod 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aici, `alpha` este **rata de Ã®nvÄƒÈ›are**, care defineÈ™te Ã®n ce mÄƒsurÄƒ ar trebui sÄƒ ajustÄƒm valorile curente din Q-Table la fiecare pas. Ãn lecÈ›ia anterioarÄƒ am Ã®nceput cu 1 È™i apoi am scÄƒzut `alpha` la valori mai mici Ã®n timpul antrenamentului. Ãn acest exemplu, Ã®l vom menÈ›ine constant doar pentru simplitate, iar tu poÈ›i experimenta ajustarea valorilor `alpha` mai tÃ¢rziu.

    `gamma` este **factorul de reducere**, care aratÄƒ Ã®n ce mÄƒsurÄƒ ar trebui sÄƒ prioritizÄƒm recompensa viitoare faÈ›Äƒ de recompensa curentÄƒ.

    `epsilon` este **factorul de explorare/exploatare**, care determinÄƒ dacÄƒ ar trebui sÄƒ preferÄƒm explorarea sau exploatarea. Ãn algoritmul nostru, vom selecta acÈ›iunea urmÄƒtoare conform valorilor din Q-Table Ã®n procentul `epsilon` din cazuri, iar Ã®n restul cazurilor vom executa o acÈ›iune aleatorie. Acest lucru ne va permite sÄƒ explorÄƒm zone ale spaÈ›iului de cÄƒutare pe care nu le-am vÄƒzut niciodatÄƒ.

    âœ… Ãn termeni de echilibrare - alegerea unei acÈ›iuni aleatorii (explorare) ar acÈ›iona ca o Ã®mpingere aleatorie Ã®n direcÈ›ia greÈ™itÄƒ, iar stÃ¢lpul ar trebui sÄƒ Ã®nveÈ›e cum sÄƒ-È™i recupereze echilibrul din aceste "greÈ™eli".

### ÃmbunÄƒtÄƒÈ›irea algoritmului

Putem face douÄƒ Ã®mbunÄƒtÄƒÈ›iri algoritmului nostru din lecÈ›ia anterioarÄƒ:

- **Calcularea recompensei cumulative medii**, pe parcursul unui numÄƒr de simulÄƒri. Vom afiÈ™a progresul la fiecare 5000 de iteraÈ›ii È™i vom calcula media recompensei cumulative pe acea perioadÄƒ de timp. Asta Ã®nseamnÄƒ cÄƒ, dacÄƒ obÈ›inem mai mult de 195 de puncte, putem considera problema rezolvatÄƒ, cu o calitate chiar mai mare decÃ¢t cea cerutÄƒ.

- **Calcularea rezultatului cumulativ mediu maxim**, `Qmax`, È™i vom stoca Q-Table corespunzÄƒtor acelui rezultat. CÃ¢nd rulezi antrenamentul, vei observa cÄƒ uneori rezultatul cumulativ mediu Ã®ncepe sÄƒ scadÄƒ, È™i vrem sÄƒ pÄƒstrÄƒm valorile din Q-Table care corespund celui mai bun model observat Ã®n timpul antrenamentului.

1. ColecteazÄƒ toate recompensele cumulative la fiecare simulare Ã®n vectorul `rewards` pentru a le reprezenta grafic ulterior. (bloc de cod 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Ce poÈ›i observa din aceste rezultate:

- **Aproape de obiectivul nostru**. Suntem foarte aproape de a atinge obiectivul de a obÈ›ine 195 de recompense cumulative pe parcursul a 100+ rulÄƒri consecutive ale simulÄƒrii, sau poate chiar l-am atins! Chiar dacÄƒ obÈ›inem numere mai mici, Ã®ncÄƒ nu È™tim, deoarece calculÄƒm media pe 5000 de rulÄƒri, iar doar 100 de rulÄƒri sunt necesare conform criteriilor formale.

- **Recompensa Ã®ncepe sÄƒ scadÄƒ**. Uneori recompensa Ã®ncepe sÄƒ scadÄƒ, ceea ce Ã®nseamnÄƒ cÄƒ putem "distruge" valorile deja Ã®nvÄƒÈ›ate din Q-Table cu cele care fac situaÈ›ia mai rea.

AceastÄƒ observaÈ›ie este mai clar vizibilÄƒ dacÄƒ reprezentÄƒm grafic progresul antrenamentului.

## Reprezentarea graficÄƒ a progresului antrenamentului

Ãn timpul antrenamentului, am colectat valoarea recompensei cumulative la fiecare dintre iteraÈ›ii Ã®n vectorul `rewards`. IatÄƒ cum aratÄƒ cÃ¢nd o reprezentÄƒm grafic Ã®n funcÈ›ie de numÄƒrul de iteraÈ›ii:

```python
plt.plot(rewards)
```

![progres brut](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Din acest grafic, nu putem spune nimic, deoarece, datoritÄƒ naturii procesului de antrenament stochastic, durata sesiunilor de antrenament variazÄƒ foarte mult. Pentru a Ã®nÈ›elege mai bine acest grafic, putem calcula **media mobilÄƒ** pe o serie de experimente, sÄƒ zicem 100. Acest lucru poate fi realizat convenabil folosind `np.convolve`: (bloc de cod 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progres antrenament](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variarea hiperparametrilor

Pentru a face Ã®nvÄƒÈ›area mai stabilÄƒ, are sens sÄƒ ajustÄƒm unii dintre hiperparametrii noÈ™tri Ã®n timpul antrenamentului. Ãn special:

- **Pentru rata de Ã®nvÄƒÈ›are**, `alpha`, putem Ã®ncepe cu valori apropiate de 1 È™i apoi sÄƒ continuÄƒm sÄƒ scÄƒdem parametrul. Cu timpul, vom obÈ›ine valori bune de probabilitate Ã®n Q-Table È™i, astfel, ar trebui sÄƒ le ajustÄƒm uÈ™or, fÄƒrÄƒ sÄƒ le suprascriem complet cu valori noi.

- **CreÈ™terea lui epsilon**. Poate fi util sÄƒ creÈ™tem `epsilon` treptat, pentru a explora mai puÈ›in È™i a exploata mai mult. Probabil are sens sÄƒ Ã®ncepem cu o valoare mai micÄƒ pentru `epsilon` È™i sÄƒ o creÈ™tem pÃ¢nÄƒ aproape de 1.
> **Sarcina 1**: JoacÄƒ-te cu valorile hiperparametrilor È™i vezi dacÄƒ poÈ›i obÈ›ine o recompensÄƒ cumulativÄƒ mai mare. Ajungi peste 195?
> **Sarcina 2**: Pentru a rezolva formal problema, trebuie sÄƒ obÈ›ii o recompensÄƒ medie de 195 pe parcursul a 100 runde consecutive. MÄƒsoarÄƒ acest lucru Ã®n timpul antrenamentului È™i asigurÄƒ-te cÄƒ ai rezolvat problema Ã®n mod formal!

## Vizualizarea rezultatului Ã®n acÈ›iune

Ar fi interesant sÄƒ vedem cum se comportÄƒ modelul antrenat. SÄƒ rulÄƒm simularea È™i sÄƒ urmÄƒm aceeaÈ™i strategie de selecÈ›ie a acÈ›iunilor ca Ã®n timpul antrenamentului, eÈ™antionÃ¢nd conform distribuÈ›iei de probabilitate din Q-Table: (bloc de cod 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Ar trebui sÄƒ vezi ceva de genul acesta:

![un cartpole echilibrat](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€Provocare

> **Sarcina 3**: Aici, am folosit copia finalÄƒ a Q-Table, care poate sÄƒ nu fie cea mai bunÄƒ. AminteÈ™te-È›i cÄƒ am salvat cea mai performantÄƒ Q-Table Ã®n variabila `Qbest`! ÃncearcÄƒ acelaÈ™i exemplu cu cea mai performantÄƒ Q-Table, copiazÄƒ `Qbest` peste `Q` È™i vezi dacÄƒ observi vreo diferenÈ›Äƒ.

> **Sarcina 4**: Aici nu am selectat cea mai bunÄƒ acÈ›iune la fiecare pas, ci am eÈ™antionat conform distribuÈ›iei de probabilitate corespunzÄƒtoare. Ar avea mai mult sens sÄƒ selectÄƒm mereu cea mai bunÄƒ acÈ›iune, cu cea mai mare valoare din Q-Table? Acest lucru poate fi realizat folosind funcÈ›ia `np.argmax` pentru a afla numÄƒrul acÈ›iunii corespunzÄƒtoare celei mai mari valori din Q-Table. ImplementeazÄƒ aceastÄƒ strategie È™i vezi dacÄƒ Ã®mbunÄƒtÄƒÈ›eÈ™te echilibrarea.

## [Quiz post-lecturÄƒ](https://ff-quizzes.netlify.app/en/ml/)

## TemÄƒ
[AntreneazÄƒ un Mountain Car](assignment.md)

## Concluzie

Am Ã®nvÄƒÈ›at acum cum sÄƒ antrenÄƒm agenÈ›i pentru a obÈ›ine rezultate bune doar oferindu-le o funcÈ›ie de recompensÄƒ care defineÈ™te starea doritÄƒ a jocului È™i oferindu-le oportunitatea de a explora inteligent spaÈ›iul de cÄƒutare. Am aplicat cu succes algoritmul Q-Learning Ã®n cazurile de medii discrete È™i continue, dar cu acÈ›iuni discrete.

Este important sÄƒ studiem È™i situaÈ›iile Ã®n care starea acÈ›iunii este continuÄƒ È™i cÃ¢nd spaÈ›iul de observaÈ›ie este mult mai complex, cum ar fi imaginea de pe ecranul unui joc Atari. Ãn astfel de probleme, deseori trebuie sÄƒ folosim tehnici de Ã®nvÄƒÈ›are automatÄƒ mai puternice, cum ar fi reÈ›elele neuronale, pentru a obÈ›ine rezultate bune. Aceste subiecte mai avansate vor fi abordate Ã®n cursul nostru viitor de AI avansat.

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.