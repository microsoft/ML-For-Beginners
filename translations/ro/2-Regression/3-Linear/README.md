<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T15:12:24+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "ro"
}
-->
# Construirea unui model de regresie folosind Scikit-learn: patru metode de regresie

![Infografic regresie liniarÄƒ vs regresie polinomialÄƒ](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografic de [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ml/)

> ### [AceastÄƒ lecÈ›ie este disponibilÄƒ È™i Ã®n R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introducere

PÃ¢nÄƒ acum, ai explorat ce este regresia folosind date de exemplu colectate din setul de date privind preÈ›urile dovlecilor, pe care Ã®l vom folosi pe parcursul acestei lecÈ›ii. De asemenea, ai vizualizat aceste date folosind Matplotlib.

Acum eÈ™ti pregÄƒtit sÄƒ aprofundezi regresia pentru ML. DeÈ™i vizualizarea te ajutÄƒ sÄƒ Ã®nÈ›elegi datele, adevÄƒrata putere a Machine Learning vine din _antrenarea modelelor_. Modelele sunt antrenate pe date istorice pentru a captura automat dependenÈ›ele dintre date È™i permit prezicerea rezultatelor pentru date noi, pe care modelul nu le-a vÄƒzut anterior.

Ãn aceastÄƒ lecÈ›ie, vei Ã®nvÄƒÈ›a mai multe despre douÄƒ tipuri de regresie: _regresia liniarÄƒ de bazÄƒ_ È™i _regresia polinomialÄƒ_, Ã®mpreunÄƒ cu o parte din matematica care stÄƒ la baza acestor tehnici. Aceste modele ne vor permite sÄƒ prezicem preÈ›urile dovlecilor Ã®n funcÈ›ie de diferite date de intrare.

[![ML pentru Ã®ncepÄƒtori - ÃnÈ›elegerea regresiei liniare](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pentru Ã®ncepÄƒtori - ÃnÈ›elegerea regresiei liniare")

> ğŸ¥ FÄƒ clic pe imaginea de mai sus pentru un scurt videoclip despre regresia liniarÄƒ.

> Pe parcursul acestui curriculum, presupunem cunoÈ™tinÈ›e minime de matematicÄƒ È™i Ã®ncercÄƒm sÄƒ o facem accesibilÄƒ pentru studenÈ›ii din alte domenii, aÈ™a cÄƒ fii atent la notiÈ›e, ğŸ§® explicaÈ›ii, diagrame È™i alte instrumente de Ã®nvÄƒÈ›are care sÄƒ ajute la Ã®nÈ›elegere.

### CerinÈ›e preliminare

Ar trebui sÄƒ fii familiarizat pÃ¢nÄƒ acum cu structura datelor despre dovleci pe care le examinÄƒm. Le poÈ›i gÄƒsi preÃ®ncÄƒrcate È™i pre-curÄƒÈ›ate Ã®n fiÈ™ierul _notebook.ipynb_ al acestei lecÈ›ii. Ãn fiÈ™ier, preÈ›ul dovlecilor este afiÈ™at per bushel Ã®ntr-un nou cadru de date. AsigurÄƒ-te cÄƒ poÈ›i rula aceste notebook-uri Ã®n kernel-uri din Visual Studio Code.

### PregÄƒtire

Ca o reamintire, Ã®ncarci aceste date pentru a pune Ã®ntrebÄƒri despre ele.

- Care este cel mai bun moment pentru a cumpÄƒra dovleci?
- Ce preÈ› pot sÄƒ mÄƒ aÈ™tept pentru o cutie de dovleci miniaturali?
- Ar trebui sÄƒ Ã®i cumpÄƒr Ã®n coÈ™uri de jumÄƒtate de bushel sau Ã®n cutii de 1 1/9 bushel?
SÄƒ continuÄƒm sÄƒ explorÄƒm aceste date.

Ãn lecÈ›ia anterioarÄƒ, ai creat un cadru de date Pandas È™i l-ai populat cu o parte din setul de date original, standardizÃ¢nd preÈ›urile pe bushel. ProcedÃ¢nd astfel, Ã®nsÄƒ, ai reuÈ™it sÄƒ colectezi doar aproximativ 400 de puncte de date È™i doar pentru lunile de toamnÄƒ.

AruncÄƒ o privire la datele preÃ®ncÄƒrcate Ã®n notebook-ul care Ã®nsoÈ›eÈ™te aceastÄƒ lecÈ›ie. Datele sunt preÃ®ncÄƒrcate È™i un grafic iniÈ›ial de tip scatterplot este creat pentru a arÄƒta datele lunare. Poate putem obÈ›ine mai multe detalii despre natura datelor curÄƒÈ›Ã¢ndu-le mai mult.

## O linie de regresie liniarÄƒ

AÈ™a cum ai Ã®nvÄƒÈ›at Ã®n LecÈ›ia 1, scopul unui exerciÈ›iu de regresie liniarÄƒ este sÄƒ poÈ›i trasa o linie pentru:

- **AfiÈ™area relaÈ›iilor dintre variabile**. AfiÈ™area relaÈ›iei dintre variabile
- **Realizarea de predicÈ›ii**. Realizarea de predicÈ›ii precise despre unde ar cÄƒdea un nou punct de date Ã®n raport cu acea linie.

Este tipic pentru **Regresia Least-Squares** sÄƒ traseze acest tip de linie. Termenul 'least-squares' Ã®nseamnÄƒ cÄƒ toate punctele de date din jurul liniei de regresie sunt ridicate la pÄƒtrat È™i apoi adunate. Ideal, suma finalÄƒ este cÃ¢t mai micÄƒ posibil, deoarece dorim un numÄƒr redus de erori, sau `least-squares`.

Facem acest lucru deoarece dorim sÄƒ modelÄƒm o linie care are cea mai micÄƒ distanÈ›Äƒ cumulativÄƒ faÈ›Äƒ de toate punctele noastre de date. De asemenea, ridicÄƒm termenii la pÄƒtrat Ã®nainte de a-i aduna, deoarece ne preocupÄƒ magnitudinea lor, nu direcÈ›ia.

> **ğŸ§® AratÄƒ-mi matematica**
>
> AceastÄƒ linie, numitÄƒ _linia de cea mai bunÄƒ potrivire_, poate fi exprimatÄƒ prin [o ecuaÈ›ie](https://en.wikipedia.org/wiki/Simple_linear_regression):
>
> ```
> Y = a + bX
> ```
>
> `X` este 'variabila explicativÄƒ'. `Y` este 'variabila dependentÄƒ'. Panta liniei este `b`, iar `a` este interceptul pe axa Y, care se referÄƒ la valoarea lui `Y` cÃ¢nd `X = 0`.
>
>![calcularea pantei](../../../../2-Regression/3-Linear/images/slope.png)
>
> Mai Ã®ntÃ¢i, calculeazÄƒ panta `b`. Infografic de [Jen Looper](https://twitter.com/jenlooper)
>
> Cu alte cuvinte, referindu-ne la Ã®ntrebarea originalÄƒ despre datele dovlecilor: "prezice preÈ›ul unui dovleac per bushel Ã®n funcÈ›ie de lunÄƒ", `X` ar fi preÈ›ul, iar `Y` ar fi luna vÃ¢nzÄƒrii.
>
>![completarea ecuaÈ›iei](../../../../2-Regression/3-Linear/images/calculation.png)
>
> CalculeazÄƒ valoarea lui Y. DacÄƒ plÄƒteÈ™ti Ã®n jur de 4 dolari, trebuie sÄƒ fie aprilie! Infografic de [Jen Looper](https://twitter.com/jenlooper)
>
> Matematica care calculeazÄƒ linia trebuie sÄƒ demonstreze panta liniei, care depinde È™i de interceptul, sau unde se aflÄƒ `Y` cÃ¢nd `X = 0`.
>
> PoÈ›i observa metoda de calcul pentru aceste valori pe site-ul [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). ViziteazÄƒ È™i [acest calculator Least-squares](https://www.mathsisfun.com/data/least-squares-calculator.html) pentru a vedea cum valorile numerelor influenÈ›eazÄƒ linia.

## CorelaÈ›ie

Un alt termen de Ã®nÈ›eles este **Coeficientul de CorelaÈ›ie** Ã®ntre variabilele X È™i Y date. Folosind un scatterplot, poÈ›i vizualiza rapid acest coeficient. Un grafic cu puncte de date distribuite Ã®ntr-o linie ordonatÄƒ are o corelaÈ›ie mare, dar un grafic cu puncte de date distribuite aleatoriu Ã®ntre X È™i Y are o corelaÈ›ie micÄƒ.

Un model de regresie liniarÄƒ bun va fi unul care are un Coeficient de CorelaÈ›ie mare (mai aproape de 1 decÃ¢t de 0) folosind metoda Least-Squares Regression cu o linie de regresie.

âœ… RuleazÄƒ notebook-ul care Ã®nsoÈ›eÈ™te aceastÄƒ lecÈ›ie È™i uitÄƒ-te la scatterplot-ul LunÄƒ-PreÈ›. Datele care asociazÄƒ Luna cu PreÈ›ul pentru vÃ¢nzÄƒrile de dovleci par sÄƒ aibÄƒ o corelaÈ›ie mare sau micÄƒ, conform interpretÄƒrii tale vizuale a scatterplot-ului? Se schimbÄƒ acest lucru dacÄƒ foloseÈ™ti o mÄƒsurÄƒ mai detaliatÄƒ Ã®n loc de `LunÄƒ`, de exemplu *ziua anului* (adicÄƒ numÄƒrul de zile de la Ã®nceputul anului)?

Ãn codul de mai jos, vom presupune cÄƒ am curÄƒÈ›at datele È™i am obÈ›inut un cadru de date numit `new_pumpkins`, similar cu urmÄƒtorul:

ID | LunÄƒ | ZiuaAnului | Tip | OraÈ™ | Pachet | PreÈ› Minim | PreÈ› Maxim | PreÈ›
---|------|------------|-----|------|--------|------------|------------|------
70 | 9 | 267 | TIP PLÄ‚CINTÄ‚ | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | TIP PLÄ‚CINTÄ‚ | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | TIP PLÄ‚CINTÄ‚ | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | TIP PLÄ‚CINTÄ‚ | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | TIP PLÄ‚CINTÄ‚ | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Codul pentru curÄƒÈ›area datelor este disponibil Ã®n [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Am efectuat aceleaÈ™i paÈ™i de curÄƒÈ›are ca Ã®n lecÈ›ia anterioarÄƒ È™i am calculat coloana `ZiuaAnului` folosind urmÄƒtoarea expresie:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Acum cÄƒ ai o Ã®nÈ›elegere a matematicii din spatele regresiei liniare, sÄƒ creÄƒm un model de regresie pentru a vedea dacÄƒ putem prezice care pachet de dovleci va avea cele mai bune preÈ›uri. Cineva care cumpÄƒrÄƒ dovleci pentru o grÄƒdinÄƒ tematicÄƒ de sÄƒrbÄƒtori ar putea dori aceste informaÈ›ii pentru a optimiza achiziÈ›iile de pachete de dovleci pentru grÄƒdinÄƒ.

## CÄƒutarea corelaÈ›iei

[![ML pentru Ã®ncepÄƒtori - CÄƒutarea corelaÈ›iei: cheia regresiei liniare](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pentru Ã®ncepÄƒtori - CÄƒutarea corelaÈ›iei: cheia regresiei liniare")

> ğŸ¥ FÄƒ clic pe imaginea de mai sus pentru un scurt videoclip despre corelaÈ›ie.

Din lecÈ›ia anterioarÄƒ, probabil ai observat cÄƒ preÈ›ul mediu pentru diferite luni aratÄƒ astfel:

<img alt="PreÈ› mediu pe lunÄƒ" src="../2-Data/images/barchart.png" width="50%"/>

Acest lucru sugereazÄƒ cÄƒ ar trebui sÄƒ existe o anumitÄƒ corelaÈ›ie, iar noi putem Ã®ncerca sÄƒ antrenÄƒm un model de regresie liniarÄƒ pentru a prezice relaÈ›ia dintre `LunÄƒ` È™i `PreÈ›`, sau dintre `ZiuaAnului` È™i `PreÈ›`. IatÄƒ scatterplot-ul care aratÄƒ ultima relaÈ›ie:

<img alt="Scatterplot PreÈ› vs. Ziua Anului" src="images/scatter-dayofyear.png" width="50%" />

SÄƒ vedem dacÄƒ existÄƒ o corelaÈ›ie folosind funcÈ›ia `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Se pare cÄƒ corelaÈ›ia este destul de micÄƒ, -0.15 pentru `LunÄƒ` È™i -0.17 pentru `ZiuaAnului`, dar ar putea exista o altÄƒ relaÈ›ie importantÄƒ. Se pare cÄƒ existÄƒ diferite grupuri de preÈ›uri corespunzÄƒtoare diferitelor varietÄƒÈ›i de dovleci. Pentru a confirma aceastÄƒ ipotezÄƒ, sÄƒ reprezentÄƒm fiecare categorie de dovleci folosind o culoare diferitÄƒ. Prin transmiterea unui parametru `ax` funcÈ›iei de plotare `scatter`, putem reprezenta toate punctele pe acelaÈ™i grafic:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Scatterplot PreÈ› vs. Ziua Anului" src="images/scatter-dayofyear-color.png" width="50%" />

InvestigaÈ›ia noastrÄƒ sugereazÄƒ cÄƒ varietatea are un efect mai mare asupra preÈ›ului general decÃ¢t data efectivÄƒ de vÃ¢nzare. Putem vedea acest lucru cu un grafic de tip barÄƒ:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Grafic de tip barÄƒ preÈ› vs varietate" src="images/price-by-variety.png" width="50%" />

SÄƒ ne concentrÄƒm pentru moment doar pe o singurÄƒ varietate de dovleci, 'tip plÄƒcintÄƒ', È™i sÄƒ vedem ce efect are data asupra preÈ›ului:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Scatterplot PreÈ› vs. Ziua Anului" src="images/pie-pumpkins-scatter.png" width="50%" />

DacÄƒ acum calculÄƒm corelaÈ›ia dintre `PreÈ›` È™i `ZiuaAnului` folosind funcÈ›ia `corr`, vom obÈ›ine ceva Ã®n jur de `-0.27` - ceea ce Ã®nseamnÄƒ cÄƒ antrenarea unui model predictiv are sens.

> Ãnainte de a antrena un model de regresie liniarÄƒ, este important sÄƒ ne asigurÄƒm cÄƒ datele noastre sunt curate. Regresia liniarÄƒ nu funcÈ›ioneazÄƒ bine cu valori lipsÄƒ, astfel Ã®ncÃ¢t este logic sÄƒ eliminÄƒm toate celulele goale:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

O altÄƒ abordare ar fi sÄƒ completÄƒm valorile lipsÄƒ cu valorile medii din coloana corespunzÄƒtoare.

## Regresie liniarÄƒ simplÄƒ

[![ML pentru Ã®ncepÄƒtori - Regresie liniarÄƒ È™i polinomialÄƒ folosind Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pentru Ã®ncepÄƒtori - Regresie liniarÄƒ È™i polinomialÄƒ folosind Scikit-learn")

> ğŸ¥ FÄƒ clic pe imaginea de mai sus pentru un scurt videoclip despre regresia liniarÄƒ È™i polinomialÄƒ.

Pentru a antrena modelul nostru de regresie liniarÄƒ, vom folosi biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Ãncepem prin separarea valorilor de intrare (caracteristici) È™i a ieÈ™irii aÈ™teptate (eticheta) Ã®n array-uri numpy separate:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> ObservÄƒ cÄƒ a trebuit sÄƒ aplicÄƒm `reshape` pe datele de intrare pentru ca pachetul de regresie liniarÄƒ sÄƒ le Ã®nÈ›eleagÄƒ corect. Regresia liniarÄƒ aÈ™teaptÄƒ un array 2D ca intrare, unde fiecare rÃ¢nd al array-ului corespunde unui vector de caracteristici de intrare. Ãn cazul nostru, deoarece avem doar o singurÄƒ intrare - avem nevoie de un array cu forma NÃ—1, unde N este dimensiunea setului de date.

Apoi, trebuie sÄƒ Ã®mpÄƒrÈ›im datele Ã®n seturi de antrenament È™i test, astfel Ã®ncÃ¢t sÄƒ putem valida modelul nostru dupÄƒ antrenament:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Ãn cele din urmÄƒ, antrenarea modelului de regresie liniarÄƒ propriu-zis dureazÄƒ doar douÄƒ linii de cod. Definim obiectul `LinearRegression` È™i Ã®l ajustÄƒm la datele noastre folosind metoda `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Obiectul `LinearRegression` dupÄƒ ajustare conÈ›ine toate coeficienÈ›ii regresiei, care pot fi accesaÈ›i folosind proprietatea `.coef_`. Ãn cazul nostru, existÄƒ doar un coeficient, care ar trebui sÄƒ fie Ã®n jur de `-0.017`. Acest lucru Ã®nseamnÄƒ cÄƒ preÈ›urile par sÄƒ scadÄƒ puÈ›in Ã®n timp, dar nu prea mult, aproximativ 2 cenÈ›i pe zi. De asemenea, putem accesa punctul de intersecÈ›ie al regresiei cu axa Y folosind `lin_reg.intercept_` - va fi Ã®n jur de `21` Ã®n cazul nostru, indicÃ¢nd preÈ›ul de la Ã®nceputul anului.

Pentru a vedea cÃ¢t de precis este modelul nostru, putem prezice preÈ›urile pe un set de date de test È™i apoi mÄƒsura cÃ¢t de apropiate sunt predicÈ›iile noastre de valorile aÈ™teptate. Acest lucru poate fi realizat folosind metrica mean square error (MSE), care este media tuturor diferenÈ›elor pÄƒtrate dintre valoarea aÈ™teptatÄƒ È™i cea prezisÄƒ.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Eroarea noastrÄƒ pare sÄƒ fie Ã®n jur de 2 puncte, ceea ce reprezintÄƒ ~17%. Nu prea bine. Un alt indicator al calitÄƒÈ›ii modelului este **coeficientul de determinare**, care poate fi obÈ›inut astfel:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
DacÄƒ valoarea este 0, Ã®nseamnÄƒ cÄƒ modelul nu ia Ã®n considerare datele de intrare È™i acÈ›ioneazÄƒ ca *cel mai slab predictor liniar*, care este pur È™i simplu valoarea medie a rezultatului. Valoarea de 1 Ã®nseamnÄƒ cÄƒ putem prezice perfect toate rezultatele aÈ™teptate. Ãn cazul nostru, coeficientul este Ã®n jur de 0.06, ceea ce este destul de scÄƒzut.

Putem, de asemenea, sÄƒ reprezentÄƒm grafic datele de testare Ã®mpreunÄƒ cu linia de regresie pentru a vedea mai bine cum funcÈ›ioneazÄƒ regresia Ã®n cazul nostru:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="Regresie liniarÄƒ" src="images/linear-results.png" width="50%" />

## Regresie PolinomialÄƒ

Un alt tip de regresie liniarÄƒ este regresia polinomialÄƒ. DeÈ™i uneori existÄƒ o relaÈ›ie liniarÄƒ Ã®ntre variabile - cu cÃ¢t dovleacul este mai mare Ã®n volum, cu atÃ¢t preÈ›ul este mai mare - uneori aceste relaÈ›ii nu pot fi reprezentate ca un plan sau o linie dreaptÄƒ.

âœ… IatÄƒ [cÃ¢teva exemple](https://online.stat.psu.edu/stat501/lesson/9/9.8) de date care ar putea utiliza regresia polinomialÄƒ.

PriveÈ™te din nou relaÈ›ia dintre DatÄƒ È™i PreÈ›. Acest scatterplot pare sÄƒ fie analizat neapÄƒrat printr-o linie dreaptÄƒ? Nu pot preÈ›urile sÄƒ fluctueze? Ãn acest caz, poÈ›i Ã®ncerca regresia polinomialÄƒ.

âœ… Polinoamele sunt expresii matematice care pot consta din una sau mai multe variabile È™i coeficienÈ›i.

Regresia polinomialÄƒ creeazÄƒ o linie curbatÄƒ pentru a se potrivi mai bine datelor neliniare. Ãn cazul nostru, dacÄƒ includem o variabilÄƒ pÄƒtraticÄƒ `DayOfYear` Ã®n datele de intrare, ar trebui sÄƒ putem ajusta datele noastre cu o curbÄƒ parabolicÄƒ, care va avea un minim Ã®ntr-un anumit punct al anului.

Scikit-learn include o [API pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) utilÄƒ pentru a combina diferite etape de procesare a datelor. Un **pipeline** este un lanÈ› de **estimatori**. Ãn cazul nostru, vom crea un pipeline care mai Ã®ntÃ¢i adaugÄƒ caracteristici polinomiale modelului nostru, apoi antreneazÄƒ regresia:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

Utilizarea `PolynomialFeatures(2)` Ã®nseamnÄƒ cÄƒ vom include toate polinoamele de gradul doi din datele de intrare. Ãn cazul nostru, aceasta va Ã®nsemna doar `DayOfYear`<sup>2</sup>, dar avÃ¢nd douÄƒ variabile de intrare X È™i Y, aceasta va adÄƒuga X<sup>2</sup>, XY È™i Y<sup>2</sup>. Putem folosi È™i polinoame de grad mai mare dacÄƒ dorim.

Pipeline-urile pot fi utilizate Ã®n acelaÈ™i mod ca obiectul original `LinearRegression`, adicÄƒ putem aplica `fit` pipeline-ului È™i apoi utiliza `predict` pentru a obÈ›ine rezultatele predicÈ›iei. IatÄƒ graficul care aratÄƒ datele de testare È™i curba de aproximare:

<img alt="Regresie polinomialÄƒ" src="images/poly-results.png" width="50%" />

Folosind regresia polinomialÄƒ, putem obÈ›ine un MSE uÈ™or mai mic È™i un coeficient de determinare mai mare, dar nu semnificativ. Trebuie sÄƒ luÄƒm Ã®n considerare alte caracteristici!

> PoÈ›i observa cÄƒ preÈ›urile minime ale dovlecilor sunt observate undeva Ã®n jurul Halloween-ului. Cum poÈ›i explica acest lucru?

ğŸƒ FelicitÄƒri, tocmai ai creat un model care poate ajuta la prezicerea preÈ›ului dovlecilor pentru plÄƒcintÄƒ. Probabil poÈ›i repeta aceeaÈ™i procedurÄƒ pentru toate tipurile de dovleci, dar ar fi obositor. SÄƒ Ã®nvÄƒÈ›Äƒm acum cum sÄƒ luÄƒm Ã®n considerare varietatea dovlecilor Ã®n modelul nostru!

## Caracteristici Categoriale

Ãn lumea idealÄƒ, ne dorim sÄƒ putem prezice preÈ›urile pentru diferite varietÄƒÈ›i de dovleci folosind acelaÈ™i model. TotuÈ™i, coloana `Variety` este oarecum diferitÄƒ de coloane precum `Month`, deoarece conÈ›ine valori non-numerice. Astfel de coloane sunt numite **categoriale**.

[![ML pentru Ã®ncepÄƒtori - PredicÈ›ii cu caracteristici categoriale folosind regresia liniarÄƒ](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pentru Ã®ncepÄƒtori - PredicÈ›ii cu caracteristici categoriale folosind regresia liniarÄƒ")

> ğŸ¥ FÄƒ clic pe imaginea de mai sus pentru un scurt videoclip despre utilizarea caracteristicilor categoriale.

Aici poÈ›i vedea cum preÈ›ul mediu depinde de varietate:

<img alt="PreÈ› mediu pe varietate" src="images/price-by-variety.png" width="50%" />

Pentru a lua Ã®n considerare varietatea, mai Ã®ntÃ¢i trebuie sÄƒ o convertim Ã®ntr-o formÄƒ numericÄƒ, sau sÄƒ o **codificÄƒm**. ExistÄƒ mai multe moduri Ã®n care putem face acest lucru:

* Codificarea numericÄƒ simplÄƒ va construi un tabel cu diferite varietÄƒÈ›i È™i apoi va Ã®nlocui numele varietÄƒÈ›ii cu un index din acel tabel. Aceasta nu este cea mai bunÄƒ idee pentru regresia liniarÄƒ, deoarece regresia liniarÄƒ ia valoarea numericÄƒ realÄƒ a indexului È™i o adaugÄƒ la rezultat, Ã®nmulÈ›ind-o cu un coeficient. Ãn cazul nostru, relaÈ›ia dintre numÄƒrul indexului È™i preÈ› este clar neliniarÄƒ, chiar dacÄƒ ne asigurÄƒm cÄƒ indicii sunt ordonaÈ›i Ã®ntr-un mod specific.
* **Codificarea one-hot** va Ã®nlocui coloana `Variety` cu 4 coloane diferite, cÃ¢te una pentru fiecare varietate. Fiecare coloanÄƒ va conÈ›ine `1` dacÄƒ rÃ¢ndul corespunzÄƒtor este de o anumitÄƒ varietate È™i `0` Ã®n caz contrar. Aceasta Ã®nseamnÄƒ cÄƒ vor exista patru coeficienÈ›i Ã®n regresia liniarÄƒ, cÃ¢te unul pentru fiecare varietate de dovleac, responsabili pentru "preÈ›ul de pornire" (sau mai degrabÄƒ "preÈ›ul suplimentar") pentru acea varietate specificÄƒ.

Codul de mai jos aratÄƒ cum putem codifica one-hot o varietate:

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

Pentru a antrena regresia liniarÄƒ folosind varietatea codificatÄƒ one-hot ca intrare, trebuie doar sÄƒ iniÈ›ializÄƒm corect datele `X` È™i `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

Restul codului este acelaÈ™i cu cel pe care l-am folosit mai sus pentru a antrena regresia liniarÄƒ. DacÄƒ Ã®ncerci, vei vedea cÄƒ eroarea medie pÄƒtraticÄƒ este aproximativ aceeaÈ™i, dar obÈ›inem un coeficient de determinare mult mai mare (~77%). Pentru a obÈ›ine predicÈ›ii È™i mai precise, putem lua Ã®n considerare mai multe caracteristici categoriale, precum È™i caracteristici numerice, cum ar fi `Month` sau `DayOfYear`. Pentru a obÈ›ine un array mare de caracteristici, putem folosi `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

Aici luÄƒm Ã®n considerare È™i `City` È™i tipul de `Package`, ceea ce ne oferÄƒ un MSE de 2.84 (10%) È™i un coeficient de determinare de 0.94!

## PunÃ¢nd totul Ã®mpreunÄƒ

Pentru a crea cel mai bun model, putem folosi date combinate (categoriale codificate one-hot + numerice) din exemplul de mai sus Ã®mpreunÄƒ cu regresia polinomialÄƒ. IatÄƒ codul complet pentru confortul tÄƒu:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Acest lucru ar trebui sÄƒ ne ofere cel mai bun coeficient de determinare de aproape 97% È™i MSE=2.23 (~8% eroare de predicÈ›ie).

| Model | MSE | Determinare |  
|-------|-----|-------------|  
| `DayOfYear` Liniar | 2.77 (17.2%) | 0.07 |  
| `DayOfYear` Polinomial | 2.73 (17.0%) | 0.08 |  
| `Variety` Liniar | 5.24 (19.7%) | 0.77 |  
| Toate caracteristicile Liniar | 2.84 (10.5%) | 0.94 |  
| Toate caracteristicile Polinomial | 2.23 (8.25%) | 0.97 |  

ğŸ† Bravo! Ai creat patru modele de regresie Ã®ntr-o singurÄƒ lecÈ›ie È™i ai Ã®mbunÄƒtÄƒÈ›it calitatea modelului la 97%. Ãn secÈ›iunea finalÄƒ despre regresie, vei Ã®nvÄƒÈ›a despre regresia logisticÄƒ pentru a determina categorii.

---

## ğŸš€Provocare

TesteazÄƒ mai multe variabile diferite Ã®n acest notebook pentru a vedea cum corelaÈ›ia corespunde cu acurateÈ›ea modelului.

## [Quiz post-lecturÄƒ](https://ff-quizzes.netlify.app/en/ml/)

## Recapitulare & Studiu Individual

Ãn aceastÄƒ lecÈ›ie am Ã®nvÄƒÈ›at despre regresia liniarÄƒ. ExistÄƒ alte tipuri importante de regresie. CiteÈ™te despre tehnicile Stepwise, Ridge, Lasso È™i Elasticnet. Un curs bun pentru a Ã®nvÄƒÈ›a mai multe este [cursul de Ã®nvÄƒÈ›are statisticÄƒ de la Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## TemÄƒ

[ConstruieÈ™te un model](assignment.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.