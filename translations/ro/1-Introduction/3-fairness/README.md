<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T16:01:28+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "ro"
}
-->
# Construirea soluÈ›iilor de Ã®nvÄƒÈ›are automatÄƒ cu AI responsabil

![Rezumat al AI responsabil Ã®n Ã®nvÄƒÈ›area automatÄƒ Ã®ntr-o schiÈ›Äƒ](../../../../sketchnotes/ml-fairness.png)
> SchiÈ›Äƒ realizatÄƒ de [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Chestionar Ã®nainte de lecÈ›ie](https://ff-quizzes.netlify.app/en/ml/)

## Introducere

Ãn acest curriculum, vei Ã®ncepe sÄƒ descoperi cum Ã®nvÄƒÈ›area automatÄƒ poate È™i influenÈ›eazÄƒ vieÈ›ile noastre de zi cu zi. Chiar È™i acum, sistemele È™i modelele sunt implicate Ã®n sarcini de luare a deciziilor zilnice, cum ar fi diagnosticarea Ã®n sÄƒnÄƒtate, aprobarea Ã®mprumuturilor sau detectarea fraudei. De aceea, este important ca aceste modele sÄƒ funcÈ›ioneze bine pentru a oferi rezultate de Ã®ncredere. La fel ca orice aplicaÈ›ie software, sistemele AI pot sÄƒ nu Ã®ndeplineascÄƒ aÈ™teptÄƒrile sau sÄƒ aibÄƒ un rezultat nedorit. De aceea, este esenÈ›ial sÄƒ putem Ã®nÈ›elege È™i explica comportamentul unui model AI.

ImagineazÄƒ-È›i ce se poate Ã®ntÃ¢mpla atunci cÃ¢nd datele pe care le foloseÈ™ti pentru a construi aceste modele nu includ anumite demografii, cum ar fi rasa, genul, opiniile politice, religia sau reprezintÄƒ disproporÈ›ionat astfel de demografii. Ce se Ã®ntÃ¢mplÄƒ cÃ¢nd rezultatul modelului este interpretat astfel Ã®ncÃ¢t sÄƒ favorizeze o anumitÄƒ demografie? Care este consecinÈ›a pentru aplicaÈ›ie? Ãn plus, ce se Ã®ntÃ¢mplÄƒ cÃ¢nd modelul are un rezultat advers È™i este dÄƒunÄƒtor pentru oameni? Cine este responsabil pentru comportamentul sistemelor AI? Acestea sunt cÃ¢teva Ã®ntrebÄƒri pe care le vom explora Ã®n acest curriculum.

Ãn aceastÄƒ lecÈ›ie, vei:

- ConÈ™tientiza importanÈ›a echitÄƒÈ›ii Ã®n Ã®nvÄƒÈ›area automatÄƒ È™i daunele legate de echitate.
- Deveni familiar cu practica de explorare a valorilor extreme È™i scenariilor neobiÈ™nuite pentru a asigura fiabilitatea È™i siguranÈ›a.
- ÃnÈ›elege necesitatea de a Ã®mputernici pe toatÄƒ lumea prin proiectarea de sisteme incluzive.
- Explora cÃ¢t de vital este sÄƒ protejezi confidenÈ›ialitatea È™i securitatea datelor È™i a oamenilor.
- Observa importanÈ›a unei abordÄƒri transparente pentru a explica comportamentul modelelor AI.
- Fi conÈ™tient de faptul cÄƒ responsabilitatea este esenÈ›ialÄƒ pentru a construi Ã®ncrederea Ã®n sistemele AI.

## Prerechizite

Ca prerechizite, te rugÄƒm sÄƒ urmezi "Principiile AI Responsabil" pe Learn Path È™i sÄƒ vizionezi videoclipul de mai jos pe acest subiect:

AflÄƒ mai multe despre AI Responsabil urmÃ¢nd acest [Learning Path](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Abordarea Microsoft pentru AI Responsabil](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Abordarea Microsoft pentru AI Responsabil")

> ğŸ¥ Click pe imaginea de mai sus pentru un videoclip: Abordarea Microsoft pentru AI Responsabil

## Echitate

Sistemele AI ar trebui sÄƒ trateze pe toatÄƒ lumea Ã®n mod echitabil È™i sÄƒ evite sÄƒ afecteze grupuri similare de oameni Ã®n moduri diferite. De exemplu, atunci cÃ¢nd sistemele AI oferÄƒ recomandÄƒri pentru tratamente medicale, aplicaÈ›ii de Ã®mprumut sau angajare, acestea ar trebui sÄƒ facÄƒ aceleaÈ™i recomandÄƒri pentru toÈ›i cei cu simptome, circumstanÈ›e financiare sau calificÄƒri profesionale similare. Fiecare dintre noi, ca oameni, purtÄƒm prejudecÄƒÈ›i moÈ™tenite care ne afecteazÄƒ deciziile È™i acÈ›iunile. Aceste prejudecÄƒÈ›i pot fi evidente Ã®n datele pe care le folosim pentru a antrena sistemele AI. Astfel de manipulÄƒri pot apÄƒrea uneori neintenÈ›ionat. Este adesea dificil sÄƒ conÈ™tientizezi cÃ¢nd introduci prejudecÄƒÈ›i Ã®n date.

**â€Nedreptateaâ€** cuprinde impacturi negative sau â€dauneâ€ pentru un grup de oameni, cum ar fi cei definiÈ›i Ã®n termeni de rasÄƒ, gen, vÃ¢rstÄƒ sau statut de dizabilitate. Principalele daune legate de echitate pot fi clasificate astfel:

- **Alocare**, dacÄƒ un gen sau o etnie, de exemplu, este favorizatÄƒ Ã®n detrimentul alteia.
- **Calitatea serviciului**. DacÄƒ antrenezi datele pentru un scenariu specific, dar realitatea este mult mai complexÄƒ, aceasta duce la un serviciu slab. De exemplu, un dozator de sÄƒpun care nu pare sÄƒ detecteze persoanele cu pielea Ã®nchisÄƒ la culoare. [ReferinÈ›Äƒ](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigrare**. Criticarea È™i etichetarea nedreaptÄƒ a ceva sau a cuiva. De exemplu, o tehnologie de etichetare a imaginilor a etichetat Ã®n mod infam imagini ale persoanelor cu pielea Ã®nchisÄƒ la culoare ca fiind gorile.
- **Reprezentare excesivÄƒ sau insuficientÄƒ**. Ideea este cÄƒ un anumit grup nu este vÄƒzut Ã®ntr-o anumitÄƒ profesie, iar orice serviciu sau funcÈ›ie care continuÄƒ sÄƒ promoveze acest lucru contribuie la daune.
- **Stereotipizare**. Asocierea unui grup dat cu atribute predefinite. De exemplu, un sistem de traducere Ã®ntre englezÄƒ È™i turcÄƒ poate avea inexactitÄƒÈ›i din cauza cuvintelor asociate stereotipic cu genul.

![traducere Ã®n turcÄƒ](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> traducere Ã®n turcÄƒ

![traducere Ã®napoi Ã®n englezÄƒ](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> traducere Ã®napoi Ã®n englezÄƒ

CÃ¢nd proiectÄƒm È™i testÄƒm sisteme AI, trebuie sÄƒ ne asigurÄƒm cÄƒ AI este echitabil È™i nu este programat sÄƒ ia decizii pÄƒrtinitoare sau discriminatorii, pe care nici oamenii nu au voie sÄƒ le ia. Garantarea echitÄƒÈ›ii Ã®n AI È™i Ã®nvÄƒÈ›area automatÄƒ rÄƒmÃ¢ne o provocare complexÄƒ sociotehnicÄƒ.

### Fiabilitate È™i siguranÈ›Äƒ

Pentru a construi Ã®ncredere, sistemele AI trebuie sÄƒ fie fiabile, sigure È™i consistente Ã®n condiÈ›ii normale È™i neaÈ™teptate. Este important sÄƒ È™tim cum se vor comporta sistemele AI Ã®ntr-o varietate de situaÈ›ii, mai ales cÃ¢nd sunt valori extreme. CÃ¢nd construim soluÈ›ii AI, trebuie sÄƒ ne concentrÄƒm substanÈ›ial pe modul de gestionare a unei varietÄƒÈ›i largi de circumstanÈ›e pe care soluÈ›iile AI le-ar putea Ã®ntÃ¢lni. De exemplu, o maÈ™inÄƒ autonomÄƒ trebuie sÄƒ punÄƒ siguranÈ›a oamenilor pe primul loc. Ca rezultat, AI-ul care alimenteazÄƒ maÈ™ina trebuie sÄƒ ia Ã®n considerare toate scenariile posibile pe care maÈ™ina le-ar putea Ã®ntÃ¢lni, cum ar fi noaptea, furtuni, viscol, copii care traverseazÄƒ strada, animale de companie, construcÈ›ii pe drum etc. CÃ¢t de bine poate gestiona un sistem AI o gamÄƒ largÄƒ de condiÈ›ii Ã®n mod fiabil È™i sigur reflectÄƒ nivelul de anticipare pe care l-a avut specialistul Ã®n date sau dezvoltatorul AI Ã®n timpul proiectÄƒrii sau testÄƒrii sistemului.

> [ğŸ¥ Click aici pentru un videoclip: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Incluziune

Sistemele AI ar trebui sÄƒ fie proiectate pentru a implica È™i Ã®mputernici pe toatÄƒ lumea. CÃ¢nd proiecteazÄƒ È™i implementeazÄƒ sisteme AI, specialiÈ™tii Ã®n date È™i dezvoltatorii AI identificÄƒ È™i abordeazÄƒ barierele potenÈ›iale din sistem care ar putea exclude neintenÈ›ionat oamenii. De exemplu, existÄƒ 1 miliard de persoane cu dizabilitÄƒÈ›i Ã®n Ã®ntreaga lume. Cu avansarea AI, acestea pot accesa o gamÄƒ largÄƒ de informaÈ›ii È™i oportunitÄƒÈ›i mai uÈ™or Ã®n viaÈ›a lor de zi cu zi. Abordarea barierelor creeazÄƒ oportunitÄƒÈ›i de a inova È™i dezvolta produse AI cu experienÈ›e mai bune care beneficiazÄƒ pe toatÄƒ lumea.

> [ğŸ¥ Click aici pentru un videoclip: incluziunea Ã®n AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Securitate È™i confidenÈ›ialitate

Sistemele AI ar trebui sÄƒ fie sigure È™i sÄƒ respecte confidenÈ›ialitatea oamenilor. Oamenii au mai puÈ›inÄƒ Ã®ncredere Ã®n sistemele care le pun Ã®n pericol confidenÈ›ialitatea, informaÈ›iile sau vieÈ›ile. CÃ¢nd antrenÄƒm modele de Ã®nvÄƒÈ›are automatÄƒ, ne bazÄƒm pe date pentru a produce cele mai bune rezultate. Ãn acest proces, originea datelor È™i integritatea lor trebuie luate Ã®n considerare. De exemplu, datele au fost trimise de utilizatori sau sunt disponibile public? Apoi, Ã®n timp ce lucrÄƒm cu datele, este crucial sÄƒ dezvoltÄƒm sisteme AI care pot proteja informaÈ›iile confidenÈ›iale È™i rezista atacurilor. Pe mÄƒsurÄƒ ce AI devine mai rÄƒspÃ¢ndit, protejarea confidenÈ›ialitÄƒÈ›ii È™i securizarea informaÈ›iilor personale È™i de afaceri devin din ce Ã®n ce mai critice È™i complexe. Problemele de confidenÈ›ialitate È™i securitate a datelor necesitÄƒ o atenÈ›ie deosebitÄƒ pentru AI, deoarece accesul la date este esenÈ›ial pentru ca sistemele AI sÄƒ facÄƒ predicÈ›ii È™i decizii precise È™i informate despre oameni.

> [ğŸ¥ Click aici pentru un videoclip: securitatea Ã®n AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Industria a fÄƒcut progrese semnificative Ã®n confidenÈ›ialitate È™i securitate, alimentate Ã®n mod semnificativ de reglementÄƒri precum GDPR (Regulamentul General privind ProtecÈ›ia Datelor).
- TotuÈ™i, cu sistemele AI trebuie sÄƒ recunoaÈ™tem tensiunea dintre necesitatea mai multor date personale pentru a face sistemele mai personale È™i eficiente â€“ È™i confidenÈ›ialitatea.
- La fel ca la naÈ™terea computerelor conectate la internet, vedem o creÈ™tere semnificativÄƒ a numÄƒrului de probleme de securitate legate de AI.
- Ãn acelaÈ™i timp, am vÄƒzut AI fiind folosit pentru a Ã®mbunÄƒtÄƒÈ›i securitatea. De exemplu, majoritatea scanerelor antivirus moderne sunt alimentate de euristici AI.
- Trebuie sÄƒ ne asigurÄƒm cÄƒ procesele noastre de È™tiinÈ›a datelor se Ã®mbinÄƒ armonios cu cele mai recente practici de confidenÈ›ialitate È™i securitate.

### TransparenÈ›Äƒ

Sistemele AI ar trebui sÄƒ fie uÈ™or de Ã®nÈ›eles. O parte crucialÄƒ a transparenÈ›ei este explicarea comportamentului sistemelor AI È™i a componentelor acestora. ÃmbunÄƒtÄƒÈ›irea Ã®nÈ›elegerii sistemelor AI necesitÄƒ ca pÄƒrÈ›ile interesate sÄƒ Ã®nÈ›eleagÄƒ cum È™i de ce funcÈ›ioneazÄƒ acestea, astfel Ã®ncÃ¢t sÄƒ poatÄƒ identifica potenÈ›iale probleme de performanÈ›Äƒ, preocupÄƒri legate de siguranÈ›Äƒ È™i confidenÈ›ialitate, prejudecÄƒÈ›i, practici de excludere sau rezultate neintenÈ›ionate. De asemenea, credem cÄƒ cei care folosesc sistemele AI ar trebui sÄƒ fie sinceri È™i deschiÈ™i cu privire la momentul, motivul È™i modul Ã®n care aleg sÄƒ le implementeze. La fel ca È™i limitÄƒrile sistemelor pe care le folosesc. De exemplu, dacÄƒ o bancÄƒ foloseÈ™te un sistem AI pentru a sprijini deciziile de creditare pentru consumatori, este important sÄƒ examineze rezultatele È™i sÄƒ Ã®nÈ›eleagÄƒ ce date influenÈ›eazÄƒ recomandÄƒrile sistemului. Guvernele Ã®ncep sÄƒ reglementeze AI Ã®n diverse industrii, astfel Ã®ncÃ¢t specialiÈ™tii Ã®n date È™i organizaÈ›iile trebuie sÄƒ explice dacÄƒ un sistem AI Ã®ndeplineÈ™te cerinÈ›ele de reglementare, mai ales atunci cÃ¢nd existÄƒ un rezultat nedorit.

> [ğŸ¥ Click aici pentru un videoclip: transparenÈ›a Ã®n AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Deoarece sistemele AI sunt atÃ¢t de complexe, este dificil sÄƒ Ã®nÈ›elegem cum funcÈ›ioneazÄƒ È™i sÄƒ interpretÄƒm rezultatele.
- AceastÄƒ lipsÄƒ de Ã®nÈ›elegere afecteazÄƒ modul Ã®n care aceste sisteme sunt gestionate, operaÈ›ionalizate È™i documentate.
- Mai important, aceastÄƒ lipsÄƒ de Ã®nÈ›elegere afecteazÄƒ deciziile luate pe baza rezultatelor produse de aceste sisteme.

### Responsabilitate

Persoanele care proiecteazÄƒ È™i implementeazÄƒ sisteme AI trebuie sÄƒ fie responsabile pentru modul Ã®n care funcÈ›ioneazÄƒ sistemele lor. Necesitatea responsabilitÄƒÈ›ii este deosebit de crucialÄƒ Ã®n cazul tehnologiilor sensibile, cum ar fi recunoaÈ™terea facialÄƒ. Recent, a existat o cerere tot mai mare pentru tehnologia de recunoaÈ™tere facialÄƒ, mai ales din partea organizaÈ›iilor de aplicare a legii care vÄƒd potenÈ›ialul tehnologiei Ã®n utilizÄƒri precum gÄƒsirea copiilor dispÄƒruÈ›i. Cu toate acestea, aceste tehnologii ar putea fi utilizate de un guvern pentru a pune Ã®n pericol libertÄƒÈ›ile fundamentale ale cetÄƒÈ›enilor sÄƒi, de exemplu, prin supravegherea continuÄƒ a unor indivizi specifici. Prin urmare, specialiÈ™tii Ã®n date È™i organizaÈ›iile trebuie sÄƒ fie responsabili pentru modul Ã®n care sistemul lor AI afecteazÄƒ indivizii sau societatea.

[![CercetÄƒtor de top Ã®n AI avertizeazÄƒ asupra supravegherii masive prin recunoaÈ™tere facialÄƒ](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Abordarea Microsoft pentru AI Responsabil")

> ğŸ¥ Click pe imaginea de mai sus pentru un videoclip: Avertismente despre supravegherea masivÄƒ prin recunoaÈ™tere facialÄƒ

Ãn cele din urmÄƒ, una dintre cele mai mari Ã®ntrebÄƒri pentru generaÈ›ia noastrÄƒ, ca prima generaÈ›ie care aduce AI Ã®n societate, este cum sÄƒ ne asigurÄƒm cÄƒ computerele vor rÄƒmÃ¢ne responsabile faÈ›Äƒ de oameni È™i cum sÄƒ ne asigurÄƒm cÄƒ persoanele care proiecteazÄƒ computere rÄƒmÃ¢n responsabile faÈ›Äƒ de toÈ›i ceilalÈ›i.

## Evaluarea impactului

Ãnainte de a antrena un model de Ã®nvÄƒÈ›are automatÄƒ, este important sÄƒ efectuezi o evaluare a impactului pentru a Ã®nÈ›elege scopul sistemului AI; utilizarea intenÈ›ionatÄƒ; unde va fi implementat; È™i cine va interacÈ›iona cu sistemul. Acestea sunt utile pentru evaluatorii sau testerii care analizeazÄƒ sistemul pentru a È™ti ce factori sÄƒ ia Ã®n considerare atunci cÃ¢nd identificÄƒ riscuri potenÈ›iale È™i consecinÈ›e aÈ™teptate.

UrmÄƒtoarele sunt domenii de interes atunci cÃ¢nd se efectueazÄƒ o evaluare a impactului:

* **Impact negativ asupra indivizilor**. ConÈ™tientizarea oricÄƒror restricÈ›ii sau cerinÈ›e, utilizÄƒri neacceptate sau limitÄƒri cunoscute care Ã®mpiedicÄƒ performanÈ›a sistemului este vitalÄƒ pentru a ne asigura cÄƒ sistemul nu este utilizat Ã®ntr-un mod care ar putea dÄƒuna indivizilor.
* **CerinÈ›e de date**. ÃnÈ›elegerea modului È™i locului Ã®n care sistemul va utiliza datele permite evaluatorilor sÄƒ exploreze orice cerinÈ›e de date de care trebuie sÄƒ fii conÈ™tient (de exemplu, reglementÄƒrile GDPR sau HIPPA). Ãn plus, examineazÄƒ dacÄƒ sursa sau cantitatea de date este suficientÄƒ pentru antrenare.
* **Rezumatul impactului**. AdunÄƒ o listÄƒ de potenÈ›iale daune care ar putea apÄƒrea din utilizarea sistemului. Pe parcursul ciclului de viaÈ›Äƒ al ML, verificÄƒ dacÄƒ problemele identificate sunt atenuate sau abordate.
* **Obiective aplicabile** pentru fiecare dintre cele È™ase principii de bazÄƒ. EvalueazÄƒ dacÄƒ obiectivele fiecÄƒrui principiu sunt Ã®ndeplinite È™i dacÄƒ existÄƒ lacune.

## Debugging cu AI responsabil

Similar cu depanarea unei aplicaÈ›ii software, depanarea unui sistem AI este un proces necesar de identificare È™i rezolvare a problemelor din sistem. ExistÄƒ mulÈ›i factori care ar putea afecta un model sÄƒ nu funcÈ›ioneze conform aÈ™teptÄƒrilor sau responsabil. Majoritatea metricilor tradiÈ›ionale de performanÈ›Äƒ ale modelului sunt agregate cantitative ale performanÈ›ei modelului, care nu sunt suficiente pentru a analiza modul Ã®n care un model Ã®ncalcÄƒ principiile AI responsabil. Mai mult, un model de Ã®nvÄƒÈ›are automatÄƒ este o cutie neagrÄƒ, ceea ce face dificilÄƒ Ã®nÈ›elegerea a ceea ce determinÄƒ rezultatul sÄƒu sau oferirea unei explicaÈ›ii atunci cÃ¢nd face o greÈ™ealÄƒ. Mai tÃ¢rziu Ã®n acest curs, vom Ã®nvÄƒÈ›a cum sÄƒ folosim tabloul de bord AI Responsabil pentru a ajuta la depanarea sistemelor AI. Tabloul de bord oferÄƒ un instrument holistic pentru specialiÈ™tii Ã®n date È™i dezvoltatorii AI pentru a efectua:

* **Analiza erorilor**. Pentru a identifica distribuÈ›ia erorilor modelului care poate afecta echitatea sau fiabilitatea sistemului.
* **Prezentarea generalÄƒ a modelului**. Pentru a descoperi unde existÄƒ disparitÄƒÈ›i Ã®n performanÈ›a modelului Ã®n diferite cohorte de date.
* **Analiza datelor**. Pentru a Ã®nÈ›elege distribuÈ›ia datelor È™i a identifica orice potenÈ›ialÄƒ prejudecatÄƒ Ã®n date care ar putea duce la probleme de echitate, incluziune È™i fiabilitate.
* **Interpretabilitatea modelului**. Pentru a Ã®nÈ›elege ce afecteazÄƒ sau influenÈ›eazÄƒ predicÈ›iile modelului. Acest lucru ajutÄƒ la explicarea comportamentului modelului, ceea ce este important pentru transparenÈ›Äƒ È™i
UrmÄƒreÈ™te acest workshop pentru a aprofunda subiectele:

- Ãn cÄƒutarea AI responsabil: Aplicarea principiilor Ã®n practicÄƒ de Besmira Nushi, Mehrnoosh Sameki È™i Amit Sharma

[![Responsible AI Toolbox: Un cadru open-source pentru construirea AI responsabil](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Un cadru open-source pentru construirea AI responsabil")


> ğŸ¥ Click pe imaginea de mai sus pentru un videoclip: RAI Toolbox: Un cadru open-source pentru construirea AI responsabil de Besmira Nushi, Mehrnoosh Sameki È™i Amit Sharma

De asemenea, citeÈ™te:

- Centrul de resurse RAI al Microsoft: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Grupul de cercetare FATE al Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox:

- [Repository-ul GitHub Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

CiteÈ™te despre instrumentele Azure Machine Learning pentru asigurarea echitÄƒÈ›ii:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## TemÄƒ

[ExploreazÄƒ RAI Toolbox](assignment.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim sÄƒ asigurÄƒm acurateÈ›ea, vÄƒ rugÄƒm sÄƒ fiÈ›i conÈ™tienÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa natalÄƒ ar trebui considerat sursa autoritarÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm responsabilitatea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite care pot apÄƒrea din utilizarea acestei traduceri.