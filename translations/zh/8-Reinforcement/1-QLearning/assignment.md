<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-09-03T18:37:16+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "zh"
}
-->
# 一个更真实的世界

在我们的场景中，Peter几乎可以不感到疲惫或饥饿地四处移动。在一个更真实的世界中，他需要时不时地坐下来休息，还需要进食。让我们通过实现以下规则，使我们的世界更加真实：

1. 每次从一个地方移动到另一个地方，Peter会失去**能量**并增加一些**疲劳**。
2. Peter可以通过吃苹果来获得更多能量。
3. Peter可以通过在树下或草地上休息来消除疲劳（即走到棋盘上有树或草的地方——绿色区域）。
4. Peter需要找到并杀死狼。
5. 为了杀死狼，Peter需要达到一定的能量和疲劳水平，否则他会输掉战斗。

## 指导

使用原始的 [notebook.ipynb](notebook.ipynb) 笔记本作为解决方案的起点。

根据游戏规则修改上述奖励函数，运行强化学习算法以学习赢得游戏的最佳策略，并将随机游走的结果与您的算法进行比较，比较赢得和输掉的游戏数量。

> **Note**: 在您的新世界中，状态更加复杂，除了人的位置，还包括疲劳和能量水平。您可以选择将状态表示为一个元组 (Board,energy,fatigue)，或者为状态定义一个类（您可能还希望从 `Board` 派生），甚至修改原始的 `Board` 类（位于 [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py) 中）。

在您的解决方案中，请保留负责随机游走策略的代码，并在最后将您的算法结果与随机游走进行比较。

> **Note**: 您可能需要调整超参数以使其正常工作，尤其是训练的轮数。由于游戏的成功（与狼战斗）是一个罕见事件，您可以预期更长的训练时间。

## 评分标准

| 标准     | 卓越表现                                                                                                                                                                                             | 合格表现                                                                                                                                                                                | 需要改进                                                                                                                          |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
|          | 提供了一个笔记本，其中定义了新的世界规则、Q学习算法以及一些文字说明。Q学习能够显著改善与随机游走相比的结果。                                                                                         | 提供了笔记本，Q学习已实现并改善了与随机游走相比的结果，但改善不显著；或者笔记本文档较差，代码结构不够清晰。                                                                               | 对重新定义世界规则做了一些尝试，但Q学习算法未能正常工作，或者奖励函数未完全定义。                                                                                           |

---

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们不对因使用此翻译而产生的任何误解或误读承担责任。