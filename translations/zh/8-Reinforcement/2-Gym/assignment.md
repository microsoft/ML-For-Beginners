<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-03T18:44:12+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "zh"
}
-->
# 训练山地车

[OpenAI Gym](http://gym.openai.com) 的设计使得所有环境都提供相同的 API——即相同的方法 `reset`、`step` 和 `render`，以及相同的 **动作空间** 和 **观察空间** 抽象。因此，可以通过最小的代码更改，将相同的强化学习算法适配到不同的环境中。

## 山地车环境

[山地车环境](https://gym.openai.com/envs/MountainCar-v0/) 包含一辆被困在山谷中的小车：

目标是通过以下动作之一，在每一步中让小车驶出山谷并夺取旗帜：

| 值 | 含义 |
|---|---|
| 0 | 向左加速 |
| 1 | 不加速 |
| 2 | 向右加速 |

然而，这个问题的主要难点在于，小车的引擎动力不足，无法一次性爬上山顶。因此，唯一的成功方法是通过来回移动来积累动能。

观察空间仅包含两个值：

| 编号 | 观察值  | 最小值 | 最大值 |
|-----|--------------|-----|-----|
|  0  | 小车位置 | -1.2 | 0.6 |
|  1  | 小车速度 | -0.07 | 0.07 |

山地车的奖励系统相当复杂：

 * 如果智能体到达山顶的旗帜位置（位置 = 0.5），奖励为 0。
 * 如果智能体的位置小于 0.5，奖励为 -1。

当小车位置超过 0.5 或者回合长度超过 200 时，回合终止。

## 指导说明

将我们的强化学习算法适配到山地车问题中。以现有的 [notebook.ipynb](notebook.ipynb) 代码为起点，替换新的环境，修改状态离散化函数，并尝试通过最小的代码修改使现有算法能够进行训练。通过调整超参数来优化结果。

> **注意**: 可能需要调整超参数以使算法收敛。

## 评分标准

| 标准 | 优秀 | 合格 | 需要改进 |
| -------- | --------- | -------- | ----------------- |
|          | 成功从 CartPole 示例中适配 Q-Learning 算法，代码修改最小，能够在 200 步内解决夺旗问题。 | 从网上采用了新的 Q-Learning 算法，但文档记录良好；或者采用了现有算法，但未达到预期结果。 | 未能成功采用任何算法，但在解决方案上迈出了重要一步（实现了状态离散化、Q 表数据结构等）。 |

---

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保准确性，但请注意，自动翻译可能包含错误或不准确之处。应以原始语言的文档作为权威来源。对于关键信息，建议使用专业人工翻译。对于因使用本翻译而引起的任何误解或误读，我们概不负责。