# å°è½¦æ†å¹³è¡¡

åœ¨å‰ä¸€è¯¾ä¸­æˆ‘ä»¬è§£å†³çš„é—®é¢˜çœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªç©å…·é—®é¢˜ï¼Œä¼¼ä¹å¹¶ä¸é€‚ç”¨äºå®é™…åœºæ™¯ã€‚ä½†äº‹å®å¹¶éå¦‚æ­¤ï¼Œå› ä¸ºè®¸å¤šç°å®ä¸–ç•Œçš„é—®é¢˜ä¹Ÿæœ‰ç±»ä¼¼çš„åœºæ™¯ï¼ŒåŒ…æ‹¬ä¸‹æ£‹æˆ–å›´æ£‹ã€‚å®ƒä»¬æ˜¯ç›¸ä¼¼çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿæœ‰ä¸€ä¸ªå¸¦æœ‰ç»™å®šè§„åˆ™çš„æ£‹ç›˜å’Œä¸€ä¸ª**ç¦»æ•£çŠ¶æ€**ã€‚

## [è¯¾å‰æµ‹éªŒ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)

## ä»‹ç»

åœ¨æœ¬è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠQå­¦ä¹ çš„ç›¸åŒåŸç†åº”ç”¨åˆ°ä¸€ä¸ª**è¿ç»­çŠ¶æ€**çš„é—®é¢˜ä¸Šï¼Œå³ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå®æ•°ç»™å®šçš„çŠ¶æ€ã€‚æˆ‘ä»¬å°†å¤„ç†ä»¥ä¸‹é—®é¢˜ï¼š

> **é—®é¢˜**ï¼šå¦‚æœå½¼å¾—æƒ³è¦é€ƒç¦»ç‹¼ï¼Œä»–éœ€è¦èƒ½å¤Ÿæ›´å¿«åœ°ç§»åŠ¨ã€‚æˆ‘ä»¬å°†çœ‹çœ‹å½¼å¾—å¦‚ä½•å­¦ä¹ æ»‘å†°ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡Qå­¦ä¹ ä¿æŒå¹³è¡¡ã€‚

![å¤§é€ƒäº¡ï¼](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.zh.png)

> å½¼å¾—å’Œä»–çš„æœ‹å‹ä»¬æƒ³å‡ºå¦™æ‹›é€ƒç¦»ç‹¼ï¼å›¾ç‰‡æ¥è‡ª [Jen Looper](https://twitter.com/jenlooper)

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§ç§°ä¸º**å°è½¦æ†**é—®é¢˜çš„ç®€åŒ–å¹³è¡¡ç‰ˆæœ¬ã€‚åœ¨å°è½¦æ†ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¯ä»¥å·¦å³ç§»åŠ¨çš„æ°´å¹³æ»‘å—ï¼Œç›®æ ‡æ˜¯è®©å‚ç›´æ†åœ¨æ»‘å—ä¸Šä¿æŒå¹³è¡¡ã€‚

## å…ˆå†³æ¡ä»¶

åœ¨æœ¬è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåä¸º**OpenAI Gym**çš„åº“æ¥æ¨¡æ‹Ÿä¸åŒçš„**ç¯å¢ƒ**ã€‚ä½ å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œæœ¬è¯¾çš„ä»£ç ï¼ˆä¾‹å¦‚åœ¨Visual Studio Codeä¸­ï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡æ‹Ÿå°†ä¼šåœ¨ä¸€ä¸ªæ–°çª—å£ä¸­æ‰“å¼€ã€‚å½“åœ¨çº¿è¿è¡Œä»£ç æ—¶ï¼Œä½ å¯èƒ½éœ€è¦å¯¹ä»£ç è¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œå¦‚[è¿™é‡Œ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7)æ‰€è¿°ã€‚

## OpenAI Gym

åœ¨å‰ä¸€è¯¾ä¸­ï¼Œæ¸¸æˆçš„è§„åˆ™å’ŒçŠ¶æ€ç”±æˆ‘ä»¬è‡ªå·±å®šä¹‰çš„`Board`ç±»ç»™å‡ºã€‚è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„**æ¨¡æ‹Ÿç¯å¢ƒ**ï¼Œå®ƒå°†æ¨¡æ‹Ÿå¹³è¡¡æ†èƒŒåçš„ç‰©ç†ç°è±¡ã€‚è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•æœ€æµè¡Œçš„æ¨¡æ‹Ÿç¯å¢ƒä¹‹ä¸€æ˜¯ç”±[OpenAI](https://openai.com/)ç»´æŠ¤çš„[Gym](https://gym.openai.com/)ã€‚é€šè¿‡ä½¿ç”¨è¿™ä¸ªgymï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä»å°è½¦æ†æ¨¡æ‹Ÿåˆ°Atariæ¸¸æˆçš„ä¸åŒ**ç¯å¢ƒ**ã€‚

> **æ³¨æ„**ï¼šä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://gym.openai.com/envs/#classic_control)æŸ¥çœ‹OpenAI Gymæä¾›çš„å…¶ä»–ç¯å¢ƒã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®‰è£…gymå¹¶å¯¼å…¥æ‰€éœ€çš„åº“ï¼ˆä»£ç å—1ï¼‰ï¼š

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## ç»ƒä¹  - åˆå§‹åŒ–ä¸€ä¸ªå°è½¦æ†ç¯å¢ƒ

è¦å¤„ç†å°è½¦æ†å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ–ç›¸åº”çš„ç¯å¢ƒã€‚æ¯ä¸ªç¯å¢ƒéƒ½ä¸ä¸€ä¸ªï¼š

- **è§‚å¯Ÿç©ºé—´**ç›¸å…³ï¼Œå®šä¹‰æˆ‘ä»¬ä»ç¯å¢ƒä¸­æ¥æ”¶çš„ä¿¡æ¯ç»“æ„ã€‚å¯¹äºå°è½¦æ†é—®é¢˜ï¼Œæˆ‘ä»¬æ¥æ”¶æ†çš„ä½ç½®ã€é€Ÿåº¦å’Œå…¶ä»–ä¸€äº›å€¼ã€‚

- **åŠ¨ä½œç©ºé—´**ç›¸å…³ï¼Œå®šä¹‰å¯èƒ½çš„åŠ¨ä½œã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼ŒåŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£çš„ï¼Œç”±ä¸¤ä¸ªåŠ¨ä½œç»„æˆ - **å·¦**å’Œ**å³**ã€‚ï¼ˆä»£ç å—2ï¼‰

1. è¦åˆå§‹åŒ–ï¼Œè¯·è¾“å…¥ä»¥ä¸‹ä»£ç ï¼š

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

è¦æŸ¥çœ‹ç¯å¢ƒå¦‚ä½•å·¥ä½œï¼Œè®©æˆ‘ä»¬è¿è¡Œä¸€ä¸ª100æ­¥çš„ç®€çŸ­æ¨¡æ‹Ÿã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªè¦é‡‡å–çš„åŠ¨ä½œ - åœ¨æ­¤æ¨¡æ‹Ÿä¸­ï¼Œæˆ‘ä»¬åªæ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªæ¥è‡ª`action_space`çš„åŠ¨ä½œã€‚

1. è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œçœ‹çœ‹ä¼šå¯¼è‡´ä»€ä¹ˆç»“æœã€‚

    âœ… è®°ä½ï¼Œæœ€å¥½åœ¨æœ¬åœ°Pythonå®‰è£…ä¸­è¿è¡Œæ­¤ä»£ç ï¼ï¼ˆä»£ç å—3ï¼‰

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    ä½ åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„å›¾åƒï¼š

    ![æ— æ³•å¹³è¡¡çš„å°è½¦æ†](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. åœ¨æ¨¡æ‹Ÿè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è·å–è§‚å¯Ÿå€¼ä»¥å†³å®šå¦‚ä½•è¡ŒåŠ¨ã€‚äº‹å®ä¸Šï¼Œstepå‡½æ•°è¿”å›å½“å‰çš„è§‚å¯Ÿå€¼ã€å¥–åŠ±å‡½æ•°å’Œä¸€ä¸ªè¡¨ç¤ºæ˜¯å¦ç»§ç»­æ¨¡æ‹Ÿçš„å®Œæˆæ ‡å¿—ï¼šï¼ˆä»£ç å—4ï¼‰

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    ä½ å°†åœ¨ç¬”è®°æœ¬è¾“å‡ºä¸­çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„å†…å®¹ï¼š

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    åœ¨æ¨¡æ‹Ÿçš„æ¯ä¸€æ­¥è¿”å›çš„è§‚å¯Ÿå‘é‡åŒ…å«ä»¥ä¸‹å€¼ï¼š
    - å°è½¦çš„ä½ç½®
    - å°è½¦çš„é€Ÿåº¦
    - æ†çš„è§’åº¦
    - æ†çš„æ—‹è½¬é€Ÿç‡

1. è·å–è¿™äº›æ•°å­—çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼šï¼ˆä»£ç å—5ï¼‰

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    ä½ å¯èƒ½è¿˜ä¼šæ³¨æ„åˆ°ï¼Œæ¯ä¸€æ­¥æ¨¡æ‹Ÿçš„å¥–åŠ±å€¼æ€»æ˜¯1ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°½å¯èƒ½é•¿æ—¶é—´åœ°ç”Ÿå­˜ï¼Œå³å°½å¯èƒ½é•¿æ—¶é—´åœ°ä¿æŒæ†åœ¨åˆç†çš„å‚ç›´ä½ç½®ã€‚

    âœ… å®é™…ä¸Šï¼Œå¦‚æœæˆ‘ä»¬èƒ½åœ¨100æ¬¡è¿ç»­è¯•éªŒä¸­å¹³å‡è·å¾—195çš„å¥–åŠ±ï¼Œå°è½¦æ†æ¨¡æ‹Ÿå°±è¢«è®¤ä¸ºæ˜¯è§£å†³äº†ã€‚

## çŠ¶æ€ç¦»æ•£åŒ–

åœ¨Qå­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºQè¡¨ï¼Œå®šä¹‰åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹è¯¥åšä»€ä¹ˆã€‚ä¸ºäº†èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦çŠ¶æ€æ˜¯**ç¦»æ•£çš„**ï¼Œæ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒåº”è¯¥åŒ…å«æœ‰é™æ•°é‡çš„ç¦»æ•£å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä»¥æŸç§æ–¹å¼**ç¦»æ•£åŒ–**æˆ‘ä»¬çš„è§‚å¯Ÿå€¼ï¼Œå°†å®ƒä»¬æ˜ å°„åˆ°æœ‰é™çš„çŠ¶æ€é›†åˆã€‚

æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼š

- **åˆ’åˆ†ä¸ºç®±å­**ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“æŸä¸ªå€¼çš„åŒºé—´ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯¥åŒºé—´åˆ’åˆ†ä¸ºå¤šä¸ª**ç®±å­**ï¼Œç„¶åç”¨å®ƒæ‰€å±çš„ç®±å­ç¼–å·æ›¿æ¢è¯¥å€¼ã€‚è¿™å¯ä»¥ä½¿ç”¨numpyçš„[`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html)æ–¹æ³•æ¥å®Œæˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†å‡†ç¡®çŸ¥é“çŠ¶æ€çš„å¤§å°ï¼Œå› ä¸ºå®ƒå°†å–å†³äºæˆ‘ä»¬ä¸ºæ•°å­—åŒ–é€‰æ‹©çš„ç®±å­æ•°é‡ã€‚
  
âœ… æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çº¿æ€§æ’å€¼å°†å€¼å¸¦åˆ°æŸä¸ªæœ‰é™åŒºé—´ï¼ˆä¾‹å¦‚ï¼Œä»-20åˆ°20ï¼‰ï¼Œç„¶åé€šè¿‡å››èˆäº”å…¥å°†æ•°å­—è½¬æ¢ä¸ºæ•´æ•°ã€‚è¿™ç»™äº†æˆ‘ä»¬å¯¹çŠ¶æ€å¤§å°çš„æ§åˆ¶ç¨å¾®å°‘ä¸€äº›ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæˆ‘ä»¬ä¸çŸ¥é“è¾“å…¥å€¼çš„ç¡®åˆ‡èŒƒå›´ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ4ä¸ªå€¼ä¸­çš„2ä¸ªæ²¡æœ‰ä¸Š/ä¸‹é™ï¼Œè¿™å¯èƒ½å¯¼è‡´æ— é™æ•°é‡çš„çŠ¶æ€ã€‚

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒç§æ–¹æ³•ã€‚æ­£å¦‚ä½ å¯èƒ½ç¨åæ³¨æ„åˆ°çš„ï¼Œå°½ç®¡æ²¡æœ‰æ˜ç¡®çš„ä¸Š/ä¸‹é™ï¼Œè¿™äº›å€¼å¾ˆå°‘ä¼šå–åˆ°æŸäº›æœ‰é™åŒºé—´ä¹‹å¤–çš„å€¼ï¼Œå› æ­¤é‚£äº›å…·æœ‰æç«¯å€¼çš„çŠ¶æ€å°†éå¸¸ç½•è§ã€‚

1. è¿™æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­è·å–è§‚å¯Ÿå€¼å¹¶ç”Ÿæˆä¸€ä¸ªåŒ…å«4ä¸ªæ•´æ•°å€¼çš„å…ƒç»„ï¼šï¼ˆä»£ç å—6ï¼‰

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. è®©æˆ‘ä»¬è¿˜æ¢ç´¢å¦ä¸€ç§ä½¿ç”¨ç®±å­çš„ç¦»æ•£åŒ–æ–¹æ³•ï¼šï¼ˆä»£ç å—7ï¼‰

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸ªç®€çŸ­çš„æ¨¡æ‹Ÿå¹¶è§‚å¯Ÿè¿™äº›ç¦»æ•£çš„ç¯å¢ƒå€¼ã€‚éšæ„å°è¯•`discretize` and `discretize_bins`ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰å·®å¼‚ã€‚

    âœ… discretize_binsè¿”å›ç®±å­ç¼–å·ï¼Œè¿™æ˜¯ä»0å¼€å§‹çš„ã€‚å› æ­¤ï¼Œå¯¹äºæ¥è¿‘0çš„è¾“å…¥å˜é‡å€¼ï¼Œå®ƒè¿”å›åŒºé—´ä¸­é—´çš„æ•°å­—ï¼ˆ10ï¼‰ã€‚åœ¨discretizeä¸­ï¼Œæˆ‘ä»¬ä¸å…³å¿ƒè¾“å‡ºå€¼çš„èŒƒå›´ï¼Œå…è®¸å®ƒä»¬ä¸ºè´Ÿï¼Œå› æ­¤çŠ¶æ€å€¼æ²¡æœ‰åç§»ï¼Œ0å¯¹åº”äº0ã€‚ï¼ˆä»£ç å—8ï¼‰

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… å¦‚æœä½ æƒ³æŸ¥çœ‹ç¯å¢ƒçš„æ‰§è¡Œæƒ…å†µï¼Œè¯·å–æ¶ˆæ³¨é‡Šä»¥env.renderå¼€å¤´çš„è¡Œã€‚å¦åˆ™ï¼Œä½ å¯ä»¥åœ¨åå°æ‰§è¡Œå®ƒï¼Œè¿™æ ·æ›´å¿«ã€‚åœ¨æˆ‘ä»¬çš„Qå­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ç§â€œéšå½¢â€æ‰§è¡Œã€‚

## Qè¡¨ç»“æ„

åœ¨å‰ä¸€è¯¾ä¸­ï¼ŒçŠ¶æ€æ˜¯ä»0åˆ°8çš„ç®€å•æ•°å¯¹ï¼Œå› æ­¤ç”¨å½¢çŠ¶ä¸º8x8x2çš„numpyå¼ é‡è¡¨ç¤ºQè¡¨æ˜¯æ–¹ä¾¿çš„ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç®±å­ç¦»æ•£åŒ–ï¼ŒçŠ¶æ€å‘é‡çš„å¤§å°ä¹Ÿæ˜¯å·²çŸ¥çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ï¼Œç”¨å½¢çŠ¶ä¸º20x20x10x10x2çš„æ•°ç»„è¡¨ç¤ºçŠ¶æ€ï¼ˆè¿™é‡Œ2æ˜¯åŠ¨ä½œç©ºé—´çš„ç»´åº¦ï¼Œå‰é¢çš„ç»´åº¦å¯¹åº”äºæˆ‘ä»¬ä¸ºè§‚å¯Ÿç©ºé—´ä¸­çš„æ¯ä¸ªå‚æ•°é€‰æ‹©çš„ç®±å­æ•°é‡ï¼‰ã€‚

ç„¶è€Œï¼Œæœ‰æ—¶è§‚å¯Ÿç©ºé—´çš„ç¡®åˆ‡ç»´åº¦æ˜¯ä¸çŸ¥é“çš„ã€‚åœ¨`discretize`å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½æ°¸è¿œæ— æ³•ç¡®å®šæˆ‘ä»¬çš„çŠ¶æ€æ˜¯å¦ä¿æŒåœ¨æŸäº›é™åˆ¶å†…ï¼Œå› ä¸ºæŸäº›åŸå§‹å€¼æ²¡æœ‰é™åˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§ç¨å¾®ä¸åŒçš„æ–¹æ³•ï¼Œç”¨å­—å…¸è¡¨ç¤ºQè¡¨ã€‚

1. ä½¿ç”¨å¯¹*(state,action)*ä½œä¸ºå­—å…¸é”®ï¼Œå€¼å¯¹åº”äºQè¡¨æ¡ç›®å€¼ã€‚ï¼ˆä»£ç å—9ï¼‰

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    è¿™é‡Œæˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°`qvalues()`ï¼Œå®ƒè¿”å›ç»™å®šçŠ¶æ€å¯¹åº”äºæ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„Qè¡¨å€¼åˆ—è¡¨ã€‚å¦‚æœQè¡¨ä¸­æ²¡æœ‰è¯¥æ¡ç›®ï¼Œæˆ‘ä»¬å°†è¿”å›0ä½œä¸ºé»˜è®¤å€¼ã€‚

## å¼€å§‹Qå­¦ä¹ 

ç°åœ¨æˆ‘ä»¬å‡†å¤‡æ•™å½¼å¾—ä¿æŒå¹³è¡¡äº†ï¼

1. é¦–å…ˆï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€äº›è¶…å‚æ•°ï¼šï¼ˆä»£ç å—10ï¼‰

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    è¿™é‡Œï¼Œ`alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.

    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.

    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. 

    âœ… In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those "mistakes"

### Improve the algorithm

We can also make two improvements to our algorithm from the previous lesson:

- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.
  
- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.

1. Collect all cumulative rewards at each simulation at `rewards`å‘é‡ä»¥ä¾¿è¿›ä¸€æ­¥ç»˜å›¾ã€‚ï¼ˆä»£ç å—11ï¼‰

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

ä½ å¯èƒ½ä»è¿™äº›ç»“æœä¸­æ³¨æ„åˆ°ï¼š

- **æ¥è¿‘æˆ‘ä»¬çš„ç›®æ ‡**ã€‚æˆ‘ä»¬éå¸¸æ¥è¿‘å®ç°ç›®æ ‡ï¼Œå³åœ¨100+æ¬¡è¿ç»­è¿è¡Œæ¨¡æ‹Ÿä¸­è·å¾—195çš„ç´¯è®¡å¥–åŠ±ï¼Œæˆ–è€…æˆ‘ä»¬å®é™…ä¸Šå·²ç»å®ç°äº†ï¼å³ä½¿æˆ‘ä»¬å¾—åˆ°è¾ƒå°çš„æ•°å­—ï¼Œæˆ‘ä»¬ä¹Ÿä¸çŸ¥é“ï¼Œå› ä¸ºæˆ‘ä»¬å¹³å‡è¶…è¿‡5000æ¬¡è¿è¡Œï¼Œè€Œæ­£å¼æ ‡å‡†ä»…éœ€è¦100æ¬¡è¿è¡Œã€‚
  
- **å¥–åŠ±å¼€å§‹ä¸‹é™**ã€‚æœ‰æ—¶å¥–åŠ±å¼€å§‹ä¸‹é™ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯èƒ½ç”¨ä½¿æƒ…å†µå˜å¾—æ›´ç³Ÿçš„æ–°å€¼â€œç ´åâ€äº†Qè¡¨ä¸­å·²ç»å­¦ä¹ åˆ°çš„å€¼ã€‚

å¦‚æœæˆ‘ä»¬ç»˜åˆ¶è®­ç»ƒè¿›åº¦ï¼Œè¿™ä¸€è§‚å¯Ÿç»“æœä¼šæ›´åŠ æ˜æ˜¾ã€‚

## ç»˜åˆ¶è®­ç»ƒè¿›åº¦

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å·²ç»å°†æ¯æ¬¡è¿­ä»£çš„ç´¯è®¡å¥–åŠ±å€¼æ”¶é›†åˆ°`rewards`å‘é‡ä¸­ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†å…¶ä¸è¿­ä»£æ¬¡æ•°ä¸€èµ·ç»˜åˆ¶æ—¶çš„æ ·å­ï¼š

```python
plt.plot(rewards)
```

![åŸå§‹è¿›åº¦](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.zh.png)

ä»è¿™ä¸ªå›¾è¡¨ä¸­ï¼Œæˆ‘ä»¬æ— æ³•åˆ¤æ–­ä»»ä½•äº‹æƒ…ï¼Œå› ä¸ºç”±äºéšæœºè®­ç»ƒè¿‡ç¨‹çš„æ€§è´¨ï¼Œè®­ç»ƒä¼šè¯çš„é•¿åº¦å˜åŒ–å¾ˆå¤§ã€‚ä¸ºäº†ä½¿è¿™ä¸ªå›¾è¡¨æ›´æœ‰æ„ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸€ç³»åˆ—å®éªŒçš„**ç§»åŠ¨å¹³å‡å€¼**ï¼Œæ¯”å¦‚100ã€‚è¿™å¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨`np.convolve`å®Œæˆï¼šï¼ˆä»£ç å—12ï¼‰

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![è®­ç»ƒè¿›åº¦](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.zh.png)

## è°ƒæ•´è¶…å‚æ•°

ä¸ºäº†ä½¿å­¦ä¹ æ›´åŠ ç¨³å®šï¼Œæœ‰å¿…è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´ä¸€äº›è¶…å‚æ•°ã€‚ç‰¹åˆ«æ˜¯ï¼š

- **å¯¹äºå­¦ä¹ ç‡**ï¼Œ`alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.

- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon`ï¼Œå¹¶ä¸Šå‡åˆ°å‡ ä¹1ã€‚

> **ä»»åŠ¡1**ï¼šå°è¯•è°ƒæ•´è¶…å‚æ•°å€¼ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½è·å¾—æ›´é«˜çš„ç´¯è®¡å¥–åŠ±ã€‚ä½ èƒ½è¾¾åˆ°195ä»¥ä¸Šå—ï¼Ÿ

> **ä»»åŠ¡2**ï¼šè¦æ­£å¼è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦åœ¨100æ¬¡è¿ç»­è¿è¡Œä¸­è·å¾—195çš„å¹³å‡å¥–åŠ±ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æµ‹é‡è¿™ä¸€ç‚¹ï¼Œå¹¶ç¡®ä¿ä½ å·²ç»æ­£å¼è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼

## æŸ¥çœ‹ç»“æœ

å®é™…ä¸Šçœ‹åˆ°è®­ç»ƒæ¨¡å‹çš„è¡Œä¸ºä¼šå¾ˆæœ‰è¶£ã€‚è®©æˆ‘ä»¬è¿è¡Œæ¨¡æ‹Ÿï¼Œå¹¶æŒ‰ç…§è®­ç»ƒæœŸé—´çš„ç›¸åŒåŠ¨ä½œé€‰æ‹©ç­–ç•¥ï¼Œæ ¹æ®Qè¡¨ä¸­çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ï¼šï¼ˆä»£ç å—13ï¼‰

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

ä½ åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„å›¾åƒï¼š

![å¹³è¡¡çš„å°è½¦æ†](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€æŒ‘æˆ˜

> **ä»»åŠ¡3**ï¼šè¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯Qè¡¨çš„æœ€ç»ˆå‰¯æœ¬ï¼Œè¿™å¯èƒ½ä¸æ˜¯æœ€å¥½çš„ã€‚è®°ä½æˆ‘ä»¬å·²ç»å°†è¡¨ç°æœ€å¥½çš„Qè¡¨å­˜å‚¨åœ¨`Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.

> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax`å‡½æ•°æ¥æ‰¾åˆ°å¯¹åº”äºæœ€é«˜Qè¡¨å€¼çš„åŠ¨ä½œç¼–å·ã€‚å®ç°è¿™ä¸€ç­–ç•¥ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æ”¹å–„äº†å¹³è¡¡ã€‚

## [è¯¾åæµ‹éªŒ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## ä½œä¸š
[è®­ç»ƒä¸€è¾†å±±åœ°è½¦](assignment.md)

## ç»“è®º

æˆ‘ä»¬ç°åœ¨å·²ç»å­¦ä¼šäº†å¦‚ä½•é€šè¿‡æä¾›å®šä¹‰æ¸¸æˆæœŸæœ›çŠ¶æ€çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶ç»™ä»–ä»¬æœºä¼šæ™ºèƒ½åœ°æ¢ç´¢æœç´¢ç©ºé—´ï¼Œæ¥è®­ç»ƒæ™ºèƒ½ä½“ä»¥å–å¾—è‰¯å¥½çš„ç»“æœã€‚æˆ‘ä»¬å·²ç»æˆåŠŸåœ°åœ¨ç¦»æ•£å’Œè¿ç»­ç¯å¢ƒä¸­åº”ç”¨äº†Qå­¦ä¹ ç®—æ³•ï¼Œä½†åŠ¨ä½œä»ç„¶æ˜¯ç¦»æ•£çš„ã€‚

é‡è¦çš„æ˜¯è¿˜è¦ç ”ç©¶åŠ¨ä½œçŠ¶æ€ä¹Ÿæ˜¯è¿ç»­çš„æƒ…å†µï¼Œä»¥åŠè§‚å¯Ÿç©ºé—´æ›´å¤æ‚çš„æƒ…å†µï¼Œä¾‹å¦‚æ¥è‡ªAtariæ¸¸æˆå±å¹•çš„å›¾åƒã€‚åœ¨è¿™äº›é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä½¿ç”¨æ›´å¼ºå¤§çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œï¼Œä»¥å–å¾—è‰¯å¥½çš„ç»“æœã€‚è¿™äº›æ›´é«˜çº§çš„è¯é¢˜æ˜¯æˆ‘ä»¬å³å°†åˆ°æ¥çš„æ›´é«˜çº§AIè¯¾ç¨‹çš„ä¸»é¢˜ã€‚

**å…è´£å£°æ˜**ï¼š
æœ¬æ–‡æ¡£æ˜¯ä½¿ç”¨åŸºäºæœºå™¨çš„äººå·¥æ™ºèƒ½ç¿»è¯‘æœåŠ¡ç¿»è¯‘çš„ã€‚å°½ç®¡æˆ‘ä»¬åŠªåŠ›ç¡®ä¿å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åº”å°†åŸæ–‡æ¡£çš„æ¯è¯­ç‰ˆæœ¬è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºå…³é”®ä¿¡æ¯ï¼Œå»ºè®®è¿›è¡Œä¸“ä¸šçš„äººå·¥ç¿»è¯‘ã€‚æˆ‘ä»¬ä¸å¯¹ä½¿ç”¨æ­¤ç¿»è¯‘æ‰€äº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯é‡Šæ‰¿æ‹…è´£ä»»ã€‚