# ä½¿ç”¨ Scikit-learn æ„å»ºå›å½’æ¨¡å‹ï¼šå››ç§å›å½’æ–¹æ³•

![çº¿æ€§å›å½’ä¸å¤šé¡¹å¼å›å½’ä¿¡æ¯å›¾](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.zh.png)
> ä¿¡æ¯å›¾ä½œè€… [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [è¯¾å‰æµ‹éªŒ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [è¿™èŠ‚è¯¾ä¹Ÿæœ‰ R ç‰ˆæœ¬ï¼](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### ä»‹ç»

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»ç”¨å—ç“œå®šä»·æ•°æ®é›†çš„ç¤ºä¾‹æ•°æ®æ¢ç´¢äº†ä»€ä¹ˆæ˜¯å›å½’ï¼Œå¹¶ç”¨ Matplotlib å¯¹å…¶è¿›è¡Œäº†å¯è§†åŒ–ã€‚

ç°åœ¨ä½ å·²ç»å‡†å¤‡å¥½æ·±å…¥äº†è§£æœºå™¨å­¦ä¹ ä¸­çš„å›å½’ã€‚è™½ç„¶å¯è§†åŒ–å¯ä»¥å¸®åŠ©ä½ ç†è§£æ•°æ®ï¼Œä½†æœºå™¨å­¦ä¹ çš„çœŸæ­£åŠ›é‡åœ¨äº_è®­ç»ƒæ¨¡å‹_ã€‚æ¨¡å‹åœ¨å†å²æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥è‡ªåŠ¨æ•æ‰æ•°æ®ä¾èµ–å…³ç³»ï¼Œå¹¶å…è®¸ä½ é¢„æµ‹æ¨¡å‹ä»æœªè§è¿‡çš„æ–°æ•°æ®çš„ç»“æœã€‚

åœ¨æœ¬è¯¾ä¸­ï¼Œä½ å°†å­¦ä¹ ä¸¤ç§ç±»å‹çš„å›å½’ï¼š_åŸºæœ¬çº¿æ€§å›å½’_å’Œ_å¤šé¡¹å¼å›å½’_ï¼Œä»¥åŠè¿™äº›æŠ€æœ¯èƒŒåçš„ä¸€äº›æ•°å­¦åŸç†ã€‚è¿™äº›æ¨¡å‹å°†å…è®¸æˆ‘ä»¬æ ¹æ®ä¸åŒçš„è¾“å…¥æ•°æ®é¢„æµ‹å—ç“œä»·æ ¼ã€‚

[![åˆå­¦è€…çš„æœºå™¨å­¦ä¹  - ç†è§£çº¿æ€§å›å½’](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "åˆå­¦è€…çš„æœºå™¨å­¦ä¹  - ç†è§£çº¿æ€§å›å½’")

> ğŸ¥ ç‚¹å‡»ä¸Šé¢çš„å›¾ç‰‡è§‚çœ‹å…³äºçº¿æ€§å›å½’çš„ç®€çŸ­è§†é¢‘æ¦‚è¿°ã€‚

> åœ¨æ•´ä¸ªè¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æ•°å­¦çŸ¥è¯†æœ€ä½ï¼Œå¹¶åŠªåŠ›ä½¿å…¶å¯¹æ¥è‡ªå…¶ä»–é¢†åŸŸçš„å­¦ç”Ÿå¯è®¿é—®ï¼Œå› æ­¤è¯·æ³¨æ„ç¬”è®°ã€ğŸ§® æ ‡æ³¨ã€å›¾è¡¨å’Œå…¶ä»–å­¦ä¹ å·¥å…·ä»¥å¸®åŠ©ç†è§£ã€‚

### å‰ææ¡ä»¶

åˆ°ç°åœ¨ä¸ºæ­¢ï¼Œä½ åº”è¯¥ç†Ÿæ‚‰æˆ‘ä»¬æ­£åœ¨æ£€æŸ¥çš„å—ç“œæ•°æ®çš„ç»“æ„ã€‚ä½ å¯ä»¥åœ¨æœ¬è¯¾çš„_notebook.ipynb_æ–‡ä»¶ä¸­æ‰¾åˆ°é¢„åŠ è½½å’Œé¢„æ¸…ç†çš„æ•°æ®ã€‚åœ¨æ–‡ä»¶ä¸­ï¼Œå—ç“œä»·æ ¼æŒ‰è’²å¼è€³æ˜¾ç¤ºåœ¨ä¸€ä¸ªæ–°çš„æ•°æ®æ¡†ä¸­ã€‚ç¡®ä¿ä½ å¯ä»¥åœ¨ Visual Studio Code çš„å†…æ ¸ä¸­è¿è¡Œè¿™äº›ç¬”è®°æœ¬ã€‚

### å‡†å¤‡å·¥ä½œ

æé†’ä¸€ä¸‹ï¼Œä½ æ­£åœ¨åŠ è½½è¿™äº›æ•°æ®ä»¥ä¾¿æå‡ºé—®é¢˜ã€‚

- ä»€ä¹ˆæ—¶å€™æ˜¯è´­ä¹°å—ç“œçš„æœ€ä½³æ—¶é—´ï¼Ÿ
- æˆ‘å¯ä»¥æœŸå¾…ä¸€ä¸ªè¿·ä½ å—ç“œç›’çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ
- æˆ‘åº”è¯¥è´­ä¹°åŠè’²å¼è€³çš„ç¯®å­è¿˜æ˜¯1 1/9è’²å¼è€³çš„ç›’å­ï¼Ÿ
è®©æˆ‘ä»¬ç»§ç»­æ·±å…¥æŒ–æ˜è¿™äº›æ•°æ®ã€‚

åœ¨ä¸Šä¸€è¯¾ä¸­ï¼Œä½ åˆ›å»ºäº†ä¸€ä¸ª Pandas æ•°æ®æ¡†ï¼Œå¹¶ç”¨éƒ¨åˆ†åŸå§‹æ•°æ®é›†å¡«å……å®ƒï¼ŒæŒ‰è’²å¼è€³æ ‡å‡†åŒ–ä»·æ ¼ã€‚ç„¶è€Œï¼Œé€šè¿‡è¿™æ ·åšï¼Œä½ åªèƒ½æ”¶é›†åˆ°çº¦400ä¸ªæ•°æ®ç‚¹ï¼Œè€Œä¸”åªæ˜¯åœ¨ç§‹å­£æœˆä»½ã€‚

çœ‹çœ‹æˆ‘ä»¬åœ¨æœ¬è¯¾é…å¥—ç¬”è®°æœ¬ä¸­é¢„åŠ è½½çš„æ•°æ®ã€‚æ•°æ®å·²é¢„åŠ è½½ï¼Œå¹¶ç»˜åˆ¶äº†åˆå§‹æ•£ç‚¹å›¾ä»¥æ˜¾ç¤ºæœˆä»½æ•°æ®ã€‚ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿›ä¸€æ­¥æ¸…ç†æ•°æ®æ¥äº†è§£æ›´å¤šå…³äºæ•°æ®çš„æ€§è´¨ã€‚

## çº¿æ€§å›å½’çº¿

æ­£å¦‚ä½ åœ¨ç¬¬ä¸€è¯¾ä¸­æ‰€å­¦ï¼Œçº¿æ€§å›å½’ç»ƒä¹ çš„ç›®æ ‡æ˜¯èƒ½å¤Ÿç»˜åˆ¶ä¸€æ¡çº¿æ¥ï¼š

- **æ˜¾ç¤ºå˜é‡å…³ç³»**ã€‚æ˜¾ç¤ºå˜é‡ä¹‹é—´çš„å…³ç³»
- **åšå‡ºé¢„æµ‹**ã€‚å‡†ç¡®é¢„æµ‹æ–°æ•°æ®ç‚¹åœ¨è¯¥çº¿ä¸Šçš„ä½ç½®ã€‚

è¿™ç§ç±»å‹çš„çº¿é€šå¸¸æ˜¯é€šè¿‡**æœ€å°äºŒä¹˜å›å½’**ç»˜åˆ¶çš„ã€‚æœ¯è¯­â€œæœ€å°äºŒä¹˜â€æ„å‘³ç€æ‰€æœ‰å›´ç»•å›å½’çº¿çš„æ•°æ®ç‚¹éƒ½è¢«å¹³æ–¹ç„¶åç›¸åŠ ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæœ€ç»ˆçš„æ€»å’Œå°½å¯èƒ½å°ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›é”™è¯¯æ•°é‡ä½ï¼Œå³`least-squares`ã€‚

æˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬å¸Œæœ›å»ºæ¨¡ä¸€æ¡çº¿ï¼Œä½¿å…¶ä¸æ‰€æœ‰æ•°æ®ç‚¹çš„ç´¯è®¡è·ç¦»æœ€å°ã€‚æˆ‘ä»¬åœ¨ç›¸åŠ ä¹‹å‰å°†é¡¹å¹³æ–¹ï¼Œå› ä¸ºæˆ‘ä»¬å…³å¿ƒçš„æ˜¯å®ƒçš„å¤§å°è€Œä¸æ˜¯æ–¹å‘ã€‚

> **ğŸ§® ç»™æˆ‘å±•ç¤ºæ•°å­¦**
>
> è¿™æ¡çº¿ï¼Œç§°ä¸º_æœ€ä½³æ‹Ÿåˆçº¿_ï¼Œå¯ä»¥é€šè¿‡[ä¸€ä¸ªæ–¹ç¨‹](https://en.wikipedia.org/wiki/Simple_linear_regression)æ¥è¡¨ç¤ºï¼š
>
> ```
> Y = a + bX
> ```
>
> `X` is the 'explanatory variable'. `Y` is the 'dependent variable'. The slope of the line is `b` and `a` is the y-intercept, which refers to the value of `Y` when `X = 0`. 
>
>![calculate the slope](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.zh.png)
>
> First, calculate the slope `b`. Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> In other words, and referring to our pumpkin data's original question: "predict the price of a pumpkin per bushel by month", `X` would refer to the price and `Y` would refer to the month of sale. 
>
>![complete the equation](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.zh.png)
>
> Calculate the value of Y. If you're paying around $4, it must be April! Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where `Y` is situated when `X = 0`.
>
> You can observe the method of calculation for these values on the [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) web site. Also visit [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) to watch how the numbers' values impact the line.

## Correlation

One more term to understand is the **Correlation Coefficient** between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.

A good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.

âœ… Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of `Month`, eg. *day of the year* (i.e. number of days since the beginning of the year)?

In the code below, we will assume that we have cleaned up the data, and obtained a data frame called `new_pumpkins`, similar to the following:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> The code to clean the data is available in [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). We have performed the same cleaning steps as in the previous lesson, and have calculated `DayOfYear` åˆ—ä½¿ç”¨ä»¥ä¸‹è¡¨è¾¾å¼ï¼š

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

ç°åœ¨ä½ å·²ç»äº†è§£äº†çº¿æ€§å›å½’èƒŒåçš„æ•°å­¦åŸç†ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå›å½’æ¨¡å‹ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥é¢„æµ‹å“ªç§å—ç“œåŒ…è£…çš„ä»·æ ¼æœ€å¥½ã€‚æœ‰äººä¸ºèŠ‚æ—¥å—ç“œç”°è´­ä¹°å—ç“œå¯èƒ½éœ€è¦è¿™äº›ä¿¡æ¯ï¼Œä»¥ä¼˜åŒ–ä»–ä»¬çš„å—ç“œåŒ…è´­ä¹°ã€‚

## å¯»æ‰¾ç›¸å…³æ€§

[![åˆå­¦è€…çš„æœºå™¨å­¦ä¹  - å¯»æ‰¾ç›¸å…³æ€§ï¼šçº¿æ€§å›å½’çš„å…³é”®](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "åˆå­¦è€…çš„æœºå™¨å­¦ä¹  - å¯»æ‰¾ç›¸å…³æ€§ï¼šçº¿æ€§å›å½’çš„å…³é”®")

> ğŸ¥ ç‚¹å‡»ä¸Šé¢çš„å›¾ç‰‡è§‚çœ‹å…³äºç›¸å…³æ€§çš„ç®€çŸ­è§†é¢‘æ¦‚è¿°ã€‚

ä»ä¸Šä¸€è¯¾ä¸­ä½ å¯èƒ½å·²ç»çœ‹åˆ°ï¼Œä¸åŒæœˆä»½çš„å¹³å‡ä»·æ ¼å¦‚ä¸‹æ‰€ç¤ºï¼š

<img alt="æŒ‰æœˆå¹³å‡ä»·æ ¼" src="../2-Data/images/barchart.png" width="50%"/>

è¿™è¡¨æ˜åº”è¯¥æœ‰ä¸€äº›ç›¸å…³æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹æ¥é¢„æµ‹`Month` and `Price`, or between `DayOfYear` and `Price`. Here is the scatter plot that shows the latter relationship:

<img alt="Scatter plot of Price vs. Day of Year" src="images/scatter-dayofyear.png" width="50%" /> 

Let's see if there is a correlation using the `corr`å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼š

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

çœ‹èµ·æ¥ç›¸å…³æ€§å¾ˆå°ï¼Œ-0.15 é€šè¿‡`Month` and -0.17 by the `DayOfMonth`, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an `ax` parameter to the `scatter`ç»˜å›¾å‡½æ•°æˆ‘ä»¬å¯ä»¥åœ¨åŒä¸€å¼ å›¾ä¸Šç»˜åˆ¶æ‰€æœ‰ç‚¹ï¼š

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="ä»·æ ¼ä¸å¹´åº¦å¤©æ•°çš„æ•£ç‚¹å›¾" src="images/scatter-dayofyear-color.png" width="50%" />

æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œå“ç§å¯¹æ€»ä½“ä»·æ ¼çš„å½±å“æ¯”å®é™…é”€å”®æ—¥æœŸæ›´å¤§ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ±çŠ¶å›¾çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="ä»·æ ¼ä¸å“ç§çš„æŸ±çŠ¶å›¾" src="images/price-by-variety.png" width="50%" />

è®©æˆ‘ä»¬æš‚æ—¶åªå…³æ³¨ä¸€ç§å—ç“œå“ç§ï¼Œâ€œæ´¾å‹â€ï¼Œçœ‹çœ‹æ—¥æœŸå¯¹ä»·æ ¼çš„å½±å“ï¼š

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="ä»·æ ¼ä¸å¹´åº¦å¤©æ•°çš„æ•£ç‚¹å›¾" src="images/pie-pumpkins-scatter.png" width="50%" />

å¦‚æœæˆ‘ä»¬ç°åœ¨è®¡ç®—`Price` and `DayOfYear` using `corr` function, we will get something like `-0.27`ä¹‹é—´çš„ç›¸å…³æ€§ - è¿™æ„å‘³ç€è®­ç»ƒé¢„æµ‹æ¨¡å‹æ˜¯æœ‰æ„ä¹‰çš„ã€‚

> åœ¨è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹ä¹‹å‰ï¼Œç¡®ä¿æ•°æ®å¹²å‡€æ˜¯å¾ˆé‡è¦çš„ã€‚çº¿æ€§å›å½’ä¸é€‚ç”¨äºç¼ºå¤±å€¼ï¼Œå› æ­¤æ¸…é™¤æ‰€æœ‰ç©ºå•å…ƒæ ¼æ˜¯æœ‰æ„ä¹‰çš„ï¼š

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

å¦ä¸€ç§æ–¹æ³•æ˜¯ç”¨ç›¸åº”åˆ—çš„å¹³å‡å€¼å¡«å……è¿™äº›ç©ºå€¼ã€‚

## ç®€å•çº¿æ€§å›å½’

[![åˆå­¦è€…çš„æœºå™¨å­¦ä¹  - ä½¿ç”¨ Scikit-learn çš„çº¿æ€§å’Œå¤šé¡¹å¼å›å½’](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "åˆå­¦è€…çš„æœºå™¨å­¦ä¹  - ä½¿ç”¨ Scikit-learn çš„çº¿æ€§å’Œå¤šé¡¹å¼å›å½’")

> ğŸ¥ ç‚¹å‡»ä¸Šé¢çš„å›¾ç‰‡è§‚çœ‹å…³äºçº¿æ€§å’Œå¤šé¡¹å¼å›å½’çš„ç®€çŸ­è§†é¢‘æ¦‚è¿°ã€‚

ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**Scikit-learn**åº“ã€‚

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

æˆ‘ä»¬é¦–å…ˆå°†è¾“å…¥å€¼ï¼ˆç‰¹å¾ï¼‰å’Œé¢„æœŸè¾“å‡ºï¼ˆæ ‡ç­¾ï¼‰åˆ†ç¦»åˆ°å•ç‹¬çš„ numpy æ•°ç»„ä¸­ï¼š

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹è¾“å…¥æ•°æ®æ‰§è¡Œ`reshape`æ“ä½œï¼Œä»¥ä¾¿çº¿æ€§å›å½’åŒ…èƒ½å¤Ÿæ­£ç¡®ç†è§£å®ƒã€‚çº¿æ€§å›å½’æœŸæœ›è¾“å…¥ä¸ºäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ•°ç»„çš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªè¾“å…¥ç‰¹å¾å‘é‡ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç”±äºæˆ‘ä»¬åªæœ‰ä¸€ä¸ªè¾“å…¥ - æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå½¢çŠ¶ä¸º NÃ—1 çš„æ•°ç»„ï¼Œå…¶ä¸­ N æ˜¯æ•°æ®é›†å¤§å°ã€‚

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®åˆ†æˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒåéªŒè¯æˆ‘ä»¬çš„æ¨¡å‹ï¼š

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

æœ€åï¼Œè®­ç»ƒå®é™…çš„çº¿æ€§å›å½’æ¨¡å‹åªéœ€è¦ä¸¤è¡Œä»£ç ã€‚æˆ‘ä»¬å®šä¹‰`LinearRegression` object, and fit it to our data using the `fit`æ–¹æ³•ï¼š

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression` object after `fit`-ting contains all the coefficients of the regression, which can be accessed using `.coef_` property. In our case, there is just one coefficient, which should be around `-0.017`. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using `lin_reg.intercept_` - it will be around `21`åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¡¨ç¤ºå¹´åˆçš„ä»·æ ¼ã€‚

ä¸ºäº†æŸ¥çœ‹æˆ‘ä»¬çš„æ¨¡å‹æœ‰å¤šå‡†ç¡®ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šé¢„æµ‹ä»·æ ¼ï¼Œç„¶åæµ‹é‡æˆ‘ä»¬çš„é¢„æµ‹ä¸é¢„æœŸå€¼çš„æ¥è¿‘ç¨‹åº¦ã€‚è¿™å¯ä»¥ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŒ‡æ ‡æ¥å®Œæˆï¼Œè¿™æ˜¯æ‰€æœ‰é¢„æœŸå€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„å¹³æ–¹å·®çš„å¹³å‡å€¼ã€‚

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

æˆ‘ä»¬çš„è¯¯å·®ä¼¼ä¹åœ¨2ç‚¹å·¦å³ï¼Œå¤§çº¦æ˜¯17%ã€‚å¦ä¸€ä¸ªæ¨¡å‹è´¨é‡çš„æŒ‡æ ‡æ˜¯**å†³å®šç³»æ•°**ï¼Œå¯ä»¥è¿™æ ·è·å¾—ï¼š

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
å¦‚æœå€¼ä¸º0ï¼Œåˆ™æ„å‘³ç€æ¨¡å‹ä¸è€ƒè™‘è¾“å…¥æ•°æ®ï¼Œå¹¶ä½œä¸º*æœ€å·®çº¿æ€§é¢„æµ‹å™¨*ï¼Œå³ç»“æœçš„ç®€å•å¹³å‡å€¼ã€‚å€¼ä¸º1æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å®Œç¾é¢„æµ‹æ‰€æœ‰é¢„æœŸè¾“å‡ºã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œç³»æ•°çº¦ä¸º0.06ï¼Œè¿™ç›¸å½“ä½ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥å°†æµ‹è¯•æ•°æ®ä¸å›å½’çº¿ä¸€èµ·ç»˜åˆ¶ï¼Œä»¥æ›´å¥½åœ°äº†è§£å›å½’åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="çº¿æ€§å›å½’" src="images/linear-results.png" width="50%" />

## å¤šé¡¹å¼å›å½’

å¦ä¸€ç§çº¿æ€§å›å½’æ˜¯å¤šé¡¹å¼å›å½’ã€‚æœ‰æ—¶å˜é‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³» - å—ç“œä½“ç§¯è¶Šå¤§ï¼Œä»·æ ¼è¶Šé«˜ - æœ‰æ—¶è¿™äº›å…³ç³»ä¸èƒ½ç»˜åˆ¶ä¸ºå¹³é¢æˆ–ç›´çº¿ã€‚

âœ… è¿™é‡Œæœ‰[æ›´å¤šç¤ºä¾‹](https://online.stat.psu.edu/stat501/lesson/9/9.8)è¯´æ˜å¯ä»¥ä½¿ç”¨å¤šé¡¹å¼å›å½’çš„æ•°æ®

å†çœ‹çœ‹æ—¥æœŸä¸ä»·æ ¼ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸ªæ•£ç‚¹å›¾çœ‹èµ·æ¥æ˜¯å¦ä¸€å®šè¦ç”¨ç›´çº¿æ¥åˆ†æï¼Ÿä»·æ ¼ä¸èƒ½æ³¢åŠ¨å—ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å°è¯•å¤šé¡¹å¼å›å½’ã€‚

âœ… å¤šé¡¹å¼æ˜¯å¯èƒ½åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå˜é‡å’Œç³»æ•°çš„æ•°å­¦è¡¨è¾¾å¼

å¤šé¡¹å¼å›å½’åˆ›å»ºä¸€æ¡æ›²çº¿ä»¥æ›´å¥½åœ°æ‹Ÿåˆéçº¿æ€§æ•°æ®ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¦‚æœæˆ‘ä»¬åœ¨è¾“å…¥æ•°æ®ä¸­åŒ…å«ä¸€ä¸ªå¹³æ–¹çš„`DayOfYear`å˜é‡ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿç”¨ä¸€æ¡æŠ›ç‰©çº¿æ¥æ‹Ÿåˆæˆ‘ä»¬çš„æ•°æ®ï¼Œè¯¥æŠ›ç‰©çº¿å°†åœ¨ä¸€å¹´ä¸­çš„æŸä¸€ç‚¹è¾¾åˆ°æœ€ä½ç‚¹ã€‚

Scikit-learn åŒ…å«ä¸€ä¸ªæœ‰ç”¨çš„[pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline)ï¼Œå¯ä»¥å°†ä¸åŒçš„æ•°æ®å¤„ç†æ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ã€‚**ç®¡é“**æ˜¯**ä¼°è®¡å™¨**çš„é“¾ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®¡é“ï¼Œé¦–å…ˆå°†å¤šé¡¹å¼ç‰¹å¾æ·»åŠ åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ï¼Œç„¶åè®­ç»ƒå›å½’ï¼š

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

ä½¿ç”¨`PolynomialFeatures(2)` means that we will include all second-degree polynomials from the input data. In our case it will just mean `DayOfYear`<sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.

Pipelines can be used in the same manner as the original `LinearRegression` object, i.e. we can `fit` the pipeline, and then use `predict` to get the prediction results. Here is the graph showing test data, and the approximation curve:

<img alt="Polynomial regression" src="images/poly-results.png" width="50%" />

Using Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!

> You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? 

ğŸƒ Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!

## Categorical Features

In the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the `Variety` column is somewhat different from columns like `Month`, because it contains non-numeric values. Such columns are called **categorical**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> ğŸ¥ Click the image above for a short video overview of using categorical features.

Here you can see how average price depends on variety:

<img alt="Average price by variety" src="images/price-by-variety.png" width="50%" />

To take variety into account, we first need to convert it to numeric form, or **encode** it. There are several way we can do it:

* Simple **numeric encoding** will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.
* **One-hot encoding** will replace the `Variety` column by 4 different columns, one for each variety. Each column will contain `1` if the corresponding row is of a given variety, and `0` å¦åˆ™ã€‚è¿™æ„å‘³ç€çº¿æ€§å›å½’ä¸­å°†æœ‰å››ä¸ªç³»æ•°ï¼Œæ¯ä¸ªå—ç“œå“ç§ä¸€ä¸ªï¼Œè´Ÿè´£è¯¥ç‰¹å®šå“ç§çš„â€œèµ·å§‹ä»·æ ¼â€ï¼ˆæˆ–â€œé™„åŠ ä»·æ ¼â€ï¼‰ã€‚

ä¸‹é¢çš„ä»£ç æ˜¾ç¤ºäº†æˆ‘ä»¬å¦‚ä½•å¯¹ä¸€ä¸ªå“ç§è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼š

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

è¦ä½¿ç”¨ç‹¬çƒ­ç¼–ç å“ç§ä½œä¸ºè¾“å…¥è®­ç»ƒçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬åªéœ€è¦æ­£ç¡®åˆå§‹åŒ–`X` and `y`æ•°æ®ï¼š

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

å…¶ä½™ä»£ç ä¸æˆ‘ä»¬ä¸Šé¢ç”¨äºè®­ç»ƒçº¿æ€§å›å½’çš„ä»£ç ç›¸åŒã€‚å¦‚æœä½ å°è¯•ä¸€ä¸‹ï¼Œä½ ä¼šå‘ç°å‡æ–¹è¯¯å·®å¤§è‡´ç›¸åŒï¼Œä½†æˆ‘ä»¬å¾—åˆ°äº†æ›´é«˜çš„å†³å®šç³»æ•°ï¼ˆçº¦77%ï¼‰ã€‚ä¸ºäº†è·å¾—æ›´å‡†ç¡®çš„é¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘æ›´å¤šçš„åˆ†ç±»ç‰¹å¾ï¼Œä»¥åŠæ•°å€¼ç‰¹å¾ï¼Œå¦‚`Month` or `DayOfYear`. To get one large array of features, we can use `join`ï¼š

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

åœ¨è¿™é‡Œæˆ‘ä»¬è¿˜è€ƒè™‘äº†`City` and `Package`ç±»å‹ï¼Œè¿™ç»™æˆ‘ä»¬å¸¦æ¥äº†MSE 2.84ï¼ˆ10%ï¼‰å’Œå†³å®šç³»æ•°0.94ï¼

## ç»¼åˆèµ·æ¥

ä¸ºäº†åˆ¶ä½œæœ€ä½³æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸Šè¿°ç¤ºä¾‹ä¸­çš„ç»„åˆï¼ˆç‹¬çƒ­ç¼–ç åˆ†ç±» + æ•°å€¼ï¼‰æ•°æ®ä¸å¤šé¡¹å¼å›å½’ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´ä»£ç ï¼Œä¾›ä½ å‚è€ƒï¼š

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

è¿™åº”è¯¥ç»™æˆ‘ä»¬å‡ ä¹97%çš„æœ€ä½³å†³å®šç³»æ•°ï¼ŒMSE=2.23ï¼ˆ~8%çš„é¢„æµ‹è¯¯å·®ï¼‰ã€‚

| æ¨¡å‹ | MSE | å†³å®šç³»æ•° |
|-------|-----|-----------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |
| `Variety` çº¿æ€§ | 5.24 (19.7%) | 0.77 |
| æ‰€æœ‰ç‰¹å¾çº¿æ€§ | 2.84 (10.5%) | 0.94 |
| æ‰€æœ‰ç‰¹å¾å¤šé¡¹å¼ | 2.23 (8.25%) | 0.97 |

ğŸ† åšå¾—å¥½ï¼ä½ åœ¨ä¸€èŠ‚è¯¾ä¸­åˆ›å»ºäº†å››ä¸ªå›å½’æ¨¡å‹ï¼Œå¹¶å°†æ¨¡å‹è´¨é‡æé«˜åˆ°97%ã€‚åœ¨å›å½’çš„æœ€åä¸€éƒ¨åˆ†ä¸­ï¼Œä½ å°†å­¦ä¹ é€»è¾‘å›å½’ä»¥ç¡®å®šç±»åˆ«ã€‚

---
## ğŸš€æŒ‘æˆ˜

åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­æµ‹è¯•å‡ ä¸ªä¸åŒçš„å˜é‡ï¼Œçœ‹çœ‹ç›¸å…³æ€§å¦‚ä½•å¯¹åº”äºæ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

## [è¯¾åæµ‹éªŒ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)

## å¤ä¹ ä¸è‡ªå­¦

åœ¨æœ¬è¯¾ä¸­æˆ‘ä»¬å­¦ä¹ äº†çº¿æ€§å›å½’ã€‚è¿˜æœ‰å…¶ä»–é‡è¦çš„å›å½’ç±»å‹ã€‚é˜…è¯»é€æ­¥å›å½’ã€å²­å›å½’ã€å¥—ç´¢å›å½’å’Œå¼¹æ€§ç½‘å›å½’æŠ€æœ¯ã€‚ä¸€ä¸ªå¾ˆå¥½çš„è¯¾ç¨‹æ˜¯[æ–¯å¦ç¦ç»Ÿè®¡å­¦ä¹ è¯¾ç¨‹](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## ä½œä¸š

[æ„å»ºä¸€ä¸ªæ¨¡å‹](assignment.md)

**å…è´£å£°æ˜**:
æœ¬æ–‡ä»¶ä½¿ç”¨æœºå™¨ç¿»è¯‘æœåŠ¡è¿›è¡Œç¿»è¯‘ã€‚è™½ç„¶æˆ‘ä»¬åŠªåŠ›ç¡®ä¿å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åº”å°†åŸæ–‡æ¡£çš„æ¯è¯­ç‰ˆæœ¬è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºå…³é”®ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚å¯¹äºå› ä½¿ç”¨æœ¬ç¿»è¯‘è€Œäº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯è¯»ï¼Œæˆ‘ä»¬ä¸æ‰¿æ‹…è´£ä»»ã€‚