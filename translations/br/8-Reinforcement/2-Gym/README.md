<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9660fbd80845c59c15715cb418cd6e23",
  "translation_date": "2025-08-29T22:14:26+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "br"
}
-->
## Pr√©-requisitos

Nesta li√ß√£o, utilizaremos uma biblioteca chamada **OpenAI Gym** para simular diferentes **ambientes**. Voc√™ pode executar o c√≥digo desta li√ß√£o localmente (por exemplo, no Visual Studio Code), caso em que a simula√ß√£o ser√° aberta em uma nova janela. Ao executar o c√≥digo online, pode ser necess√°rio fazer alguns ajustes no c√≥digo, conforme descrito [aqui](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Na li√ß√£o anterior, as regras do jogo e o estado eram definidos pela classe `Board`, que criamos manualmente. Aqui, utilizaremos um **ambiente de simula√ß√£o** especial, que simular√° a f√≠sica por tr√°s do equil√≠brio do p√™ndulo. Um dos ambientes de simula√ß√£o mais populares para treinar algoritmos de aprendizado por refor√ßo √© chamado de [Gym](https://gym.openai.com/), mantido pela [OpenAI](https://openai.com/). Usando este Gym, podemos criar diferentes **ambientes**, desde simula√ß√µes de p√™ndulos at√© jogos de Atari.

> **Nota**: Voc√™ pode ver outros ambientes dispon√≠veis no OpenAI Gym [aqui](https://gym.openai.com/envs/#classic_control).

Primeiro, vamos instalar o Gym e importar as bibliotecas necess√°rias (bloco de c√≥digo 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Exerc√≠cio - inicializar um ambiente de CartPole

Para trabalhar com o problema de equil√≠brio do p√™ndulo, precisamos inicializar o ambiente correspondente. Cada ambiente est√° associado a:

- **Espa√ßo de observa√ß√£o**, que define a estrutura das informa√ß√µes que recebemos do ambiente. No problema do p√™ndulo, recebemos a posi√ß√£o do p√™ndulo, a velocidade e outros valores.

- **Espa√ßo de a√ß√£o**, que define as a√ß√µes poss√≠veis. No nosso caso, o espa√ßo de a√ß√£o √© discreto e consiste em duas a√ß√µes: **esquerda** e **direita**. (bloco de c√≥digo 2)

1. Para inicializar, digite o seguinte c√≥digo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Para entender como o ambiente funciona, vamos executar uma simula√ß√£o curta de 100 passos. Em cada passo, fornecemos uma das a√ß√µes a serem realizadas - nesta simula√ß√£o, selecionamos aleatoriamente uma a√ß√£o do `action_space`.

1. Execute o c√≥digo abaixo e veja o que acontece.

    ‚úÖ Lembre-se de que √© prefer√≠vel executar este c√≥digo em uma instala√ß√£o local do Python! (bloco de c√≥digo 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Voc√™ deve ver algo semelhante a esta imagem:

    ![p√™ndulo sem equil√≠brio](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Durante a simula√ß√£o, precisamos obter observa√ß√µes para decidir como agir. Na verdade, a fun√ß√£o step retorna as observa√ß√µes atuais, uma fun√ß√£o de recompensa e um indicador `done` que mostra se faz sentido continuar a simula√ß√£o ou n√£o: (bloco de c√≥digo 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Voc√™ ver√° algo como isto na sa√≠da do notebook:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    O vetor de observa√ß√£o retornado em cada passo da simula√ß√£o cont√©m os seguintes valores:
    - Posi√ß√£o do carrinho
    - Velocidade do carrinho
    - √Çngulo do p√™ndulo
    - Taxa de rota√ß√£o do p√™ndulo

1. Obtenha os valores m√≠nimos e m√°ximos desses n√∫meros: (bloco de c√≥digo 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Voc√™ tamb√©m pode notar que o valor da recompensa em cada passo da simula√ß√£o √© sempre 1. Isso ocorre porque nosso objetivo √© sobreviver o maior tempo poss√≠vel, ou seja, manter o p√™ndulo em uma posi√ß√£o razoavelmente vertical pelo maior per√≠odo de tempo.

    ‚úÖ Na verdade, a simula√ß√£o do CartPole √© considerada resolvida se conseguirmos obter uma recompensa m√©dia de 195 em 100 tentativas consecutivas.

## Discretiza√ß√£o do estado

No Q-Learning, precisamos construir uma Q-Table que define o que fazer em cada estado. Para isso, o estado precisa ser **discreto**, ou seja, deve conter um n√∫mero finito de valores discretos. Assim, precisamos de alguma forma **discretizar** nossas observa√ß√µes, mapeando-as para um conjunto finito de estados.

Existem algumas maneiras de fazer isso:

- **Dividir em intervalos**. Se conhecemos o intervalo de um determinado valor, podemos dividi-lo em um n√∫mero de **intervalos** e, em seguida, substituir o valor pelo n√∫mero do intervalo ao qual ele pertence. Isso pode ser feito usando o m√©todo [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) do numpy. Nesse caso, saberemos exatamente o tamanho do estado, pois ele depender√° do n√∫mero de intervalos que selecionarmos para a digitaliza√ß√£o.

‚úÖ Podemos usar interpola√ß√£o linear para trazer os valores para algum intervalo finito (por exemplo, de -20 a 20) e, em seguida, converter os n√∫meros em inteiros arredondando-os. Isso nos d√° um pouco menos de controle sobre o tamanho do estado, especialmente se n√£o conhecermos os intervalos exatos dos valores de entrada. Por exemplo, no nosso caso, 2 dos 4 valores n√£o t√™m limites superiores/inferiores, o que pode resultar em um n√∫mero infinito de estados.

No nosso exemplo, usaremos a segunda abordagem. Como voc√™ pode notar mais tarde, apesar de os valores n√£o terem limites definidos, eles raramente assumem valores fora de certos intervalos finitos, tornando os estados com valores extremos muito raros.

1. Aqui est√° a fun√ß√£o que pegar√° a observa√ß√£o do nosso modelo e produzir√° uma tupla de 4 valores inteiros: (bloco de c√≥digo 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Vamos tamb√©m explorar outro m√©todo de discretiza√ß√£o usando intervalos: (bloco de c√≥digo 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Agora, execute uma simula√ß√£o curta e observe esses valores discretos do ambiente. Sinta-se √† vontade para testar tanto `discretize` quanto `discretize_bins` e veja se h√° alguma diferen√ßa.

    ‚úÖ `discretize_bins` retorna o n√∫mero do intervalo, que come√ßa em 0. Assim, para valores da vari√°vel de entrada pr√≥ximos de 0, ele retorna o n√∫mero do meio do intervalo (10). Em `discretize`, n√£o nos preocupamos com o intervalo dos valores de sa√≠da, permitindo que sejam negativos, de modo que os valores do estado n√£o s√£o deslocados, e 0 corresponde a 0. (bloco de c√≥digo 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Descomente a linha que come√ßa com `env.render` se quiser ver como o ambiente √© executado. Caso contr√°rio, voc√™ pode execut√°-lo em segundo plano, o que √© mais r√°pido. Usaremos essa execu√ß√£o "invis√≠vel" durante nosso processo de Q-Learning.

## Estrutura da Q-Table

Na li√ß√£o anterior, o estado era um simples par de n√∫meros de 0 a 8, e, portanto, era conveniente representar a Q-Table como um tensor numpy com formato 8x8x2. Se usarmos a discretiza√ß√£o por intervalos, o tamanho do nosso vetor de estado tamb√©m ser√° conhecido, ent√£o podemos usar a mesma abordagem e representar o estado como um array com formato 20x20x10x10x2 (aqui, 2 √© a dimens√£o do espa√ßo de a√ß√£o, e as primeiras dimens√µes correspondem ao n√∫mero de intervalos que selecionamos para cada par√¢metro no espa√ßo de observa√ß√£o).

No entanto, √†s vezes as dimens√µes precisas do espa√ßo de observa√ß√£o n√£o s√£o conhecidas. No caso da fun√ß√£o `discretize`, nunca podemos ter certeza de que nosso estado permanecer√° dentro de certos limites, porque alguns dos valores originais n√£o t√™m limites. Assim, usaremos uma abordagem ligeiramente diferente e representaremos a Q-Table como um dicion√°rio.

1. Use o par *(state, action)* como chave do dicion√°rio, e o valor corresponder√° ao valor da entrada na Q-Table. (bloco de c√≥digo 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aqui tamb√©m definimos uma fun√ß√£o `qvalues()`, que retorna uma lista de valores da Q-Table para um estado dado que corresponde a todas as a√ß√µes poss√≠veis. Se a entrada n√£o estiver presente na Q-Table, retornaremos 0 como padr√£o.

## Vamos come√ßar o Q-Learning

Agora estamos prontos para ensinar Peter a equilibrar!

1. Primeiro, vamos definir alguns hiperpar√¢metros: (bloco de c√≥digo 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aqui, `alpha` √© a **taxa de aprendizado**, que define em que medida devemos ajustar os valores atuais da Q-Table em cada passo. Na li√ß√£o anterior, come√ßamos com 1 e depois reduzimos `alpha` para valores menores durante o treinamento. Neste exemplo, manteremos constante apenas para simplificar, e voc√™ pode experimentar ajustar os valores de `alpha` mais tarde.

    `gamma` √© o **fator de desconto**, que mostra em que medida devemos priorizar a recompensa futura em rela√ß√£o √† recompensa atual.

    `epsilon` √© o **fator de explora√ß√£o/explora√ß√£o**, que determina se devemos preferir explorar ou explorar mais. Em nosso algoritmo, em `epsilon` por cento dos casos, selecionaremos a pr√≥xima a√ß√£o de acordo com os valores da Q-Table, e no restante dos casos executaremos uma a√ß√£o aleat√≥ria. Isso nos permitir√° explorar √°reas do espa√ßo de busca que nunca vimos antes.

    ‚úÖ Em termos de equil√≠brio - escolher uma a√ß√£o aleat√≥ria (explora√ß√£o) agiria como um empurr√£o aleat√≥rio na dire√ß√£o errada, e o p√™ndulo teria que aprender a recuperar o equil√≠brio desses "erros".

### Melhorar o algoritmo

Podemos tamb√©m fazer duas melhorias no nosso algoritmo da li√ß√£o anterior:

- **Calcular a recompensa cumulativa m√©dia**, ao longo de v√°rias simula√ß√µes. Imprimiremos o progresso a cada 5000 itera√ß√µes e faremos a m√©dia da recompensa cumulativa nesse per√≠odo de tempo. Isso significa que, se obtivermos mais de 195 pontos, podemos considerar o problema resolvido, com qualidade ainda maior do que a exigida.

- **Calcular o resultado cumulativo m√©dio m√°ximo**, `Qmax`, e armazenaremos a Q-Table correspondente a esse resultado. Quando voc√™ executar o treinamento, notar√° que, √†s vezes, o resultado cumulativo m√©dio come√ßa a cair, e queremos manter os valores da Q-Table que correspondem ao melhor modelo observado durante o treinamento.

1. Colete todas as recompensas cumulativas em cada simula√ß√£o no vetor `rewards` para posterior plotagem. (bloco de c√≥digo 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

O que voc√™ pode notar a partir desses resultados:

- **Perto do nosso objetivo**. Estamos muito pr√≥ximos de alcan√ßar o objetivo de obter 195 recompensas cumulativas em mais de 100 execu√ß√µes consecutivas da simula√ß√£o, ou talvez j√° tenhamos alcan√ßado! Mesmo que obtenhamos n√∫meros menores, ainda n√£o sabemos, porque fazemos a m√©dia em 5000 execu√ß√µes, e apenas 100 execu√ß√µes s√£o necess√°rias no crit√©rio formal.

- **A recompensa come√ßa a cair**. √Äs vezes, a recompensa come√ßa a cair, o que significa que podemos "destruir" valores j√° aprendidos na Q-Table com novos valores que pioram a situa√ß√£o.

Essa observa√ß√£o √© mais claramente vis√≠vel se plotarmos o progresso do treinamento.

## Plotando o progresso do treinamento

Durante o treinamento, coletamos o valor da recompensa cumulativa em cada uma das itera√ß√µes no vetor `rewards`. Veja como ele fica quando o plotamos contra o n√∫mero de itera√ß√µes:

```python
plt.plot(rewards)
```

![progresso bruto](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.br.png)

A partir deste gr√°fico, n√£o √© poss√≠vel dizer muita coisa, porque, devido √† natureza do processo de treinamento estoc√°stico, a dura√ß√£o das sess√µes de treinamento varia muito. Para dar mais sentido a este gr√°fico, podemos calcular a **m√©dia m√≥vel** ao longo de uma s√©rie de experimentos, digamos 100. Isso pode ser feito convenientemente usando `np.convolve`: (bloco de c√≥digo 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progresso do treinamento](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.br.png)

## Variando os hiperpar√¢metros

Para tornar o aprendizado mais est√°vel, faz sentido ajustar alguns de nossos hiperpar√¢metros durante o treinamento. Em particular:

- **Para a taxa de aprendizado**, `alpha`, podemos come√ßar com valores pr√≥ximos de 1 e, em seguida, diminuir o par√¢metro gradualmente. Com o tempo, obteremos boas probabilidades na Q-Table, e, assim, devemos ajust√°-las levemente, em vez de sobrescrev√™-las completamente com novos valores.

- **Aumentar epsilon**. Podemos querer aumentar o `epsilon` lentamente, para explorar menos e explorar mais. Provavelmente faz sentido come√ßar com um valor menor de `epsilon` e aument√°-lo gradualmente at√© quase 1.
> **Tarefa 1**: Experimente ajustar os valores dos hiperpar√¢metros e veja se consegue alcan√ßar uma recompensa acumulada maior. Voc√™ est√° conseguindo acima de 195?
> **Tarefa 2**: Para resolver formalmente o problema, voc√™ precisa alcan√ßar uma recompensa m√©dia de 195 em 100 execu√ß√µes consecutivas. Me√ßa isso durante o treinamento e certifique-se de que voc√™ resolveu o problema formalmente!

## Vendo o resultado em a√ß√£o

Seria interessante realmente ver como o modelo treinado se comporta. Vamos rodar a simula√ß√£o e seguir a mesma estrat√©gia de sele√ß√£o de a√ß√µes usada durante o treinamento, amostrando de acordo com a distribui√ß√£o de probabilidade na Q-Table: (bloco de c√≥digo 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Voc√™ deve ver algo como isto:

![um carrinho equilibrando](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄDesafio

> **Tarefa 3**: Aqui, est√°vamos usando a c√≥pia final da Q-Table, que pode n√£o ser a melhor. Lembre-se de que armazenamos a Q-Table com melhor desempenho na vari√°vel `Qbest`! Tente o mesmo exemplo com a Q-Table de melhor desempenho copiando `Qbest` para `Q` e veja se voc√™ percebe alguma diferen√ßa.

> **Tarefa 4**: Aqui, n√£o est√°vamos selecionando a melhor a√ß√£o em cada passo, mas sim amostrando com a distribui√ß√£o de probabilidade correspondente. Faria mais sentido sempre selecionar a melhor a√ß√£o, com o maior valor na Q-Table? Isso pode ser feito usando a fun√ß√£o `np.argmax` para descobrir o n√∫mero da a√ß√£o correspondente ao maior valor na Q-Table. Implemente essa estrat√©gia e veja se isso melhora o equil√≠brio.

## [Quiz p√≥s-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## Tarefa
[Treine um Mountain Car](assignment.md)

## Conclus√£o

Agora aprendemos como treinar agentes para alcan√ßar bons resultados apenas fornecendo a eles uma fun√ß√£o de recompensa que define o estado desejado do jogo e dando-lhes a oportunidade de explorar inteligentemente o espa√ßo de busca. Aplicamos com sucesso o algoritmo de Q-Learning nos casos de ambientes discretos e cont√≠nuos, mas com a√ß√µes discretas.

√â importante tamb√©m estudar situa√ß√µes em que o estado das a√ß√µes tamb√©m √© cont√≠nuo e quando o espa√ßo de observa√ß√£o √© muito mais complexo, como a imagem da tela de um jogo Atari. Nesses problemas, muitas vezes precisamos usar t√©cnicas de aprendizado de m√°quina mais poderosas, como redes neurais, para alcan√ßar bons resultados. Esses t√≥picos mais avan√ßados ser√£o abordados em nosso pr√≥ximo curso de IA avan√ßada.

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.