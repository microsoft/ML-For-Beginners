<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-04T21:42:23+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "br"
}
-->
# Introdu√ß√£o ao Aprendizado por Refor√ßo e Q-Learning

![Resumo do aprendizado por refor√ßo em machine learning em um sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

O aprendizado por refor√ßo envolve tr√™s conceitos importantes: o agente, alguns estados e um conjunto de a√ß√µes por estado. Ao executar uma a√ß√£o em um estado espec√≠fico, o agente recebe uma recompensa. Imagine novamente o jogo de computador Super Mario. Voc√™ √© o Mario, est√° em um n√≠vel do jogo, parado ao lado de um penhasco. Acima de voc√™ h√° uma moeda. Voc√™, sendo o Mario, em um n√≠vel do jogo, em uma posi√ß√£o espec√≠fica... esse √© o seu estado. Mover um passo para a direita (uma a√ß√£o) o levar√° para o penhasco, o que resultar√° em uma pontua√ß√£o num√©rica baixa. No entanto, pressionar o bot√£o de pular permitir√° que voc√™ marque um ponto e continue vivo. Esse √© um resultado positivo e deve lhe conceder uma pontua√ß√£o num√©rica positiva.

Usando aprendizado por refor√ßo e um simulador (o jogo), voc√™ pode aprender a jogar para maximizar a recompensa, que √© permanecer vivo e marcar o maior n√∫mero de pontos poss√≠vel.

[![Introdu√ß√£o ao Aprendizado por Refor√ßo](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Clique na imagem acima para ouvir Dmitry falar sobre Aprendizado por Refor√ßo

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

## Pr√©-requisitos e Configura√ß√£o

Nesta li√ß√£o, experimentaremos com algum c√≥digo em Python. Voc√™ deve ser capaz de executar o c√≥digo do Jupyter Notebook desta li√ß√£o, seja no seu computador ou em algum lugar na nuvem.

Voc√™ pode abrir [o notebook da li√ß√£o](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) e acompanhar esta li√ß√£o para construir.

> **Nota:** Se voc√™ estiver abrindo este c√≥digo na nuvem, tamb√©m precisar√° buscar o arquivo [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), que √© usado no c√≥digo do notebook. Adicione-o ao mesmo diret√≥rio do notebook.

## Introdu√ß√£o

Nesta li√ß√£o, exploraremos o mundo de **[Pedro e o Lobo](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirado por um conto musical de fadas de um compositor russo, [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Usaremos o **Aprendizado por Refor√ßo** para permitir que Pedro explore seu ambiente, colete ma√ß√£s saborosas e evite encontrar o lobo.

O **Aprendizado por Refor√ßo** (RL) √© uma t√©cnica de aprendizado que nos permite aprender o comportamento ideal de um **agente** em algum **ambiente** realizando muitos experimentos. Um agente nesse ambiente deve ter algum **objetivo**, definido por uma **fun√ß√£o de recompensa**.

## O ambiente

Para simplificar, vamos considerar o mundo de Pedro como um tabuleiro quadrado de tamanho `largura` x `altura`, como este:

![Ambiente de Pedro](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Cada c√©lula neste tabuleiro pode ser:

* **ch√£o**, onde Pedro e outras criaturas podem andar.
* **√°gua**, onde obviamente voc√™ n√£o pode andar.
* uma **√°rvore** ou **grama**, um lugar onde voc√™ pode descansar.
* uma **ma√ß√£**, que representa algo que Pedro ficaria feliz em encontrar para se alimentar.
* um **lobo**, que √© perigoso e deve ser evitado.

H√° um m√≥dulo Python separado, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), que cont√©m o c√≥digo para trabalhar com este ambiente. Como este c√≥digo n√£o √© importante para entender nossos conceitos, importaremos o m√≥dulo e o usaremos para criar o tabuleiro de exemplo (bloco de c√≥digo 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Este c√≥digo deve imprimir uma imagem do ambiente semelhante √† mostrada acima.

## A√ß√µes e pol√≠tica

No nosso exemplo, o objetivo de Pedro seria encontrar uma ma√ß√£, enquanto evita o lobo e outros obst√°culos. Para isso, ele pode essencialmente andar pelo tabuleiro at√© encontrar uma ma√ß√£.

Portanto, em qualquer posi√ß√£o, ele pode escolher entre uma das seguintes a√ß√µes: cima, baixo, esquerda e direita.

Definiremos essas a√ß√µes como um dicion√°rio e as mapearemos para pares de mudan√ßas de coordenadas correspondentes. Por exemplo, mover para a direita (`R`) corresponderia ao par `(1,0)`. (bloco de c√≥digo 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Resumindo, a estrat√©gia e o objetivo deste cen√°rio s√£o os seguintes:

- **A estrat√©gia** do nosso agente (Pedro) √© definida por uma chamada **pol√≠tica**. Uma pol√≠tica √© uma fun√ß√£o que retorna a a√ß√£o em qualquer estado dado. No nosso caso, o estado do problema √© representado pelo tabuleiro, incluindo a posi√ß√£o atual do jogador.

- **O objetivo** do aprendizado por refor√ßo √© eventualmente aprender uma boa pol√≠tica que nos permita resolver o problema de forma eficiente. No entanto, como linha de base, vamos considerar a pol√≠tica mais simples chamada **caminhada aleat√≥ria**.

## Caminhada aleat√≥ria

Vamos primeiro resolver nosso problema implementando uma estrat√©gia de caminhada aleat√≥ria. Com a caminhada aleat√≥ria, escolheremos aleatoriamente a pr√≥xima a√ß√£o entre as a√ß√µes permitidas, at√© alcan√ßarmos a ma√ß√£ (bloco de c√≥digo 3).

1. Implemente a caminhada aleat√≥ria com o c√≥digo abaixo:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    A chamada para `walk` deve retornar o comprimento do caminho correspondente, que pode variar de uma execu√ß√£o para outra.

1. Execute o experimento de caminhada v√°rias vezes (digamos, 100) e imprima as estat√≠sticas resultantes (bloco de c√≥digo 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Note que o comprimento m√©dio de um caminho √© em torno de 30-40 passos, o que √© bastante, dado que a dist√¢ncia m√©dia at√© a ma√ß√£ mais pr√≥xima √© de cerca de 5-6 passos.

    Voc√™ tamb√©m pode ver como √© o movimento de Pedro durante a caminhada aleat√≥ria:

    ![Caminhada Aleat√≥ria de Pedro](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Fun√ß√£o de recompensa

Para tornar nossa pol√≠tica mais inteligente, precisamos entender quais movimentos s√£o "melhores" que outros. Para isso, precisamos definir nosso objetivo.

O objetivo pode ser definido em termos de uma **fun√ß√£o de recompensa**, que retornar√° algum valor de pontua√ß√£o para cada estado. Quanto maior o n√∫mero, melhor a fun√ß√£o de recompensa. (bloco de c√≥digo 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Uma coisa interessante sobre fun√ß√µes de recompensa √© que, na maioria dos casos, *s√≥ recebemos uma recompensa substancial no final do jogo*. Isso significa que nosso algoritmo deve, de alguma forma, lembrar os "bons" passos que levaram a uma recompensa positiva no final e aumentar sua import√¢ncia. Da mesma forma, todos os movimentos que levam a resultados ruins devem ser desencorajados.

## Q-Learning

O algoritmo que discutiremos aqui √© chamado de **Q-Learning**. Neste algoritmo, a pol√≠tica √© definida por uma fun√ß√£o (ou uma estrutura de dados) chamada de **Q-Table**. Ela registra a "qualidade" de cada uma das a√ß√µes em um determinado estado.

√â chamada de Q-Table porque muitas vezes √© conveniente represent√°-la como uma tabela ou matriz multidimensional. Como nosso tabuleiro tem dimens√µes `largura` x `altura`, podemos representar a Q-Table usando um array numpy com forma `largura` x `altura` x `len(actions)`: (bloco de c√≥digo 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Observe que inicializamos todos os valores da Q-Table com um valor igual, no nosso caso - 0.25. Isso corresponde √† pol√≠tica de "caminhada aleat√≥ria", porque todos os movimentos em cada estado s√£o igualmente bons. Podemos passar a Q-Table para a fun√ß√£o `plot` para visualizar a tabela no tabuleiro: `m.plot(Q)`.

![Ambiente de Pedro](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

No centro de cada c√©lula h√° uma "seta" que indica a dire√ß√£o preferida de movimento. Como todas as dire√ß√µes s√£o iguais, √© exibido um ponto.

Agora precisamos executar a simula√ß√£o, explorar nosso ambiente e aprender uma melhor distribui√ß√£o de valores na Q-Table, o que nos permitir√° encontrar o caminho para a ma√ß√£ muito mais rapidamente.

## Ess√™ncia do Q-Learning: Equa√ß√£o de Bellman

Uma vez que come√ßamos a nos mover, cada a√ß√£o ter√° uma recompensa correspondente, ou seja, teoricamente podemos selecionar a pr√≥xima a√ß√£o com base na maior recompensa imediata. No entanto, na maioria dos estados, o movimento n√£o alcan√ßar√° nosso objetivo de chegar √† ma√ß√£, e assim n√£o podemos decidir imediatamente qual dire√ß√£o √© melhor.

> Lembre-se de que n√£o √© o resultado imediato que importa, mas sim o resultado final, que obteremos no final da simula√ß√£o.

Para levar em conta essa recompensa atrasada, precisamos usar os princ√≠pios da **[programa√ß√£o din√¢mica](https://en.wikipedia.org/wiki/Dynamic_programming)**, que nos permitem pensar sobre nosso problema de forma recursiva.

Suponha que estamos agora no estado *s* e queremos nos mover para o pr√≥ximo estado *s'*. Ao fazer isso, receberemos a recompensa imediata *r(s,a)*, definida pela fun√ß√£o de recompensa, mais alguma recompensa futura. Se supusermos que nossa Q-Table reflete corretamente a "atratividade" de cada a√ß√£o, ent√£o no estado *s'* escolheremos uma a√ß√£o *a* que corresponda ao valor m√°ximo de *Q(s',a')*. Assim, a melhor recompensa futura poss√≠vel que poder√≠amos obter no estado *s* ser√° definida como `max`

## Verificando a pol√≠tica

Como a Q-Table lista a "atratividade" de cada a√ß√£o em cada estado, √© bastante f√°cil us√°-la para definir a navega√ß√£o eficiente em nosso mundo. No caso mais simples, podemos selecionar a a√ß√£o correspondente ao maior valor da Q-Table: (bloco de c√≥digo 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Se voc√™ tentar o c√≥digo acima v√°rias vezes, pode perceber que, √†s vezes, ele "trava", e voc√™ precisa pressionar o bot√£o STOP no notebook para interromp√™-lo. Isso acontece porque podem haver situa√ß√µes em que dois estados "apontam" um para o outro em termos de valor Q-√ìtimo, fazendo com que o agente acabe se movendo entre esses estados indefinidamente.

## üöÄDesafio

> **Tarefa 1:** Modifique a fun√ß√£o `walk` para limitar o comprimento m√°ximo do caminho a um certo n√∫mero de passos (digamos, 100), e observe o c√≥digo acima retornar esse valor de tempos em tempos.

> **Tarefa 2:** Modifique a fun√ß√£o `walk` para que ela n√£o volte aos lugares onde j√° esteve anteriormente. Isso evitar√° que `walk` entre em loop, no entanto, o agente ainda pode acabar ficando "preso" em um local do qual n√£o consegue escapar.

## Navega√ß√£o

Uma pol√≠tica de navega√ß√£o melhor seria aquela que usamos durante o treinamento, que combina explora√ß√£o e aproveitamento. Nessa pol√≠tica, selecionamos cada a√ß√£o com uma certa probabilidade, proporcional aos valores na Q-Table. Essa estrat√©gia ainda pode fazer com que o agente volte a uma posi√ß√£o que j√° explorou, mas, como voc√™ pode ver no c√≥digo abaixo, resulta em um caminho m√©dio muito curto at√© o local desejado (lembre-se de que `print_statistics` executa a simula√ß√£o 100 vezes): (bloco de c√≥digo 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Ap√≥s executar este c√≥digo, voc√™ deve obter um comprimento m√©dio de caminho muito menor do que antes, na faixa de 3-6.

## Investigando o processo de aprendizado

Como mencionamos, o processo de aprendizado √© um equil√≠brio entre explora√ß√£o e aproveitamento do conhecimento adquirido sobre a estrutura do espa√ßo do problema. Vimos que os resultados do aprendizado (a capacidade de ajudar um agente a encontrar um caminho curto at√© o objetivo) melhoraram, mas tamb√©m √© interessante observar como o comprimento m√©dio do caminho se comporta durante o processo de aprendizado:

## Os aprendizados podem ser resumidos como:

- **O comprimento m√©dio do caminho aumenta**. O que vemos aqui √© que, no in√≠cio, o comprimento m√©dio do caminho aumenta. Isso provavelmente ocorre porque, quando n√£o sabemos nada sobre o ambiente, √© mais prov√°vel que fiquemos presos em estados ruins, como √°gua ou lobo. √Ä medida que aprendemos mais e come√ßamos a usar esse conhecimento, podemos explorar o ambiente por mais tempo, mas ainda n√£o sabemos muito bem onde est√£o as ma√ß√£s.

- **O comprimento do caminho diminui, conforme aprendemos mais**. Uma vez que aprendemos o suficiente, torna-se mais f√°cil para o agente alcan√ßar o objetivo, e o comprimento do caminho come√ßa a diminuir. No entanto, ainda estamos abertos √† explora√ß√£o, ent√£o frequentemente nos desviamos do melhor caminho e exploramos novas op√ß√µes, tornando o caminho mais longo do que o ideal.

- **O comprimento aumenta abruptamente**. O que tamb√©m observamos nesse gr√°fico √© que, em algum momento, o comprimento aumentou abruptamente. Isso indica a natureza estoc√°stica do processo e que, em algum momento, podemos "estragar" os coeficientes da Q-Table ao sobrescrev√™-los com novos valores. Isso idealmente deve ser minimizado diminuindo a taxa de aprendizado (por exemplo, no final do treinamento, ajustamos os valores da Q-Table apenas por um pequeno valor).

No geral, √© importante lembrar que o sucesso e a qualidade do processo de aprendizado dependem significativamente de par√¢metros, como taxa de aprendizado, decaimento da taxa de aprendizado e fator de desconto. Esses par√¢metros s√£o frequentemente chamados de **hiperpar√¢metros**, para distingui-los dos **par√¢metros**, que otimizamos durante o treinamento (por exemplo, os coeficientes da Q-Table). O processo de encontrar os melhores valores de hiperpar√¢metros √© chamado de **otimiza√ß√£o de hiperpar√¢metros**, e merece um t√≥pico √† parte.

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Tarefa 
[Um Mundo Mais Realista](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.