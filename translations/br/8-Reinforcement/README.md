<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-08-29T22:02:32+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "br"
}
-->
# Introdu√ß√£o ao aprendizado por refor√ßo

O aprendizado por refor√ßo, RL, √© considerado um dos paradigmas b√°sicos de aprendizado de m√°quina, ao lado do aprendizado supervisionado e n√£o supervisionado. RL trata de decis√µes: tomar as decis√µes certas ou, pelo menos, aprender com elas.

Imagine que voc√™ tem um ambiente simulado, como o mercado de a√ß√µes. O que acontece se voc√™ impuser uma determinada regulamenta√ß√£o? Isso ter√° um efeito positivo ou negativo? Se algo negativo acontecer, voc√™ precisa aceitar esse _refor√ßo negativo_, aprender com ele e mudar de dire√ß√£o. Se o resultado for positivo, voc√™ precisa construir sobre esse _refor√ßo positivo_.

![Pedro e o lobo](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.br.png)

> Pedro e seus amigos precisam escapar do lobo faminto! Imagem por [Jen Looper](https://twitter.com/jenlooper)

## T√≥pico regional: Pedro e o Lobo (R√∫ssia)

[Pedro e o Lobo](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) √© um conto musical escrito pelo compositor russo [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). √â uma hist√≥ria sobre o jovem pioneiro Pedro, que corajosamente sai de sua casa para a clareira da floresta para perseguir o lobo. Nesta se√ß√£o, treinaremos algoritmos de aprendizado de m√°quina que ajudar√£o Pedro a:

- **Explorar** a √°rea ao redor e construir um mapa de navega√ß√£o ideal.
- **Aprender** a usar um skate e se equilibrar nele, para se mover mais r√°pido.

[![Pedro e o Lobo](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Clique na imagem acima para ouvir Pedro e o Lobo de Prokofiev

## Aprendizado por refor√ßo

Nas se√ß√µes anteriores, voc√™ viu dois exemplos de problemas de aprendizado de m√°quina:

- **Supervisionado**, onde temos conjuntos de dados que sugerem solu√ß√µes de exemplo para o problema que queremos resolver. [Classifica√ß√£o](../4-Classification/README.md) e [regress√£o](../2-Regression/README.md) s√£o tarefas de aprendizado supervisionado.
- **N√£o supervisionado**, no qual n√£o temos dados de treinamento rotulados. O principal exemplo de aprendizado n√£o supervisionado √© [Agrupamento](../5-Clustering/README.md).

Nesta se√ß√£o, apresentaremos um novo tipo de problema de aprendizado que n√£o requer dados de treinamento rotulados. Existem v√°rios tipos de problemas desse tipo:

- **[Aprendizado semi-supervisionado](https://wikipedia.org/wiki/Semi-supervised_learning)**, onde temos muitos dados n√£o rotulados que podem ser usados para pr√©-treinar o modelo.
- **[Aprendizado por refor√ßo](https://wikipedia.org/wiki/Reinforcement_learning)**, no qual um agente aprende como se comportar realizando experimentos em algum ambiente simulado.

### Exemplo - jogo de computador

Suponha que voc√™ queira ensinar um computador a jogar um jogo, como xadrez ou [Super Mario](https://wikipedia.org/wiki/Super_Mario). Para que o computador jogue, precisamos que ele preveja qual movimento fazer em cada estado do jogo. Embora isso possa parecer um problema de classifica√ß√£o, n√£o √© - porque n√£o temos um conjunto de dados com estados e a√ß√µes correspondentes. Embora possamos ter alguns dados, como partidas de xadrez existentes ou grava√ß√µes de jogadores jogando Super Mario, √© prov√°vel que esses dados n√£o cubram suficientemente um n√∫mero grande de estados poss√≠veis.

Em vez de procurar dados existentes do jogo, o **Aprendizado por Refor√ßo** (RL) baseia-se na ideia de *fazer o computador jogar* muitas vezes e observar o resultado. Assim, para aplicar o Aprendizado por Refor√ßo, precisamos de duas coisas:

- **Um ambiente** e **um simulador** que nos permitam jogar muitas vezes. Esse simulador definiria todas as regras do jogo, bem como os estados e a√ß√µes poss√≠veis.

- **Uma fun√ß√£o de recompensa**, que nos diria qu√£o bem nos sa√≠mos durante cada movimento ou partida.

A principal diferen√ßa entre outros tipos de aprendizado de m√°quina e RL √© que, no RL, geralmente n√£o sabemos se ganhamos ou perdemos at√© terminarmos o jogo. Assim, n√£o podemos dizer se um determinado movimento isolado √© bom ou n√£o - s√≥ recebemos uma recompensa no final do jogo. Nosso objetivo √© projetar algoritmos que nos permitam treinar um modelo sob condi√ß√µes incertas. Vamos aprender sobre um algoritmo de RL chamado **Q-learning**.

## Li√ß√µes

1. [Introdu√ß√£o ao aprendizado por refor√ßo e Q-Learning](1-QLearning/README.md)
2. [Usando um ambiente de simula√ß√£o Gym](2-Gym/README.md)

## Cr√©ditos

"Introdu√ß√£o ao Aprendizado por Refor√ßo" foi escrito com ‚ô•Ô∏è por [Dmitry Soshnikov](http://soshnikov.com)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.