<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "085d571097d201810720df4cd379f8c2",
  "translation_date": "2025-08-29T21:06:22+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "br"
}
-->
# K-Means clustering

## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)

Nesta li√ß√£o, voc√™ aprender√° como criar clusters usando Scikit-learn e o conjunto de dados de m√∫sica nigeriana que voc√™ importou anteriormente. Vamos abordar os fundamentos do K-Means para Clustering. Lembre-se de que, como voc√™ aprendeu na li√ß√£o anterior, existem v√°rias maneiras de trabalhar com clusters, e o m√©todo que voc√™ usa depende dos seus dados. Vamos experimentar o K-Means, pois √© a t√©cnica de clustering mais comum. Vamos come√ßar!

Termos que voc√™ aprender√°:

- Pontua√ß√£o de Silhouette
- M√©todo do cotovelo
- In√©rcia
- Vari√¢ncia

## Introdu√ß√£o

[K-Means Clustering](https://wikipedia.org/wiki/K-means_clustering) √© um m√©todo derivado do dom√≠nio de processamento de sinais. Ele √© usado para dividir e particionar grupos de dados em 'k' clusters usando uma s√©rie de observa√ß√µes. Cada observa√ß√£o trabalha para agrupar um determinado ponto de dados mais pr√≥ximo de sua 'm√©dia' mais pr√≥xima, ou o ponto central de um cluster.

Os clusters podem ser visualizados como [diagramas de Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), que incluem um ponto (ou 'semente') e sua regi√£o correspondente.

![voronoi diagram](../../../../translated_images/voronoi.1dc1613fb0439b9564615eca8df47a4bcd1ce06217e7e72325d2406ef2180795.br.png)

> Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)

O processo de clustering K-Means [√© executado em tr√™s etapas](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. O algoritmo seleciona um n√∫mero k de pontos centrais ao amostrar do conjunto de dados. Depois disso, ele entra em um loop:
    1. Ele atribui cada amostra ao centr√≥ide mais pr√≥ximo.
    2. Ele cria novos centr√≥ides calculando o valor m√©dio de todas as amostras atribu√≠das aos centr√≥ides anteriores.
    3. Em seguida, calcula a diferen√ßa entre os novos e antigos centr√≥ides e repete at√© que os centr√≥ides sejam estabilizados.

Uma desvantagem do uso do K-Means √© que voc√™ precisar√° estabelecer 'k', ou seja, o n√∫mero de centr√≥ides. Felizmente, o 'm√©todo do cotovelo' ajuda a estimar um bom valor inicial para 'k'. Voc√™ experimentar√° isso em breve.

## Pr√©-requisito

Voc√™ trabalhar√° no arquivo [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) desta li√ß√£o, que inclui a importa√ß√£o de dados e a limpeza preliminar que voc√™ fez na √∫ltima li√ß√£o.

## Exerc√≠cio - prepara√ß√£o

Comece revisando os dados das m√∫sicas.

1. Crie um boxplot, chamando `boxplot()` para cada coluna:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Esses dados est√£o um pouco ruidosos: ao observar cada coluna como um boxplot, voc√™ pode ver outliers.

    ![outliers](../../../../translated_images/boxplots.8228c29dabd0f29227dd38624231a175f411f1d8d4d7c012cb770e00e4fdf8b6.br.png)

Voc√™ poderia percorrer o conjunto de dados e remover esses outliers, mas isso tornaria os dados bastante reduzidos.

1. Por enquanto, escolha quais colunas voc√™ usar√° para o exerc√≠cio de clustering. Escolha aquelas com intervalos semelhantes e codifique a coluna `artist_top_genre` como dados num√©ricos:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Agora voc√™ precisa escolher quantos clusters deseja segmentar. Voc√™ sabe que h√° 3 g√™neros musicais que extra√≠mos do conjunto de dados, ent√£o vamos tentar 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Voc√™ ver√° um array impresso com clusters previstos (0, 1 ou 2) para cada linha do dataframe.

1. Use esse array para calcular uma 'pontua√ß√£o de silhouette':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Pontua√ß√£o de Silhouette

Procure uma pontua√ß√£o de silhouette mais pr√≥xima de 1. Essa pontua√ß√£o varia de -1 a 1, e se a pontua√ß√£o for 1, o cluster √© denso e bem separado dos outros clusters. Um valor pr√≥ximo de 0 representa clusters sobrepostos com amostras muito pr√≥ximas do limite de decis√£o dos clusters vizinhos. [(Fonte)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Nossa pontua√ß√£o √© **0,53**, bem no meio. Isso indica que nossos dados n√£o s√£o particularmente adequados para esse tipo de clustering, mas vamos continuar.

### Exerc√≠cio - construir um modelo

1. Importe `KMeans` e inicie o processo de clustering.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    H√° algumas partes aqui que merecem explica√ß√£o.

    > üéì range: Estas s√£o as itera√ß√µes do processo de clustering.

    > üéì random_state: "Determina a gera√ß√£o de n√∫meros aleat√≥rios para a inicializa√ß√£o dos centr√≥ides." [Fonte](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "soma dos quadrados dentro do cluster" mede a dist√¢ncia m√©dia ao quadrado de todos os pontos dentro de um cluster at√© o centr√≥ide do cluster. [Fonte](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì In√©rcia: Os algoritmos K-Means tentam escolher centr√≥ides para minimizar a 'in√©rcia', "uma medida de qu√£o coerentes internamente os clusters s√£o." [Fonte](https://scikit-learn.org/stable/modules/clustering.html). O valor √© adicionado √† vari√°vel wcss em cada itera√ß√£o.

    > üéì k-means++: No [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means), voc√™ pode usar a otimiza√ß√£o 'k-means++', que "inicializa os centr√≥ides para serem (geralmente) distantes uns dos outros, levando a resultados provavelmente melhores do que a inicializa√ß√£o aleat√≥ria."

### M√©todo do cotovelo

Anteriormente, voc√™ presumiu que, como segmentou 3 g√™neros musicais, deveria escolher 3 clusters. Mas ser√° que √© isso mesmo?

1. Use o 'm√©todo do cotovelo' para ter certeza.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Use a vari√°vel `wcss` que voc√™ construiu na etapa anterior para criar um gr√°fico mostrando onde est√° a 'curva' no cotovelo, que indica o n√∫mero ideal de clusters. Talvez seja **3**!

    ![elbow method](../../../../translated_images/elbow.72676169eed744ff03677e71334a16c6b8f751e9e716e3d7f40dd7cdef674cca.br.png)

## Exerc√≠cio - exibir os clusters

1. Tente o processo novamente, desta vez configurando tr√™s clusters, e exiba os clusters como um gr√°fico de dispers√£o:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Verifique a precis√£o do modelo:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    A precis√£o deste modelo n√£o √© muito boa, e o formato dos clusters d√° uma pista do porqu√™.

    ![clusters](../../../../translated_images/clusters.b635354640d8e4fd4a49ef545495518e7be76172c97c13bd748f5b79f171f69a.br.png)

    Esses dados s√£o muito desequilibrados, pouco correlacionados e h√° muita vari√¢ncia entre os valores das colunas para formar bons clusters. Na verdade, os clusters que se formam provavelmente s√£o fortemente influenciados ou enviesados pelas tr√™s categorias de g√™neros que definimos acima. Foi um processo de aprendizado!

    Na documenta√ß√£o do Scikit-learn, voc√™ pode ver que um modelo como este, com clusters n√£o muito bem demarcados, tem um problema de 'vari√¢ncia':

    ![problem models](../../../../translated_images/problems.f7fb539ccd80608e1f35c319cf5e3ad1809faa3c08537aead8018c6b5ba2e33a.br.png)
    > Infogr√°fico do Scikit-learn

## Vari√¢ncia

Vari√¢ncia √© definida como "a m√©dia das diferen√ßas ao quadrado em rela√ß√£o √† m√©dia" [(Fonte)](https://www.mathsisfun.com/data/standard-deviation.html). No contexto deste problema de clustering, refere-se a dados em que os n√∫meros do nosso conjunto tendem a divergir um pouco demais da m√©dia.

‚úÖ Este √© um √≥timo momento para pensar em todas as maneiras de corrigir esse problema. Ajustar os dados um pouco mais? Usar colunas diferentes? Utilizar um algoritmo diferente? Dica: Experimente [escalar seus dados](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) para normaliz√°-los e testar outras colunas.

> Experimente este '[calculador de vari√¢ncia](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' para entender melhor o conceito.

---

## üöÄDesafio

Passe algum tempo com este notebook ajustando os par√¢metros. Voc√™ consegue melhorar a precis√£o do modelo limpando mais os dados (removendo outliers, por exemplo)? Voc√™ pode usar pesos para dar mais import√¢ncia a determinadas amostras de dados. O que mais voc√™ pode fazer para criar clusters melhores?

Dica: Experimente escalar seus dados. H√° c√≥digo comentado no notebook que adiciona escalonamento padr√£o para fazer as colunas de dados se parecerem mais em termos de intervalo. Voc√™ descobrir√° que, embora a pontua√ß√£o de silhouette diminua, a 'curva' no gr√°fico do cotovelo se suaviza. Isso ocorre porque deixar os dados sem escala permite que dados com menos vari√¢ncia tenham mais peso. Leia um pouco mais sobre esse problema [aqui](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)

## Revis√£o e Autoestudo

D√™ uma olhada em um simulador de K-Means [como este](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Voc√™ pode usar esta ferramenta para visualizar pontos de dados de amostra e determinar seus centr√≥ides. Voc√™ pode editar a aleatoriedade dos dados, o n√∫mero de clusters e o n√∫mero de centr√≥ides. Isso ajuda voc√™ a ter uma ideia de como os dados podem ser agrupados?

Al√©m disso, confira [este material sobre K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) da Stanford.

## Tarefa

[Experimente diferentes m√©todos de clustering](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.