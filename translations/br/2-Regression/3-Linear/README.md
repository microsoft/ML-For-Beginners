<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-04T21:20:40+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "br"
}
-->
# Construir um modelo de regress√£o usando Scikit-learn: regress√£o de quatro maneiras

![Infogr√°fico de regress√£o linear vs polinomial](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infogr√°fico por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

> ### [Esta li√ß√£o est√° dispon√≠vel em R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introdu√ß√£o 

At√© agora, voc√™ explorou o que √© regress√£o com dados de exemplo coletados do conjunto de dados de pre√ßos de ab√≥bora que usaremos ao longo desta li√ß√£o. Voc√™ tamb√©m os visualizou usando Matplotlib.

Agora voc√™ est√° pronto para mergulhar mais fundo na regress√£o para aprendizado de m√°quina. Embora a visualiza√ß√£o permita que voc√™ compreenda os dados, o verdadeiro poder do aprendizado de m√°quina vem do _treinamento de modelos_. Modelos s√£o treinados com dados hist√≥ricos para capturar automaticamente as depend√™ncias dos dados e permitem prever resultados para novos dados que o modelo ainda n√£o viu.

Nesta li√ß√£o, voc√™ aprender√° mais sobre dois tipos de regress√£o: _regress√£o linear b√°sica_ e _regress√£o polinomial_, juntamente com algumas das matem√°ticas subjacentes a essas t√©cnicas. Esses modelos nos permitir√£o prever os pre√ßos das ab√≥boras dependendo de diferentes dados de entrada.

[![ML para iniciantes - Entendendo a Regress√£o Linear](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para iniciantes - Entendendo a Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo sobre regress√£o linear.

> Ao longo deste curr√≠culo, assumimos conhecimento m√≠nimo de matem√°tica e buscamos torn√°-lo acess√≠vel para estudantes de outras √°reas. Fique atento a notas, üßÆ chamadas, diagramas e outras ferramentas de aprendizado para ajudar na compreens√£o.

### Pr√©-requisito

Voc√™ j√° deve estar familiarizado com a estrutura dos dados de ab√≥bora que estamos analisando. Voc√™ pode encontr√°-los pr√©-carregados e pr√©-limpos no arquivo _notebook.ipynb_ desta li√ß√£o. No arquivo, o pre√ßo da ab√≥bora √© exibido por alqueire em um novo data frame. Certifique-se de que pode executar esses notebooks em kernels no Visual Studio Code.

### Prepara√ß√£o

Como lembrete, voc√™ est√° carregando esses dados para fazer perguntas sobre eles.

- Qual √© o melhor momento para comprar ab√≥boras?
- Qual pre√ßo posso esperar por uma caixa de ab√≥boras em miniatura?
- Devo compr√°-las em cestas de meio alqueire ou em caixas de 1 1/9 alqueire?
Vamos continuar explorando esses dados.

Na li√ß√£o anterior, voc√™ criou um data frame do Pandas e o preencheu com parte do conjunto de dados original, padronizando os pre√ßos por alqueire. Ao fazer isso, no entanto, voc√™ s√≥ conseguiu reunir cerca de 400 pontos de dados e apenas para os meses de outono.

D√™ uma olhada nos dados que pr√©-carregamos no notebook que acompanha esta li√ß√£o. Os dados est√£o pr√©-carregados e um gr√°fico de dispers√£o inicial √© tra√ßado para mostrar os dados por m√™s. Talvez possamos obter um pouco mais de detalhes sobre a natureza dos dados ao limp√°-los mais.

## Uma linha de regress√£o linear

Como voc√™ aprendeu na Li√ß√£o 1, o objetivo de um exerc√≠cio de regress√£o linear √© ser capaz de tra√ßar uma linha para:

- **Mostrar rela√ß√µes entre vari√°veis**. Mostrar a rela√ß√£o entre vari√°veis
- **Fazer previs√µes**. Fazer previs√µes precisas sobre onde um novo ponto de dados cairia em rela√ß√£o a essa linha.

√â t√≠pico da **Regress√£o de M√≠nimos Quadrados** tra√ßar esse tipo de linha. O termo 'm√≠nimos quadrados' significa que todos os pontos de dados ao redor da linha de regress√£o s√£o elevados ao quadrado e depois somados. Idealmente, essa soma final √© o menor poss√≠vel, porque queremos um n√∫mero baixo de erros, ou `m√≠nimos quadrados`.

Fazemos isso porque queremos modelar uma linha que tenha a menor dist√¢ncia cumulativa de todos os nossos pontos de dados. Tamb√©m elevamos os termos ao quadrado antes de som√°-los, pois estamos preocupados com sua magnitude, em vez de sua dire√ß√£o.

> **üßÆ Mostre-me a matem√°tica** 
> 
> Esta linha, chamada de _linha de melhor ajuste_, pode ser expressa por [uma equa√ß√£o](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` √© a 'vari√°vel explicativa'. `Y` √© a 'vari√°vel dependente'. A inclina√ß√£o da linha √© `b` e `a` √© o intercepto no eixo Y, que se refere ao valor de `Y` quando `X = 0`. 
>
>![calcular a inclina√ß√£o](../../../../2-Regression/3-Linear/images/slope.png)
>
> Primeiro, calcule a inclina√ß√£o `b`. Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> Em outras palavras, e referindo-se √† pergunta original dos dados de ab√≥bora: "prever o pre√ßo de uma ab√≥bora por alqueire por m√™s", `X` se referiria ao pre√ßo e `Y` ao m√™s de venda. 
>
>![completar a equa√ß√£o](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Calcule o valor de Y. Se voc√™ est√° pagando cerca de $4, deve ser abril! Infogr√°fico por [Jen Looper](https://twitter.com/jenlooper)
>
> A matem√°tica que calcula a linha deve demonstrar a inclina√ß√£o da linha, que tamb√©m depende do intercepto, ou onde `Y` est√° situado quando `X = 0`.
>
> Voc√™ pode observar o m√©todo de c√°lculo desses valores no site [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Tamb√©m visite [este calculador de m√≠nimos quadrados](https://www.mathsisfun.com/data/least-squares-calculator.html) para ver como os valores dos n√∫meros impactam a linha.

## Correla√ß√£o

Outro termo importante para entender √© o **Coeficiente de Correla√ß√£o** entre as vari√°veis X e Y fornecidas. Usando um gr√°fico de dispers√£o, voc√™ pode visualizar rapidamente esse coeficiente. Um gr√°fico com pontos de dados espalhados em uma linha ordenada tem alta correla√ß√£o, mas um gr√°fico com pontos de dados espalhados por toda parte entre X e Y tem baixa correla√ß√£o.

Um bom modelo de regress√£o linear ser√° aquele que tem um Coeficiente de Correla√ß√£o alto (mais pr√≥ximo de 1 do que de 0) usando o m√©todo de M√≠nimos Quadrados com uma linha de regress√£o.

‚úÖ Execute o notebook que acompanha esta li√ß√£o e observe o gr√°fico de dispers√£o de M√™s para Pre√ßo. Os dados que associam M√™s ao Pre√ßo para vendas de ab√≥bora parecem ter alta ou baixa correla√ß√£o, de acordo com sua interpreta√ß√£o visual do gr√°fico de dispers√£o? Isso muda se voc√™ usar uma medida mais detalhada em vez de `M√™s`, como *dia do ano* (ou seja, n√∫mero de dias desde o in√≠cio do ano)?

No c√≥digo abaixo, assumiremos que limpamos os dados e obtivemos um data frame chamado `new_pumpkins`, semelhante ao seguinte:

ID | M√™s | DiaDoAno | Variedade | Cidade | Embalagem | Pre√ßo Baixo | Pre√ßo Alto | Pre√ßo
---|-----|----------|-----------|--------|-----------|-------------|------------|-------
70 | 9 | 267 | TIPO TORTA | BALTIMORE | 1 1/9 caixas de alqueire | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | TIPO TORTA | BALTIMORE | 1 1/9 caixas de alqueire | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | TIPO TORTA | BALTIMORE | 1 1/9 caixas de alqueire | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | TIPO TORTA | BALTIMORE | 1 1/9 caixas de alqueire | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | TIPO TORTA | BALTIMORE | 1 1/9 caixas de alqueire | 15.0 | 15.0 | 13.636364

> O c√≥digo para limpar os dados est√° dispon√≠vel em [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Realizamos os mesmos passos de limpeza da li√ß√£o anterior e calculamos a coluna `DiaDoAno` usando a seguinte express√£o: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Agora que voc√™ entende a matem√°tica por tr√°s da regress√£o linear, vamos criar um modelo de regress√£o para ver se conseguimos prever qual embalagem de ab√≥boras ter√° os melhores pre√ßos. Algu√©m comprando ab√≥boras para um campo de ab√≥boras de feriado pode querer essa informa√ß√£o para otimizar suas compras de embalagens de ab√≥boras para o campo.

## Procurando por Correla√ß√£o

[![ML para iniciantes - Procurando por Correla√ß√£o: A Chave para Regress√£o Linear](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para iniciantes - Procurando por Correla√ß√£o: A Chave para Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo sobre correla√ß√£o.

Na li√ß√£o anterior, voc√™ provavelmente viu que o pre√ßo m√©dio para diferentes meses se parece com isto:

<img alt="Pre√ßo m√©dio por m√™s" src="../2-Data/images/barchart.png" width="50%"/>

Isso sugere que deve haver alguma correla√ß√£o, e podemos tentar treinar um modelo de regress√£o linear para prever a rela√ß√£o entre `M√™s` e `Pre√ßo`, ou entre `DiaDoAno` e `Pre√ßo`. Aqui est√° o gr√°fico de dispers√£o que mostra a √∫ltima rela√ß√£o:

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/scatter-dayofyear.png" width="50%" /> 

Vamos ver se h√° correla√ß√£o usando a fun√ß√£o `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que a correla√ß√£o √© bem pequena, -0.15 por `M√™s` e -0.17 por `DiaDoAno`, mas pode haver outra rela√ß√£o importante. Parece que h√° diferentes agrupamentos de pre√ßos correspondendo a diferentes variedades de ab√≥bora. Para confirmar essa hip√≥tese, vamos plotar cada categoria de ab√≥bora usando uma cor diferente. Passando um par√¢metro `ax` para a fun√ß√£o de plotagem `scatter`, podemos plotar todos os pontos no mesmo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/scatter-dayofyear-color.png" width="50%" /> 

Nossa investiga√ß√£o sugere que a variedade tem mais efeito no pre√ßo geral do que a data de venda. Podemos ver isso com um gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de pre√ßo vs variedade" src="images/price-by-variety.png" width="50%" /> 

Vamos focar por enquanto apenas em uma variedade de ab√≥bora, o 'tipo torta', e ver qual efeito a data tem no pre√ßo:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Se agora calcularmos a correla√ß√£o entre `Pre√ßo` e `DiaDoAno` usando a fun√ß√£o `corr`, obteremos algo como `-0.27` - o que significa que treinar um modelo preditivo faz sentido.

> Antes de treinar um modelo de regress√£o linear, √© importante garantir que nossos dados estejam limpos. A regress√£o linear n√£o funciona bem com valores ausentes, portanto, faz sentido eliminar todas as c√©lulas vazias:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Outra abordagem seria preencher esses valores vazios com valores m√©dios da coluna correspondente.

## Regress√£o Linear Simples

[![ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn")

> üé• Clique na imagem acima para um breve v√≠deo sobre regress√£o linear e polinomial.

Para treinar nosso modelo de regress√£o linear, usaremos a biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Come√ßamos separando os valores de entrada (features) e a sa√≠da esperada (label) em arrays numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Note que tivemos que realizar `reshape` nos dados de entrada para que o pacote de regress√£o linear os entendesse corretamente. A regress√£o linear espera um array 2D como entrada, onde cada linha do array corresponde a um vetor de caracter√≠sticas de entrada. No nosso caso, como temos apenas uma entrada, precisamos de um array com formato N√ó1, onde N √© o tamanho do conjunto de dados.

Depois, precisamos dividir os dados em conjuntos de treino e teste, para que possamos validar nosso modelo ap√≥s o treinamento:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, treinar o modelo de regress√£o linear real leva apenas duas linhas de c√≥digo. Definimos o objeto `LinearRegression` e ajustamos aos nossos dados usando o m√©todo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

O objeto `LinearRegression` ap√≥s o ajuste (`fit`) cont√©m todos os coeficientes da regress√£o, que podem ser acessados usando a propriedade `.coef_`. No nosso caso, h√° apenas um coeficiente, que deve ser em torno de `-0.017`. Isso significa que os pre√ßos parecem cair um pouco com o tempo, mas n√£o muito, cerca de 2 centavos por dia. Tamb√©m podemos acessar o ponto de interse√ß√£o da regress√£o com o eixo Y usando `lin_reg.intercept_` - ser√° em torno de `21` no nosso caso, indicando o pre√ßo no in√≠cio do ano.

Para ver qu√£o preciso nosso modelo √©, podemos prever pre√ßos em um conjunto de dados de teste e, em seguida, medir qu√£o pr√≥ximas nossas previs√µes est√£o dos valores esperados. Isso pode ser feito usando a m√©trica de erro quadr√°tico m√©dio (MSE), que √© a m√©dia de todas as diferen√ßas quadradas entre o valor esperado e o valor previsto.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Nosso erro parece estar em torno de 2 pontos, o que equivale a ~17%. N√£o √© muito bom. Outro indicador da qualidade do modelo √© o **coeficiente de determina√ß√£o**, que pode ser obtido assim:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Se o valor for 0, significa que o modelo n√£o leva os dados de entrada em considera√ß√£o e age como o *pior preditor linear*, que √© simplesmente o valor m√©dio do resultado. O valor 1 significa que podemos prever perfeitamente todos os resultados esperados. No nosso caso, o coeficiente est√° em torno de 0,06, o que √© bastante baixo.

Tamb√©m podemos plotar os dados de teste junto com a linha de regress√£o para entender melhor como a regress√£o funciona no nosso caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Regress√£o linear" src="images/linear-results.png" width="50%" />

## Regress√£o Polinomial

Outro tipo de Regress√£o Linear √© a Regress√£o Polinomial. Embora √†s vezes haja uma rela√ß√£o linear entre as vari√°veis - quanto maior o volume da ab√≥bora, maior o pre√ßo - √†s vezes essas rela√ß√µes n√£o podem ser representadas como um plano ou linha reta.

‚úÖ Aqui est√£o [alguns exemplos](https://online.stat.psu.edu/stat501/lesson/9/9.8) de dados que poderiam usar Regress√£o Polinomial.

Observe novamente a rela√ß√£o entre Data e Pre√ßo. Esse gr√°fico de dispers√£o parece que deveria ser analisado necessariamente por uma linha reta? Os pre√ßos n√£o podem flutuar? Nesse caso, voc√™ pode tentar a regress√£o polinomial.

‚úÖ Polin√¥mios s√£o express√µes matem√°ticas que podem consistir em uma ou mais vari√°veis e coeficientes.

A regress√£o polinomial cria uma linha curva para ajustar melhor os dados n√£o lineares. No nosso caso, se incluirmos uma vari√°vel `DayOfYear` ao quadrado nos dados de entrada, devemos ser capazes de ajustar nossos dados com uma curva parab√≥lica, que ter√° um m√≠nimo em um determinado ponto do ano.

O Scikit-learn inclui uma [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) √∫til para combinar diferentes etapas de processamento de dados. Um **pipeline** √© uma cadeia de **estimadores**. No nosso caso, criaremos um pipeline que primeiro adiciona caracter√≠sticas polinomiais ao nosso modelo e, em seguida, treina a regress√£o:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usar `PolynomialFeatures(2)` significa que incluiremos todos os polin√¥mios de segundo grau dos dados de entrada. No nosso caso, isso significar√° apenas `DayOfYear`<sup>2</sup>, mas, dado duas vari√°veis de entrada X e Y, isso adicionar√° X<sup>2</sup>, XY e Y<sup>2</sup>. Tamb√©m podemos usar polin√¥mios de grau superior, se quisermos.

Os pipelines podem ser usados da mesma maneira que o objeto original `LinearRegression`, ou seja, podemos usar `fit` no pipeline e, em seguida, usar `predict` para obter os resultados da previs√£o. Aqui est√° o gr√°fico mostrando os dados de teste e a curva de aproxima√ß√£o:

<img alt="Regress√£o polinomial" src="images/poly-results.png" width="50%" />

Usando Regress√£o Polinomial, podemos obter um MSE ligeiramente menor e um coeficiente de determina√ß√£o maior, mas n√£o significativamente. Precisamos levar em conta outras caracter√≠sticas!

> Voc√™ pode ver que os pre√ßos m√≠nimos das ab√≥boras s√£o observados em algum momento pr√≥ximo ao Halloween. Como voc√™ explicaria isso?

üéÉ Parab√©ns, voc√™ acabou de criar um modelo que pode ajudar a prever o pre√ßo das ab√≥boras para torta. Voc√™ provavelmente pode repetir o mesmo procedimento para todos os tipos de ab√≥bora, mas isso seria tedioso. Vamos aprender agora como levar em conta a variedade de ab√≥bora no nosso modelo!

## Caracter√≠sticas Categ√≥ricas

No mundo ideal, queremos ser capazes de prever pre√ßos para diferentes variedades de ab√≥bora usando o mesmo modelo. No entanto, a coluna `Variety` √© um pouco diferente de colunas como `Month`, porque cont√©m valores n√£o num√©ricos. Essas colunas s√£o chamadas de **categ√≥ricas**.

[![ML para iniciantes - Previs√µes com caracter√≠sticas categ√≥ricas usando Regress√£o Linear](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML para iniciantes - Previs√µes com caracter√≠sticas categ√≥ricas usando Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo sobre o uso de caracter√≠sticas categ√≥ricas.

Aqui voc√™ pode ver como o pre√ßo m√©dio depende da variedade:

<img alt="Pre√ßo m√©dio por variedade" src="images/price-by-variety.png" width="50%" />

Para levar a variedade em conta, primeiro precisamos convert√™-la para forma num√©rica, ou **codific√°-la**. Existem v√°rias maneiras de fazer isso:

* A **codifica√ß√£o num√©rica simples** criar√° uma tabela de diferentes variedades e, em seguida, substituir√° o nome da variedade por um √≠ndice nessa tabela. Essa n√£o √© a melhor ideia para regress√£o linear, porque a regress√£o linear leva o valor num√©rico real do √≠ndice e o adiciona ao resultado, multiplicando por algum coeficiente. No nosso caso, a rela√ß√£o entre o n√∫mero do √≠ndice e o pre√ßo √© claramente n√£o linear, mesmo que garantamos que os √≠ndices sejam ordenados de alguma forma espec√≠fica.
* A **codifica√ß√£o one-hot** substituir√° a coluna `Variety` por 4 colunas diferentes, uma para cada variedade. Cada coluna conter√° `1` se a linha correspondente for de uma determinada variedade e `0` caso contr√°rio. Isso significa que haver√° quatro coeficientes na regress√£o linear, um para cada variedade de ab√≥bora, respons√°vel pelo "pre√ßo inicial" (ou melhor, "pre√ßo adicional") para aquela variedade espec√≠fica.

O c√≥digo abaixo mostra como podemos codificar uma variedade usando one-hot:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para treinar a regress√£o linear usando a variedade codificada como one-hot nos dados de entrada, s√≥ precisamos inicializar os dados `X` e `y` corretamente:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

O restante do c√≥digo √© o mesmo que usamos acima para treinar a Regress√£o Linear. Se voc√™ tentar, ver√° que o erro m√©dio quadr√°tico √© aproximadamente o mesmo, mas obtemos um coeficiente de determina√ß√£o muito maior (~77%). Para obter previs√µes ainda mais precisas, podemos levar em conta mais caracter√≠sticas categ√≥ricas, bem como caracter√≠sticas num√©ricas, como `Month` ou `DayOfYear`. Para obter um grande array de caracter√≠sticas, podemos usar `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqui tamb√©m levamos em conta `City` e o tipo de `Package`, o que nos d√° um MSE de 2.84 (10%) e um coeficiente de determina√ß√£o de 0.94!

## Juntando tudo

Para criar o melhor modelo, podemos usar dados combinados (categ√≥ricos codificados como one-hot + num√©ricos) do exemplo acima junto com a Regress√£o Polinomial. Aqui est√° o c√≥digo completo para sua conveni√™ncia:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Isso deve nos dar o melhor coeficiente de determina√ß√£o de quase 97% e MSE=2.23 (~8% de erro de previs√£o).

| Modelo | MSE | Determina√ß√£o |
|--------|-----|--------------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polinomial | 2.73 (17.0%) | 0.08 |
| `Variety` Linear | 5.24 (19.7%) | 0.77 |
| Todas as caracter√≠sticas Linear | 2.84 (10.5%) | 0.94 |
| Todas as caracter√≠sticas Polinomial | 2.23 (8.25%) | 0.97 |

üèÜ Muito bem! Voc√™ criou quatro modelos de Regress√£o em uma √∫nica li√ß√£o e melhorou a qualidade do modelo para 97%. Na se√ß√£o final sobre Regress√£o, voc√™ aprender√° sobre Regress√£o Log√≠stica para determinar categorias.

---
## üöÄDesafio

Teste v√°rias vari√°veis diferentes neste notebook para ver como a correla√ß√£o corresponde √† precis√£o do modelo.

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Revis√£o e Autoestudo

Nesta li√ß√£o, aprendemos sobre Regress√£o Linear. Existem outros tipos importantes de Regress√£o. Leia sobre as t√©cnicas Stepwise, Ridge, Lasso e Elasticnet. Um bom curso para aprender mais √© o [curso de Aprendizado Estat√≠stico de Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Tarefa

[Construa um Modelo](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.