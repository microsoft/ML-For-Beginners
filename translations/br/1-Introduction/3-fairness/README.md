<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-04T21:34:04+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "br"
}
-->
# Construindo solu√ß√µes de Machine Learning com IA respons√°vel

![Resumo de IA respons√°vel em Machine Learning em um sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

## Introdu√ß√£o

Neste curr√≠culo, voc√™ come√ßar√° a descobrir como o aprendizado de m√°quina pode impactar e j√° est√° impactando nossas vidas cotidianas. Atualmente, sistemas e modelos est√£o envolvidos em tarefas de tomada de decis√£o di√°ria, como diagn√≥sticos de sa√∫de, aprova√ß√µes de empr√©stimos ou detec√ß√£o de fraudes. Por isso, √© importante que esses modelos funcionem bem para fornecer resultados confi√°veis. Assim como qualquer aplica√ß√£o de software, sistemas de IA podem n√£o atender √†s expectativas ou ter resultados indesej√°veis. √â por isso que √© essencial entender e explicar o comportamento de um modelo de IA.

Imagine o que pode acontecer quando os dados usados para construir esses modelos carecem de certos grupos demogr√°ficos, como ra√ßa, g√™nero, vis√£o pol√≠tica, religi√£o, ou representam esses grupos de forma desproporcional. E se a sa√≠da do modelo for interpretada de forma a favorecer algum grupo demogr√°fico? Qual √© a consequ√™ncia para a aplica√ß√£o? Al√©m disso, o que acontece quando o modelo tem um resultado adverso e prejudica pessoas? Quem √© respons√°vel pelo comportamento dos sistemas de IA? Estas s√£o algumas das quest√µes que exploraremos neste curr√≠culo.

Nesta li√ß√£o, voc√™ ir√°:

- Aumentar sua conscientiza√ß√£o sobre a import√¢ncia da equidade no aprendizado de m√°quina e os danos relacionados √† falta de equidade.
- Familiarizar-se com a pr√°tica de explorar outliers e cen√°rios incomuns para garantir confiabilidade e seguran√ßa.
- Compreender a necessidade de capacitar todos ao projetar sistemas inclusivos.
- Explorar como √© vital proteger a privacidade e a seguran√ßa dos dados e das pessoas.
- Ver a import√¢ncia de ter uma abordagem transparente para explicar o comportamento dos modelos de IA.
- Ser consciente de como a responsabilidade √© essencial para construir confian√ßa em sistemas de IA.

## Pr√©-requisito

Como pr√©-requisito, fa√ßa o "Caminho de Aprendizado sobre Princ√≠pios de IA Respons√°vel" e assista ao v√≠deo abaixo sobre o tema:

Saiba mais sobre IA Respons√°vel seguindo este [Caminho de Aprendizado](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Abordagem da Microsoft para IA Respons√°vel](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Abordagem da Microsoft para IA Respons√°vel")

> üé• Clique na imagem acima para assistir ao v√≠deo: Abordagem da Microsoft para IA Respons√°vel

## Equidade

Sistemas de IA devem tratar todos de forma justa e evitar afetar grupos semelhantes de pessoas de maneiras diferentes. Por exemplo, quando sistemas de IA fornecem orienta√ß√µes sobre tratamento m√©dico, solicita√ß√µes de empr√©stimos ou emprego, eles devem fazer as mesmas recomenda√ß√µes para todos com sintomas, circunst√¢ncias financeiras ou qualifica√ß√µes profissionais semelhantes. Cada um de n√≥s, como seres humanos, carrega preconceitos herdados que afetam nossas decis√µes e a√ß√µes. Esses preconceitos podem estar evidentes nos dados que usamos para treinar sistemas de IA. Essa manipula√ß√£o pode, √†s vezes, acontecer de forma n√£o intencional. Muitas vezes, √© dif√≠cil perceber conscientemente quando voc√™ est√° introduzindo preconceitos nos dados.

**‚ÄúInjusti√ßa‚Äù** abrange impactos negativos, ou ‚Äúdanos‚Äù, para um grupo de pessoas, como aqueles definidos em termos de ra√ßa, g√™nero, idade ou status de defici√™ncia. Os principais danos relacionados √† equidade podem ser classificados como:

- **Aloca√ß√£o**, quando, por exemplo, um g√™nero ou etnia √© favorecido em detrimento de outro.
- **Qualidade do servi√ßo**. Se voc√™ treinar os dados para um cen√°rio espec√≠fico, mas a realidade for muito mais complexa, isso leva a um servi√ßo de desempenho ruim. Por exemplo, um dispensador de sab√£o que n√£o consegue detectar pessoas com pele escura. [Refer√™ncia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigra√ß√£o**. Criticar ou rotular algo ou algu√©m de forma injusta. Por exemplo, uma tecnologia de rotulagem de imagens que infamemente rotulou imagens de pessoas de pele escura como gorilas.
- **Super ou sub-representa√ß√£o**. A ideia de que um determinado grupo n√£o √© visto em uma certa profiss√£o, e qualquer servi√ßo ou fun√ß√£o que continue promovendo isso est√° contribuindo para o dano.
- **Estereotipagem**. Associar um grupo espec√≠fico a atributos pr√©-definidos. Por exemplo, um sistema de tradu√ß√£o entre ingl√™s e turco pode ter imprecis√µes devido a palavras com associa√ß√µes estereotipadas de g√™nero.

![tradu√ß√£o para turco](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> tradu√ß√£o para turco

![tradu√ß√£o de volta para ingl√™s](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> tradu√ß√£o de volta para ingl√™s

Ao projetar e testar sistemas de IA, precisamos garantir que a IA seja justa e n√£o programada para tomar decis√µes tendenciosas ou discriminat√≥rias, que tamb√©m s√£o proibidas para seres humanos. Garantir equidade em IA e aprendizado de m√°quina continua sendo um desafio sociot√©cnico complexo.

### Confiabilidade e seguran√ßa

Para construir confian√ßa, sistemas de IA precisam ser confi√°veis, seguros e consistentes em condi√ß√µes normais e inesperadas. √â importante saber como os sistemas de IA se comportar√£o em uma variedade de situa√ß√µes, especialmente quando s√£o casos extremos. Ao construir solu√ß√µes de IA, √© necess√°rio um foco substancial em como lidar com uma ampla variedade de circunst√¢ncias que as solu√ß√µes de IA podem encontrar. Por exemplo, um carro aut√¥nomo precisa priorizar a seguran√ßa das pessoas. Como resultado, a IA que alimenta o carro precisa considerar todos os cen√°rios poss√≠veis que o carro pode enfrentar, como noite, tempestades, nevascas, crian√ßas correndo pela rua, animais de estima√ß√£o, constru√ß√µes na estrada, etc. Qu√£o bem um sistema de IA pode lidar com uma ampla gama de condi√ß√µes de forma confi√°vel e segura reflete o n√≠vel de antecipa√ß√£o que o cientista de dados ou desenvolvedor de IA considerou durante o design ou teste do sistema.

> [üé• Clique aqui para assistir ao v√≠deo: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclus√£o

Sistemas de IA devem ser projetados para engajar e capacitar todos. Ao projetar e implementar sistemas de IA, cientistas de dados e desenvolvedores de IA identificam e abordam poss√≠veis barreiras no sistema que poderiam excluir pessoas de forma n√£o intencional. Por exemplo, existem 1 bilh√£o de pessoas com defici√™ncia em todo o mundo. Com o avan√ßo da IA, elas podem acessar uma ampla gama de informa√ß√µes e oportunidades mais facilmente em suas vidas di√°rias. Ao abordar as barreiras, cria-se oportunidades para inovar e desenvolver produtos de IA com melhores experi√™ncias que beneficiem todos.

> [üé• Clique aqui para assistir ao v√≠deo: inclus√£o em IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Seguran√ßa e privacidade

Sistemas de IA devem ser seguros e respeitar a privacidade das pessoas. As pessoas t√™m menos confian√ßa em sistemas que colocam sua privacidade, informa√ß√µes ou vidas em risco. Ao treinar modelos de aprendizado de m√°quina, dependemos de dados para produzir os melhores resultados. Ao fazer isso, a origem dos dados e sua integridade devem ser consideradas. Por exemplo, os dados foram enviados por usu√°rios ou est√£o dispon√≠veis publicamente? Al√©m disso, ao trabalhar com os dados, √© crucial desenvolver sistemas de IA que possam proteger informa√ß√µes confidenciais e resistir a ataques. √Ä medida que a IA se torna mais prevalente, proteger a privacidade e garantir a seguran√ßa de informa√ß√µes pessoais e empresariais importantes est√° se tornando mais cr√≠tico e complexo. Quest√µes de privacidade e seguran√ßa de dados exigem aten√ß√£o especial para IA porque o acesso aos dados √© essencial para que os sistemas de IA fa√ßam previs√µes e decis√µes precisas e informadas sobre as pessoas.

> [üé• Clique aqui para assistir ao v√≠deo: seguran√ßa em IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como ind√∫stria, fizemos avan√ßos significativos em privacidade e seguran√ßa, impulsionados significativamente por regulamenta√ß√µes como o GDPR (Regulamento Geral de Prote√ß√£o de Dados).
- No entanto, com sistemas de IA, devemos reconhecer a tens√£o entre a necessidade de mais dados pessoais para tornar os sistemas mais personalizados e eficazes ‚Äì e a privacidade.
- Assim como com o nascimento de computadores conectados √† internet, tamb√©m estamos vendo um grande aumento no n√∫mero de problemas de seguran√ßa relacionados √† IA.
- Ao mesmo tempo, vimos a IA sendo usada para melhorar a seguran√ßa. Por exemplo, a maioria dos scanners antiv√≠rus modernos √© alimentada por heur√≠sticas de IA.
- Precisamos garantir que nossos processos de ci√™ncia de dados se harmonizem com as pr√°ticas mais recentes de privacidade e seguran√ßa.

### Transpar√™ncia

Sistemas de IA devem ser compreens√≠veis. Uma parte crucial da transpar√™ncia √© explicar o comportamento dos sistemas de IA e seus componentes. Melhorar a compreens√£o dos sistemas de IA exige que as partes interessadas compreendam como e por que eles funcionam, para que possam identificar poss√≠veis problemas de desempenho, preocupa√ß√µes de seguran√ßa e privacidade, preconceitos, pr√°ticas excludentes ou resultados indesejados. Tamb√©m acreditamos que aqueles que usam sistemas de IA devem ser honestos e transparentes sobre quando, por que e como escolhem implant√°-los, bem como sobre as limita√ß√µes dos sistemas que utilizam. Por exemplo, se um banco usa um sistema de IA para apoiar suas decis√µes de empr√©stimos ao consumidor, √© importante examinar os resultados e entender quais dados influenciam as recomenda√ß√µes do sistema. Governos est√£o come√ßando a regulamentar a IA em diferentes ind√∫strias, ent√£o cientistas de dados e organiza√ß√µes devem explicar se um sistema de IA atende aos requisitos regulat√≥rios, especialmente quando h√° um resultado indesej√°vel.

> [üé• Clique aqui para assistir ao v√≠deo: transpar√™ncia em IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como os sistemas de IA s√£o t√£o complexos, √© dif√≠cil entender como eles funcionam e interpretar os resultados.
- Essa falta de compreens√£o afeta a forma como esses sistemas s√£o gerenciados, operacionalizados e documentados.
- Mais importante ainda, essa falta de compreens√£o afeta as decis√µes tomadas com base nos resultados que esses sistemas produzem.

### Responsabilidade

As pessoas que projetam e implantam sistemas de IA devem ser respons√°veis pelo funcionamento de seus sistemas. A necessidade de responsabilidade √© particularmente crucial com tecnologias sens√≠veis, como reconhecimento facial. Recentemente, houve uma demanda crescente por tecnologia de reconhecimento facial, especialmente de organiza√ß√µes de aplica√ß√£o da lei que veem o potencial da tecnologia em usos como encontrar crian√ßas desaparecidas. No entanto, essas tecnologias podem ser usadas por um governo para colocar em risco as liberdades fundamentais de seus cidad√£os, por exemplo, permitindo vigil√¢ncia cont√≠nua de indiv√≠duos espec√≠ficos. Portanto, cientistas de dados e organiza√ß√µes precisam ser respons√°veis pelo impacto de seus sistemas de IA sobre indiv√≠duos ou a sociedade.

[![Pesquisador l√≠der em IA alerta sobre vigil√¢ncia em massa por meio de reconhecimento facial](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Abordagem da Microsoft para IA Respons√°vel")

> üé• Clique na imagem acima para assistir ao v√≠deo: Alertas sobre vigil√¢ncia em massa por meio de reconhecimento facial

No final, uma das maiores quest√µes para nossa gera√ß√£o, como a primeira gera√ß√£o que est√° trazendo IA para a sociedade, √© como garantir que os computadores permane√ßam respons√°veis perante as pessoas e como garantir que as pessoas que projetam computadores permane√ßam respons√°veis perante todos os outros.

## Avalia√ß√£o de impacto

Antes de treinar um modelo de aprendizado de m√°quina, √© importante realizar uma avalia√ß√£o de impacto para entender o prop√≥sito do sistema de IA; qual √© o uso pretendido; onde ele ser√° implantado; e quem interagir√° com o sistema. Essas informa√ß√µes s√£o √∫teis para os revisores ou testadores avaliarem o sistema e saberem quais fatores considerar ao identificar riscos potenciais e consequ√™ncias esperadas.

As seguintes √°reas devem ser focadas ao realizar uma avalia√ß√£o de impacto:

* **Impacto adverso sobre indiv√≠duos**. Estar ciente de quaisquer restri√ß√µes ou requisitos, uso n√£o suportado ou limita√ß√µes conhecidas que possam prejudicar o desempenho do sistema √© vital para garantir que o sistema n√£o seja usado de forma a causar danos √†s pessoas.
* **Requisitos de dados**. Compreender como e onde o sistema usar√° dados permite que os revisores explorem quaisquer requisitos de dados que voc√™ precise considerar (por exemplo, regulamenta√ß√µes de dados como GDPR ou HIPAA). Al√©m disso, examine se a origem ou quantidade de dados √© substancial para o treinamento.
* **Resumo do impacto**. Re√∫na uma lista de poss√≠veis danos que podem surgir do uso do sistema. Ao longo do ciclo de vida do aprendizado de m√°quina, revise se os problemas identificados foram mitigados ou resolvidos.
* **Metas aplic√°veis** para cada um dos seis princ√≠pios fundamentais. Avalie se as metas de cada princ√≠pio foram atendidas e se h√° lacunas.

## Depura√ß√£o com IA respons√°vel

Semelhante √† depura√ß√£o de uma aplica√ß√£o de software, depurar um sistema de IA √© um processo necess√°rio para identificar e resolver problemas no sistema. H√° muitos fatores que podem afetar o desempenho de um modelo ou sua responsabilidade. A maioria das m√©tricas tradicionais de desempenho de modelos s√£o agregados quantitativos do desempenho de um modelo, o que n√£o √© suficiente para analisar como um modelo viola os princ√≠pios de IA respons√°vel. Al√©m disso, um modelo de aprendizado de m√°quina √© uma "caixa preta", o que dificulta entender o que impulsiona seus resultados ou fornecer explica√ß√µes quando ele comete erros. Mais adiante neste curso, aprenderemos como usar o painel de IA Respons√°vel para ajudar a depurar sistemas de IA. O painel fornece uma ferramenta hol√≠stica para cientistas de dados e desenvolvedores de IA realizarem:

* **An√°lise de erros**. Para identificar a distribui√ß√£o de erros do modelo que pode afetar a equidade ou confiabilidade do sistema.
* **Vis√£o geral do modelo**. Para descobrir onde h√° disparidades no desempenho do modelo entre diferentes grupos de dados.
* **An√°lise de dados**. Para entender a distribui√ß√£o dos dados e identificar poss√≠veis preconceitos nos dados que possam levar a problemas de equidade, inclus√£o e confiabilidade.
* **Interpretabilidade do modelo**. Para entender o que afeta ou influencia as previs√µes do modelo. Isso ajuda a explicar o comportamento do modelo, o que √© importante para transpar√™ncia e responsabilidade.

## üöÄ Desafio

Para evitar que danos sejam introduzidos desde o in√≠cio, devemos:

- ter diversidade de origens e perspectivas entre as pessoas que trabalham nos sistemas
- investir em conjuntos de dados que reflitam a diversidade de nossa sociedade
- desenvolver melhores m√©todos ao longo do ciclo de vida do aprendizado de m√°quina para detectar e corrigir problemas de IA respons√°vel quando eles ocorrerem

Pense em cen√°rios da vida real onde a falta de confiabilidade de um modelo √© evidente na constru√ß√£o e uso do modelo. O que mais devemos considerar?

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)

## Revis√£o e Autoestudo

Nesta li√ß√£o, voc√™ aprendeu alguns conceitos b√°sicos sobre equidade e injusti√ßa no aprendizado de m√°quina.
Assista a este workshop para se aprofundar nos t√≥picos:

- Em busca de IA respons√°vel: Colocando princ√≠pios em pr√°tica por Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

[![Responsible AI Toolbox: Um framework de c√≥digo aberto para construir IA respons√°vel](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Um framework de c√≥digo aberto para construir IA respons√°vel")

> üé• Clique na imagem acima para assistir ao v√≠deo: RAI Toolbox: Um framework de c√≥digo aberto para construir IA respons√°vel por Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

Al√©m disso, leia:

- Centro de recursos de IA respons√°vel da Microsoft: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de pesquisa FATE da Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Reposit√≥rio GitHub do Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Leia sobre as ferramentas do Azure Machine Learning para garantir equidade:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Tarefa

[Explore o RAI Toolbox](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.