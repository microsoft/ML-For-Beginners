<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-04T21:32:46+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "br"
}
-->
# P√≥s-escrito: Depura√ß√£o de Modelos de Machine Learning usando componentes do painel de IA Respons√°vel

## [Quiz pr√©-aula](https://ff-quizzes.netlify.app/en/ml/)

## Introdu√ß√£o

O aprendizado de m√°quina impacta nossas vidas cotidianas. A IA est√° se inserindo em alguns dos sistemas mais importantes que nos afetam como indiv√≠duos e como sociedade, desde sa√∫de, finan√ßas, educa√ß√£o e emprego. Por exemplo, sistemas e modelos est√£o envolvidos em tarefas di√°rias de tomada de decis√£o, como diagn√≥sticos m√©dicos ou detec√ß√£o de fraudes. Consequentemente, os avan√ßos na IA, juntamente com sua ado√ß√£o acelerada, est√£o sendo acompanhados por expectativas sociais em evolu√ß√£o e regulamenta√ß√µes crescentes em resposta. Constantemente vemos √°reas onde os sistemas de IA continuam a n√£o atender √†s expectativas; eles exp√µem novos desafios; e os governos est√£o come√ßando a regulamentar solu√ß√µes de IA. Por isso, √© importante que esses modelos sejam analisados para fornecer resultados justos, confi√°veis, inclusivos, transparentes e respons√°veis para todos.

Neste curr√≠culo, exploraremos ferramentas pr√°ticas que podem ser usadas para avaliar se um modelo apresenta problemas relacionados √† IA respons√°vel. T√©cnicas tradicionais de depura√ß√£o de aprendizado de m√°quina tendem a ser baseadas em c√°lculos quantitativos, como precis√£o agregada ou perda m√©dia de erro. Imagine o que pode acontecer quando os dados que voc√™ est√° usando para construir esses modelos carecem de certos grupos demogr√°ficos, como ra√ßa, g√™nero, vis√£o pol√≠tica, religi√£o, ou representam esses grupos de forma desproporcional. E se a sa√≠da do modelo for interpretada de forma a favorecer algum grupo demogr√°fico? Isso pode introduzir uma super ou sub-representa√ß√£o desses grupos sens√≠veis, resultando em problemas de justi√ßa, inclus√£o ou confiabilidade no modelo. Outro fator √© que os modelos de aprendizado de m√°quina s√£o considerados "caixas-pretas", o que dificulta entender e explicar o que impulsiona as previs√µes de um modelo. Todos esses s√£o desafios enfrentados por cientistas de dados e desenvolvedores de IA quando n√£o possuem ferramentas adequadas para depurar e avaliar a justi√ßa ou confiabilidade de um modelo.

Nesta li√ß√£o, voc√™ aprender√° a depurar seus modelos usando:

- **An√°lise de Erros**: identificar onde na distribui√ß√£o de dados o modelo apresenta altas taxas de erro.
- **Vis√£o Geral do Modelo**: realizar an√°lises comparativas entre diferentes coortes de dados para descobrir disparidades nas m√©tricas de desempenho do modelo.
- **An√°lise de Dados**: investigar onde pode haver super ou sub-representa√ß√£o nos dados que podem enviesar o modelo para favorecer um grupo demogr√°fico em detrimento de outro.
- **Import√¢ncia das Features**: entender quais caracter√≠sticas est√£o impulsionando as previs√µes do modelo em n√≠vel global ou local.

## Pr√©-requisito

Como pr√©-requisito, revise [Ferramentas de IA Respons√°vel para desenvolvedores](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif sobre Ferramentas de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## An√°lise de Erros

M√©tricas tradicionais de desempenho de modelos usadas para medir precis√£o s√£o, na maioria das vezes, c√°lculos baseados em previs√µes corretas versus incorretas. Por exemplo, determinar que um modelo √© preciso 89% do tempo com uma perda de erro de 0,001 pode ser considerado um bom desempenho. No entanto, os erros geralmente n√£o s√£o distribu√≠dos uniformemente no conjunto de dados subjacente. Voc√™ pode obter uma pontua√ß√£o de precis√£o de 89%, mas descobrir que existem diferentes regi√µes nos dados em que o modelo falha 42% do tempo. As consequ√™ncias desses padr√µes de falha em certos grupos de dados podem levar a problemas de justi√ßa ou confiabilidade. √â essencial entender as √°reas onde o modelo est√° se saindo bem ou n√£o. As regi√µes de dados com um alto n√∫mero de imprecis√µes no modelo podem acabar sendo um grupo demogr√°fico importante.

![Analisar e depurar erros do modelo](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

O componente de An√°lise de Erros no painel de IA Respons√°vel ilustra como as falhas do modelo est√£o distribu√≠das entre v√°rios coortes com uma visualiza√ß√£o em √°rvore. Isso √© √∫til para identificar caracter√≠sticas ou √°reas onde h√° uma alta taxa de erro no conjunto de dados. Ao ver de onde v√™m a maioria das imprecis√µes do modelo, voc√™ pode come√ßar a investigar a causa raiz. Tamb√©m √© poss√≠vel criar coortes de dados para realizar an√°lises. Esses coortes de dados ajudam no processo de depura√ß√£o para determinar por que o desempenho do modelo √© bom em um coorte, mas apresenta erros em outro.

![An√°lise de Erros](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

Os indicadores visuais no mapa de √°rvore ajudam a localizar as √°reas problem√°ticas mais rapidamente. Por exemplo, quanto mais escuro o tom de vermelho em um n√≥ da √°rvore, maior a taxa de erro.

O mapa de calor √© outra funcionalidade de visualiza√ß√£o que os usu√°rios podem usar para investigar a taxa de erro usando uma ou duas caracter√≠sticas para encontrar contribui√ß√µes para os erros do modelo em todo o conjunto de dados ou coortes.

![Mapa de calor da An√°lise de Erros](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Use a an√°lise de erros quando voc√™ precisar:

* Obter uma compreens√£o profunda de como as falhas do modelo est√£o distribu√≠das em um conjunto de dados e em v√°rias dimens√µes de entrada e caracter√≠sticas.
* Dividir as m√©tricas de desempenho agregadas para descobrir automaticamente coortes com erros e informar suas etapas de mitiga√ß√£o direcionadas.

## Vis√£o Geral do Modelo

Avaliar o desempenho de um modelo de aprendizado de m√°quina requer uma compreens√£o hol√≠stica de seu comportamento. Isso pode ser alcan√ßado revisando mais de uma m√©trica, como taxa de erro, precis√£o, recall, precis√£o ou MAE (Erro Absoluto M√©dio), para encontrar disparidades entre as m√©tricas de desempenho. Uma m√©trica de desempenho pode parecer √≥tima, mas imprecis√µes podem ser expostas em outra m√©trica. Al√©m disso, comparar as m√©tricas para disparidades em todo o conjunto de dados ou coortes ajuda a esclarecer onde o modelo est√° se saindo bem ou n√£o. Isso √© especialmente importante para observar o desempenho do modelo entre caracter√≠sticas sens√≠veis e insens√≠veis (por exemplo, ra√ßa, g√™nero ou idade de pacientes) para descobrir poss√≠veis injusti√ßas que o modelo possa ter. Por exemplo, descobrir que o modelo √© mais impreciso em um coorte que possui caracter√≠sticas sens√≠veis pode revelar poss√≠veis injusti√ßas no modelo.

O componente Vis√£o Geral do Modelo do painel de IA Respons√°vel ajuda n√£o apenas na an√°lise das m√©tricas de desempenho da representa√ß√£o de dados em um coorte, mas tamb√©m oferece aos usu√°rios a capacidade de comparar o comportamento do modelo entre diferentes coortes.

![Coortes de conjunto de dados - vis√£o geral do modelo no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

A funcionalidade de an√°lise baseada em caracter√≠sticas do componente permite que os usu√°rios delimitem subgrupos de dados dentro de uma caracter√≠stica espec√≠fica para identificar anomalias em um n√≠vel granular. Por exemplo, o painel possui intelig√™ncia integrada para gerar automaticamente coortes para uma caracter√≠stica selecionada pelo usu√°rio (ex., *"tempo_no_hospital < 3"* ou *"tempo_no_hospital >= 7"*). Isso permite que o usu√°rio isole uma caracter√≠stica espec√≠fica de um grupo maior de dados para verificar se ela √© um influenciador chave dos resultados err√¥neos do modelo.

![Coortes de caracter√≠sticas - vis√£o geral do modelo no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

O componente Vis√£o Geral do Modelo suporta duas classes de m√©tricas de disparidade:

**Disparidade no desempenho do modelo**: Esses conjuntos de m√©tricas calculam a disparidade (diferen√ßa) nos valores da m√©trica de desempenho selecionada entre subgrupos de dados. Aqui est√£o alguns exemplos:

* Disparidade na taxa de precis√£o
* Disparidade na taxa de erro
* Disparidade na precis√£o
* Disparidade no recall
* Disparidade no erro absoluto m√©dio (MAE)

**Disparidade na taxa de sele√ß√£o**: Essa m√©trica cont√©m a diferen√ßa na taxa de sele√ß√£o (previs√£o favor√°vel) entre subgrupos. Um exemplo disso √© a disparidade nas taxas de aprova√ß√£o de empr√©stimos. Taxa de sele√ß√£o significa a fra√ß√£o de pontos de dados em cada classe classificados como 1 (em classifica√ß√£o bin√°ria) ou a distribui√ß√£o dos valores de previs√£o (em regress√£o).

## An√°lise de Dados

> "Se voc√™ torturar os dados por tempo suficiente, eles confessar√£o qualquer coisa" - Ronald Coase

Essa afirma√ß√£o soa extrema, mas √© verdade que os dados podem ser manipulados para apoiar qualquer conclus√£o. Tal manipula√ß√£o √†s vezes pode acontecer de forma n√£o intencional. Como seres humanos, todos temos preconceitos, e muitas vezes √© dif√≠cil saber conscientemente quando estamos introduzindo preconceitos nos dados. Garantir justi√ßa na IA e no aprendizado de m√°quina continua sendo um desafio complexo.

Os dados s√£o um grande ponto cego para m√©tricas tradicionais de desempenho de modelos. Voc√™ pode ter altas pontua√ß√µes de precis√£o, mas isso nem sempre reflete o vi√©s subjacente que pode estar presente no conjunto de dados. Por exemplo, se um conjunto de dados de funcion√°rios possui 27% de mulheres em cargos executivos em uma empresa e 73% de homens no mesmo n√≠vel, um modelo de IA de publicidade de empregos treinado com esses dados pode direcionar principalmente um p√∫blico masculino para posi√ß√µes de alto n√≠vel. Ter esse desequil√≠brio nos dados enviesou a previs√£o do modelo para favorecer um g√™nero. Isso revela um problema de justi√ßa onde h√° um vi√©s de g√™nero no modelo de IA.

O componente de An√°lise de Dados no painel de IA Respons√°vel ajuda a identificar √°reas onde h√° super ou sub-representa√ß√£o no conjunto de dados. Ele ajuda os usu√°rios a diagnosticar a causa raiz de erros e problemas de justi√ßa introduzidos por desequil√≠brios nos dados ou falta de representa√ß√£o de um grupo de dados espec√≠fico. Isso d√° aos usu√°rios a capacidade de visualizar conjuntos de dados com base em resultados previstos e reais, grupos de erros e caracter√≠sticas espec√≠ficas. √Äs vezes, descobrir um grupo de dados sub-representado tamb√©m pode revelar que o modelo n√£o est√° aprendendo bem, da√≠ as altas imprecis√µes. Ter um modelo com vi√©s nos dados n√£o √© apenas um problema de justi√ßa, mas mostra que o modelo n√£o √© inclusivo ou confi√°vel.

![Componente de An√°lise de Dados no painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Use a an√°lise de dados quando voc√™ precisar:

* Explorar as estat√≠sticas do seu conjunto de dados selecionando diferentes filtros para dividir seus dados em diferentes dimens√µes (tamb√©m conhecidas como coortes).
* Entender a distribui√ß√£o do seu conjunto de dados entre diferentes coortes e grupos de caracter√≠sticas.
* Determinar se suas descobertas relacionadas √† justi√ßa, an√°lise de erros e causalidade (derivadas de outros componentes do painel) s√£o resultado da distribui√ß√£o do seu conjunto de dados.
* Decidir em quais √°reas coletar mais dados para mitigar erros que surgem de problemas de representa√ß√£o, ru√≠do de r√≥tulo, ru√≠do de caracter√≠sticas, vi√©s de r√≥tulo e fatores semelhantes.

## Interpretabilidade do Modelo

Modelos de aprendizado de m√°quina tendem a ser "caixas-pretas". Entender quais caracter√≠sticas-chave dos dados impulsionam as previs√µes de um modelo pode ser desafiador. √â importante fornecer transpar√™ncia sobre por que um modelo faz uma determinada previs√£o. Por exemplo, se um sistema de IA prev√™ que um paciente diab√©tico est√° em risco de ser readmitido em um hospital em menos de 30 dias, ele deve ser capaz de fornecer dados de suporte que levaram √† sua previs√£o. Ter indicadores de dados de suporte traz transpar√™ncia para ajudar cl√≠nicos ou hospitais a tomarem decis√µes bem informadas. Al√©m disso, ser capaz de explicar por que um modelo fez uma previs√£o para um paciente individual permite responsabilidade com as regulamenta√ß√µes de sa√∫de. Quando voc√™ usa modelos de aprendizado de m√°quina de maneiras que afetam a vida das pessoas, √© crucial entender e explicar o que influencia o comportamento de um modelo. A explicabilidade e interpretabilidade do modelo ajudam a responder perguntas em cen√°rios como:

* Depura√ß√£o do modelo: Por que meu modelo cometeu esse erro? Como posso melhorar meu modelo?
* Colabora√ß√£o humano-IA: Como posso entender e confiar nas decis√µes do modelo?
* Conformidade regulat√≥ria: Meu modelo atende aos requisitos legais?

O componente Import√¢ncia das Features do painel de IA Respons√°vel ajuda voc√™ a depurar e obter uma compreens√£o abrangente de como um modelo faz previs√µes. Tamb√©m √© uma ferramenta √∫til para profissionais de aprendizado de m√°quina e tomadores de decis√£o explicarem e mostrarem evid√™ncias das caracter√≠sticas que influenciam o comportamento do modelo para conformidade regulat√≥ria. Em seguida, os usu√°rios podem explorar explica√ß√µes globais e locais para validar quais caracter√≠sticas impulsionam as previs√µes do modelo. Explica√ß√µes globais listam as principais caracter√≠sticas que afetaram a previs√£o geral do modelo. Explica√ß√µes locais exibem quais caracter√≠sticas levaram √† previs√£o do modelo para um caso individual. A capacidade de avaliar explica√ß√µes locais tamb√©m √© √∫til na depura√ß√£o ou auditoria de um caso espec√≠fico para entender e interpretar melhor por que um modelo fez uma previs√£o precisa ou imprecisa.

![Componente Import√¢ncia das Features do painel de IA Respons√°vel](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Explica√ß√µes globais: Por exemplo, quais caracter√≠sticas afetam o comportamento geral de um modelo de readmiss√£o hospitalar para diabetes?
* Explica√ß√µes locais: Por exemplo, por que um paciente diab√©tico com mais de 60 anos e hospitaliza√ß√µes anteriores foi previsto como readmitido ou n√£o readmitido em menos de 30 dias em um hospital?

No processo de depura√ß√£o ao examinar o desempenho de um modelo entre diferentes coortes, a Import√¢ncia das Features mostra o n√≠vel de impacto que uma caracter√≠stica tem entre os coortes. Ela ajuda a revelar anomalias ao comparar o n√≠vel de influ√™ncia que a caracter√≠stica tem em impulsionar as previs√µes err√¥neas do modelo. O componente Import√¢ncia das Features pode mostrar quais valores em uma caracter√≠stica influenciaram positivamente ou negativamente o resultado do modelo. Por exemplo, se um modelo fez uma previs√£o imprecisa, o componente d√° a capacidade de detalhar e identificar quais caracter√≠sticas ou valores de caracter√≠sticas impulsionaram a previs√£o. Esse n√≠vel de detalhe ajuda n√£o apenas na depura√ß√£o, mas tamb√©m fornece transpar√™ncia e responsabilidade em situa√ß√µes de auditoria. Por fim, o componente pode ajudar a identificar problemas de justi√ßa. Para ilustrar, se uma caracter√≠stica sens√≠vel como etnia ou g√™nero √© altamente influente em impulsionar a previs√£o do modelo, isso pode ser um sinal de vi√©s de ra√ßa ou g√™nero no modelo.

![Import√¢ncia das Features](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Use interpretabilidade quando voc√™ precisar:

* Determinar o qu√£o confi√°veis s√£o as previs√µes do seu sistema de IA, entendendo quais caracter√≠sticas s√£o mais importantes para as previs√µes.
* Abordar a depura√ß√£o do seu modelo entendendo-o primeiro e identificando se o modelo est√° usando caracter√≠sticas saud√°veis ou apenas correla√ß√µes falsas.
* Descobrir poss√≠veis fontes de injusti√ßa entendendo se o modelo est√° baseando previs√µes em caracter√≠sticas sens√≠veis ou em caracter√≠sticas altamente correlacionadas com elas.
* Construir confian√ßa do usu√°rio nas decis√µes do modelo gerando explica√ß√µes locais para ilustrar seus resultados.
* Completar uma auditoria regulat√≥ria de um sistema de IA para validar modelos e monitorar o impacto das decis√µes do modelo sobre os seres humanos.

## Conclus√£o

Todos os componentes do painel de IA Respons√°vel s√£o ferramentas pr√°ticas para ajudar voc√™ a construir modelos de aprendizado de m√°quina que sejam menos prejudiciais e mais confi√°veis para a sociedade. Eles melhoram a preven√ß√£o de amea√ßas aos direitos humanos; discrimina√ß√£o ou exclus√£o de certos grupos de oportunidades de vida; e o risco de danos f√≠sicos ou psicol√≥gicos. Tamb√©m ajudam a construir confian√ßa nas decis√µes do modelo, gerando explica√ß√µes locais para ilustrar seus resultados. Alguns dos poss√≠veis danos podem ser classificados como:

- **Aloca√ß√£o**, se um g√™nero ou etnia, por exemplo, for favorecido em detrimento de outro.
- **Qualidade do servi√ßo**. Se voc√™ treinar os dados para um cen√°rio espec√≠fico, mas a realidade for muito mais complexa, isso leva a um servi√ßo de desempenho ruim.
- **Estereotipagem**. Associar um determinado grupo a atributos pr√©-atribu√≠dos.
- **Denigra√ß√£o**. Criticar injustamente e rotular algo ou algu√©m.
- **Representa√ß√£o excessiva ou insuficiente**. A ideia √© que um determinado grupo n√£o seja visto em uma certa profiss√£o, e qualquer servi√ßo ou fun√ß√£o que continue promovendo isso est√° contribuindo para o problema.

### Painel Azure RAI

O [Painel Azure RAI](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) √© baseado em ferramentas de c√≥digo aberto desenvolvidas por institui√ß√µes acad√™micas e organiza√ß√µes l√≠deres, incluindo a Microsoft. Ele √© essencial para cientistas de dados e desenvolvedores de IA entenderem melhor o comportamento dos modelos, identificarem e mitigarem problemas indesej√°veis em modelos de IA.

- Aprenda como usar os diferentes componentes consultando a [documenta√ß√£o do painel RAI.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Confira alguns [notebooks de exemplo do painel RAI](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) para depurar cen√°rios de IA mais respons√°veis no Azure Machine Learning.

---
## üöÄ Desafio

Para evitar que vieses estat√≠sticos ou de dados sejam introduzidos desde o in√≠cio, devemos:

- ter diversidade de origens e perspectivas entre as pessoas que trabalham nos sistemas
- investir em conjuntos de dados que reflitam a diversidade da nossa sociedade
- desenvolver melhores m√©todos para detectar e corrigir vieses quando eles ocorrerem

Pense em cen√°rios da vida real onde a injusti√ßa √© evidente na constru√ß√£o e uso de modelos. O que mais devemos considerar?

## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ml/)
## Revis√£o e Autoestudo

Nesta li√ß√£o, voc√™ aprendeu algumas ferramentas pr√°ticas para incorporar IA respons√°vel no aprendizado de m√°quina.

Assista a este workshop para se aprofundar nos t√≥picos:

- Painel de IA Respons√°vel: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica, por Besmira Nushi e Mehrnoosh Sameki

[![Painel de IA Respons√°vel: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Painel de IA Respons√°vel: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica")


> üé• Clique na imagem acima para assistir ao v√≠deo: Painel de IA Respons√°vel: Uma solu√ß√£o completa para operacionalizar IA respons√°vel na pr√°tica, por Besmira Nushi e Mehrnoosh Sameki

Consulte os seguintes materiais para aprender mais sobre IA respons√°vel e como construir modelos mais confi√°veis:

- Ferramentas do painel RAI da Microsoft para depura√ß√£o de modelos de aprendizado de m√°quina: [Recursos de ferramentas de IA respons√°vel](https://aka.ms/rai-dashboard)

- Explore o kit de ferramentas de IA respons√°vel: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Centro de recursos de IA respons√°vel da Microsoft: [Recursos de IA Respons√°vel ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de pesquisa FATE da Microsoft: [FATE: Justi√ßa, Responsabilidade, Transpar√™ncia e √âtica em IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Tarefa

[Explore o painel RAI](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.