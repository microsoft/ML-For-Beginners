<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-10-11T11:16:15+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "et"
}
-->
# CartPole Uisutamine

Probleem, mida me eelmises tunnis lahendasime, v√µib tunduda m√§nguline ja mitte eriti elul√§hedane. Tegelikult see nii ei ole, sest paljud p√§riselulised probleemid jagavad sama stsenaariumi ‚Äì n√§iteks malet v√µi Go-d m√§ngides. Need on sarnased, kuna meil on samuti m√§ngulaud kindlate reeglitega ja **diskreetne olek**.

## [Eeltesti k√ºsimustik](https://ff-quizzes.netlify.app/en/ml/)

## Sissejuhatus

Selles tunnis rakendame Q-√µppe p√µhim√µtteid probleemile, millel on **j√§tkuv olek**, st olek, mida kirjeldavad √ºks v√µi mitu reaalarvu. Me tegeleme j√§rgmise probleemiga:

> **Probleem**: Kui Peeter tahab hundi eest p√µgeneda, peab ta liikuma kiiremini. Me n√§eme, kuidas Peeter saab √µppida uisutama, t√§psemalt tasakaalu hoidma, kasutades Q-√µpet.

![Suur p√µgenemine!](../../../../translated_images/escape.18862db9930337e3.et.png)

> Peeter ja tema s√µbrad muutuvad loovaks, et hundi eest p√µgeneda! Pilt: [Jen Looper](https://twitter.com/jenlooper)

Kasutame tasakaalu lihtsustatud versiooni, mida tuntakse kui **CartPole** probleem. CartPole maailmas on meil horisontaalne liugur, mis saab liikuda vasakule v√µi paremale, ja eesm√§rk on hoida vertikaalset posti liuguri peal tasakaalus.

<img alt="CartPole" src="../../../../translated_images/cartpole.b5609cc0494a14f7.et.png" width="200"/>

## Eeltingimused

Selles tunnis kasutame raamatukogu nimega **OpenAI Gym**, et simuleerida erinevaid **keskkondi**. Saate selle tunni koodi k√§ivitada lokaalselt (nt Visual Studio Code'is), sel juhul avaneb simulatsioon uues aknas. Kui k√§ivitate koodi veebis, peate v√µib-olla koodi veidi kohandama, nagu kirjeldatud [siin](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Eelmises tunnis m√§√§ras m√§ngu reeglid ja oleku meie enda defineeritud `Board` klass. Siin kasutame spetsiaalset **simulatsioonikeskkonda**, mis simuleerib tasakaalu posti f√º√ºsikat. √úks populaarsemaid simulatsioonikeskkondi, mida kasutatakse tugevdus√µppe algoritmide treenimiseks, on [Gym](https://gym.openai.com/), mida haldab [OpenAI](https://openai.com/). Selle Gymi abil saame luua erinevaid **keskkondi**, alates CartPole simulatsioonist kuni Atari m√§ngudeni.

> **M√§rkus**: Teisi OpenAI Gymi keskkondi saate vaadata [siit](https://gym.openai.com/envs/#classic_control).

K√µigepealt installime Gymi ja impordime vajalikud teegid (koodiplokk 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Harjutus - CartPole keskkonna initsialiseerimine

CartPole tasakaalu probleemi lahendamiseks peame initsialiseerima vastava keskkonna. Iga keskkond on seotud:

- **Vaatlusruumiga**, mis m√§√§ratleb struktuuri, mille kaudu saame keskkonnast teavet. CartPole probleemi puhul saame posti asukoha, kiiruse ja m√µned muud v√§√§rtused.

- **Tegevusruumiga**, mis m√§√§ratleb v√µimalikud tegevused. Meie puhul on tegevusruum diskreetne ja koosneb kahest tegevusest ‚Äì **vasakule** ja **paremale**. (koodiplokk 2)

1. Initsialiseerimiseks sisestage j√§rgmine kood:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Et n√§ha, kuidas keskkond t√∂√∂tab, k√§ivitame l√ºhikese simulatsiooni 100 sammu jooksul. Igal sammul anname √ºhe tegevuse, mida tuleb teha ‚Äì selles simulatsioonis valime juhuslikult tegevuse `action_space` hulgast.

1. K√§ivitage allolev kood ja vaadake, mis juhtub.

    ‚úÖ Pidage meeles, et eelistatav on k√§ivitada see kood lokaalses Python'i installatsioonis! (koodiplokk 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Peaksite n√§gema midagi sarnast sellele pildile:

    ![tasakaalustamata CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Simulatsiooni ajal peame saama vaatlusi, et otsustada, kuidas tegutseda. Tegelikult tagastab `step` funktsioon praegused vaatlused, tasu funktsiooni ja `done` lipu, mis n√§itab, kas simulatsiooni j√§tkamine on m√µistlik v√µi mitte: (koodiplokk 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    N√§ete midagi sellist oma m√§rkmiku v√§ljundis:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Simulatsiooni iga sammu jooksul tagastatud vaatlusvektor sisaldab j√§rgmisi v√§√§rtusi:
    - K√§ru asukoht
    - K√§ru kiirus
    - Posti nurk
    - Posti p√∂√∂rlemiskiirus

1. Leidke nende numbrite minimaalne ja maksimaalne v√§√§rtus: (koodiplokk 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Samuti v√µite m√§rgata, et tasu v√§√§rtus igal simulatsiooni sammul on alati 1. See on sellep√§rast, et meie eesm√§rk on ellu j√§√§da nii kaua kui v√µimalik, st hoida post m√µistlikult vertikaalses asendis v√µimalikult pika aja jooksul.

    ‚úÖ Tegelikult loetakse CartPole simulatsioon lahendatuks, kui suudame saavutada keskmise tasu 195 √ºle 100 j√§rjestikuse katse.

## Oleku diskretiseerimine

Q-√µppes peame looma Q-tabeli, mis m√§√§ratleb, mida teha igas olekus. Selleks peab olek olema **diskreetne**, t√§psemalt, see peaks sisaldama piiratud arvu diskreetseid v√§√§rtusi. Seega peame kuidagi **diskretiseerima** oma vaatlused, kaardistades need piiratud olekute hulgale.

Selleks on mitu v√µimalust:

- **Jagamine vahemikeks**. Kui teame teatud v√§√§rtuse intervalli, saame selle intervalli jagada mitmeks **vahemikuks** ja seej√§rel asendada v√§√§rtuse vahemiku numbriga, kuhu see kuulub. Seda saab teha numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) meetodiga. Sel juhul teame t√§pselt oleku suurust, kuna see s√µltub valitud vahemike arvust.

‚úÖ V√µime kasutada lineaarset interpolatsiooni, et tuua v√§√§rtused teatud piiratud intervalli (n√§iteks -20 kuni 20), ja seej√§rel teisendada numbrid t√§isarvudeks, neid √ºmardades. See annab meile oleku suuruse √ºle v√§hem kontrolli, eriti kui me ei tea sisendv√§√§rtuste t√§pseid vahemikke. N√§iteks meie puhul ei ole 2-st 4-st v√§√§rtusest √ºlemisi/alumisi piire, mis v√µib viia l√µpmatu arvu olekuteni.

Meie n√§ites kasutame teist l√§henemist. Nagu hiljem m√§rkate, hoolimata m√§√§ramata √ºlemistest/alumistest piiridest, v√µtavad need v√§√§rtused harva teatud piiratud intervallidest v√§ljapoole j√§√§vaid v√§√§rtusi, seega on √§√§rmuslike v√§√§rtustega olekud v√§ga haruldased.

1. Siin on funktsioon, mis v√µtab meie mudelist vaatluse ja toodab 4 t√§isarvu tuple'i: (koodiplokk 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Uurime ka teist diskretiseerimismeetodit, kasutades vahemikke: (koodiplokk 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. K√§ivitame n√º√ºd l√ºhikese simulatsiooni ja vaatleme neid diskreetseid keskkonna v√§√§rtusi. Proovige julgelt nii `discretize` kui ka `discretize_bins` ja vaadake, kas on erinevusi.

    ‚úÖ `discretize_bins` tagastab vahemiku numbri, mis on 0-p√µhine. Seega sisendmuutuja v√§√§rtuste puhul, mis on umbes 0, tagastab see intervalli keskelt numbri (10). `discretize` puhul ei hoolinud me v√§ljundv√§√§rtuste vahemikust, lubades neil olla negatiivsed, seega oleku v√§√§rtused ei ole nihutatud ja 0 vastab 0-le. (koodiplokk 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Kommenteerige lahti rida, mis algab `env.render`, kui soovite n√§ha, kuidas keskkond t√§itub. Vastasel juhul saate selle taustal t√§ita, mis on kiirem. Kasutame seda "n√§htamatut" t√§itmist Q-√µppe protsessi ajal.

## Q-tabeli struktuur

Eelmises tunnis oli olek lihtne paar numbreid vahemikus 0 kuni 8, mist√µttu oli mugav esitada Q-tabelit numpy tensorina kujuga 8x8x2. Kui kasutame vahemike diskretiseerimist, on meie olekuvektori suurus samuti teada, seega saame kasutada sama l√§henemist ja esitada oleku massiivina kujuga 20x20x10x10x2 (siin 2 on tegevusruumi dimensioon ja esimesed dimensioonid vastavad vahemike arvule, mida oleme valinud iga vaatlusruumi parameetri jaoks).

Kuid m√µnikord ei ole vaatlusruumi t√§psed dimensioonid teada. `discretize` funktsiooni puhul ei pruugi me kunagi olla kindlad, et meie olek j√§√§b teatud piiridesse, sest m√µned algsed v√§√§rtused ei ole piiratud. Seet√µttu kasutame veidi teistsugust l√§henemist ja esitame Q-tabeli s√µnastikuna.

1. Kasutage paari *(olek, tegevus)* s√µnastiku v√µtmena ja v√§√§rtus vastaks Q-tabeli kirje v√§√§rtusele. (koodiplokk 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Siin m√§√§ratleme ka funktsiooni `qvalues()`, mis tagastab loendi Q-tabeli v√§√§rtustest antud oleku jaoks, mis vastab k√µigile v√µimalikele tegevustele. Kui kirje ei ole Q-tabelis olemas, tagastame vaikimisi 0.

## Alustame Q-√µpet

N√º√ºd oleme valmis √µpetama Peetrit tasakaalu hoidma!

1. K√µigepealt m√§√§rame m√µned h√ºperparameetrid: (koodiplokk 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Siin on `alpha` **√µppem√§√§r**, mis m√§√§rab, mil m√§√§ral peaksime Q-tabeli praeguseid v√§√§rtusi igal sammul kohandama. Eelmises tunnis alustasime v√§√§rtusega 1 ja v√§hendasime seej√§rel `alpha` v√§√§rtusi treeningu ajal. Selles n√§ites hoiame selle lihtsuse huvides konstantsena ja saate hiljem katsetada `alpha` v√§√§rtuste kohandamist.

    `gamma` on **diskontom√§√§r**, mis n√§itab, mil m√§√§ral peaksime eelistama tulevast tasu praeguse tasu ees.

    `epsilon` on **uurimise/kasutamise tegur**, mis m√§√§rab, kas peaksime eelistama uurimist v√µi kasutamist. Meie algoritmis valime `epsilon` protsendil juhtudest j√§rgmise tegevuse vastavalt Q-tabeli v√§√§rtustele ja √ºlej√§√§nud juhtudel teeme juhusliku tegevuse. See v√µimaldab meil uurida otsinguruumi piirkondi, mida me pole kunagi varem n√§inud.

    ‚úÖ Tasakaalu osas ‚Äì juhusliku tegevuse valimine (uurimine) toimiks juhusliku l√∂√∂gina vales suunas ja post peaks √µppima, kuidas nendest "vigadest" tasakaalu taastada.

### Paranda algoritmi

Saame oma algoritmi eelmise tunni p√µhjal kahel viisil t√§iustada:

- **Arvuta keskmine kumulatiivne tasu** mitme simulatsiooni jooksul. Tr√ºkime progressi iga 5000 iteratsiooni j√§rel ja keskmistame kumulatiivse tasu selle aja jooksul. See t√§hendab, et kui saame rohkem kui 195 punkti, v√µime probleemi lahendatuks pidada, isegi k√µrgema kvaliteediga kui n√µutud.

- **Arvuta maksimaalne keskmine kumulatiivne tulemus**, `Qmax`, ja salvestame Q-tabeli, mis vastab sellele tulemusele. Kui treeningu ajal m√§rkate, et keskmine kumulatiivne tulemus hakkab langema, tahame s√§ilitada Q-tabeli v√§√§rtused, mis vastavad treeningu parimale mudelile.

1. Koguge k√µik kumulatiivsed tasud igal simulatsioonil `rewards` vektorisse edasiseks joonistamiseks. (koodiplokk 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Mida v√µite nendest tulemustest m√§rgata:

- **Eesm√§rgile l√§hedal**. Oleme v√§ga l√§hedal eesm√§rgi saavutamisele, saades 195 kumulatiivset tasu √ºle 100+ j√§rjestikuse simulatsiooni v√µi oleme selle tegelikult saavutanud! Isegi kui saame v√§iksemaid numbreid, ei tea me seda, sest keskmistame 5000 jooksu jooksul ja ametlikus kriteeriumis on n√µutud ainult 100 jooksu.

- **Tasu hakkab langema**. M√µnikord hakkab tasu langema, mis t√§hendab, et v√µime "h√§vitada" juba √µpitud v√§√§rtused Q-tabelis v√§√§rtustega, mis olukorda halvendavad.

See t√§helepanek on selgem, kui joonistame treeningu progressi.

## Treeningu progressi joonistamine

Treeningu ajal kogusime kumulatiivse tasu v√§√§rtuse igal iteratsioonil `rewards` vektorisse. Siin on, kuidas see v√§lja n√§eb, kui joonistame selle iteratsiooni numbri vastu:

```python
plt.plot(rewards)
```

![toores progress](../../../../translated_images/train_progress_raw.2adfdf2daea09c59.et.png)

Sellest graafikust ei ole v√µimalik midagi j√§reldada, sest stohhastilise treeningprotsessi olemuse t√µttu varieerub treeningseansside pikkus suuresti. Selle graafiku m√µistlikumaks muutmiseks saame arvutada **jooksva keskmise** mitme katse jooksul, n√§iteks 100. Seda saab mugavalt teha `np.convolve` abil: (koodiplokk 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![treeningu progress](../../../../translated_images/train_progress_runav.c71694a8fa9ab359.et.png)

## H√ºperparameetrite muutmine

Et √µppimine oleks stabiilsem, on m√µistlik treeningu ajal m√µningaid h√ºperparameetreid kohandada. Eelk√µige:

- **√ïppem√§√§ra**, `alpha`, puhul v√µime alustada v√§√§rtustega, mis on l√§hedased 1-le, ja seej√§rel seda parameetrit j√§rk-j√§rgult v√§hendada. Aja jooksul saame Q-tabelis h√§id t√µen√§osusv√§√§rtusi ja seega peaksime neid veidi kohandama, mitte t√§ielikult uute v√§√§rtustega √ºle kirjutama.

- **Suurenda epsilonit**. V√µime soovida `epsilon` v√§√§rtust aeglaselt suurendada, et uurida v√§hem ja kasutada rohkem. T√µen√§oliselt on m√µistlik alustada madalama `epsilon` v√§√§rtusega ja liikuda peaaegu 1-ni.

> **√úlesanne 1**: M√§ngige h√ºperparameetrite v√§√§rtustega ja vaadake, kas suudate saavutada k√µrgema kumulatiivse tasu. Kas j√µuate √ºle 195?
> **√úlesanne 2**: Probleemi ametlikuks lahendamiseks peate saavutama 195 keskmise tasu 100 j√§rjestikuse jooksu jooksul. M√µ√µtke seda treeningu ajal ja veenduge, et olete probleemi ametlikult lahendanud!

## Tulemust tegevuses n√§ha

Oleks huvitav n√§ha, kuidas treenitud mudel tegelikult k√§itub. K√§ivitame simulatsiooni ja j√§rgime sama tegevuse valimise strateegiat nagu treeningu ajal, valides tegevusi vastavalt Q-tabeli t√µen√§osusjaotusele: (koodiplokk 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Peaksite n√§gema midagi sellist:

![tasakaalu hoidev cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄV√§ljakutse

> **√úlesanne 3**: Siin kasutasime Q-tabeli l√µplikku koopiat, mis ei pruugi olla parim. Pidage meeles, et oleme salvestanud parima tulemusega Q-tabeli `Qbest` muutujasse! Proovige sama n√§idet parima tulemusega Q-tabeliga, kopeerides `Qbest` √ºle `Q`-sse ja vaadake, kas m√§rkate erinevust.

> **√úlesanne 4**: Siin me ei valinud igal sammul parimat tegevust, vaid pigem valisime tegevusi vastavalt t√µen√§osusjaotusele. Kas oleks m√µistlikum alati valida parim tegevus, millel on Q-tabelis k√µrgeim v√§√§rtus? Seda saab teha, kasutades `np.argmax` funktsiooni, et leida tegevuse number, mis vastab k√µrgeimale Q-tabeli v√§√§rtusele. Rakendage see strateegia ja vaadake, kas see parandab tasakaalu hoidmist.

## [Loengu j√§rgne viktoriin](https://ff-quizzes.netlify.app/en/ml/)

## √úlesanne
[Treeni Mountain Car](assignment.md)

## Kokkuv√µte

Oleme n√º√ºd √µppinud, kuidas treenida agente saavutama h√§id tulemusi, pakkudes neile ainult tasufunktsiooni, mis m√§√§ratleb m√§ngu soovitud oleku, ja andes neile v√µimaluse intelligentselt otsinguruumi uurida. Oleme edukalt rakendanud Q-√µppe algoritmi nii diskreetsete kui ka pidevate keskkondade puhul, kuid diskreetsete tegevustega.

Oluline on uurida ka olukordi, kus tegevusruum on samuti pidev ja kui vaatlusruum on palju keerulisem, n√§iteks pilt Atari m√§ngu ekraanilt. Selliste probleemide lahendamiseks on sageli vaja kasutada v√µimsamaid masin√µppe tehnikaid, nagu n√§iteks tehisn√§rviv√µrgud, et saavutada h√§id tulemusi. Need keerukamad teemad on meie tulevase edasij√µudnute AI kursuse fookuses.

---

**Lahti√ºtlus**:  
See dokument on t√µlgitud AI t√µlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi p√º√ºame tagada t√§psust, palume arvestada, et automaatsed t√µlked v√µivad sisaldada vigu v√µi ebat√§psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimt√µlget. Me ei vastuta selle t√µlke kasutamisest tulenevate arusaamatuste v√µi valesti t√µlgenduste eest.