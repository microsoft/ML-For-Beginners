<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T12:18:40+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "hr"
}
-->
# K-Means klasteriranje

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

U ovoj lekciji nauƒçit ƒáete kako kreirati klastere koristeƒái Scikit-learn i nigerijski glazbeni dataset koji ste ranije uvezli. Pokrit ƒáemo osnove K-Means metode za klasteriranje. Imajte na umu da, kao ≈°to ste nauƒçili u prethodnoj lekciji, postoji mnogo naƒçina za rad s klasterima, a metoda koju koristite ovisi o va≈°im podacima. Isprobat ƒáemo K-Means jer je to najƒçe≈°ƒáe kori≈°tena tehnika klasteriranja. Krenimo!

Pojmovi koje ƒáete nauƒçiti:

- Silhouette ocjenjivanje
- Metoda lakta
- Inercija
- Varijanca

## Uvod

[K-Means klasteriranje](https://wikipedia.org/wiki/K-means_clustering) je metoda koja potjeƒçe iz podruƒçja obrade signala. Koristi se za podjelu i grupiranje podataka u 'k' klastera pomoƒáu niza opa≈æanja. Svako opa≈æanje radi na grupiranju odreƒëenog podatka najbli≈æem njegovom 'prosjeku', odnosno sredi≈°njoj toƒçki klastera.

Klasteri se mogu vizualizirati kao [Voronoi dijagrami](https://wikipedia.org/wiki/Voronoi_diagram), koji ukljuƒçuju toƒçku (ili 'sjeme') i njezinu odgovarajuƒáu regiju.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografika od [Jen Looper](https://twitter.com/jenlooper)

Proces K-Means klasteriranja [izvodi se u tri koraka](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritam odabire k-broj sredi≈°njih toƒçaka uzorkovanjem iz skupa podataka. Nakon toga, ponavlja:
    1. Dodjeljuje svaki uzorak najbli≈æem centroidu.
    2. Stvara nove centre uzimajuƒái prosjeƒçnu vrijednost svih uzoraka dodijeljenih prethodnim centrima.
    3. Zatim izraƒçunava razliku izmeƒëu novih i starih centara i ponavlja dok se centri ne stabiliziraju.

Jedan nedostatak kori≈°tenja K-Means metode je ƒçinjenica da morate odrediti 'k', odnosno broj centara. Sreƒáom, 'metoda lakta' poma≈æe procijeniti dobru poƒçetnu vrijednost za 'k'. Uskoro ƒáete je isprobati.

## Preduvjeti

Radit ƒáete u datoteci [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) koja ukljuƒçuje uvoz podataka i preliminarno ƒçi≈°ƒáenje koje ste obavili u prethodnoj lekciji.

## Vje≈æba - priprema

Zapoƒçnite tako da ponovno pogledate podatke o pjesmama.

1. Kreirajte boxplot pozivajuƒái `boxplot()` za svaki stupac:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Ovi podaci su malo buƒçni: promatrajuƒái svaki stupac kao boxplot, mo≈æete vidjeti outliere.

    ![outliers](../../../../5-Clustering/2-K-Means/images/boxplots.png)

Mo≈æete proƒái kroz skup podataka i ukloniti ove outliere, ali to bi uƒçinilo podatke priliƒçno minimalnima.

1. Za sada odaberite koje stupce ƒáete koristiti za vje≈æbu klasteriranja. Odaberite one sa sliƒçnim rasponima i kodirajte stupac `artist_top_genre` kao numeriƒçke podatke:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Sada trebate odabrati koliko klastera ciljati. Znate da postoje 3 glazbena ≈æanra koja smo izdvojili iz skupa podataka, pa poku≈°ajmo s 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Vidite ispisan niz s predviƒëenim klasterima (0, 1 ili 2) za svaki redak dataframea.

1. Koristite ovaj niz za izraƒçunavanje 'silhouette ocjene':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhouette ocjena

Tra≈æite silhouette ocjenu bli≈æu 1. Ova ocjena varira od -1 do 1, a ako je ocjena 1, klaster je gust i dobro odvojen od drugih klastera. Vrijednost blizu 0 predstavlja preklapajuƒáe klastere s uzorcima vrlo blizu granice odluke susjednih klastera. [(Izvor)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Na≈°a ocjena je **.53**, dakle toƒçno u sredini. To ukazuje da na≈°i podaci nisu osobito prikladni za ovu vrstu klasteriranja, ali nastavimo.

### Vje≈æba - izgradnja modela

1. Uvezite `KMeans` i zapoƒçnite proces klasteriranja.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Postoji nekoliko dijelova koji zaslu≈æuju obja≈°njenje.

    > üéì range: Ovo su iteracije procesa klasteriranja

    > üéì random_state: "Odreƒëuje generiranje sluƒçajnih brojeva za inicijalizaciju centara." [Izvor](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "sume kvadrata unutar klastera" mjere prosjeƒçnu kvadratnu udaljenost svih toƒçaka unutar klastera do centra klastera. [Izvor](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce). 

    > üéì Inercija: K-Means algoritmi poku≈°avaju odabrati centre kako bi minimizirali 'inerciju', "mjeru koliko su klasteri interno koherentni." [Izvor](https://scikit-learn.org/stable/modules/clustering.html). Vrijednost se dodaje varijabli wcss pri svakoj iteraciji.

    > üéì k-means++: U [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) mo≈æete koristiti 'k-means++' optimizaciju, koja "inicijalizira centre tako da budu (opƒáenito) udaljeni jedni od drugih, ≈°to dovodi do vjerojatno boljih rezultata od sluƒçajne inicijalizacije.

### Metoda lakta

Ranije ste pretpostavili da, buduƒái da ste ciljali 3 glazbena ≈æanra, trebate odabrati 3 klastera. Ali je li to sluƒçaj?

1. Koristite 'metodu lakta' da budete sigurni.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Koristite varijablu `wcss` koju ste izgradili u prethodnom koraku za kreiranje grafikona koji pokazuje gdje je 'savijanje' u laktu, ≈°to ukazuje na optimalan broj klastera. Mo≈æda je to **zaista** 3!

    ![elbow method](../../../../5-Clustering/2-K-Means/images/elbow.png)

## Vje≈æba - prikaz klastera

1. Poku≈°ajte ponovno proces, ovaj put postavljajuƒái tri klastera, i prika≈æite klastere kao scatterplot:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Provjerite toƒçnost modela:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    Toƒçnost ovog modela nije ba≈° dobra, a oblik klastera daje vam nagovje≈°taj za≈°to.

    ![clusters](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Ovi podaci su previ≈°e neuravnote≈æeni, premalo korelirani i postoji prevelika varijanca izmeƒëu vrijednosti stupaca da bi se dobro klasterirali. Zapravo, klasteri koji se formiraju vjerojatno su jako pod utjecajem ili iskrivljeni zbog tri kategorije ≈æanrova koje smo gore definirali. To je bio proces uƒçenja!

    U dokumentaciji Scikit-learn mo≈æete vidjeti da model poput ovog, s klasterima koji nisu dobro razgraniƒçeni, ima problem 'varijance':

    ![problem models](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografika iz Scikit-learn

## Varijanca

Varijanca je definirana kao "prosjek kvadrata razlika od srednje vrijednosti" [(Izvor)](https://www.mathsisfun.com/data/standard-deviation.html). U kontekstu ovog problema klasteriranja, odnosi se na podatke kod kojih brojevi na≈°eg skupa podataka imaju tendenciju previ≈°e odstupati od srednje vrijednosti.

‚úÖ Ovo je odliƒçan trenutak da razmislite o svim naƒçinima na koje biste mogli ispraviti ovaj problem. Malo vi≈°e prilagoditi podatke? Koristiti razliƒçite stupce? Koristiti drugaƒçiji algoritam? Savjet: Poku≈°ajte [skalirati svoje podatke](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) kako biste ih normalizirali i testirali druge stupce.

> Poku≈°ajte ovaj '[kalkulator varijance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' kako biste bolje razumjeli koncept.

---

## üöÄIzazov

Provedite neko vrijeme s ovim notebookom, prilagoƒëavajuƒái parametre. Mo≈æete li pobolj≈°ati toƒçnost modela ƒçi≈°ƒáenjem podataka (na primjer, uklanjanjem outliera)? Mo≈æete koristiti te≈æine kako biste dali veƒáu te≈æinu odreƒëenim uzorcima podataka. ≈†to jo≈° mo≈æete uƒçiniti kako biste stvorili bolje klastere?

Savjet: Poku≈°ajte skalirati svoje podatke. U notebooku postoji komentirani kod koji dodaje standardno skaliranje kako bi stupci podataka vi≈°e nalikovali jedni drugima u smislu raspona. Primijetit ƒáete da, iako silhouette ocjena opada, 'savijanje' u grafu lakta postaje glaƒëe. To je zato ≈°to ostavljanje podataka neskaliranima omoguƒáuje podacima s manje varijance da imaju veƒáu te≈æinu. Proƒçitajte malo vi≈°e o ovom problemu [ovdje](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Pregled i samostalno uƒçenje

Pogledajte simulator K-Means [poput ovog](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Mo≈æete koristiti ovaj alat za vizualizaciju uzoraka podataka i odreƒëivanje njihovih centara. Mo≈æete ureƒëivati sluƒçajnost podataka, broj klastera i broj centara. Poma≈æe li vam ovo da steknete ideju o tome kako se podaci mogu grupirati?

Takoƒëer, pogledajte [ovaj materijal o K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) sa Stanforda.

## Zadatak

[Isprobajte razliƒçite metode klasteriranja](assignment.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden kori≈°tenjem AI usluge za prevoƒëenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toƒçnost, imajte na umu da automatski prijevodi mogu sadr≈æavati pogre≈°ke ili netoƒçnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuƒçne informacije preporuƒçuje se profesionalni prijevod od strane struƒçnjaka. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogre≈°na tumaƒçenja koja mogu proizaƒái iz kori≈°tenja ovog prijevoda.