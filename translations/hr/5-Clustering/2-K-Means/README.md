<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T12:18:40+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "hr"
}
-->
# K-Means klasteriranje

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

U ovoj lekciji nauÄit Ä‡ete kako kreirati klastere koristeÄ‡i Scikit-learn i nigerijski glazbeni dataset koji ste ranije uvezli. Pokrit Ä‡emo osnove K-Means metode za klasteriranje. Imajte na umu da, kao Å¡to ste nauÄili u prethodnoj lekciji, postoji mnogo naÄina za rad s klasterima, a metoda koju koristite ovisi o vaÅ¡im podacima. Isprobat Ä‡emo K-Means jer je to najÄeÅ¡Ä‡e koriÅ¡tena tehnika klasteriranja. Krenimo!

Pojmovi koje Ä‡ete nauÄiti:

- Silhouette ocjenjivanje
- Metoda lakta
- Inercija
- Varijanca

## Uvod

[K-Means klasteriranje](https://wikipedia.org/wiki/K-means_clustering) je metoda koja potjeÄe iz podruÄja obrade signala. Koristi se za podjelu i grupiranje podataka u 'k' klastera pomoÄ‡u niza opaÅ¾anja. Svako opaÅ¾anje radi na grupiranju odreÄ‘enog podatka najbliÅ¾em njegovom 'prosjeku', odnosno srediÅ¡njoj toÄki klastera.

Klasteri se mogu vizualizirati kao [Voronoi dijagrami](https://wikipedia.org/wiki/Voronoi_diagram), koji ukljuÄuju toÄku (ili 'sjeme') i njezinu odgovarajuÄ‡u regiju.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografika od [Jen Looper](https://twitter.com/jenlooper)

Proces K-Means klasteriranja [izvodi se u tri koraka](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritam odabire k-broj srediÅ¡njih toÄaka uzorkovanjem iz skupa podataka. Nakon toga, ponavlja:
    1. Dodjeljuje svaki uzorak najbliÅ¾em centroidu.
    2. Stvara nove centre uzimajuÄ‡i prosjeÄnu vrijednost svih uzoraka dodijeljenih prethodnim centrima.
    3. Zatim izraÄunava razliku izmeÄ‘u novih i starih centara i ponavlja dok se centri ne stabiliziraju.

Jedan nedostatak koriÅ¡tenja K-Means metode je Äinjenica da morate odrediti 'k', odnosno broj centara. SreÄ‡om, 'metoda lakta' pomaÅ¾e procijeniti dobru poÄetnu vrijednost za 'k'. Uskoro Ä‡ete je isprobati.

## Preduvjeti

Radit Ä‡ete u datoteci [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) koja ukljuÄuje uvoz podataka i preliminarno ÄiÅ¡Ä‡enje koje ste obavili u prethodnoj lekciji.

## VjeÅ¾ba - priprema

ZapoÄnite tako da ponovno pogledate podatke o pjesmama.

1. Kreirajte boxplot pozivajuÄ‡i `boxplot()` za svaki stupac:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Ovi podaci su malo buÄni: promatrajuÄ‡i svaki stupac kao boxplot, moÅ¾ete vidjeti outliere.

    ![outliers](../../../../5-Clustering/2-K-Means/images/boxplots.png)

MoÅ¾ete proÄ‡i kroz skup podataka i ukloniti ove outliere, ali to bi uÄinilo podatke priliÄno minimalnima.

1. Za sada odaberite koje stupce Ä‡ete koristiti za vjeÅ¾bu klasteriranja. Odaberite one sa sliÄnim rasponima i kodirajte stupac `artist_top_genre` kao numeriÄke podatke:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Sada trebate odabrati koliko klastera ciljati. Znate da postoje 3 glazbena Å¾anra koja smo izdvojili iz skupa podataka, pa pokuÅ¡ajmo s 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Vidite ispisan niz s predviÄ‘enim klasterima (0, 1 ili 2) za svaki redak dataframea.

1. Koristite ovaj niz za izraÄunavanje 'silhouette ocjene':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhouette ocjena

TraÅ¾ite silhouette ocjenu bliÅ¾u 1. Ova ocjena varira od -1 do 1, a ako je ocjena 1, klaster je gust i dobro odvojen od drugih klastera. Vrijednost blizu 0 predstavlja preklapajuÄ‡e klastere s uzorcima vrlo blizu granice odluke susjednih klastera. [(Izvor)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

NaÅ¡a ocjena je **.53**, dakle toÄno u sredini. To ukazuje da naÅ¡i podaci nisu osobito prikladni za ovu vrstu klasteriranja, ali nastavimo.

### VjeÅ¾ba - izgradnja modela

1. Uvezite `KMeans` i zapoÄnite proces klasteriranja.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Postoji nekoliko dijelova koji zasluÅ¾uju objaÅ¡njenje.

    > ğŸ“ range: Ovo su iteracije procesa klasteriranja

    > ğŸ“ random_state: "OdreÄ‘uje generiranje sluÄajnih brojeva za inicijalizaciju centara." [Izvor](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > ğŸ“ WCSS: "sume kvadrata unutar klastera" mjere prosjeÄnu kvadratnu udaljenost svih toÄaka unutar klastera do centra klastera. [Izvor](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce). 

    > ğŸ“ Inercija: K-Means algoritmi pokuÅ¡avaju odabrati centre kako bi minimizirali 'inerciju', "mjeru koliko su klasteri interno koherentni." [Izvor](https://scikit-learn.org/stable/modules/clustering.html). Vrijednost se dodaje varijabli wcss pri svakoj iteraciji.

    > ğŸ“ k-means++: U [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) moÅ¾ete koristiti 'k-means++' optimizaciju, koja "inicijalizira centre tako da budu (opÄ‡enito) udaljeni jedni od drugih, Å¡to dovodi do vjerojatno boljih rezultata od sluÄajne inicijalizacije.

### Metoda lakta

Ranije ste pretpostavili da, buduÄ‡i da ste ciljali 3 glazbena Å¾anra, trebate odabrati 3 klastera. Ali je li to sluÄaj?

1. Koristite 'metodu lakta' da budete sigurni.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Koristite varijablu `wcss` koju ste izgradili u prethodnom koraku za kreiranje grafikona koji pokazuje gdje je 'savijanje' u laktu, Å¡to ukazuje na optimalan broj klastera. MoÅ¾da je to **zaista** 3!

    ![elbow method](../../../../5-Clustering/2-K-Means/images/elbow.png)

## VjeÅ¾ba - prikaz klastera

1. PokuÅ¡ajte ponovno proces, ovaj put postavljajuÄ‡i tri klastera, i prikaÅ¾ite klastere kao scatterplot:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Provjerite toÄnost modela:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    ToÄnost ovog modela nije baÅ¡ dobra, a oblik klastera daje vam nagovjeÅ¡taj zaÅ¡to.

    ![clusters](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Ovi podaci su previÅ¡e neuravnoteÅ¾eni, premalo korelirani i postoji prevelika varijanca izmeÄ‘u vrijednosti stupaca da bi se dobro klasterirali. Zapravo, klasteri koji se formiraju vjerojatno su jako pod utjecajem ili iskrivljeni zbog tri kategorije Å¾anrova koje smo gore definirali. To je bio proces uÄenja!

    U dokumentaciji Scikit-learn moÅ¾ete vidjeti da model poput ovog, s klasterima koji nisu dobro razgraniÄeni, ima problem 'varijance':

    ![problem models](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografika iz Scikit-learn

## Varijanca

Varijanca je definirana kao "prosjek kvadrata razlika od srednje vrijednosti" [(Izvor)](https://www.mathsisfun.com/data/standard-deviation.html). U kontekstu ovog problema klasteriranja, odnosi se na podatke kod kojih brojevi naÅ¡eg skupa podataka imaju tendenciju previÅ¡e odstupati od srednje vrijednosti.

âœ… Ovo je odliÄan trenutak da razmislite o svim naÄinima na koje biste mogli ispraviti ovaj problem. Malo viÅ¡e prilagoditi podatke? Koristiti razliÄite stupce? Koristiti drugaÄiji algoritam? Savjet: PokuÅ¡ajte [skalirati svoje podatke](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) kako biste ih normalizirali i testirali druge stupce.

> PokuÅ¡ajte ovaj '[kalkulator varijance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' kako biste bolje razumjeli koncept.

---

## ğŸš€Izazov

Provedite neko vrijeme s ovim notebookom, prilagoÄ‘avajuÄ‡i parametre. MoÅ¾ete li poboljÅ¡ati toÄnost modela ÄiÅ¡Ä‡enjem podataka (na primjer, uklanjanjem outliera)? MoÅ¾ete koristiti teÅ¾ine kako biste dali veÄ‡u teÅ¾inu odreÄ‘enim uzorcima podataka. Å to joÅ¡ moÅ¾ete uÄiniti kako biste stvorili bolje klastere?

Savjet: PokuÅ¡ajte skalirati svoje podatke. U notebooku postoji komentirani kod koji dodaje standardno skaliranje kako bi stupci podataka viÅ¡e nalikovali jedni drugima u smislu raspona. Primijetit Ä‡ete da, iako silhouette ocjena opada, 'savijanje' u grafu lakta postaje glaÄ‘e. To je zato Å¡to ostavljanje podataka neskaliranima omoguÄ‡uje podacima s manje varijance da imaju veÄ‡u teÅ¾inu. ProÄitajte malo viÅ¡e o ovom problemu [ovdje](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Pregled i samostalno uÄenje

Pogledajte simulator K-Means [poput ovog](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). MoÅ¾ete koristiti ovaj alat za vizualizaciju uzoraka podataka i odreÄ‘ivanje njihovih centara. MoÅ¾ete ureÄ‘ivati sluÄajnost podataka, broj klastera i broj centara. PomaÅ¾e li vam ovo da steknete ideju o tome kako se podaci mogu grupirati?

TakoÄ‘er, pogledajte [ovaj materijal o K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) sa Stanforda.

## Zadatak

[Isprobajte razliÄite metode klasteriranja](assignment.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.