<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T11:34:39+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "hr"
}
-->
# LogistiÄka regresija za predviÄ‘anje kategorija

![Infografika: LogistiÄka vs. linearna regresija](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ova lekcija je dostupna i na R jeziku!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Uvod

U ovoj zavrÅ¡noj lekciji o regresiji, jednoj od osnovnih _klasiÄnih_ tehnika strojnog uÄenja, prouÄit Ä‡emo logistiÄku regresiju. Ovu tehniku koristite za otkrivanje obrazaca kako biste predvidjeli binarne kategorije. Je li ovaj slatkiÅ¡ Äokolada ili nije? Je li ova bolest zarazna ili nije? HoÄ‡e li ovaj kupac odabrati ovaj proizvod ili neÄ‡e?

U ovoj lekciji nauÄit Ä‡ete:

- Novi alat za vizualizaciju podataka
- Tehnike logistiÄke regresije

âœ… Produbite svoje razumijevanje rada s ovom vrstom regresije u ovom [modulu za uÄenje](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Preduvjeti

Radili smo s podacima o bundevama i sada smo dovoljno upoznati s njima da shvatimo kako postoji jedna binarna kategorija s kojom moÅ¾emo raditi: `Boja`.

Izgradimo model logistiÄke regresije kako bismo predvidjeli, na temelju nekih varijabli, _koje je boje odreÄ‘ena bundeva_ (naranÄasta ğŸƒ ili bijela ğŸ‘»).

> ZaÅ¡to govorimo o binarnoj klasifikaciji u lekciji o regresiji? Samo radi jeziÄne praktiÄnosti, jer je logistiÄka regresija [zapravo metoda klasifikacije](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), iako se temelji na linearnim modelima. Saznajte viÅ¡e o drugim naÄinima klasifikacije podataka u sljedeÄ‡oj grupi lekcija.

## Definirajte pitanje

Za naÅ¡e potrebe, izrazit Ä‡emo ovo kao binarnu kategoriju: 'Bijela' ili 'Nije bijela'. U naÅ¡em skupu podataka postoji i kategorija 'prugasta', ali ima malo primjera te kategorije, pa je neÄ‡emo koristiti. Ionako nestaje kada uklonimo null vrijednosti iz skupa podataka.

> ğŸƒ Zanimljivost: bijele bundeve ponekad nazivamo 'duh' bundevama. Nisu baÅ¡ jednostavne za rezbarenje, pa nisu toliko popularne kao naranÄaste, ali izgledaju zanimljivo! Tako bismo svoje pitanje mogli preformulirati i kao: 'Duh' ili 'Nije duh'. ğŸ‘»

## O logistiÄkoj regresiji

LogistiÄka regresija razlikuje se od linearne regresije, o kojoj ste ranije uÄili, u nekoliko vaÅ¾nih aspekata.

[![ML za poÄetnike - Razumijevanje logistiÄke regresije za klasifikaciju](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML za poÄetnike - Razumijevanje logistiÄke regresije za klasifikaciju")

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled logistiÄke regresije.

### Binarna klasifikacija

LogistiÄka regresija ne nudi iste moguÄ‡nosti kao linearna regresija. Prva nudi predviÄ‘anje binarne kategorije ("bijela ili nije bijela"), dok druga moÅ¾e predvidjeti kontinuirane vrijednosti, na primjer, s obzirom na podrijetlo bundeve i vrijeme berbe, _koliko Ä‡e joj cijena porasti_.

![Model klasifikacije bundeva](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika: [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Druge vrste klasifikacije

Postoje i druge vrste logistiÄke regresije, ukljuÄujuÄ‡i multinomijalnu i ordinalnu:

- **Multinomijalna**, koja ukljuÄuje viÅ¡e od jedne kategorije - "NaranÄasta, Bijela i Prugasta".
- **Ordinalna**, koja ukljuÄuje ureÄ‘ene kategorije, korisno ako Å¾elimo logiÄki poredati ishode, poput bundeva koje su poredane prema veliÄini (mini, sm, med, lg, xl, xxl).

![Multinomijalna vs ordinalna regresija](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Varijable NE moraju biti povezane

SjeÄ‡ate se kako je linearna regresija bolje funkcionirala s viÅ¡e povezanih varijabli? LogistiÄka regresija je suprotna - varijable ne moraju biti povezane. To odgovara ovim podacima koji imaju priliÄno slabe korelacije.

### Potrebno je puno Äistih podataka

LogistiÄka regresija daje toÄnije rezultate ako koristite viÅ¡e podataka; naÅ¡ mali skup podataka nije optimalan za ovaj zadatak, pa to imajte na umu.

[![ML za poÄetnike - Analiza i priprema podataka za logistiÄku regresiju](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML za poÄetnike - Analiza i priprema podataka za logistiÄku regresiju")

âœ… Razmislite o vrstama podataka koje bi bile prikladne za logistiÄku regresiju.

## VjeÅ¾ba - priprema podataka

Prvo, oÄistite podatke, uklonite null vrijednosti i odaberite samo neke stupce:

1. Dodajte sljedeÄ‡i kod:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Uvijek moÅ¾ete zaviriti u svoj novi dataframe:

    ```python
    pumpkins.info
    ```

### Vizualizacija - kategorijalni graf

Do sada ste ponovno uÄitali [poÄetnu biljeÅ¾nicu](../../../../2-Regression/4-Logistic/notebook.ipynb) s podacima o bundevama i oÄistili je kako biste saÄuvali skup podataka koji sadrÅ¾i nekoliko varijabli, ukljuÄujuÄ‡i `Boju`. Vizualizirajmo dataframe u biljeÅ¾nici koristeÄ‡i drugu biblioteku: [Seaborn](https://seaborn.pydata.org/index.html), koja je izgraÄ‘ena na Matplotlibu koji smo ranije koristili.

Seaborn nudi zanimljive naÄine za vizualizaciju podataka. Na primjer, moÅ¾ete usporediti distribucije podataka za svaku `Varijantu` i `Boju` u kategorijalnom grafu.

1. Stvorite takav graf koristeÄ‡i funkciju `catplot`, koristeÄ‡i naÅ¡e podatke o bundevama `pumpkins` i specificirajuÄ‡i mapiranje boja za svaku kategoriju bundeva (naranÄasta ili bijela):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![MreÅ¾a vizualiziranih podataka](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    PromatrajuÄ‡i podatke, moÅ¾ete vidjeti kako se podaci o boji odnose na varijantu.

    âœ… Na temelju ovog kategorijalnog grafa, koje zanimljive analize moÅ¾ete zamisliti?

### Predobrada podataka: kodiranje znaÄajki i oznaka

NaÅ¡ skup podataka o bundevama sadrÅ¾i tekstualne vrijednosti za sve stupce. Rad s kategorijalnim podacima intuitivan je za ljude, ali ne i za strojeve. Algoritmi strojnog uÄenja bolje rade s brojevima. Zato je kodiranje vrlo vaÅ¾an korak u fazi predobrade podataka jer nam omoguÄ‡uje pretvaranje kategorijalnih podataka u numeriÄke, bez gubitka informacija. Dobro kodiranje vodi do izgradnje dobrog modela.

Za kodiranje znaÄajki postoje dvije glavne vrste kodera:

1. Ordinalni koder: dobro odgovara za ordinalne varijable, koje su kategorijalne varijable Äiji podaci slijede logiÄki redoslijed, poput stupca `VeliÄina predmeta` u naÅ¡em skupu podataka. Stvara mapiranje tako da je svaka kategorija predstavljena brojem, koji odgovara redoslijedu kategorije u stupcu.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorijalni koder: dobro odgovara za nominalne varijable, koje su kategorijalne varijable Äiji podaci ne slijede logiÄki redoslijed, poput svih znaÄajki osim `VeliÄine predmeta` u naÅ¡em skupu podataka. To je kodiranje s jednom vruÄ‡om vrijednoÅ¡Ä‡u, Å¡to znaÄi da je svaka kategorija predstavljena binarnim stupcem: kodirana varijabla jednaka je 1 ako bundeva pripada toj varijanti, a 0 inaÄe.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```

Zatim se `ColumnTransformer` koristi za kombiniranje viÅ¡e kodera u jedan korak i njihovu primjenu na odgovarajuÄ‡e stupce.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```

S druge strane, za kodiranje oznaka koristimo klasu `LabelEncoder` iz scikit-learn biblioteke, koja je pomoÄ‡na klasa za normalizaciju oznaka tako da sadrÅ¾e samo vrijednosti izmeÄ‘u 0 i n_klasa-1 (ovdje, 0 i 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

Nakon Å¡to smo kodirali znaÄajke i oznake, moÅ¾emo ih spojiti u novi dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

âœ… Koje su prednosti koriÅ¡tenja ordinalnog kodera za stupac `VeliÄina predmeta`?

### Analiza odnosa izmeÄ‘u varijabli

Sada kada smo obradili podatke, moÅ¾emo analizirati odnose izmeÄ‘u znaÄajki i oznaka kako bismo stekli ideju o tome koliko Ä‡e model biti uspjeÅ¡an u predviÄ‘anju oznaka na temelju znaÄajki. Najbolji naÄin za izvoÄ‘enje ove vrste analize je grafiÄko prikazivanje podataka. Ponovno Ä‡emo koristiti funkciju `catplot` iz Seaborna za vizualizaciju odnosa izmeÄ‘u `VeliÄine predmeta`, `Varijante` i `Boje` u kategorijalnom grafu. Za bolje prikazivanje podataka koristit Ä‡emo kodirani stupac `VeliÄina predmeta` i nekodirani stupac `Varijanta`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

![Kategorijalni graf vizualiziranih podataka](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### KoriÅ¡tenje swarm grafa

BuduÄ‡i da je `Boja` binarna kategorija (Bijela ili Nije), potrebna je '[posebna metoda](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) za vizualizaciju'. Postoje i drugi naÄini za vizualizaciju odnosa ove kategorije s drugim varijablama.

MoÅ¾ete vizualizirati varijable usporedno pomoÄ‡u Seaborn grafova.

1. Isprobajte 'swarm' graf za prikaz distribucije vrijednosti:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Swarm graf vizualiziranih podataka](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Napomena**: kod iznad moÅ¾e generirati upozorenje jer Seaborn ne moÅ¾e prikazati toliku koliÄinu podataka u swarm grafu. MoguÄ‡e rjeÅ¡enje je smanjenje veliÄine markera pomoÄ‡u parametra 'size'. MeÄ‘utim, imajte na umu da to moÅ¾e utjecati na Äitljivost grafa.

> **ğŸ§® Matematika iza toga**
>
> LogistiÄka regresija temelji se na konceptu 'maksimalne vjerodostojnosti' koristeÄ‡i [sigmoidne funkcije](https://wikipedia.org/wiki/Sigmoid_function). 'Sigmoidna funkcija' na grafu izgleda poput oblika slova 'S'. Uzima vrijednost i mapira je na raspon izmeÄ‘u 0 i 1. Njezina krivulja takoÄ‘er se naziva 'logistiÄka krivulja'. Formula izgleda ovako:
>
> ![logistiÄka funkcija](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> gdje se sredina sigmoidne funkcije nalazi na x-ovoj 0 toÄki, L je maksimalna vrijednost krivulje, a k je strmina krivulje. Ako je ishod funkcije veÄ‡i od 0.5, oznaka Ä‡e biti dodijeljena klasi '1' binarnog izbora. Ako nije, bit Ä‡e klasificirana kao '0'.

## Izgradite svoj model

Izgradnja modela za pronalaÅ¾enje binarne klasifikacije iznenaÄ‘ujuÄ‡e je jednostavna u Scikit-learn biblioteci.

[![ML za poÄetnike - LogistiÄka regresija za klasifikaciju podataka](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML za poÄetnike - LogistiÄka regresija za klasifikaciju podataka")

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled izgradnje modela logistiÄke regresije.

1. Odaberite varijable koje Å¾elite koristiti u svom modelu klasifikacije i podijelite skup podataka na trening i testni pozivom `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Sada moÅ¾ete trenirati svoj model pozivom `fit()` s vaÅ¡im trening podacima i ispisati njegov rezultat:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Pogledajte rezultat svog modela. Nije loÅ¡e, s obzirom na to da imate samo oko 1000 redaka podataka:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Bolje razumijevanje putem matrice konfuzije

Iako moÅ¾ete dobiti izvjeÅ¡taj o rezultatima modela [termini](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) ispisivanjem gore navedenih stavki, moÅ¾da Ä‡ete bolje razumjeti svoj model koriÅ¡tenjem [matrice konfuzije](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) koja pomaÅ¾e razumjeti kako model funkcionira.

> ğŸ“ '[Matrica konfuzije](https://wikipedia.org/wiki/Confusion_matrix)' (ili 'matrica pogreÅ¡aka') je tablica koja izraÅ¾ava stvarne i laÅ¾ne pozitivne i negativne rezultate vaÅ¡eg modela, Äime se procjenjuje toÄnost predviÄ‘anja.

1. Za koriÅ¡tenje matrice konfuzije, pozovite `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Pogledajte matricu konfuzije svog modela:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

U Scikit-learn biblioteci, redovi (os x) predstavljaju stvarne oznake, a stupci (os y) predviÄ‘ene oznake.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Å to se ovdje dogaÄ‘a? Recimo da naÅ¡ model treba klasificirati bundeve izmeÄ‘u dvije binarne kategorije, kategorije 'bijela' i kategorije 'nije bijela'.

- Ako vaÅ¡ model predvidi bundevu kao 'nije bijela', a ona stvarno pripada kategoriji 'nije bijela', to nazivamo pravim negativnim rezultatom, prikazanim u gornjem lijevom kutu.
- Ako vaÅ¡ model predvidi bundevu kao 'bijela', a ona stvarno pripada kategoriji 'nije bijela', to nazivamo laÅ¾nim negativnim rezultatom, prikazanim u donjem lijevom kutu.
- Ako vaÅ¡ model predvidi bundevu kao 'nije bijela', a ona stvarno pripada kategoriji 'bijela', to nazivamo laÅ¾nim pozitivnim rezultatom, prikazanim u gornjem desnom kutu.
- Ako vaÅ¡ model predvidi bundevu kao 'bijela', a ona stvarno pripada kategoriji 'bijela', to nazivamo pravim pozitivnim rezultatom, prikazanim u donjem desnom kutu.

Kao Å¡to ste mogli pretpostaviti, poÅ¾eljno je imati veÄ‡i broj pravih pozitivnih i pravih negativnih rezultata te manji broj laÅ¾nih pozitivnih i laÅ¾nih negativnih rezultata, Å¡to implicira da model bolje funkcionira.
Kako se matrica zabune odnosi na preciznost i odziv? Zapamtite, izvjeÅ¡taj o klasifikaciji prikazan iznad pokazao je preciznost (0.85) i odziv (0.67).

Preciznost = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461

Odziv = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666

âœ… P: Prema matrici zabune, kako je model proÅ¡ao? O: Nije loÅ¡e; postoji dobar broj toÄnih negativnih, ali i nekoliko laÅ¾nih negativnih.

Ponovno Ä‡emo pogledati pojmove koje smo ranije vidjeli uz pomoÄ‡ mapiranja TP/TN i FP/FN u matrici zabune:

ğŸ“ Preciznost: TP/(TP + FP) Omjer relevantnih instanci meÄ‘u pronaÄ‘enim instancama (npr. koje oznake su dobro oznaÄene).

ğŸ“ Odziv: TP/(TP + FN) Omjer relevantnih instanci koje su pronaÄ‘ene, bez obzira jesu li dobro oznaÄene ili ne.

ğŸ“ f1-score: (2 * preciznost * odziv)/(preciznost + odziv) Ponderirani prosjek preciznosti i odziva, gdje je najbolji rezultat 1, a najgori 0.

ğŸ“ PodrÅ¡ka: Broj pojavljivanja svake pronaÄ‘ene oznake.

ğŸ“ ToÄnost: (TP + TN)/(TP + TN + FP + FN) Postotak oznaka koje su toÄno predviÄ‘ene za uzorak.

ğŸ“ Makro prosjek: IzraÄunavanje neponderiranog srednjeg rezultata za svaku oznaku, ne uzimajuÄ‡i u obzir neravnoteÅ¾u oznaka.

ğŸ“ Ponderirani prosjek: IzraÄunavanje srednjeg rezultata za svaku oznaku, uzimajuÄ‡i u obzir neravnoteÅ¾u oznaka ponderiranjem prema njihovoj podrÅ¡ci (broj toÄnih instanci za svaku oznaku).

âœ… MoÅ¾ete li zamisliti koji bi metrik trebali pratiti ako Å¾elite da vaÅ¡ model smanji broj laÅ¾nih negativnih?

## Vizualizirajte ROC krivulju ovog modela

[![ML za poÄetnike - Analiza performansi logistiÄke regresije s ROC krivuljama](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML za poÄetnike - Analiza performansi logistiÄke regresije s ROC krivuljama")

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled ROC krivulja.

Napravimo joÅ¡ jednu vizualizaciju kako bismo vidjeli takozvanu 'ROC' krivulju:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

KoristeÄ‡i Matplotlib, nacrtajte [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) ili ROC krivulju modela. ROC krivulje se Äesto koriste za pregled izlaza klasifikatora u smislu njegovih toÄnih i laÅ¾nih pozitivnih. "ROC krivulje obiÄno prikazuju stopu toÄnih pozitivnih na Y osi, a stopu laÅ¾nih pozitivnih na X osi." Dakle, strmina krivulje i prostor izmeÄ‘u srediÅ¡nje linije i krivulje su vaÅ¾ni: Å¾elite krivulju koja brzo ide gore i preko linije. U naÅ¡em sluÄaju, postoje laÅ¾ni pozitivni na poÄetku, a zatim linija ide gore i preko pravilno:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Na kraju, koristite Scikit-learnov [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) za izraÄun stvarne 'PovrÅ¡ine ispod krivulje' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Rezultat je `0.9749908725812341`. S obzirom na to da AUC varira od 0 do 1, Å¾elite visok rezultat, jer model koji je 100% toÄan u svojim predviÄ‘anjima ima AUC od 1; u ovom sluÄaju, model je _priliÄno dobar_.

U buduÄ‡im lekcijama o klasifikacijama nauÄit Ä‡ete kako iterirati kako biste poboljÅ¡ali rezultate svog modela. Ali za sada, Äestitamo! ZavrÅ¡ili ste ove lekcije o regresiji!

---
## ğŸš€Izazov

Ima joÅ¡ puno toga za istraÅ¾iti o logistiÄkoj regresiji! No, najbolji naÄin za uÄenje je eksperimentiranje. PronaÄ‘ite skup podataka koji se dobro uklapa u ovu vrstu analize i izradite model s njim. Å to ste nauÄili? savjet: pokuÅ¡ajte [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) za zanimljive skupove podataka.

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Pregled i samostalno uÄenje

ProÄitajte prvih nekoliko stranica [ovog rada sa Stanforda](https://web.stanford.edu/~jurafsky/slp3/5.pdf) o nekim praktiÄnim primjenama logistiÄke regresije. Razmislite o zadacima koji su bolje prilagoÄ‘eni jednoj ili drugoj vrsti regresijskih zadataka koje smo do sada prouÄavali. Å to bi najbolje funkcioniralo?

## Zadatak 

[Ponovno pokuÅ¡avanje ove regresije](assignment.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogreÅ¡ne interpretacije proizaÅ¡le iz koriÅ¡tenja ovog prijevoda.