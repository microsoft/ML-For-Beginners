<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T11:27:31+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "hr"
}
-->
# Izgradnja regresijskog modela koristeÄ‡i Scikit-learn: Äetiri naÄina regresije

![Infografika linearne i polinomne regresije](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ova lekcija je dostupna u R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Uvod 

Do sada ste istraÅ¾ili Å¡to je regresija koristeÄ‡i uzorke podataka iz skupa podataka o cijenama bundeva, koji Ä‡emo koristiti tijekom ove lekcije. TakoÄ‘er ste vizualizirali podatke koristeÄ‡i Matplotlib.

Sada ste spremni dublje zaroniti u regresiju za strojno uÄenje. Dok vizualizacija omoguÄ‡uje razumijevanje podataka, prava snaga strojnog uÄenja dolazi iz _treniranja modela_. Modeli se treniraju na povijesnim podacima kako bi automatski uhvatili ovisnosti podataka i omoguÄ‡ili predviÄ‘anje ishoda za nove podatke koje model nije prethodno vidio.

U ovoj lekciji nauÄit Ä‡ete viÅ¡e o dvije vrste regresije: _osnovnoj linearnoj regresiji_ i _polinomnoj regresiji_, zajedno s nekim matematiÄkim osnovama ovih tehnika. Ti modeli omoguÄ‡it Ä‡e nam predviÄ‘anje cijena bundeva ovisno o razliÄitim ulaznim podacima.

[![ML za poÄetnike - Razumijevanje linearne regresije](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML za poÄetnike - Razumijevanje linearne regresije")

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled linearne regresije.

> Tijekom ovog kurikuluma pretpostavljamo minimalno znanje matematike i nastojimo ga uÄiniti dostupnim studentima iz drugih podruÄja, pa obratite paÅ¾nju na biljeÅ¡ke, ğŸ§® matematiÄke primjere, dijagrame i druge alate za uÄenje koji pomaÅ¾u u razumijevanju.

### Preduvjeti

Do sada biste trebali biti upoznati sa strukturom podataka o bundevama koje analiziramo. MoÅ¾ete ih pronaÄ‡i unaprijed uÄitane i oÄiÅ¡Ä‡ene u datoteci _notebook.ipynb_ ove lekcije. U datoteci je cijena bundeve prikazana po buÅ¡elu u novom podatkovnom okviru. Provjerite moÅ¾ete li pokrenuti ove biljeÅ¾nice u kernelima u Visual Studio Codeu.

### Priprema

Podsjetnik: uÄitavate ove podatke kako biste postavili pitanja o njima.

- Kada je najbolje vrijeme za kupnju bundeva? 
- Koju cijenu mogu oÄekivati za kutiju minijaturnih bundeva?
- Trebam li ih kupiti u koÅ¡arama od pola buÅ¡ela ili u kutijama od 1 1/9 buÅ¡ela?
Nastavimo istraÅ¾ivati ove podatke.

U prethodnoj lekciji kreirali ste Pandas podatkovni okvir i popunili ga dijelom izvornog skupa podataka, standardizirajuÄ‡i cijene po buÅ¡elu. MeÄ‘utim, na taj naÄin uspjeli ste prikupiti samo oko 400 podatkovnih toÄaka i to samo za jesenske mjesece.

Pogledajte podatke koje smo unaprijed uÄitali u biljeÅ¾nici koja prati ovu lekciju. Podaci su unaprijed uÄitani, a poÄetni dijagram rasprÅ¡enja je nacrtan kako bi prikazao podatke po mjesecima. MoÅ¾da moÅ¾emo dobiti malo viÅ¡e detalja o prirodi podataka ako ih dodatno oÄistimo.

## Linija linearne regresije

Kao Å¡to ste nauÄili u Lekciji 1, cilj vjeÅ¾be linearne regresije je moÄ‡i nacrtati liniju kako bi:

- **Prikazali odnose varijabli**. Prikazali odnos izmeÄ‘u varijabli
- **Napravili predviÄ‘anja**. Napravili toÄna predviÄ‘anja o tome gdje bi nova podatkovna toÄka mogla pasti u odnosu na tu liniju.

TipiÄno je za **regresiju metodom najmanjih kvadrata** nacrtati ovu vrstu linije. Pojam 'najmanji kvadrati' znaÄi da su sve podatkovne toÄke oko regresijske linije kvadrirane i zatim zbrojene. Idealno, taj konaÄni zbroj je Å¡to manji, jer Å¾elimo mali broj pogreÅ¡aka, ili `najmanje kvadrate`.

To radimo jer Å¾elimo modelirati liniju koja ima najmanju kumulativnu udaljenost od svih naÅ¡ih podatkovnih toÄaka. TakoÄ‘er kvadriramo vrijednosti prije zbrajanja jer nas zanima njihova veliÄina, a ne smjer.

> **ğŸ§® PokaÅ¾i mi matematiku** 
> 
> Ova linija, nazvana _linija najboljeg pristajanja_, moÅ¾e se izraziti [jednadÅ¾bom](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'objaÅ¡njavajuÄ‡a varijabla'. `Y` je 'ovisna varijabla'. Nagib linije je `b`, a `a` je presjek s y-osom, koji se odnosi na vrijednost `Y` kada je `X = 0`. 
>
>![izraÄunaj nagib](../../../../2-Regression/3-Linear/images/slope.png)
>
> Prvo, izraÄunajte nagib `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Drugim rijeÄima, i referirajuÄ‡i se na izvorno pitanje o podacima o bundevama: "predvidite cijenu bundeve po buÅ¡elu po mjesecu", `X` bi se odnosio na cijenu, a `Y` na mjesec prodaje. 
>
>![dovrÅ¡ite jednadÅ¾bu](../../../../2-Regression/3-Linear/images/calculation.png)
>
> IzraÄunajte vrijednost Y. Ako plaÄ‡ate oko $4, mora da je travanj! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika koja izraÄunava liniju mora pokazati nagib linije, koji takoÄ‘er ovisi o presjeku, odnosno gdje se `Y` nalazi kada je `X = 0`.
>
> Metodu izraÄuna ovih vrijednosti moÅ¾ete vidjeti na web stranici [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). TakoÄ‘er posjetite [ovaj kalkulator najmanjih kvadrata](https://www.mathsisfun.com/data/least-squares-calculator.html) kako biste vidjeli kako vrijednosti brojeva utjeÄu na liniju.

## Korelacija

JoÅ¡ jedan pojam koji treba razumjeti je **koeficijent korelacije** izmeÄ‘u danih X i Y varijabli. KoristeÄ‡i dijagram rasprÅ¡enja, moÅ¾ete brzo vizualizirati ovaj koeficijent. Dijagram s podatkovnim toÄkama rasporeÄ‘enim u urednu liniju ima visoku korelaciju, dok dijagram s podatkovnim toÄkama rasprÅ¡enim svugdje izmeÄ‘u X i Y ima nisku korelaciju.

Dobar model linearne regresije bit Ä‡e onaj koji ima visok (bliÅ¾i 1 nego 0) koeficijent korelacije koristeÄ‡i metodu najmanjih kvadrata s regresijskom linijom.

âœ… Pokrenite biljeÅ¾nicu koja prati ovu lekciju i pogledajte dijagram rasprÅ¡enja Mjesec prema Cijeni. ÄŒini li se da podaci koji povezuju Mjesec s Cijenom za prodaju bundeva imaju visoku ili nisku korelaciju, prema vaÅ¡oj vizualnoj interpretaciji dijagrama rasprÅ¡enja? Mijenja li se to ako koristite precizniju mjeru umjesto `Mjesec`, npr. *dan u godini* (tj. broj dana od poÄetka godine)?

U kodu ispod pretpostavit Ä‡emo da smo oÄistili podatke i dobili podatkovni okvir nazvan `new_pumpkins`, sliÄan sljedeÄ‡em:

ID | Mjesec | DanUGodini | Vrsta | Grad | Paket | NajniÅ¾a cijena | NajviÅ¡a cijena | Cijena
---|--------|------------|-------|------|-------|----------------|----------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Kod za ÄiÅ¡Ä‡enje podataka dostupan je u [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Proveli smo iste korake ÄiÅ¡Ä‡enja kao u prethodnoj lekciji i izraÄunali stupac `DanUGodini` koristeÄ‡i sljedeÄ‡i izraz: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Sada kada razumijete matematiku iza linearne regresije, kreirajmo regresijski model kako bismo vidjeli moÅ¾emo li predvidjeti koji paket bundeva Ä‡e imati najbolje cijene bundeva. Netko tko kupuje bundeve za blagdanski vrt bundeva moÅ¾da Å¾eli ove informacije kako bi optimizirao svoje kupnje paketa bundeva za vrt.

## TraÅ¾enje korelacije

[![ML za poÄetnike - TraÅ¾enje korelacije: KljuÄ za linearnu regresiju](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML za poÄetnike - TraÅ¾enje korelacije: KljuÄ za linearnu regresiju")

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled korelacije.

Iz prethodne lekcije vjerojatno ste vidjeli da prosjeÄna cijena za razliÄite mjesece izgleda ovako:

<img alt="ProsjeÄna cijena po mjesecu" src="../2-Data/images/barchart.png" width="50%"/>

To sugerira da bi mogla postojati neka korelacija, i moÅ¾emo pokuÅ¡ati trenirati model linearne regresije kako bismo predvidjeli odnos izmeÄ‘u `Mjesec` i `Cijena`, ili izmeÄ‘u `DanUGodini` i `Cijena`. Evo dijagrama rasprÅ¡enja koji pokazuje potonji odnos:

<img alt="Dijagram rasprÅ¡enja Cijena vs. Dan u godini" src="images/scatter-dayofyear.png" width="50%" /> 

Pogledajmo postoji li korelacija koristeÄ‡i funkciju `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

ÄŒini se da je korelacija priliÄno mala, -0.15 za `Mjesec` i -0.17 za `DanUGodini`, ali mogla bi postojati druga vaÅ¾na veza. Izgleda da postoje razliÄiti klasteri cijena koji odgovaraju razliÄitim vrstama bundeva. Da bismo potvrdili ovu hipotezu, nacrtajmo svaku kategoriju bundeva koristeÄ‡i razliÄitu boju. ProsljeÄ‘ivanjem parametra `ax` funkciji za crtanje rasprÅ¡enja moÅ¾emo nacrtati sve toÄke na istom grafikonu:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Dijagram rasprÅ¡enja Cijena vs. Dan u godini" src="images/scatter-dayofyear-color.png" width="50%" /> 

NaÅ¡a istraga sugerira da vrsta bundeve ima veÄ‡i utjecaj na ukupnu cijenu nego stvarni datum prodaje. To moÅ¾emo vidjeti s dijagramom stupaca:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Dijagram stupaca cijena vs vrsta" src="images/price-by-variety.png" width="50%" /> 

UsredotoÄimo se za trenutak samo na jednu vrstu bundeve, 'pie type', i pogledajmo kakav uÄinak datum ima na cijenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Dijagram rasprÅ¡enja Cijena vs. Dan u godini" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Ako sada izraÄunamo korelaciju izmeÄ‘u `Cijena` i `DanUGodini` koristeÄ‡i funkciju `corr`, dobit Ä‡emo neÅ¡to poput `-0.27` - Å¡to znaÄi da treniranje prediktivnog modela ima smisla.

> Prije treniranja modela linearne regresije, vaÅ¾no je osigurati da su naÅ¡i podaci Äisti. Linearna regresija ne funkcionira dobro s nedostajuÄ‡im vrijednostima, stoga ima smisla rijeÅ¡iti se svih praznih Ä‡elija:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Drugi pristup bio bi popuniti te prazne vrijednosti srednjim vrijednostima iz odgovarajuÄ‡eg stupca.

## Jednostavna linearna regresija

[![ML za poÄetnike - Linearna i polinomna regresija koristeÄ‡i Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML za poÄetnike - Linearna i polinomna regresija koristeÄ‡i Scikit-learn")

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled linearne i polinomne regresije.

Za treniranje naÅ¡eg modela linearne regresije koristit Ä‡emo biblioteku **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

PoÄinjemo razdvajanjem ulaznih vrijednosti (znaÄajki) i oÄekivanog izlaza (oznaka) u zasebne numpy nizove:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Napomena: morali smo izvrÅ¡iti `reshape` na ulaznim podacima kako bi paket za linearnu regresiju ispravno razumio podatke. Linearna regresija oÄekuje 2D-niz kao ulaz, gdje svaki redak niza odgovara vektoru ulaznih znaÄajki. U naÅ¡em sluÄaju, buduÄ‡i da imamo samo jedan ulaz, trebamo niz oblika NÃ—1, gdje je N veliÄina skupa podataka.

Zatim trebamo podijeliti podatke na skupove za treniranje i testiranje, kako bismo mogli validirati naÅ¡ model nakon treniranja:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Na kraju, treniranje stvarnog modela linearne regresije traje samo dva retka koda. Definiramo objekt `LinearRegression` i prilagodimo ga naÅ¡im podacima koristeÄ‡i metodu `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` nakon prilagodbe (`fit`) sadrÅ¾i sve koeficijente regresije, koji se mogu pristupiti koristeÄ‡i svojstvo `.coef_`. U naÅ¡em sluÄaju postoji samo jedan koeficijent, koji bi trebao biti oko `-0.017`. To znaÄi da se cijene Äini da malo padaju s vremenom, ali ne previÅ¡e, oko 2 centa dnevno. TakoÄ‘er moÅ¾emo pristupiti toÄki presjeka regresije s Y-osom koristeÄ‡i `lin_reg.intercept_` - ona Ä‡e biti oko `21` u naÅ¡em sluÄaju, Å¡to ukazuje na cijenu na poÄetku godine.

Kako bismo vidjeli koliko je naÅ¡ model toÄan, moÅ¾emo predvidjeti cijene na testnom skupu podataka, a zatim izmjeriti koliko su naÅ¡e predikcije blizu oÄekivanim vrijednostima. To se moÅ¾e uÄiniti koristeÄ‡i metriku srednje kvadratne pogreÅ¡ke (MSE), koja je srednja vrijednost svih kvadriranih razlika izmeÄ‘u oÄekivane i predviÄ‘ene vrijednosti.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
NaÅ¡a pogreÅ¡ka Äini se da se kreÄ‡e oko 2 toÄke, Å¡to je ~17%. Nije baÅ¡ dobro. JoÅ¡ jedan pokazatelj kvalitete modela je **koeficijent determinacije**, koji se moÅ¾e dobiti ovako:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Ako je vrijednost 0, to znaÄi da model ne uzima ulazne podatke u obzir i ponaÅ¡a se kao *najgori linearni prediktor*, Å¡to je jednostavno prosjeÄna vrijednost rezultata. Vrijednost 1 znaÄi da moÅ¾emo savrÅ¡eno predvidjeti sve oÄekivane izlaze. U naÅ¡em sluÄaju, koeficijent je oko 0.06, Å¡to je priliÄno nisko.

TakoÄ‘er moÅ¾emo prikazati testne podatke zajedno s regresijskom linijom kako bismo bolje vidjeli kako regresija funkcionira u naÅ¡em sluÄaju:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="Linear regression" src="images/linear-results.png" width="50%" />

## Polinomijalna regresija  

Druga vrsta linearne regresije je polinomijalna regresija. Iako ponekad postoji linearna veza izmeÄ‘u varijabli - Å¡to je veÄ‡a bundeva po volumenu, to je viÅ¡a cijena - ponekad te veze ne mogu biti prikazane kao ravnina ili ravna linija.  

âœ… Evo [nekoliko primjera](https://online.stat.psu.edu/stat501/lesson/9/9.8) podataka koji bi mogli koristiti polinomijalnu regresiju.  

Pogledajte ponovno vezu izmeÄ‘u datuma i cijene. ÄŒini li se ovaj dijagram rasprÅ¡enosti kao da bi nuÅ¾no trebao biti analiziran ravnom linijom? Zar cijene ne mogu fluktuirati? U ovom sluÄaju moÅ¾ete pokuÅ¡ati s polinomijalnom regresijom.  

âœ… Polinomi su matematiÄki izrazi koji mogu sadrÅ¾avati jednu ili viÅ¡e varijabli i koeficijenata.  

Polinomijalna regresija stvara zakrivljenu liniju kako bi bolje odgovarala nelinearnim podacima. U naÅ¡em sluÄaju, ako ukljuÄimo kvadratnu varijablu `DayOfYear` u ulazne podatke, trebali bismo moÄ‡i prilagoditi naÅ¡e podatke paraboliÄnoj krivulji, koja Ä‡e imati minimum u odreÄ‘enom trenutku unutar godine.  

Scikit-learn ukljuÄuje koristan [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) za kombiniranje razliÄitih koraka obrade podataka. **Pipeline** je lanac **procjenitelja**. U naÅ¡em sluÄaju, stvorit Ä‡emo pipeline koji prvo dodaje polinomijalne znaÄajke naÅ¡em modelu, a zatim trenira regresiju:  

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

KoriÅ¡tenje `PolynomialFeatures(2)` znaÄi da Ä‡emo ukljuÄiti sve polinome drugog stupnja iz ulaznih podataka. U naÅ¡em sluÄaju to Ä‡e jednostavno znaÄiti `DayOfYear`<sup>2</sup>, ali s obzirom na dvije ulazne varijable X i Y, to Ä‡e dodati X<sup>2</sup>, XY i Y<sup>2</sup>. TakoÄ‘er moÅ¾emo koristiti polinome viÅ¡eg stupnja ako Å¾elimo.  

Pipeline se moÅ¾e koristiti na isti naÄin kao i originalni objekt `LinearRegression`, tj. moÅ¾emo koristiti `fit` za treniranje pipelinea, a zatim `predict` za dobivanje rezultata predikcije. Evo grafikona koji prikazuje testne podatke i aproksimacijsku krivulju:  

<img alt="Polynomial regression" src="images/poly-results.png" width="50%" />  

KoriÅ¡tenjem polinomijalne regresije moÅ¾emo dobiti neÅ¡to niÅ¾i MSE i viÅ¡i koeficijent determinacije, ali ne znaÄajno. Moramo uzeti u obzir i druge znaÄajke!  

> MoÅ¾ete vidjeti da su minimalne cijene bundeva zabiljeÅ¾ene negdje oko NoÄ‡i vjeÅ¡tica. Kako to moÅ¾ete objasniti?  

ğŸƒ ÄŒestitamo, upravo ste stvorili model koji moÅ¾e pomoÄ‡i u predviÄ‘anju cijene bundeva za pite. Vjerojatno moÅ¾ete ponoviti isti postupak za sve vrste bundeva, ali to bi bilo zamorno. Sada nauÄimo kako uzeti u obzir raznolikost bundeva u naÅ¡em modelu!  

## Kategorijalne znaÄajke  

U idealnom svijetu Å¾elimo moÄ‡i predvidjeti cijene za razliÄite vrste bundeva koristeÄ‡i isti model. MeÄ‘utim, stupac `Variety` je donekle drugaÄiji od stupaca poput `Month`, jer sadrÅ¾i nenumeriÄke vrijednosti. Takvi stupci nazivaju se **kategorijalni**.  

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")  

> ğŸ¥ Kliknite na sliku iznad za kratki video pregled koriÅ¡tenja kategorijalnih znaÄajki.  

Ovdje moÅ¾ete vidjeti kako prosjeÄna cijena ovisi o vrsti:  

<img alt="Average price by variety" src="images/price-by-variety.png" width="50%" />  

Kako bismo uzeli u obzir vrstu, prvo je moramo pretvoriti u numeriÄki oblik, odnosno **kodirati**. Postoji nekoliko naÄina kako to moÅ¾emo uÄiniti:  

* Jednostavno **numeriÄko kodiranje** Ä‡e izgraditi tablicu razliÄitih vrsta, a zatim zamijeniti naziv vrste indeksom u toj tablici. Ovo nije najbolja ideja za linearnu regresiju, jer linearna regresija uzima stvarnu numeriÄku vrijednost indeksa i dodaje je rezultatu, mnoÅ¾eÄ‡i je nekim koeficijentom. U naÅ¡em sluÄaju, veza izmeÄ‘u broja indeksa i cijene oÄito nije linearna, Äak i ako osiguramo da su indeksi poredani na neki specifiÄan naÄin.  
* **One-hot kodiranje** Ä‡e zamijeniti stupac `Variety` s 4 razliÄita stupca, po jedan za svaku vrstu. Svaki stupac Ä‡e sadrÅ¾avati `1` ako odgovarajuÄ‡i redak pripada odreÄ‘enoj vrsti, a `0` inaÄe. To znaÄi da Ä‡e u linearnom regresijskom modelu postojati Äetiri koeficijenta, po jedan za svaku vrstu bundeve, odgovorna za "poÄetnu cijenu" (ili radije "dodatnu cijenu") za tu odreÄ‘enu vrstu.  

Kod ispod pokazuje kako moÅ¾emo one-hot kodirati vrstu:  

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

Kako bismo trenirali linearnu regresiju koristeÄ‡i one-hot kodiranu vrstu kao ulaz, samo trebamo ispravno inicijalizirati podatke `X` i `y`:  

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

Ostatak koda je isti kao Å¡to smo koristili gore za treniranje linearne regresije. Ako ga isprobate, vidjet Ä‡ete da je srednja kvadratna pogreÅ¡ka otprilike ista, ali dobivamo puno viÅ¡i koeficijent determinacije (~77%). Kako bismo dobili joÅ¡ toÄnija predviÄ‘anja, moÅ¾emo uzeti u obzir viÅ¡e kategorijalnih znaÄajki, kao i numeriÄke znaÄajke poput `Month` ili `DayOfYear`. Kako bismo dobili jedan veliki niz znaÄajki, moÅ¾emo koristiti `join`:  

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

Ovdje takoÄ‘er uzimamo u obzir `City` i vrstu `Package`, Å¡to nam daje MSE 2.84 (10%) i determinaciju 0.94!  

## Sve zajedno  

Kako bismo napravili najbolji model, moÅ¾emo koristiti kombinirane (one-hot kodirane kategorijalne + numeriÄke) podatke iz gornjeg primjera zajedno s polinomijalnom regresijom. Evo kompletnog koda za vaÅ¡u praktiÄnost:  

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Ovo bi nam trebalo dati najbolji koeficijent determinacije od gotovo 97% i MSE=2.23 (~8% pogreÅ¡ke u predviÄ‘anju).  

| Model | MSE | Determinacija |  
|-------|-----|---------------|  
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |  
| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |  
| `Variety` Linear | 5.24 (19.7%) | 0.77 |  
| Sve znaÄajke Linear | 2.84 (10.5%) | 0.94 |  
| Sve znaÄajke Polynomial | 2.23 (8.25%) | 0.97 |  

ğŸ† Bravo! Stvorili ste Äetiri regresijska modela u jednoj lekciji i poboljÅ¡ali kvalitetu modela na 97%. U zavrÅ¡nom dijelu o regresiji nauÄit Ä‡ete o logistiÄkoj regresiji za odreÄ‘ivanje kategorija.  

---  
## ğŸš€Izazov  

Testirajte nekoliko razliÄitih varijabli u ovom notebooku kako biste vidjeli kako korelacija odgovara toÄnosti modela.  

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)  

## Pregled i samostalno uÄenje  

U ovoj lekciji nauÄili smo o linearnoj regresiji. Postoje i druge vaÅ¾ne vrste regresije. ProÄitajte o tehnikama Stepwise, Ridge, Lasso i Elasticnet. Dobar teÄaj za daljnje uÄenje je [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).  

## Zadatak  

[Izgradite model](assignment.md)  

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za bilo kakve nesporazume ili pogreÅ¡ne interpretacije koje proizlaze iz koriÅ¡tenja ovog prijevoda.