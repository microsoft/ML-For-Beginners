<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T12:37:22+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "hr"
}
-->
# Izgradnja rjeÅ¡enja za strojno uÄenje s odgovornom umjetnom inteligencijom

![SaÅ¾etak odgovorne umjetne inteligencije u strojnome uÄenju u obliku sketchnotea](../../../../sketchnotes/ml-fairness.png)
> Sketchnote autorice [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Uvod

U ovom kurikulumu poÄet Ä‡ete otkrivati kako strojno uÄenje moÅ¾e utjecati na naÅ¡e svakodnevne Å¾ivote. VeÄ‡ sada sustavi i modeli sudjeluju u svakodnevnim zadacima donoÅ¡enja odluka, poput dijagnoza u zdravstvenoj skrbi, odobravanja kredita ili otkrivanja prijevara. Stoga je vaÅ¾no da ti modeli dobro funkcioniraju kako bi pruÅ¾ili pouzdane rezultate. Kao i svaka softverska aplikacija, sustavi umjetne inteligencije mogu podbaciti u ispunjavanju oÄekivanja ili imati neÅ¾eljene ishode. Zato je kljuÄno razumjeti i objasniti ponaÅ¡anje AI modela.

Zamislite Å¡to se moÅ¾e dogoditi kada podaci koje koristite za izgradnju tih modela nedostaju odreÄ‘ene demografske skupine, poput rase, spola, politiÄkih stavova, religije, ili kada su te demografske skupine neproporcionalno zastupljene. Å to ako se izlaz modela interpretira tako da favorizira odreÄ‘enu demografsku skupinu? Koje su posljedice za aplikaciju? Osim toga, Å¡to se dogaÄ‘a kada model ima negativan ishod koji Å¡teti ljudima? Tko je odgovoran za ponaÅ¡anje AI sustava? Ovo su neka od pitanja koja Ä‡emo istraÅ¾iti u ovom kurikulumu.

U ovoj lekciji Ä‡ete:

- PoveÄ‡ati svijest o vaÅ¾nosti pravednosti u strojnome uÄenju i Å¡tetama povezanim s nepravednoÅ¡Ä‡u.
- Upoznati se s praksom istraÅ¾ivanja odstupanja i neobiÄnih scenarija kako biste osigurali pouzdanost i sigurnost.
- SteÄ‡i razumijevanje o potrebi osnaÅ¾ivanja svih kroz dizajniranje inkluzivnih sustava.
- IstraÅ¾iti koliko je vaÅ¾no zaÅ¡tititi privatnost i sigurnost podataka i ljudi.
- Uvidjeti vaÅ¾nost pristupa "staklene kutije" za objaÅ¡njavanje ponaÅ¡anja AI modela.
- Biti svjesni kako je odgovornost kljuÄna za izgradnju povjerenja u AI sustave.

## Preduvjeti

Kao preduvjet, molimo vas da zavrÅ¡ite "Principi odgovorne umjetne inteligencije" Learn Path i pogledate videozapis u nastavku na tu temu:

Saznajte viÅ¡e o odgovornoj umjetnoj inteligenciji prateÄ‡i ovaj [Learning Path](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Microsoftov pristup odgovornoj umjetnoj inteligenciji](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsoftov pristup odgovornoj umjetnoj inteligenciji")

> ğŸ¥ Kliknite na sliku iznad za video: Microsoftov pristup odgovornoj umjetnoj inteligenciji

## Pravednost

AI sustavi trebali bi tretirati sve ljude jednako i izbjegavati razliÄito utjecanje na sliÄne skupine ljudi. Na primjer, kada AI sustavi pruÅ¾aju smjernice o medicinskom tretmanu, aplikacijama za kredite ili zapoÅ¡ljavanju, trebali bi davati iste preporuke svima sliÄnih simptoma, financijskih okolnosti ili profesionalnih kvalifikacija. Svaki od nas kao ljudi nosi naslijeÄ‘ene pristranosti koje utjeÄu na naÅ¡e odluke i postupke. Te pristranosti mogu biti vidljive u podacima koje koristimo za treniranje AI sustava. Takva manipulacija ponekad se dogaÄ‘a nenamjerno. ÄŒesto je teÅ¡ko svjesno znati kada uvodite pristranost u podatke.

**"Nepravednost"** obuhvaÄ‡a negativne utjecaje ili "Å¡tete" za skupinu ljudi, poput onih definiranih prema rasi, spolu, dobi ili statusu invaliditeta. Glavne Å¡tete povezane s pravednoÅ¡Ä‡u mogu se klasificirati kao:

- **Dodjela**, ako je, na primjer, spol ili etniÄka pripadnost favorizirana u odnosu na drugu.
- **Kvaliteta usluge**. Ako trenirate podatke za jedan specifiÄan scenarij, ali stvarnost je mnogo sloÅ¾enija, to dovodi do loÅ¡e izvedbe usluge. Na primjer, dozator sapuna koji ne moÅ¾e prepoznati ljude s tamnom koÅ¾om. [Referenca](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **OmalovaÅ¾avanje**. Nepravedno kritiziranje i oznaÄavanje neÄega ili nekoga. Na primjer, tehnologija za oznaÄavanje slika neslavno je pogreÅ¡no oznaÄila slike tamnoputih ljudi kao gorile.
- **Prekomjerna ili nedovoljna zastupljenost**. Ideja da odreÄ‘ena skupina nije viÄ‘ena u odreÄ‘enoj profesiji, a svaka usluga ili funkcija koja to nastavlja promovirati doprinosi Å¡teti.
- **Stereotipiziranje**. Povezivanje odreÄ‘ene skupine s unaprijed dodijeljenim atributima. Na primjer, sustav za prevoÄ‘enje izmeÄ‘u engleskog i turskog moÅ¾e imati netoÄnosti zbog rijeÄi sa stereotipnim asocijacijama na spol.

![prijevod na turski](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> prijevod na turski

![prijevod natrag na engleski](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> prijevod natrag na engleski

Prilikom dizajniranja i testiranja AI sustava, moramo osigurati da AI bude pravedan i da nije programiran za donoÅ¡enje pristranih ili diskriminatornih odluka, koje su zabranjene i ljudima. Osiguravanje pravednosti u AI i strojnome uÄenju ostaje sloÅ¾en sociotehniÄki izazov.

### Pouzdanost i sigurnost

Kako bismo izgradili povjerenje, AI sustavi moraju biti pouzdani, sigurni i dosljedni u normalnim i neoÄekivanim uvjetima. VaÅ¾no je znati kako Ä‡e se AI sustavi ponaÅ¡ati u raznim situacijama, posebno kada su u pitanju odstupanja. Prilikom izgradnje AI rjeÅ¡enja, potrebno je posvetiti znaÄajnu paÅ¾nju tome kako se nositi s raznim okolnostima koje AI rjeÅ¡enja mogu susresti. Na primjer, samovozeÄ‡i automobil mora staviti sigurnost ljudi kao glavni prioritet. Kao rezultat toga, AI koji pokreÄ‡e automobil mora uzeti u obzir sve moguÄ‡e scenarije s kojima se automobil moÅ¾e susresti, poput noÄ‡i, oluja, snjeÅ¾nih meÄ‡ava, djece koja trÄe preko ulice, kuÄ‡nih ljubimaca, radova na cesti itd. Koliko dobro AI sustav moÅ¾e pouzdano i sigurno upravljati Å¡irokim rasponom uvjeta odraÅ¾ava razinu predviÄ‘anja koju je podatkovni znanstvenik ili AI programer uzeo u obzir tijekom dizajna ili testiranja sustava.

> [ğŸ¥ Kliknite ovdje za video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inkluzivnost

AI sustavi trebali bi biti dizajnirani tako da angaÅ¾iraju i osnaÅ¾uju sve ljude. Prilikom dizajniranja i implementacije AI sustava, podatkovni znanstvenici i AI programeri identificiraju i rjeÅ¡avaju potencijalne prepreke u sustavu koje bi mogle nenamjerno iskljuÄiti ljude. Na primjer, postoji 1 milijarda ljudi s invaliditetom diljem svijeta. S napretkom AI-a, oni mogu lakÅ¡e pristupiti Å¡irokom rasponu informacija i prilika u svakodnevnom Å¾ivotu. RjeÅ¡avanjem prepreka stvaraju se prilike za inovacije i razvoj AI proizvoda s boljim iskustvima koja koriste svima.

> [ğŸ¥ Kliknite ovdje za video: inkluzivnost u AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sigurnost i privatnost

AI sustavi trebali bi biti sigurni i poÅ¡tovati privatnost ljudi. Ljudi imaju manje povjerenja u sustave koji ugroÅ¾avaju njihovu privatnost, informacije ili Å¾ivote. Prilikom treniranja modela strojnog uÄenja, oslanjamo se na podatke kako bismo postigli najbolje rezultate. Pritom se mora uzeti u obzir podrijetlo podataka i njihov integritet. Na primjer, jesu li podaci korisniÄki dostavljeni ili javno dostupni? Nadalje, dok radimo s podacima, kljuÄno je razviti AI sustave koji mogu zaÅ¡tititi povjerljive informacije i odoljeti napadima. Kako AI postaje sve prisutniji, zaÅ¡tita privatnosti i osiguranje vaÅ¾nih osobnih i poslovnih informacija postaje sve kritiÄnija i sloÅ¾enija. Problemi privatnosti i sigurnosti podataka zahtijevaju posebnu paÅ¾nju za AI jer je pristup podacima kljuÄan za to da AI sustavi donose toÄne i informirane predikcije i odluke o ljudima.

> [ğŸ¥ Kliknite ovdje za video: sigurnost u AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Industrija je postigla znaÄajan napredak u privatnosti i sigurnosti, potaknut znaÄajno regulativama poput GDPR-a (OpÄ‡a uredba o zaÅ¡titi podataka).
- Ipak, s AI sustavima moramo priznati napetost izmeÄ‘u potrebe za viÅ¡e osobnih podataka kako bi sustavi bili osobniji i uÄinkovitiji â€“ i privatnosti.
- Kao i s pojavom povezanih raÄunala putem interneta, takoÄ‘er vidimo veliki porast broja sigurnosnih problema povezanih s AI-jem.
- Istovremeno, vidimo da se AI koristi za poboljÅ¡anje sigurnosti. Na primjer, veÄ‡ina modernih antivirusnih skenera danas se pokreÄ‡e AI heuristikom.
- Moramo osigurati da naÅ¡i procesi podatkovne znanosti skladno suraÄ‘uju s najnovijim praksama privatnosti i sigurnosti.

### Transparentnost

AI sustavi trebali bi biti razumljivi. KljuÄni dio transparentnosti je objaÅ¡njavanje ponaÅ¡anja AI sustava i njihovih komponenti. PoboljÅ¡anje razumijevanja AI sustava zahtijeva da dionici shvate kako i zaÅ¡to funkcioniraju kako bi mogli identificirati potencijalne probleme s izvedbom, zabrinutosti za sigurnost i privatnost, pristranosti, iskljuÄujuÄ‡e prakse ili neÅ¾eljene ishode. TakoÄ‘er vjerujemo da oni koji koriste AI sustave trebaju biti iskreni i otvoreni o tome kada, zaÅ¡to i kako odluÄuju implementirati te sustave, kao i o ograniÄenjima sustava koje koriste. Na primjer, ako banka koristi AI sustav za podrÅ¡ku svojim odlukama o kreditiranju potroÅ¡aÄa, vaÅ¾no je ispitati ishode i razumjeti koji podaci utjeÄu na preporuke sustava. Vlade poÄinju regulirati AI u razliÄitim industrijama, pa podatkovni znanstvenici i organizacije moraju objasniti ispunjava li AI sustav regulatorne zahtjeve, posebno kada doÄ‘e do neÅ¾eljenog ishoda.

> [ğŸ¥ Kliknite ovdje za video: transparentnost u AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- BuduÄ‡i da su AI sustavi toliko sloÅ¾eni, teÅ¡ko je razumjeti kako funkcioniraju i interpretirati rezultate.
- Taj nedostatak razumijevanja utjeÄe na naÄin na koji se ti sustavi upravljaju, operacionaliziraju i dokumentiraju.
- Taj nedostatak razumijevanja joÅ¡ vaÅ¾nije utjeÄe na odluke donesene na temelju rezultata koje ti sustavi proizvode.

### Odgovornost

Ljudi koji dizajniraju i implementiraju AI sustave moraju biti odgovorni za naÄin na koji njihovi sustavi funkcioniraju. Potreba za odgovornoÅ¡Ä‡u posebno je vaÅ¾na kod osjetljivih tehnologija poput prepoznavanja lica. Nedavno je porasla potraÅ¾nja za tehnologijom prepoznavanja lica, posebno od strane organizacija za provedbu zakona koje vide potencijal tehnologije u primjenama poput pronalaÅ¾enja nestale djece. MeÄ‘utim, te tehnologije potencijalno bi mogle biti koriÅ¡tene od strane vlada za ugroÅ¾avanje temeljnih sloboda graÄ‘ana, primjerice omoguÄ‡avanjem kontinuiranog nadzora odreÄ‘enih pojedinaca. Stoga podatkovni znanstvenici i organizacije moraju biti odgovorni za naÄin na koji njihov AI sustav utjeÄe na pojedince ili druÅ¡tvo.

[![VodeÄ‡i istraÅ¾ivaÄ AI-a upozorava na masovni nadzor putem prepoznavanja lica](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsoftov pristup odgovornoj umjetnoj inteligenciji")

> ğŸ¥ Kliknite na sliku iznad za video: Upozorenja o masovnom nadzoru putem prepoznavanja lica

Na kraju, jedno od najveÄ‡ih pitanja za naÅ¡u generaciju, kao prvu generaciju koja donosi AI u druÅ¡tvo, jest kako osigurati da raÄunala ostanu odgovorna ljudima i kako osigurati da ljudi koji dizajniraju raÄunala ostanu odgovorni svima ostalima.

## Procjena utjecaja

Prije treniranja modela strojnog uÄenja, vaÅ¾no je provesti procjenu utjecaja kako biste razumjeli svrhu AI sustava; koja je namjeravana upotreba; gdje Ä‡e biti implementiran; i tko Ä‡e komunicirati sa sustavom. Ovo je korisno za recenzente ili testere koji procjenjuju sustav kako bi znali koje Äimbenike uzeti u obzir prilikom identificiranja potencijalnih rizika i oÄekivanih posljedica.

SljedeÄ‡a su podruÄja fokusa prilikom provoÄ‘enja procjene utjecaja:

* **Negativan utjecaj na pojedince**. Svijest o bilo kakvim ograniÄenjima ili zahtjevima, nepodrÅ¾anoj upotrebi ili poznatim ograniÄenjima koja ometaju izvedbu sustava kljuÄna je kako bi se osiguralo da sustav nije koriÅ¡ten na naÄin koji bi mogao nanijeti Å¡tetu pojedincima.
* **Zahtjevi za podatke**. Razumijevanje kako i gdje Ä‡e sustav koristiti podatke omoguÄ‡uje recenzentima da istraÅ¾e sve zahtjeve za podatke na koje treba obratiti paÅ¾nju (npr. GDPR ili HIPPA regulative). Osim toga, provjerite je li izvor ili koliÄina podataka dovoljna za treniranje.
* **SaÅ¾etak utjecaja**. Prikupite popis potencijalnih Å¡teta koje bi mogle nastati koriÅ¡tenjem sustava. Tijekom Å¾ivotnog ciklusa ML-a, provjerite jesu li identificirani problemi ublaÅ¾eni ili rijeÅ¡eni.
* **Primjenjivi ciljevi** za svaki od Å¡est osnovnih principa. Procijenite jesu li ciljevi iz svakog principa ispunjeni i postoje li praznine.

## Otklanjanje pogreÅ¡aka s odgovornom umjetnom inteligencijom

SliÄno otklanjanju pogreÅ¡aka u softverskoj aplikaciji, otklanjanje pogreÅ¡aka u AI sustavu nuÅ¾an je proces identificiranja i rjeÅ¡avanja problema u sustavu. Postoji mnogo Äimbenika koji mogu utjecati na to da model ne funkcionira kako se oÄekuje ili odgovorno. VeÄ‡ina tradicionalnih metrika izvedbe modela su kvantitativni agregati izvedbe modela, Å¡to nije dovoljno za analizu kako model krÅ¡i principe odgovorne umjetne inteligencije. Nadalje, model strojnog uÄenja je "crna kutija" koja oteÅ¾ava razumijevanje Å¡to pokreÄ‡e njegov ishod ili pruÅ¾anje objaÅ¡njenja kada pogrijeÅ¡i. Kasnije u ovom teÄaju nauÄit Ä‡emo kako koristiti nadzornu ploÄu odgovorne umjetne inteligencije za pomoÄ‡ pri otklanjanju pogreÅ¡aka u AI sustavima. Nadzorna ploÄa pruÅ¾a holistiÄki alat za podatkovne znanstvenike i AI programere za obavljanje:

* **Analize pogreÅ¡aka**. Identificiranje distribucije pogreÅ¡aka modela koje mogu utjecati na pravednost ili pouzdanost sustava.
* **Pregleda modela**. Otkrivanje gdje postoje razlike u izvedbi modela meÄ‘u skupovima podataka.
* **Analize podataka**. Razumijevanje distribucije podataka i identificiranje potencijalne pristranosti u podacima koja bi mogla dovesti do problema s pravednoÅ¡Ä‡u, inkluzivnoÅ¡Ä‡u i pouzdanoÅ¡Ä‡u.
* **Interpretacije modela**. Razumijevanje Å¡to utjeÄe ili utjeÄe na predikcije modela. Ovo pomaÅ¾e u objaÅ¡njavanju ponaÅ¡anja modela, Å¡to je vaÅ¾no za transparentnost i odgovornost.

## ğŸš€ Izazov

Kako bismo sprijeÄili da se Å¡tete uopÄ‡e uvedu, trebali bismo:

- imati raznolikost pozadina i perspektiva meÄ‘u ljudima koji rade na sustavima
- ulagati u skupove podataka koji odraÅ¾avaju raznolikost naÅ¡eg druÅ¡tva
- razviti bolje metode tijekom Å¾ivotnog ciklusa strojnog uÄenja za otkrivanje i ispravljanje odgovorne umjetne inteligencije kada se pojavi

Razmislite o stvarnim scenarijima gdje je nepouzdanost modela oÄita u
Pogledajte ovu radionicu za dublje razumijevanje tema:

- U potrazi za odgovornom umjetnom inteligencijom: Primjena principa u praksi od Besmire Nushi, Mehrnoosh Sameki i Amita Sharme

[![Responsible AI Toolbox: Otvoreni okvir za izgradnju odgovorne umjetne inteligencije](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Otvoreni okvir za izgradnju odgovorne umjetne inteligencije")

> ğŸ¥ Kliknite na sliku iznad za video: RAI Toolbox: Otvoreni okvir za izgradnju odgovorne umjetne inteligencije od Besmire Nushi, Mehrnoosh Sameki i Amita Sharme

TakoÄ‘er, proÄitajte:

- Microsoftov centar resursa za odgovornu umjetnu inteligenciju: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsoftova FATE istraÅ¾ivaÄka grupa: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Responsible AI Toolbox GitHub repozitorij](https://github.com/microsoft/responsible-ai-toolbox)

ProÄitajte o alatima Azure Machine Learninga za osiguranje pravednosti:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Zadatak

[Istrazite RAI Toolbox](assignment.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja mogu proizaÄ‡i iz koriÅ¡tenja ovog prijevoda.