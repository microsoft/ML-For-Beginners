<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T13:35:49+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "hr"
}
-->
# Uvod u uÄenje pojaÄanjem i Q-uÄenje

![SaÅ¾etak uÄenja pojaÄanjem u strojnom uÄenju u obliku sketchnotea](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote autorice [Tomomi Imura](https://www.twitter.com/girlie_mac)

UÄenje pojaÄanjem ukljuÄuje tri vaÅ¾na koncepta: agenta, odreÄ‘ena stanja i skup akcija za svako stanje. IzvrÅ¡avanjem akcije u odreÄ‘enom stanju, agent dobiva nagradu. Zamislite raÄunalnu igru Super Mario. Vi ste Mario, nalazite se na razini igre, stojite pored ruba litice. Iznad vas je novÄiÄ‡. Vi, kao Mario, na razini igre, na odreÄ‘enoj poziciji... to je vaÅ¡e stanje. Pomicanje korak udesno (akcija) odvest Ä‡e vas preko ruba, Å¡to bi vam donijelo nisku numeriÄku ocjenu. MeÄ‘utim, pritiskom na gumb za skok osvojili biste bod i ostali Å¾ivi. To je pozitivan ishod i trebao bi vam donijeti pozitivnu numeriÄku ocjenu.

KoriÅ¡tenjem uÄenja pojaÄanjem i simulatora (igre), moÅ¾ete nauÄiti kako igrati igru kako biste maksimizirali nagradu, Å¡to znaÄi ostati Å¾iv i osvojiti Å¡to viÅ¡e bodova.

[![Uvod u uÄenje pojaÄanjem](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ Kliknite na sliku iznad kako biste Äuli Dmitryja kako govori o uÄenju pojaÄanjem

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Preduvjeti i postavljanje

U ovoj lekciji eksperimentirat Ä‡emo s nekim kodom u Pythonu. Trebali biste moÄ‡i pokrenuti Jupyter Notebook kod iz ove lekcije, bilo na svom raÄunalu ili negdje u oblaku.

MoÅ¾ete otvoriti [biljeÅ¾nicu lekcije](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) i proÄ‡i kroz ovu lekciju kako biste je izgradili.

> **Napomena:** Ako otvarate ovaj kod iz oblaka, takoÄ‘er trebate preuzeti datoteku [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), koja se koristi u kodu biljeÅ¾nice. Dodajte je u isti direktorij kao i biljeÅ¾nicu.

## Uvod

U ovoj lekciji istraÅ¾it Ä‡emo svijet **[Petra i vuka](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspiriran glazbenom bajkom ruskog skladatelja [Sergeja Prokofjeva](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Koristit Ä‡emo **uÄenje pojaÄanjem** kako bismo omoguÄ‡ili Petru da istraÅ¾i svoje okruÅ¾enje, prikupi ukusne jabuke i izbjegne susret s vukom.

**UÄenje pojaÄanjem** (RL) je tehnika uÄenja koja nam omoguÄ‡uje da nauÄimo optimalno ponaÅ¡anje **agenta** u nekom **okruÅ¾enju** izvoÄ‘enjem mnogih eksperimenata. Agent u ovom okruÅ¾enju treba imati neki **cilj**, definiran pomoÄ‡u **funkcije nagrade**.

## OkruÅ¾enje

Radi jednostavnosti, zamislimo Petrov svijet kao kvadratnu ploÄu veliÄine `Å¡irina` x `visina`, ovako:

![Petrovo okruÅ¾enje](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Svaka Ä‡elija na ovoj ploÄi moÅ¾e biti:

* **tlo**, po kojem Peter i druga biÄ‡a mogu hodati.
* **voda**, po kojoj oÄito ne moÅ¾ete hodati.
* **stablo** ili **trava**, mjesto gdje se moÅ¾ete odmoriti.
* **jabuka**, Å¡to predstavlja neÅ¡to Å¡to bi Peter rado pronaÅ¡ao kako bi se nahranio.
* **vuk**, koji je opasan i treba ga izbjegavati.

Postoji zaseban Python modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), koji sadrÅ¾i kod za rad s ovim okruÅ¾enjem. BuduÄ‡i da ovaj kod nije vaÅ¾an za razumijevanje naÅ¡ih koncepata, uvest Ä‡emo modul i koristiti ga za stvaranje uzorka ploÄe (blok koda 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Ovaj kod bi trebao ispisati sliku okruÅ¾enja sliÄnu onoj gore.

## Akcije i politika

U naÅ¡em primjeru, Petrov cilj bio bi pronaÄ‡i jabuku, dok izbjegava vuka i druge prepreke. Da bi to uÄinio, moÅ¾e se kretati dok ne pronaÄ‘e jabuku.

Dakle, na bilo kojoj poziciji, moÅ¾e birati izmeÄ‘u sljedeÄ‡ih akcija: gore, dolje, lijevo i desno.

Te Ä‡emo akcije definirati kao rjeÄnik i mapirati ih na parove odgovarajuÄ‡ih promjena koordinata. Na primjer, pomicanje udesno (`R`) odgovaralo bi paru `(1,0)`. (blok koda 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Da rezimiramo, strategija i cilj ovog scenarija su sljedeÄ‡i:

- **Strategija** naÅ¡eg agenta (Petra) definirana je tzv. **politikom**. Politika je funkcija koja vraÄ‡a akciju u bilo kojem danom stanju. U naÅ¡em sluÄaju, stanje problema predstavljeno je ploÄom, ukljuÄujuÄ‡i trenutnu poziciju igraÄa.

- **Cilj** uÄenja pojaÄanjem je na kraju nauÄiti dobru politiku koja Ä‡e nam omoguÄ‡iti uÄinkovito rjeÅ¡avanje problema. MeÄ‘utim, kao osnovnu liniju, razmotrit Ä‡emo najjednostavniju politiku zvanu **sluÄajna Å¡etnja**.

## SluÄajna Å¡etnja

Najprije Ä‡emo rijeÅ¡iti naÅ¡ problem implementacijom strategije sluÄajne Å¡etnje. Kod sluÄajne Å¡etnje, nasumiÄno Ä‡emo birati sljedeÄ‡u akciju iz dopuÅ¡tenih akcija, sve dok ne doÄ‘emo do jabuke (blok koda 3).

1. Implementirajte sluÄajnu Å¡etnju pomoÄ‡u donjeg koda:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Poziv funkcije `walk` trebao bi vratiti duljinu odgovarajuÄ‡e staze, koja moÅ¾e varirati od jednog pokretanja do drugog.

1. Pokrenite eksperiment Å¡etnje nekoliko puta (recimo, 100) i ispiÅ¡ite dobivene statistike (blok koda 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Primijetite da je prosjeÄna duljina staze oko 30-40 koraka, Å¡to je priliÄno puno, s obzirom na to da je prosjeÄna udaljenost do najbliÅ¾e jabuke oko 5-6 koraka.

    TakoÄ‘er moÅ¾ete vidjeti kako izgleda Petrov pokret tijekom sluÄajne Å¡etnje:

    ![Petrova sluÄajna Å¡etnja](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funkcija nagrade

Kako bismo naÅ¡u politiku uÄinili inteligentnijom, trebamo razumjeti koji su potezi "bolji" od drugih. Da bismo to uÄinili, trebamo definirati naÅ¡ cilj.

Cilj se moÅ¾e definirati u smislu **funkcije nagrade**, koja Ä‡e vraÄ‡ati neku vrijednost ocjene za svako stanje. Å to je broj veÄ‡i, to je funkcija nagrade bolja. (blok koda 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Zanimljivo je da u veÄ‡ini sluÄajeva *znaÄajnu nagradu dobivamo tek na kraju igre*. To znaÄi da naÅ¡ algoritam nekako treba zapamtiti "dobre" korake koji vode do pozitivne nagrade na kraju i poveÄ‡ati njihovu vaÅ¾nost. SliÄno tome, svi potezi koji vode do loÅ¡ih rezultata trebaju se obeshrabriti.

## Q-uÄenje

Algoritam koji Ä‡emo ovdje raspraviti zove se **Q-uÄenje**. U ovom algoritmu, politika je definirana funkcijom (ili strukturom podataka) zvanom **Q-Tablica**. Ona biljeÅ¾i "dobrotu" svake od akcija u danom stanju.

Zove se Q-Tablica jer je Äesto zgodno predstavljati je kao tablicu ili viÅ¡edimenzionalni niz. BuduÄ‡i da naÅ¡a ploÄa ima dimenzije `Å¡irina` x `visina`, moÅ¾emo predstaviti Q-Tablicu pomoÄ‡u numpy niza s oblikom `Å¡irina` x `visina` x `len(actions)`: (blok koda 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Primijetite da inicijaliziramo sve vrijednosti Q-Tablice s jednakom vrijednoÅ¡Ä‡u, u naÅ¡em sluÄaju - 0.25. Ovo odgovara politici "sluÄajne Å¡etnje", jer su svi potezi u svakom stanju jednako dobri. Q-Tablicu moÅ¾emo proslijediti funkciji `plot` kako bismo vizualizirali tablicu na ploÄi: `m.plot(Q)`.

![Petrovo okruÅ¾enje](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

U srediÅ¡tu svake Ä‡elije nalazi se "strelica" koja oznaÄava preferirani smjer kretanja. BuduÄ‡i da su svi smjerovi jednaki, prikazuje se toÄka.

Sada trebamo pokrenuti simulaciju, istraÅ¾iti naÅ¡e okruÅ¾enje i nauÄiti bolju raspodjelu vrijednosti Q-Tablice, Å¡to Ä‡e nam omoguÄ‡iti da mnogo brÅ¾e pronaÄ‘emo put do jabuke.

## SuÅ¡tina Q-uÄenja: Bellmanova jednadÅ¾ba

Jednom kada se poÄnemo kretati, svaka akcija imat Ä‡e odgovarajuÄ‡u nagradu, tj. teoretski moÅ¾emo odabrati sljedeÄ‡u akciju na temelju najveÄ‡e neposredne nagrade. MeÄ‘utim, u veÄ‡ini stanja potez neÄ‡e postiÄ‡i naÅ¡ cilj dolaska do jabuke, pa stoga ne moÅ¾emo odmah odluÄiti koji je smjer bolji.

> Zapamtite da nije vaÅ¾an neposredni rezultat, veÄ‡ konaÄni rezultat, koji Ä‡emo dobiti na kraju simulacije.

Kako bismo uzeli u obzir ovu odgoÄ‘enu nagradu, trebamo koristiti principe **[dinamiÄkog programiranja](https://en.wikipedia.org/wiki/Dynamic_programming)**, koji nam omoguÄ‡uju da o naÅ¡em problemu razmiÅ¡ljamo rekurzivno.

Pretpostavimo da se sada nalazimo u stanju *s* i Å¾elimo se pomaknuti u sljedeÄ‡e stanje *s'*. Time Ä‡emo dobiti neposrednu nagradu *r(s,a)*, definiranu funkcijom nagrade, plus neku buduÄ‡u nagradu. Ako pretpostavimo da naÅ¡a Q-Tablica toÄno odraÅ¾ava "privlaÄnost" svake akcije, tada Ä‡emo u stanju *s'* odabrati akciju *a* koja odgovara maksimalnoj vrijednosti *Q(s',a')*. Tako Ä‡e najbolja moguÄ‡a buduÄ‡a nagrada koju bismo mogli dobiti u stanju *s* biti definirana kao `max`

## Provjera politike

BuduÄ‡i da Q-Tablica prikazuje "privlaÄnost" svake akcije u svakom stanju, vrlo je jednostavno koristiti je za definiranje uÄinkovitog kretanja u naÅ¡em svijetu. U najjednostavnijem sluÄaju, moÅ¾emo odabrati akciju koja odgovara najveÄ‡oj vrijednosti u Q-Tablici: (kodni blok 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Ako nekoliko puta isprobate gornji kod, moÅ¾da Ä‡ete primijetiti da se ponekad "zaglavi" i morate pritisnuti gumb STOP u biljeÅ¾nici kako biste ga prekinuli. To se dogaÄ‘a jer mogu postojati situacije u kojima dva stanja "pokazuju" jedno na drugo u smislu optimalne Q-Vrijednosti, u kojem sluÄaju agent zavrÅ¡ava kreÄ‡uÄ‡i se izmeÄ‘u tih stanja beskonaÄno.

## ğŸš€Izazov

> **Zadatak 1:** Modificirajte funkciju `walk` kako biste ograniÄili maksimalnu duljinu puta na odreÄ‘eni broj koraka (recimo, 100) i promatrajte kako gornji kod povremeno vraÄ‡a ovu vrijednost.

> **Zadatak 2:** Modificirajte funkciju `walk` tako da se ne vraÄ‡a na mjesta na kojima je veÄ‡ bio. Ovo Ä‡e sprijeÄiti da se `walk` ponavlja, no agent i dalje moÅ¾e zavrÅ¡iti "zarobljen" na lokaciji s koje ne moÅ¾e pobjeÄ‡i.

## Navigacija

Bolja navigacijska politika bila bi ona koju smo koristili tijekom treninga, a koja kombinira eksploataciju i istraÅ¾ivanje. U ovoj politici odabiremo svaku akciju s odreÄ‘enom vjerojatnoÅ¡Ä‡u, proporcionalno vrijednostima u Q-Tablici. Ova strategija moÅ¾e i dalje rezultirati time da se agent vraÄ‡a na poziciju koju je veÄ‡ istraÅ¾io, ali, kao Å¡to moÅ¾ete vidjeti iz koda dolje, rezultira vrlo kratkim prosjeÄnim putem do Å¾eljene lokacije (zapamtite da `print_statistics` pokreÄ‡e simulaciju 100 puta): (kodni blok 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Nakon pokretanja ovog koda, trebali biste dobiti znatno manju prosjeÄnu duljinu puta nego prije, u rasponu od 3-6.

## IstraÅ¾ivanje procesa uÄenja

Kao Å¡to smo spomenuli, proces uÄenja je ravnoteÅ¾a izmeÄ‘u istraÅ¾ivanja i koriÅ¡tenja steÄenog znanja o strukturi prostora problema. Vidjeli smo da su rezultati uÄenja (sposobnost pomaganja agentu da pronaÄ‘e kratak put do cilja) poboljÅ¡ani, ali takoÄ‘er je zanimljivo promatrati kako se prosjeÄna duljina puta ponaÅ¡a tijekom procesa uÄenja:

## SaÅ¾etak nauÄenog:

- **ProsjeÄna duljina puta raste**. Ono Å¡to ovdje vidimo jest da na poÄetku prosjeÄna duljina puta raste. To je vjerojatno zbog Äinjenice da, kada niÅ¡ta ne znamo o okoliÅ¡u, vjerojatno Ä‡emo se zaglaviti u loÅ¡im stanjima, poput vode ili vuka. Kako uÄimo viÅ¡e i poÄnemo koristiti to znanje, moÅ¾emo dulje istraÅ¾ivati okoliÅ¡, ali joÅ¡ uvijek ne znamo dobro gdje se nalaze jabuke.

- **Duljina puta se smanjuje kako uÄimo viÅ¡e**. Kada dovoljno nauÄimo, agentu postaje lakÅ¡e postiÄ‡i cilj, a duljina puta poÄinje se smanjivati. MeÄ‘utim, joÅ¡ uvijek smo otvoreni za istraÅ¾ivanje, pa Äesto skreÄ‡emo s najboljeg puta i istraÅ¾ujemo nove opcije, Äime put postaje dulji od optimalnog.

- **Duljina naglo raste**. Ono Å¡to takoÄ‘er primjeÄ‡ujemo na ovom grafu jest da u nekom trenutku duljina naglo raste. To ukazuje na stohastiÄku prirodu procesa i da u nekom trenutku moÅ¾emo "pokvariti" koeficijente u Q-Tablici prepisivanjem novih vrijednosti. Ovo bi idealno trebalo minimizirati smanjenjem stope uÄenja (na primjer, prema kraju treninga, prilagoÄ‘avamo vrijednosti u Q-Tablici samo malim iznosom).

Sveukupno, vaÅ¾no je zapamtiti da uspjeh i kvaliteta procesa uÄenja znaÄajno ovise o parametrima, poput stope uÄenja, smanjenja stope uÄenja i faktora diskonta. Ti se parametri Äesto nazivaju **hiperparametri**, kako bi se razlikovali od **parametara**, koje optimiziramo tijekom treninga (na primjer, koeficijenti u Q-Tablici). Proces pronalaÅ¾enja najboljih vrijednosti hiperparametara naziva se **optimizacija hiperparametara**, i zasluÅ¾uje zasebnu temu.

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Zadatak 
[RealistiÄniji svijet](assignment.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koriÅ¡tenjem AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.