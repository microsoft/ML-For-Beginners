<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T13:45:45+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "hr"
}
-->
# CartPole Klizanje

Problem koji smo rjeÅ¡avali u prethodnoj lekciji moÅ¾e se Äiniti kao igraÄka, ne baÅ¡ primjenjiva u stvarnim Å¾ivotnim situacijama. No, to nije sluÄaj, jer mnogi stvarni problemi dijele sliÄan scenarij - ukljuÄujuÄ‡i igranje Å¡aha ili Go. Oni su sliÄni jer takoÄ‘er imamo ploÄu s odreÄ‘enim pravilima i **diskretno stanje**.

## [Kviz prije predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Uvod

U ovoj lekciji primijenit Ä‡emo iste principe Q-Learninga na problem s **kontinuiranim stanjem**, tj. stanjem koje je definirano jednim ili viÅ¡e realnih brojeva. Bavimo se sljedeÄ‡im problemom:

> **Problem**: Ako Peter Å¾eli pobjeÄ‡i od vuka, mora se moÄ‡i kretati brÅ¾e. Vidjet Ä‡emo kako Peter moÅ¾e nauÄiti klizati, posebno odrÅ¾avati ravnoteÅ¾u, koristeÄ‡i Q-Learning.

![Veliki bijeg!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> Peter i njegovi prijatelji postaju kreativni kako bi pobjegli od vuka! Slika: [Jen Looper](https://twitter.com/jenlooper)

Koristit Ä‡emo pojednostavljenu verziju odrÅ¾avanja ravnoteÅ¾e poznatu kao problem **CartPole**. U svijetu CartPole-a imamo horizontalni klizaÄ koji se moÅ¾e kretati lijevo ili desno, a cilj je odrÅ¾ati vertikalni stup na vrhu klizaÄa.

## Preduvjeti

U ovoj lekciji koristit Ä‡emo biblioteku pod nazivom **OpenAI Gym** za simulaciju razliÄitih **okruÅ¾enja**. Kod ove lekcije moÅ¾ete pokrenuti lokalno (npr. iz Visual Studio Code-a), u kojem sluÄaju Ä‡e se simulacija otvoriti u novom prozoru. Kada pokreÄ‡ete kod online, moÅ¾da Ä‡ete morati napraviti neke prilagodbe koda, kao Å¡to je opisano [ovdje](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

U prethodnoj lekciji pravila igre i stanje definirali smo pomoÄ‡u klase `Board` koju smo sami kreirali. Ovdje Ä‡emo koristiti posebno **simulacijsko okruÅ¾enje**, koje Ä‡e simulirati fiziku iza balansirajuÄ‡eg stupa. Jedno od najpopularnijih simulacijskih okruÅ¾enja za treniranje algoritama za uÄenje pojaÄanjem zove se [Gym](https://gym.openai.com/), kojeg odrÅ¾ava [OpenAI](https://openai.com/). KoristeÄ‡i ovaj Gym moÅ¾emo kreirati razliÄita **okruÅ¾enja**, od simulacije CartPole-a do Atari igara.

> **Napomena**: Ostala dostupna okruÅ¾enja iz OpenAI Gym-a moÅ¾ete vidjeti [ovdje](https://gym.openai.com/envs/#classic_control).

Prvo, instalirajmo Gym i uvezimo potrebne biblioteke (blok koda 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## VjeÅ¾ba - inicijalizacija okruÅ¾enja CartPole

Za rad s problemom balansiranja CartPole-a, moramo inicijalizirati odgovarajuÄ‡e okruÅ¾enje. Svako okruÅ¾enje povezano je s:

- **Prostorom opaÅ¾anja** koji definira strukturu informacija koje primamo iz okruÅ¾enja. Za problem CartPole-a primamo poziciju stupa, brzinu i neke druge vrijednosti.

- **Prostorom akcija** koji definira moguÄ‡e akcije. U naÅ¡em sluÄaju prostor akcija je diskretan i sastoji se od dvije akcije - **lijevo** i **desno**. (blok koda 2)

1. Za inicijalizaciju, upiÅ¡ite sljedeÄ‡i kod:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Da bismo vidjeli kako okruÅ¾enje funkcionira, pokrenimo kratku simulaciju od 100 koraka. Na svakom koraku pruÅ¾amo jednu od akcija koje treba poduzeti - u ovoj simulaciji nasumiÄno biramo akciju iz `action_space`.

1. Pokrenite kod ispod i pogledajte rezultat.

    âœ… Zapamtite da je poÅ¾eljno pokrenuti ovaj kod na lokalnoj Python instalaciji! (blok koda 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Trebali biste vidjeti neÅ¡to sliÄno ovoj slici:

    ![CartPole bez ravnoteÅ¾e](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Tijekom simulacije, trebamo dobiti opaÅ¾anja kako bismo odluÄili Å¡to uÄiniti. Zapravo, funkcija `step` vraÄ‡a trenutna opaÅ¾anja, funkciju nagrade i zastavicu `done` koja oznaÄava ima li smisla nastaviti simulaciju ili ne: (blok koda 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Na kraju Ä‡ete vidjeti neÅ¡to sliÄno ovome u izlazu biljeÅ¾nice:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Vektor opaÅ¾anja koji se vraÄ‡a na svakom koraku simulacije sadrÅ¾i sljedeÄ‡e vrijednosti:
    - Pozicija klizaÄa
    - Brzina klizaÄa
    - Kut stupa
    - Brzina rotacije stupa

1. Dobijte minimalne i maksimalne vrijednosti tih brojeva: (blok koda 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    TakoÄ‘er moÅ¾ete primijetiti da je vrijednost nagrade na svakom koraku simulacije uvijek 1. To je zato Å¡to je naÅ¡ cilj preÅ¾ivjeti Å¡to je dulje moguÄ‡e, tj. odrÅ¾avati stup u razumno vertikalnom poloÅ¾aju Å¡to duÅ¾e.

    âœ… Zapravo, simulacija CartPole-a smatra se rijeÅ¡enom ako uspijemo postiÄ‡i prosjeÄnu nagradu od 195 tijekom 100 uzastopnih pokuÅ¡aja.

## Diskretizacija stanja

U Q-Learningu, moramo izgraditi Q-Tablicu koja definira Å¡to uÄiniti u svakom stanju. Da bismo to mogli uÄiniti, stanje mora biti **diskretno**, preciznije, mora sadrÅ¾avati konaÄan broj diskretnih vrijednosti. Dakle, moramo nekako **diskretizirati** naÅ¡a opaÅ¾anja, mapirajuÄ‡i ih na konaÄan skup stanja.

Postoji nekoliko naÄina kako to moÅ¾emo uÄiniti:

- **Podjela u binove**. Ako znamo interval odreÄ‘ene vrijednosti, moÅ¾emo podijeliti taj interval u odreÄ‘eni broj **binova**, a zatim zamijeniti vrijednost brojem bina kojem pripada. To se moÅ¾e uÄiniti pomoÄ‡u metode numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). U ovom sluÄaju, toÄno Ä‡emo znati veliÄinu stanja, jer Ä‡e ovisiti o broju binova koje odaberemo za digitalizaciju.

âœ… MoÅ¾emo koristiti linearnu interpolaciju kako bismo vrijednosti doveli na neki konaÄan interval (recimo, od -20 do 20), a zatim pretvoriti brojeve u cijele brojeve zaokruÅ¾ivanjem. To nam daje malo manje kontrole nad veliÄinom stanja, posebno ako ne znamo toÄne raspone ulaznih vrijednosti. Na primjer, u naÅ¡em sluÄaju 2 od 4 vrijednosti nemaju gornje/donje granice svojih vrijednosti, Å¡to moÅ¾e rezultirati beskonaÄnim brojem stanja.

U naÅ¡em primjeru, koristit Ä‡emo drugi pristup. Kao Å¡to Ä‡ete kasnije primijetiti, unatoÄ neodreÄ‘enim gornjim/donjim granicama, te vrijednosti rijetko uzimaju vrijednosti izvan odreÄ‘enih konaÄnih intervala, pa Ä‡e ta stanja s ekstremnim vrijednostima biti vrlo rijetka.

1. Evo funkcije koja Ä‡e uzeti opaÅ¾anje iz naÅ¡eg modela i proizvesti tuple od 4 cijele vrijednosti: (blok koda 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. TakoÄ‘er istraÅ¾imo drugu metodu diskretizacije koristeÄ‡i binove: (blok koda 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Sada pokrenimo kratku simulaciju i promatrajmo te diskretne vrijednosti okruÅ¾enja. Slobodno isprobajte obje funkcije `discretize` i `discretize_bins` i provjerite postoji li razlika.

    âœ… `discretize_bins` vraÄ‡a broj bina, koji poÄinje od 0. Dakle, za vrijednosti ulazne varijable oko 0 vraÄ‡a broj iz sredine intervala (10). U `discretize`, nismo se brinuli o rasponu izlaznih vrijednosti, dopuÅ¡tajuÄ‡i im da budu negativne, pa vrijednosti stanja nisu pomaknute, i 0 odgovara 0. (blok koda 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Uklonite komentar s linije koja poÄinje s `env.render` ako Å¾elite vidjeti kako se okruÅ¾enje izvrÅ¡ava. InaÄe ga moÅ¾ete izvrÅ¡iti u pozadini, Å¡to je brÅ¾e. Koristit Ä‡emo ovo "nevidljivo" izvrÅ¡avanje tijekom naÅ¡eg procesa Q-Learninga.

## Struktura Q-Tablice

U naÅ¡oj prethodnoj lekciji, stanje je bilo jednostavan par brojeva od 0 do 8, pa je bilo zgodno predstaviti Q-Tablicu pomoÄ‡u numpy tensor-a oblika 8x8x2. Ako koristimo diskretizaciju binova, veliÄina naÅ¡eg vektora stanja takoÄ‘er je poznata, pa moÅ¾emo koristiti isti pristup i predstaviti stanje pomoÄ‡u niza oblika 20x20x10x10x2 (ovdje 2 predstavlja dimenziju prostora akcija, a prve dimenzije odgovaraju broju binova koje smo odabrali za svaku od parametara u prostoru opaÅ¾anja).

MeÄ‘utim, ponekad precizne dimenzije prostora opaÅ¾anja nisu poznate. U sluÄaju funkcije `discretize`, nikada ne moÅ¾emo biti sigurni da naÅ¡e stanje ostaje unutar odreÄ‘enih granica, jer neke od originalnih vrijednosti nisu ograniÄene. Stoga Ä‡emo koristiti malo drugaÄiji pristup i predstaviti Q-Tablicu pomoÄ‡u rjeÄnika.

1. Koristite par *(state,action)* kao kljuÄ rjeÄnika, a vrijednost bi odgovarala vrijednosti unosa u Q-Tablici. (blok koda 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Ovdje takoÄ‘er definiramo funkciju `qvalues()`, koja vraÄ‡a popis vrijednosti Q-Tablice za dano stanje koje odgovara svim moguÄ‡im akcijama. Ako unos nije prisutan u Q-Tablici, vratit Ä‡emo 0 kao zadanu vrijednost.

## ZapoÄnimo Q-Learning

Sada smo spremni nauÄiti Petera kako odrÅ¾avati ravnoteÅ¾u!

1. Prvo, postavimo neke hiperparametre: (blok koda 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Ovdje, `alpha` je **stopa uÄenja** koja definira u kojoj mjeri trebamo prilagoditi trenutne vrijednosti Q-Tablice na svakom koraku. U prethodnoj lekciji zapoÄeli smo s 1, a zatim smanjili `alpha` na niÅ¾e vrijednosti tijekom treninga. U ovom primjeru zadrÅ¾at Ä‡emo ga konstantnim radi jednostavnosti, a vi moÅ¾ete eksperimentirati s prilagodbom vrijednosti `alpha` kasnije.

    `gamma` je **faktor diskontiranja** koji pokazuje u kojoj mjeri trebamo prioritizirati buduÄ‡u nagradu nad trenutnom nagradom.

    `epsilon` je **faktor istraÅ¾ivanja/iskoriÅ¡tavanja** koji odreÄ‘uje trebamo li preferirati istraÅ¾ivanje ili iskoriÅ¡tavanje. U naÅ¡em algoritmu, u `epsilon` postotku sluÄajeva odabrat Ä‡emo sljedeÄ‡u akciju prema vrijednostima Q-Tablice, a u preostalom broju sluÄajeva izvrÅ¡it Ä‡emo nasumiÄnu akciju. To Ä‡e nam omoguÄ‡iti istraÅ¾ivanje podruÄja prostora pretraÅ¾ivanja koja nikada prije nismo vidjeli.

    âœ… U smislu odrÅ¾avanja ravnoteÅ¾e - odabir nasumiÄne akcije (istraÅ¾ivanje) djelovao bi kao nasumiÄni udarac u pogreÅ¡nom smjeru, a stup bi morao nauÄiti kako povratiti ravnoteÅ¾u iz tih "pogreÅ¡aka".

### PoboljÅ¡anje algoritma

MoÅ¾emo napraviti dva poboljÅ¡anja naÅ¡em algoritmu iz prethodne lekcije:

- **IzraÄunajte prosjeÄnu kumulativnu nagradu**, tijekom odreÄ‘enog broja simulacija. Ispisivat Ä‡emo napredak svakih 5000 iteracija, i prosjeÄno Ä‡emo izraÄunati kumulativnu nagradu tijekom tog vremenskog razdoblja. To znaÄi da ako postignemo viÅ¡e od 195 bodova - moÅ¾emo smatrati problem rijeÅ¡enim, Äak i s viÅ¡om kvalitetom nego Å¡to je potrebno.

- **IzraÄunajte maksimalni prosjeÄni kumulativni rezultat**, `Qmax`, i pohraniti Ä‡emo Q-Tablicu koja odgovara tom rezultatu. Kada pokrenete trening primijetit Ä‡ete da ponekad prosjeÄni kumulativni rezultat poÄinje padati, i Å¾elimo zadrÅ¾ati vrijednosti Q-Tablice koje odgovaraju najboljem modelu promatranom tijekom treninga.

1. Prikupite sve kumulativne nagrade na svakoj simulaciji u vektor `rewards` za daljnje crtanje. (blok koda 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Å to moÅ¾ete primijetiti iz tih rezultata:

- **Blizu naÅ¡eg cilja**. Vrlo smo blizu postizanja cilja od 195 kumulativnih nagrada tijekom 100+ uzastopnih simulacija, ili smo ga moÅ¾da veÄ‡ postigli! ÄŒak i ako dobijemo manje brojeve, joÅ¡ uvijek ne znamo, jer prosjeÄno raÄunamo tijekom 5000 pokuÅ¡aja, a samo 100 pokuÅ¡aja je potrebno prema formalnim kriterijima.

- **Nagrada poÄinje padati**. Ponekad nagrada poÄinje padati, Å¡to znaÄi da moÅ¾emo "uniÅ¡titi" veÄ‡ nauÄene vrijednosti u Q-Tablici s onima koje pogorÅ¡avaju situaciju.

Ovo opaÅ¾anje je jasnije vidljivo ako nacrtamo napredak treninga.

## Crtanje napretka treninga

Tijekom treninga, prikupili smo vrijednost kumulativne nagrade na svakoj iteraciji u vektor `rewards`. Evo kako izgleda kada ga nacrtamo u odnosu na broj iteracija:

```python
plt.plot(rewards)
```

![sirovi napredak](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Iz ovog grafikona nije moguÄ‡e niÅ¡ta zakljuÄiti, jer zbog prirode stohastiÄkog procesa treninga duljina sesija treninga jako varira. Da bi grafikon imao viÅ¡e smisla, moÅ¾emo izraÄunati **pokretni prosjek** tijekom serije eksperimenata, recimo 100. To se moÅ¾e zgodno uÄiniti pomoÄ‡u `np.convolve`: (blok koda 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![napredak treninga](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variranje hiperparametara

Kako bi uÄenje bilo stabilnije, ima smisla prilagoditi neke od naÅ¡ih hiperparametara tijekom treninga. Konkretno:

- **Za stopu uÄenja**, `alpha`, moÅ¾emo zapoÄeti s vrijednostima blizu 1, a zatim postupno smanjivati parametar. S vremenom Ä‡emo dobivati dobre vrijednosti vjerojatnosti u Q-Tablici, pa bismo ih trebali lagano prilagoÄ‘avati, a ne potpuno prepisivati novim vrijednostima.

- **PoveÄ‡ajte epsilon**. MoÅ¾da Ä‡emo htjeti postupno poveÄ‡avati `epsilon`, kako bismo manje istraÅ¾ivali, a viÅ¡e iskoriÅ¡tavali. Vjerojatno ima smisla zapoÄeti s niÅ¾om vrijednosti `epsilon`, i postupno je poveÄ‡avati do gotovo 1.
> **Zadatak 1**: Igrajte se s vrijednostima hiperparametara i provjerite moÅ¾ete li postiÄ‡i veÄ‡i kumulativni nagradni rezultat. DoseÅ¾ete li iznad 195?
> **Zadatak 2**: Da biste formalno rijeÅ¡ili problem, trebate postiÄ‡i prosjeÄnu nagradu od 195 kroz 100 uzastopnih pokretanja. Mjerite to tijekom treninga i uvjerite se da ste formalno rijeÅ¡ili problem!

## Promatranje rezultata u praksi

Bilo bi zanimljivo vidjeti kako se trenirani model ponaÅ¡a. Pokrenimo simulaciju i slijedimo istu strategiju odabira akcija kao tijekom treninga, uzorkujuÄ‡i prema distribuciji vjerojatnosti u Q-Tablici: (blok koda 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Trebali biste vidjeti neÅ¡to poput ovoga:

![balansirajuÄ‡i cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€Izazov

> **Zadatak 3**: Ovdje smo koristili zavrÅ¡nu verziju Q-Tablice, koja moÅ¾da nije najbolja. Zapamtite da smo najbolju Q-Tablicu spremili u varijablu `Qbest`! Isprobajte isti primjer s najboljom Q-Tablicom tako da kopirate `Qbest` u `Q` i provjerite primjeÄ‡ujete li razliku.

> **Zadatak 4**: Ovdje nismo birali najbolju akciju u svakom koraku, veÄ‡ smo uzorkovali prema odgovarajuÄ‡oj distribuciji vjerojatnosti. Bi li imalo viÅ¡e smisla uvijek birati najbolju akciju, onu s najviÅ¡om vrijednoÅ¡Ä‡u u Q-Tablici? To se moÅ¾e uÄiniti koriÅ¡tenjem funkcije `np.argmax` kako biste pronaÅ¡li broj akcije koji odgovara najviÅ¡oj vrijednosti u Q-Tablici. Implementirajte ovu strategiju i provjerite poboljÅ¡ava li balansiranje.

## [Kviz nakon predavanja](https://ff-quizzes.netlify.app/en/ml/)

## Zadatak
[Trenirajte Mountain Car](assignment.md)

## ZakljuÄak

Sada smo nauÄili kako trenirati agente da postignu dobre rezultate samo pruÅ¾anjem funkcije nagrade koja definira Å¾eljeno stanje igre i omoguÄ‡avanjem inteligentnog istraÅ¾ivanja prostora pretraÅ¾ivanja. UspjeÅ¡no smo primijenili algoritam Q-Learning u sluÄajevima diskretnih i kontinuiranih okruÅ¾enja, ali s diskretnim akcijama.

VaÅ¾no je takoÄ‘er prouÄiti situacije u kojima je stanje akcije takoÄ‘er kontinuirano, a prostor opaÅ¾anja mnogo sloÅ¾eniji, poput slike s ekrana Atari igre. U tim problemima Äesto trebamo koristiti moÄ‡nije tehnike strojnog uÄenja, poput neuronskih mreÅ¾a, kako bismo postigli dobre rezultate. Ti napredniji koncepti bit Ä‡e tema naÅ¡eg nadolazeÄ‡eg naprednog AI teÄaja.

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane struÄnjaka. Ne preuzimamo odgovornost za bilo kakva nesporazuma ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.