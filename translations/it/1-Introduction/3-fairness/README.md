<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8f819813b2ca08ec7b9f60a2c9336045",
  "translation_date": "2025-08-29T21:23:10+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "it"
}
-->
# Costruire soluzioni di Machine Learning con AI responsabile

![Riassunto dell'AI responsabile nel Machine Learning in uno sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.it.png)
> Sketchnote di [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz pre-lezione](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introduzione

In questo curriculum, inizierai a scoprire come il machine learning puÃ² influenzare e sta influenzando la nostra vita quotidiana. Anche ora, sistemi e modelli sono coinvolti in attivitÃ  decisionali quotidiane, come diagnosi sanitarie, approvazioni di prestiti o rilevamento di frodi. Ãˆ quindi importante che questi modelli funzionino bene per fornire risultati affidabili. Come qualsiasi applicazione software, i sistemi di AI possono non soddisfare le aspettative o avere risultati indesiderati. Per questo motivo, Ã¨ essenziale comprendere e spiegare il comportamento di un modello di AI.

Immagina cosa puÃ² accadere quando i dati utilizzati per costruire questi modelli mancano di determinate demografie, come razza, genere, opinioni politiche, religione, o rappresentano in modo sproporzionato tali demografie. E se l'output del modello fosse interpretato in modo da favorire alcune demografie? Quali sarebbero le conseguenze per l'applicazione? Inoltre, cosa succede quando il modello ha un risultato negativo e danneggia le persone? Chi Ã¨ responsabile del comportamento dei sistemi di AI? Queste sono alcune delle domande che esploreremo in questo curriculum.

In questa lezione, imparerai a:

- Comprendere l'importanza dell'equitÃ  nel machine learning e i danni correlati all'equitÃ .
- Familiarizzare con la pratica di esplorare anomalie e scenari insoliti per garantire affidabilitÃ  e sicurezza.
- Capire la necessitÃ  di progettare sistemi inclusivi per potenziare tutti.
- Esplorare quanto sia vitale proteggere la privacy e la sicurezza dei dati e delle persone.
- Riconoscere l'importanza di un approccio trasparente per spiegare il comportamento dei modelli di AI.
- Essere consapevoli di come la responsabilitÃ  sia essenziale per costruire fiducia nei sistemi di AI.

## Prerequisiti

Come prerequisito, segui il percorso di apprendimento "Principi di AI responsabile" e guarda il video qui sotto sull'argomento:

Scopri di piÃ¹ sull'AI responsabile seguendo questo [Percorso di Apprendimento](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Approccio di Microsoft all'AI responsabile](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Approccio di Microsoft all'AI responsabile")

> ðŸŽ¥ Clicca sull'immagine sopra per un video: Approccio di Microsoft all'AI responsabile

## EquitÃ 

I sistemi di AI dovrebbero trattare tutti in modo equo ed evitare di influenzare gruppi simili di persone in modi diversi. Ad esempio, quando i sistemi di AI forniscono indicazioni su trattamenti medici, richieste di prestiti o occupazione, dovrebbero fare le stesse raccomandazioni a tutti con sintomi, circostanze finanziarie o qualifiche professionali simili. Ognuno di noi, come esseri umani, porta con sÃ© bias ereditati che influenzano le nostre decisioni e azioni. Questi bias possono essere evidenti nei dati che utilizziamo per addestrare i sistemi di AI. Tale manipolazione puÃ² talvolta accadere involontariamente. Spesso Ã¨ difficile sapere consapevolmente quando si sta introducendo un bias nei dati.

**â€œIngiustiziaâ€** comprende impatti negativi, o â€œdanniâ€, per un gruppo di persone, come quelli definiti in termini di razza, genere, etÃ  o stato di disabilitÃ . I principali danni correlati all'equitÃ  possono essere classificati come:

- **Allocazione**, se un genere o un'etnia, ad esempio, Ã¨ favorito rispetto a un altro.
- **QualitÃ  del servizio**. Se i dati sono addestrati per uno scenario specifico ma la realtÃ  Ã¨ molto piÃ¹ complessa, si ottiene un servizio di scarsa qualitÃ . Ad esempio, un distributore di sapone che non riesce a rilevare persone con pelle scura. [Riferimento](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigrazione**. Criticare ingiustamente e etichettare qualcosa o qualcuno. Ad esempio, una tecnologia di etichettatura delle immagini ha infamemente etichettato immagini di persone con pelle scura come gorilla.
- **Sovra- o sotto-rappresentazione**. L'idea Ã¨ che un certo gruppo non sia visibile in una certa professione, e qualsiasi servizio o funzione che continua a promuovere ciÃ² contribuisce al danno.
- **Stereotipizzazione**. Associare un determinato gruppo a attributi preassegnati. Ad esempio, un sistema di traduzione tra inglese e turco potrebbe avere imprecisioni dovute a parole con associazioni stereotipate di genere.

![traduzione in turco](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.it.png)
> traduzione in turco

![traduzione di ritorno in inglese](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.it.png)
> traduzione di ritorno in inglese

Quando progettiamo e testiamo sistemi di AI, dobbiamo garantire che l'AI sia equa e non programmata per prendere decisioni discriminatorie o di parte, che anche gli esseri umani sono proibiti dal fare. Garantire l'equitÃ  nell'AI e nel machine learning rimane una sfida sociotecnica complessa.

### AffidabilitÃ  e sicurezza

Per costruire fiducia, i sistemi di AI devono essere affidabili, sicuri e coerenti in condizioni normali e inattese. Ãˆ importante sapere come i sistemi di AI si comporteranno in una varietÃ  di situazioni, specialmente quando si tratta di anomalie. Quando si costruiscono soluzioni di AI, Ã¨ necessario concentrarsi in modo sostanziale su come gestire una vasta gamma di circostanze che le soluzioni di AI potrebbero incontrare. Ad esempio, un'auto a guida autonoma deve mettere la sicurezza delle persone come prioritÃ  assoluta. Di conseguenza, l'AI che alimenta l'auto deve considerare tutti gli scenari possibili che l'auto potrebbe incontrare, come notte, temporali o bufere di neve, bambini che attraversano la strada, animali domestici, lavori stradali, ecc. Quanto bene un sistema di AI puÃ² gestire una vasta gamma di condizioni in modo affidabile e sicuro riflette il livello di anticipazione che il data scientist o sviluppatore di AI ha considerato durante la progettazione o il test del sistema.

> [ðŸŽ¥ Clicca qui per un video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### InclusivitÃ 

I sistemi di AI dovrebbero essere progettati per coinvolgere e potenziare tutti. Quando progettano e implementano sistemi di AI, i data scientist e gli sviluppatori di AI identificano e affrontano potenziali barriere nel sistema che potrebbero escludere involontariamente le persone. Ad esempio, ci sono 1 miliardo di persone con disabilitÃ  in tutto il mondo. Con l'avanzamento dell'AI, possono accedere a una vasta gamma di informazioni e opportunitÃ  piÃ¹ facilmente nella loro vita quotidiana. Affrontando le barriere, si creano opportunitÃ  per innovare e sviluppare prodotti di AI con esperienze migliori che beneficiano tutti.

> [ðŸŽ¥ Clicca qui per un video: inclusivitÃ  nell'AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sicurezza e privacy

I sistemi di AI dovrebbero essere sicuri e rispettare la privacy delle persone. Le persone hanno meno fiducia nei sistemi che mettono a rischio la loro privacy, informazioni o vite. Quando addestriamo modelli di machine learning, ci affidiamo ai dati per ottenere i migliori risultati. Facendo ciÃ², l'origine dei dati e la loro integritÃ  devono essere considerate. Ad esempio, i dati sono stati forniti dagli utenti o sono pubblicamente disponibili? Inoltre, mentre si lavora con i dati, Ã¨ cruciale sviluppare sistemi di AI che possano proteggere informazioni riservate e resistere agli attacchi. Con l'aumento dell'uso dell'AI, proteggere la privacy e garantire la sicurezza delle informazioni personali e aziendali sta diventando sempre piÃ¹ critico e complesso. Le questioni di privacy e sicurezza dei dati richiedono particolare attenzione per l'AI, poichÃ© l'accesso ai dati Ã¨ essenziale affinchÃ© i sistemi di AI possano fare previsioni e decisioni accurate e informate sulle persone.

> [ðŸŽ¥ Clicca qui per un video: sicurezza nell'AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Come industria, abbiamo fatto significativi progressi nella privacy e sicurezza, alimentati significativamente da regolamenti come il GDPR (Regolamento Generale sulla Protezione dei Dati).
- Tuttavia, con i sistemi di AI dobbiamo riconoscere la tensione tra la necessitÃ  di piÃ¹ dati personali per rendere i sistemi piÃ¹ personali ed efficaci â€“ e la privacy.
- Proprio come con la nascita dei computer connessi a Internet, stiamo anche assistendo a un enorme aumento del numero di problemi di sicurezza legati all'AI.
- Allo stesso tempo, abbiamo visto l'AI essere utilizzata per migliorare la sicurezza. Ad esempio, la maggior parte degli scanner antivirus moderni Ã¨ guidata da euristiche di AI.
- Dobbiamo garantire che i nostri processi di Data Science si armonizzino con le ultime pratiche di privacy e sicurezza.

### Trasparenza

I sistemi di AI dovrebbero essere comprensibili. Una parte cruciale della trasparenza Ã¨ spiegare il comportamento dei sistemi di AI e dei loro componenti. Migliorare la comprensione dei sistemi di AI richiede che gli stakeholder comprendano come e perchÃ© funzionano, in modo che possano identificare potenziali problemi di prestazione, preoccupazioni sulla sicurezza e privacy, bias, pratiche esclusive o risultati non intenzionali. Crediamo anche che coloro che utilizzano sistemi di AI dovrebbero essere onesti e trasparenti su quando, perchÃ© e come scelgono di implementarli, cosÃ¬ come sui limiti dei sistemi che utilizzano. Ad esempio, se una banca utilizza un sistema di AI per supportare le sue decisioni di prestito ai consumatori, Ã¨ importante esaminare i risultati e capire quali dati influenzano le raccomandazioni del sistema. I governi stanno iniziando a regolamentare l'AI in vari settori, quindi i data scientist e le organizzazioni devono spiegare se un sistema di AI soddisfa i requisiti normativi, specialmente quando c'Ã¨ un risultato indesiderato.

> [ðŸŽ¥ Clicca qui per un video: trasparenza nell'AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- PoichÃ© i sistemi di AI sono cosÃ¬ complessi, Ã¨ difficile capire come funzionano e interpretare i risultati.
- Questa mancanza di comprensione influisce sul modo in cui questi sistemi vengono gestiti, operativizzati e documentati.
- Ancora piÃ¹ importante, questa mancanza di comprensione influisce sulle decisioni prese utilizzando i risultati prodotti da questi sistemi.

### ResponsabilitÃ 

Le persone che progettano e implementano sistemi di AI devono essere responsabili di come i loro sistemi operano. La necessitÃ  di responsabilitÃ  Ã¨ particolarmente cruciale con tecnologie sensibili come il riconoscimento facciale. Recentemente, c'Ã¨ stata una crescente domanda di tecnologia di riconoscimento facciale, soprattutto da parte delle organizzazioni di polizia che vedono il potenziale della tecnologia in usi come trovare bambini scomparsi. Tuttavia, queste tecnologie potrebbero essere utilizzate da un governo per mettere a rischio le libertÃ  fondamentali dei cittadini, ad esempio, consentendo la sorveglianza continua di individui specifici. Pertanto, i data scientist e le organizzazioni devono essere responsabili di come il loro sistema di AI impatta individui o societÃ .

[![Ricercatore di AI avverte dei rischi di sorveglianza di massa attraverso il riconoscimento facciale](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.it.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Approccio di Microsoft all'AI responsabile")

> ðŸŽ¥ Clicca sull'immagine sopra per un video: Avvertimenti sulla sorveglianza di massa attraverso il riconoscimento facciale

Alla fine, una delle domande piÃ¹ grandi per la nostra generazione, come la prima generazione che sta portando l'AI nella societÃ , Ã¨ come garantire che i computer rimangano responsabili verso le persone e come garantire che le persone che progettano i computer rimangano responsabili verso tutti gli altri.

## Valutazione dell'impatto

Prima di addestrare un modello di machine learning, Ã¨ importante condurre una valutazione dell'impatto per comprendere lo scopo del sistema di AI; quale sia l'uso previsto; dove sarÃ  implementato; e chi interagirÃ  con il sistema. Questi aspetti sono utili per i revisori o tester che valutano il sistema per sapere quali fattori considerare quando identificano potenziali rischi e conseguenze attese.

Le seguenti aree sono di interesse quando si conduce una valutazione dell'impatto:

* **Impatto negativo sugli individui**. Essere consapevoli di eventuali restrizioni o requisiti, usi non supportati o limitazioni note che ostacolano le prestazioni del sistema Ã¨ vitale per garantire che il sistema non venga utilizzato in modo da causare danni agli individui.
* **Requisiti dei dati**. Comprendere come e dove il sistema utilizzerÃ  i dati consente ai revisori di esplorare eventuali requisiti di dati di cui tenere conto (ad esempio, regolamenti GDPR o HIPPA). Inoltre, esaminare se la fonte o la quantitÃ  di dati Ã¨ sufficiente per l'addestramento.
* **Riepilogo dell'impatto**. Raccogliere un elenco di potenziali danni che potrebbero derivare dall'uso del sistema. Durante il ciclo di vita del ML, verificare se i problemi identificati sono mitigati o affrontati.
* **Obiettivi applicabili** per ciascuno dei sei principi fondamentali. Valutare se gli obiettivi di ciascun principio sono soddisfatti e se ci sono lacune.

## Debugging con AI responsabile

Simile al debugging di un'applicazione software, il debugging di un sistema di AI Ã¨ un processo necessario per identificare e risolvere problemi nel sistema. Ci sono molti fattori che possono influenzare un modello che non funziona come previsto o in modo responsabile. La maggior parte delle metriche tradizionali di prestazione del modello sono aggregati quantitativi delle prestazioni del modello, che non sono sufficienti per analizzare come un modello viola i principi di AI responsabile. Inoltre, un modello di machine learning Ã¨ una scatola nera che rende difficile capire cosa guida il suo risultato o fornire spiegazioni quando commette un errore. PiÃ¹ avanti in questo corso, impareremo come utilizzare il dashboard di AI responsabile per aiutare a fare debugging dei sistemi di AI. Il dashboard fornisce uno strumento olistico per i data scientist e gli sviluppatori di AI per eseguire:

* **Analisi degli errori**. Per identificare la distribuzione degli errori del modello che puÃ² influenzare l'equitÃ  o l'affidabilitÃ  del sistema.
* **Panoramica del modello**. Per scoprire dove ci sono disparitÃ  nelle prestazioni del modello tra i diversi gruppi di dati.
* **Analisi dei dati**. Per comprendere la distribuzione dei dati e identificare eventuali bias nei dati che potrebbero portare a problemi di equitÃ , inclusivitÃ  e affidabilitÃ .
* **InterpretabilitÃ  del modello**. Per capire cosa influenza o determina le previsioni del modello. Questo aiuta a spiegare il comportamento del modello, che Ã¨ importante per la trasparenza e la responsabilitÃ .

## ðŸš€ Sfida

Per prevenire l'introduzione di danni fin dall'inizio, dovremmo:

- avere una diversitÃ  di background e prospettive tra le persone che lavorano sui sistemi
- investire in dataset che riflettano la diversitÃ  della nostra societÃ 
- sviluppare metodi migliori durante il ciclo di vita del machine learning per rilevare e correggere l'AI responsabile quando si verifica

Pensa a scenari reali in cui l'inaffidabilitÃ  di un modello Ã¨ evidente nella costruzione e nell'uso del modello. Cos'altro dovremmo considerare?

## [Quiz post-lezione](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revisione e studio autonomo
In questa lezione, hai appreso alcune basi sui concetti di equitÃ  e iniquitÃ  nell'apprendimento automatico.  

Guarda questo workshop per approfondire gli argomenti: 

- Alla ricerca di un'IA responsabile: Portare i principi nella pratica di Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

[![Responsible AI Toolbox: Un framework open-source per costruire un'IA responsabile](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Un framework open-source per costruire un'IA responsabile")


> ðŸŽ¥ Clicca sull'immagine sopra per un video: RAI Toolbox: Un framework open-source per costruire un'IA responsabile di Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

Leggi anche: 

- Centro risorse RAI di Microsoft: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Gruppo di ricerca FATE di Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox: 

- [Repository GitHub di Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Leggi gli strumenti di Azure Machine Learning per garantire l'equitÃ :

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## Compito

[Esplora RAI Toolbox](assignment.md)

---

**Disclaimer**:  
Questo documento Ã¨ stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.