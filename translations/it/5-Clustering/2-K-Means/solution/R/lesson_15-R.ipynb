{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  },
  "colab": {
   "name": "lesson_14.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "coopTranslator": {
   "original_hash": "ad65fb4aad0a156b42216e4929f490fc",
   "translation_date": "2025-08-29T23:40:29+00:00",
   "source_file": "5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb",
   "language_code": "it"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GULATlQXLXyR"
   },
   "source": [
    "## Esplora il clustering K-Means utilizzando R e i principi dei dati Tidy.\n",
    "\n",
    "### [**Quiz pre-lezione**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)\n",
    "\n",
    "In questa lezione, imparerai a creare cluster utilizzando il pacchetto Tidymodels e altri pacchetti dell'ecosistema R (li chiameremo amici üßë‚Äçü§ù‚Äçüßë), insieme al dataset di musica nigeriana che hai importato in precedenza. Tratteremo le basi del K-Means per il clustering. Tieni presente che, come hai appreso nella lezione precedente, ci sono molti modi per lavorare con i cluster e il metodo che utilizzi dipende dai tuoi dati. Proveremo il K-Means poich√© √® la tecnica di clustering pi√π comune. Iniziamo!\n",
    "\n",
    "Termini che imparerai:\n",
    "\n",
    "-   Punteggio silhouette\n",
    "\n",
    "-   Metodo del gomito\n",
    "\n",
    "-   Inerzia\n",
    "\n",
    "-   Varianza\n",
    "\n",
    "### **Introduzione**\n",
    "\n",
    "[Il clustering K-Means](https://wikipedia.org/wiki/K-means_clustering) √® un metodo derivato dal campo dell'elaborazione del segnale. Viene utilizzato per dividere e raggruppare gruppi di dati in `k cluster` basandosi sulle somiglianze delle loro caratteristiche.\n",
    "\n",
    "I cluster possono essere visualizzati come [diagrammi di Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), che includono un punto (o 'seme') e la sua corrispondente regione.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/voronoi.png\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Infografica di Jen Looper</figcaption>\n",
    "\n",
    "\n",
    "Il clustering K-Means segue i seguenti passaggi:\n",
    "\n",
    "1.  Lo scienziato dei dati inizia specificando il numero desiderato di cluster da creare.\n",
    "\n",
    "2.  Successivamente, l'algoritmo seleziona casualmente K osservazioni dal dataset per fungere da centri iniziali per i cluster (cio√®, i centroidi).\n",
    "\n",
    "3.  Ogni osservazione rimanente viene quindi assegnata al suo centroide pi√π vicino.\n",
    "\n",
    "4.  Successivamente, vengono calcolate le nuove medie di ciascun cluster e il centroide viene spostato verso la media.\n",
    "\n",
    "5.  Ora che i centri sono stati ricalcolati, ogni osservazione viene nuovamente controllata per verificare se potrebbe essere pi√π vicina a un cluster diverso. Tutti gli oggetti vengono riassegnati utilizzando le medie dei cluster aggiornate. I passaggi di assegnazione dei cluster e aggiornamento dei centroidi vengono ripetuti iterativamente fino a quando le assegnazioni dei cluster smettono di cambiare (cio√®, quando si raggiunge la convergenza). Tipicamente, l'algoritmo termina quando ogni nuova iterazione comporta uno spostamento trascurabile dei centroidi e i cluster diventano statici.\n",
    "\n",
    "<div>\n",
    "\n",
    "> Nota che, a causa della randomizzazione delle k osservazioni iniziali utilizzate come centroidi di partenza, possiamo ottenere risultati leggermente diversi ogni volta che applichiamo la procedura. Per questo motivo, la maggior parte degli algoritmi utilizza diversi *avvii casuali* e sceglie l'iterazione con il WCSS pi√π basso. Pertanto, √® fortemente consigliato eseguire sempre il K-Means con diversi valori di *nstart* per evitare un *ottimo locale indesiderato.*\n",
    "\n",
    "</div>\n",
    "\n",
    "Questa breve animazione, utilizzando le [illustrazioni](https://github.com/allisonhorst/stats-illustrations) di Allison Horst, spiega il processo di clustering:\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/kmeans.gif\"\n",
    "   width=\"550\"/>\n",
    "   <figcaption>Illustrazione di @allison_horst</figcaption>\n",
    "\n",
    "\n",
    "\n",
    "Una domanda fondamentale che sorge nel clustering √® questa: come si fa a sapere in quanti cluster separare i dati? Uno svantaggio dell'utilizzo del K-Means √® che sar√† necessario stabilire `k`, ovvero il numero di `centroidi`. Fortunatamente, il `metodo del gomito` aiuta a stimare un buon valore iniziale per `k`. Lo proverai tra poco.\n",
    "\n",
    "### \n",
    "\n",
    "**Prerequisiti**\n",
    "\n",
    "Riprenderemo esattamente da dove ci siamo fermati nella [lezione precedente](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb), dove abbiamo analizzato il dataset, creato molte visualizzazioni e filtrato il dataset per osservazioni di interesse. Assicurati di dare un'occhiata!\n",
    "\n",
    "Avremo bisogno di alcuni pacchetti per completare questo modulo. Puoi installarli con: `install.packages(c('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork'))`\n",
    "\n",
    "In alternativa, lo script qui sotto verifica se hai i pacchetti necessari per completare questo modulo e li installa per te nel caso in cui ne manchino alcuni.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ah_tBi58LXyi"
   },
   "source": [
    "suppressWarnings(if(!require(\"pacman\")) install.packages(\"pacman\"))\n",
    "\n",
    "pacman::p_load('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e--UCUTLXym"
   },
   "source": [
    "## 1. Un ballo con i dati: Restringi ai 3 generi musicali pi√π popolari\n",
    "\n",
    "Questo √® un riepilogo di ci√≤ che abbiamo fatto nella lezione precedente. Analizziamo un po' di dati!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ycamx7GGLXyn"
   },
   "source": [
    "# Load the core tidyverse and make it available in your current R session\n",
    "library(tidyverse)\n",
    "\n",
    "# Import the data into a tibble\n",
    "df <- read_csv(file = \"https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/data/nigerian-songs.csv\", show_col_types = FALSE)\n",
    "\n",
    "# Narrow down to top 3 popular genres\n",
    "nigerian_songs <- df %>% \n",
    "  # Concentrate on top 3 genres\n",
    "  filter(artist_top_genre %in% c(\"afro dancehall\", \"afropop\",\"nigerian pop\")) %>% \n",
    "  # Remove unclassified observations\n",
    "  filter(popularity != 0)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize popular genres using bar plots\n",
    "theme_set(theme_light())\n",
    "nigerian_songs %>%\n",
    "  count(artist_top_genre) %>%\n",
    "  ggplot(mapping = aes(x = artist_top_genre, y = n,\n",
    "                       fill = artist_top_genre)) +\n",
    "  geom_col(alpha = 0.8) +\n",
    "  paletteer::scale_fill_paletteer_d(\"ggsci::category10_d3\") +\n",
    "  ggtitle(\"Top genres\") +\n",
    "  theme(plot.title = element_text(hjust = 0.5))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5h5zmkPLXyp"
   },
   "source": [
    "ü§© √à andata bene!\n",
    "\n",
    "## 2. Ulteriore esplorazione dei dati.\n",
    "\n",
    "Quanto sono puliti questi dati? Controlliamo la presenza di valori anomali utilizzando i box plot. Ci concentreremo sulle colonne numeriche con meno valori anomali (anche se potresti eliminare i valori anomali). I box plot possono mostrare l'intervallo dei dati e aiutare a scegliere quali colonne utilizzare. Nota, i box plot non mostrano la varianza, un elemento importante per dati ben clusterizzabili. Consulta [questa discussione](https://stats.stackexchange.com/questions/91536/deduce-variance-from-boxplot) per ulteriori approfondimenti.\n",
    "\n",
    "I [box plot](https://en.wikipedia.org/wiki/Box_plot) vengono utilizzati per rappresentare graficamente la distribuzione dei dati `numerici`, quindi iniziamo *selezionando* tutte le colonne numeriche insieme ai generi musicali pi√π popolari.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HhNreJKLLXyq"
   },
   "source": [
    "# Select top genre column and all other numeric columns\n",
    "df_numeric <- nigerian_songs %>% \n",
    "  select(artist_top_genre, where(is.numeric)) \n",
    "\n",
    "# Display the data\n",
    "df_numeric %>% \n",
    "  slice_head(n = 5)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYXrwJRaLXyq"
   },
   "source": [
    "Vedi come il selettore `where` rende tutto pi√π semplice üíÅ? Esplora altre funzioni simili [qui](https://tidyselect.r-lib.org/).\n",
    "\n",
    "Poich√© creeremo un boxplot per ogni caratteristica numerica e vogliamo evitare di usare cicli, riformattiamo i nostri dati in un formato *pi√π lungo* che ci permetter√† di sfruttare i `facets` - sottotrame che mostrano ciascuna un sottoinsieme dei dati.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gd5bR3f8LXys"
   },
   "source": [
    "# Pivot data from wide to long\n",
    "df_numeric_long <- df_numeric %>% \n",
    "  pivot_longer(!artist_top_genre, names_to = \"feature_names\", values_to = \"values\") \n",
    "\n",
    "# Print out data\n",
    "df_numeric_long %>% \n",
    "  slice_head(n = 15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7tE1swnLXyv"
   },
   "source": [
    "Molto pi√π lungo! Ora √® il momento di alcuni `ggplots`! Quindi quale `geom` useremo?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r88bIsyuLXyy"
   },
   "source": [
    "# Make a box plot\n",
    "df_numeric_long %>% \n",
    "  ggplot(mapping = aes(x = feature_names, y = values, fill = feature_names)) +\n",
    "  geom_boxplot() +\n",
    "  facet_wrap(~ feature_names, ncol = 4, scales = \"free\") +\n",
    "  theme(legend.position = \"none\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYVyKIUELXyz"
   },
   "source": [
    "Facile-gg!\n",
    "\n",
    "Ora possiamo vedere che questi dati sono un po' rumorosi: osservando ogni colonna come un boxplot, si possono notare dei valori anomali. Potresti esaminare il dataset e rimuovere questi valori anomali, ma ci√≤ renderebbe i dati piuttosto ridotti.\n",
    "\n",
    "Per il momento, scegliamo quali colonne utilizzare per il nostro esercizio di clustering. Selezioniamo le colonne numeriche con intervalli simili. Potremmo codificare `artist_top_genre` come numerico, ma per ora lo escluderemo.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wkpINyZLXy0"
   },
   "source": [
    "# Select variables with similar ranges\n",
    "df_numeric_select <- df_numeric %>% \n",
    "  select(popularity, danceability, acousticness, loudness, energy) \n",
    "\n",
    "# Normalize data\n",
    "# df_numeric_select <- scale(df_numeric_select)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7dLzgpqLXy1"
   },
   "source": [
    "## 3. Calcolo del clustering k-means in R\n",
    "\n",
    "Possiamo calcolare il k-means in R utilizzando la funzione integrata `kmeans`, consulta `help(\"kmeans()\")`. La funzione `kmeans()` accetta un data frame con tutte le colonne numeriche come suo argomento principale.\n",
    "\n",
    "Il primo passo nell'utilizzo del clustering k-means √® specificare il numero di cluster (k) che verranno generati nella soluzione finale. Sappiamo che ci sono 3 generi musicali che abbiamo estratto dal dataset, quindi proviamo con 3:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uC4EQ5w7LXy5"
   },
   "source": [
    "set.seed(2056)\n",
    "# Kmeans clustering for 3 clusters\n",
    "kclust <- kmeans(\n",
    "  df_numeric_select,\n",
    "  # Specify the number of clusters\n",
    "  centers = 3,\n",
    "  # How many random initial configurations\n",
    "  nstart = 25\n",
    ")\n",
    "\n",
    "# Display clustering object\n",
    "kclust\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzfhscWrLXy-"
   },
   "source": [
    "L'oggetto kmeans contiene diverse informazioni ben spiegate in `help(\"kmeans()\")`. Per ora, concentriamoci su alcune di esse. Vediamo che i dati sono stati raggruppati in 3 cluster di dimensioni 65, 110, 111. L'output contiene anche i centri dei cluster (medie) per i 3 gruppi rispetto alle 5 variabili.\n",
    "\n",
    "Il vettore di clustering rappresenta l'assegnazione del cluster per ogni osservazione. Utilizziamo la funzione `augment` per aggiungere l'assegnazione del cluster al set di dati originale.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0XwwpFGQLXy_"
   },
   "source": [
    "# Add predicted cluster assignment to data set\n",
    "augment(kclust, df_numeric_select) %>% \n",
    "  relocate(.cluster) %>% \n",
    "  slice_head(n = 10)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXIVXXACLXzA"
   },
   "source": [
    "Perfetto, abbiamo appena suddiviso il nostro set di dati in un insieme di 3 gruppi. Quindi, quanto √® buono il nostro clustering ü§∑? Diamo un'occhiata al `Silhouette score`.\n",
    "\n",
    "### **Silhouette score**\n",
    "\n",
    "[L'analisi del silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)) pu√≤ essere utilizzata per studiare la distanza di separazione tra i cluster risultanti. Questo punteggio varia da -1 a 1, e se il punteggio √® vicino a 1, il cluster √® denso e ben separato dagli altri cluster. Un valore vicino a 0 rappresenta cluster sovrapposti con campioni molto vicini al confine decisionale dei cluster vicini. [fonte](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam).\n",
    "\n",
    "Il metodo della silhouette media calcola la silhouette media delle osservazioni per diversi valori di *k*. Un punteggio medio di silhouette alto indica un buon clustering.\n",
    "\n",
    "La funzione `silhouette` nel pacchetto cluster serve per calcolare la larghezza media della silhouette.\n",
    "\n",
    "> La silhouette pu√≤ essere calcolata con qualsiasi [metrica di distanza](https://en.wikipedia.org/wiki/Distance \"Distance\"), come la [distanza euclidea](https://en.wikipedia.org/wiki/Euclidean_distance \"Euclidean distance\") o la [distanza di Manhattan](https://en.wikipedia.org/wiki/Manhattan_distance \"Manhattan distance\") di cui abbiamo discusso nella [lezione precedente](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jn0McL28LXzB"
   },
   "source": [
    "# Load cluster package\n",
    "library(cluster)\n",
    "\n",
    "# Compute average silhouette score\n",
    "ss <- silhouette(kclust$cluster,\n",
    "                 # Compute euclidean distance\n",
    "                 dist = dist(df_numeric_select))\n",
    "mean(ss[, 3])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyQRn97nLXzC"
   },
   "source": [
    "Il nostro punteggio √® **.549**, quindi esattamente a met√†. Questo indica che i nostri dati non sono particolarmente adatti a questo tipo di clustering. Vediamo se possiamo confermare questa ipotesi visivamente. Il [pacchetto factoextra](https://rpkgs.datanovia.com/factoextra/index.html) fornisce funzioni (`fviz_cluster()`) per visualizzare il clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7a6Km1_FLXzD"
   },
   "source": [
    "library(factoextra)\n",
    "\n",
    "# Visualize clustering results\n",
    "fviz_cluster(kclust, df_numeric_select)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwCWt-0LXzD"
   },
   "source": [
    "La sovrapposizione nei cluster indica che i nostri dati non sono particolarmente adatti a questo tipo di clustering, ma continuiamo comunque.\n",
    "\n",
    "## 4. Determinare il numero ottimale di cluster\n",
    "\n",
    "Una domanda fondamentale che spesso emerge nel clustering K-Means √® questa: senza etichette di classe conosciute, come si pu√≤ sapere in quanti cluster suddividere i dati?\n",
    "\n",
    "Un modo per scoprirlo √® utilizzare un campione di dati per `creare una serie di modelli di clustering` con un numero crescente di cluster (ad esempio da 1 a 10) e valutare metriche di clustering come il **Silhouette score.**\n",
    "\n",
    "Determiniamo il numero ottimale di cluster calcolando l'algoritmo di clustering per diversi valori di *k* e valutando il **Within Cluster Sum of Squares** (WCSS). La somma totale dei quadrati all'interno del cluster (WCSS) misura la compattezza del clustering e vogliamo che sia il pi√π piccolo possibile, poich√© valori pi√π bassi indicano che i punti dati sono pi√π vicini tra loro.\n",
    "\n",
    "Esploriamo l'effetto di diverse scelte di `k`, da 1 a 10, su questo clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hSeIiylDLXzE"
   },
   "source": [
    "# Create a series of clustering models\n",
    "kclusts <- tibble(k = 1:10) %>% \n",
    "  # Perform kmeans clustering for 1,2,3 ... ,10 clusters\n",
    "  mutate(model = map(k, ~ kmeans(df_numeric_select, centers = .x, nstart = 25)),\n",
    "  # Farm out clustering metrics eg WCSS\n",
    "         glanced = map(model, ~ glance(.x))) %>% \n",
    "  unnest(cols = glanced)\n",
    "  \n",
    "\n",
    "# View clustering rsulsts\n",
    "kclusts\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7rS2U1eLXzE"
   },
   "source": [
    "Ora che abbiamo il totale della somma dei quadrati intra-cluster (tot.withinss) per ciascun algoritmo di clustering con centro *k*, utilizziamo il [metodo del gomito](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) per trovare il numero ottimale di cluster. Il metodo consiste nel tracciare il WCSS in funzione del numero di cluster e scegliere il [gomito della curva](https://en.wikipedia.org/wiki/Elbow_of_the_curve \"Elbow of the curve\") come il numero di cluster da utilizzare.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o_DjHGItLXzF"
   },
   "source": [
    "set.seed(2056)\n",
    "# Use elbow method to determine optimum number of clusters\n",
    "kclusts %>% \n",
    "  ggplot(mapping = aes(x = k, y = tot.withinss)) +\n",
    "  geom_line(size = 1.2, alpha = 0.8, color = \"#FF7F0EFF\") +\n",
    "  geom_point(size = 2, color = \"#FF7F0EFF\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLYyt5XSLXzG"
   },
   "source": [
    "Il grafico mostra una grande riduzione in WCSS (quindi maggiore *compattezza*) man mano che il numero di cluster aumenta da uno a due, e una ulteriore riduzione evidente da due a tre cluster. Dopo di ci√≤, la riduzione √® meno marcata, risultando in un `gomito` üí™ nel grafico intorno a tre cluster. Questo √® un buon indicatore che ci sono due o tre cluster di punti dati ragionevolmente ben separati.\n",
    "\n",
    "Possiamo ora procedere ed estrarre il modello di clustering dove `k = 3`:\n",
    "\n",
    "> `pull()`: utilizzato per estrarre una singola colonna\n",
    ">\n",
    "> `pluck()`: utilizzato per indicizzare strutture dati come liste\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JP_JPKBILXzG"
   },
   "source": [
    "# Extract k = 3 clustering\n",
    "final_kmeans <- kclusts %>% \n",
    "  filter(k == 3) %>% \n",
    "  pull(model) %>% \n",
    "  pluck(1)\n",
    "\n",
    "\n",
    "final_kmeans\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_PDTu8tLXzI"
   },
   "source": [
    "Fantastico! Procediamo a visualizzare i cluster ottenuti. Ti interessa un po' di interattivit√† utilizzando `plotly`?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dNcleFe-LXzJ"
   },
   "source": [
    "# Add predicted cluster assignment to data set\n",
    "results <-  augment(final_kmeans, df_numeric_select) %>% \n",
    "  bind_cols(df_numeric %>% select(artist_top_genre)) \n",
    "\n",
    "# Plot cluster assignments\n",
    "clust_plt <- results %>% \n",
    "  ggplot(mapping = aes(x = popularity, y = danceability, color = .cluster, shape = artist_top_genre)) +\n",
    "  geom_point(size = 2, alpha = 0.8) +\n",
    "  paletteer::scale_color_paletteer_d(\"ggthemes::Tableau_10\")\n",
    "\n",
    "ggplotly(clust_plt)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JUM_51VLXzK"
   },
   "source": [
    "Forse ci saremmo aspettati che ogni cluster (rappresentato da colori diversi) avesse generi distinti (rappresentati da forme diverse).\n",
    "\n",
    "Diamo un'occhiata alla precisione del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HdIMUGq7LXzL"
   },
   "source": [
    "# Assign genres to predefined integers\n",
    "label_count <- results %>% \n",
    "  group_by(artist_top_genre) %>% \n",
    "  mutate(id = cur_group_id()) %>% \n",
    "  ungroup() %>% \n",
    "  summarise(correct_labels = sum(.cluster == id))\n",
    "\n",
    "\n",
    "# Print results  \n",
    "cat(\"Result:\", label_count$correct_labels, \"out of\", nrow(results), \"samples were correctly labeled.\")\n",
    "\n",
    "cat(\"\\nAccuracy score:\", label_count$correct_labels/nrow(results))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C50wvaAOLXzM"
   },
   "source": [
    "La precisione di questo modello non √® male, ma nemmeno eccezionale. Potrebbe essere che i dati non si prestino bene al clustering con K-Means. Questi dati sono troppo sbilanciati, poco correlati e presentano troppa varianza tra i valori delle colonne per essere clusterizzati in modo efficace. Infatti, i cluster che si formano sono probabilmente fortemente influenzati o distorti dalle tre categorie di genere che abbiamo definito sopra.\n",
    "\n",
    "Tuttavia, √® stato un processo di apprendimento interessante!\n",
    "\n",
    "Nella documentazione di Scikit-learn, si pu√≤ vedere che un modello come questo, con cluster non molto ben definiti, presenta un problema di \"varianza\":\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/problems.png\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Infografica da Scikit-learn</figcaption>\n",
    "\n",
    "\n",
    "\n",
    "## **Varianza**\n",
    "\n",
    "La varianza √® definita come \"la media delle differenze al quadrato rispetto alla media\" [fonte](https://www.mathsisfun.com/data/standard-deviation.html). Nel contesto di questo problema di clustering, si riferisce al fatto che i numeri del nostro dataset tendono a divergere un po' troppo dalla media.\n",
    "\n",
    "‚úÖ Questo √® un ottimo momento per pensare a tutti i modi in cui potresti correggere questo problema. Modificare un po' i dati? Usare colonne diverse? Utilizzare un algoritmo differente? Suggerimento: prova a [scalare i tuoi dati](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) per normalizzarli e testare altre colonne.\n",
    "\n",
    "> Prova questo '[calcolatore di varianza](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' per comprendere meglio il concetto.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **üöÄSfida**\n",
    "\n",
    "Dedica un po' di tempo a questo notebook, modificando i parametri. Riesci a migliorare la precisione del modello pulendo ulteriormente i dati (ad esempio, rimuovendo i valori anomali)? Puoi utilizzare pesi per dare maggiore importanza a determinati campioni di dati. Cos'altro puoi fare per creare cluster migliori?\n",
    "\n",
    "Suggerimento: prova a scalare i tuoi dati. Nel notebook c'√® del codice commentato che aggiunge una scalatura standard per far s√¨ che le colonne dei dati si somiglino di pi√π in termini di intervallo. Noterai che, mentre il punteggio silhouette diminuisce, il \"gomito\" nel grafico del gomito si appiana. Questo accade perch√© lasciare i dati non scalati permette ai dati con minore varianza di avere un peso maggiore. Leggi di pi√π su questo problema [qui](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).\n",
    "\n",
    "## [**Quiz post-lezione**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)\n",
    "\n",
    "## **Revisione e Studio Autonomo**\n",
    "\n",
    "-   Dai un'occhiata a un simulatore di K-Means [come questo](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Puoi utilizzare questo strumento per visualizzare punti dati di esempio e determinare i loro centroidi. Puoi modificare la casualit√† dei dati, il numero di cluster e il numero di centroidi. Questo ti aiuta a farti un'idea di come i dati possono essere raggruppati?\n",
    "\n",
    "-   Consulta anche [questo documento su K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) di Stanford.\n",
    "\n",
    "Vuoi mettere alla prova le tue nuove competenze di clustering con dataset che si prestano bene al clustering con K-Means? Consulta:\n",
    "\n",
    "-   [Addestrare e Valutare Modelli di Clustering](https://rpubs.com/eR_ic/clustering) utilizzando Tidymodels e altri strumenti\n",
    "\n",
    "-   [Analisi dei Cluster con K-Means](https://uc-r.github.io/kmeans_clustering), Guida alla Programmazione R per l'Analisi Aziendale dell'UC\n",
    "\n",
    "- [Clustering con K-Means e principi di dati ordinati](https://www.tidymodels.org/learn/statistics/k-means/)\n",
    "\n",
    "## **Compito**\n",
    "\n",
    "[Prova metodi di clustering diversi](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/assignment.md)\n",
    "\n",
    "## GRAZIE A:\n",
    "\n",
    "[Jen Looper](https://www.twitter.com/jenlooper) per aver creato la versione originale in Python di questo modulo ‚ô•Ô∏è\n",
    "\n",
    "[`Allison Horst`](https://twitter.com/allison_horst/) per aver creato le incredibili illustrazioni che rendono R pi√π accogliente e coinvolgente. Trova altre illustrazioni nella sua [galleria](https://www.google.com/url?q=https://github.com/allisonhorst/stats-illustrations&sa=D&source=editors&ust=1626380772530000&usg=AOvVaw3zcfyCizFQZpkSLzxiiQEM).\n",
    "\n",
    "Buono studio,\n",
    "\n",
    "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/r_learners_sm.jpeg\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Illustrazione di @allison_horst</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento √® stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche potrebbero contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un esperto umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.\n"
   ]
  }
 ]
}