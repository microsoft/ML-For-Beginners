<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-06T07:14:05+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "pa"
}
-->
## ਪੂਰਵ ਸ਼ਰਤਾਂ

ਇਸ ਪਾਠ ਵਿੱਚ, ਅਸੀਂ **OpenAI Gym** ਨਾਂ ਦੀ ਲਾਇਬ੍ਰੇਰੀ ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ ਜੋ ਵੱਖ-ਵੱਖ **ਵਾਤਾਵਰਣਾਂ** ਦੀ ਨਕਲ ਕਰਦੀ ਹੈ। ਤੁਸੀਂ ਇਸ ਪਾਠ ਦਾ ਕੋਡ ਆਪਣੇ ਸਥਾਨਕ ਸਿਸਟਮ 'ਤੇ ਚਲਾ ਸਕਦੇ ਹੋ (ਜਿਵੇਂ ਕਿ Visual Studio Code ਤੋਂ), ਜਿਸ ਵਿੱਚ ਸਿਮੂਲੇਸ਼ਨ ਇੱਕ ਨਵੀਂ ਵਿੰਡੋ ਵਿੱਚ ਖੁੱਲੇਗਾ। ਜਦੋਂ ਤੁਸੀਂ ਕੋਡ ਆਨਲਾਈਨ ਚਲਾਉਂਦੇ ਹੋ, ਤਾਂ ਤੁਹਾਨੂੰ ਕੁਝ ਤਬਦੀਲੀਆਂ ਕਰਨ ਦੀ ਲੋੜ ਹੋ ਸਕਦੀ ਹੈ, ਜਿਵੇਂ ਕਿ [ਇੱਥੇ](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) ਵਰਣਨ ਕੀਤਾ ਗਿਆ ਹੈ।

## OpenAI Gym

ਪਿਛਲੇ ਪਾਠ ਵਿੱਚ, ਖੇਡ ਦੇ ਨਿਯਮ ਅਤੇ ਸਥਿਤੀ ਨੂੰ ਅਸੀਂ ਆਪਣੇ ਆਪ ਬਣਾਈ `Board` ਕਲਾਸ ਦੁਆਰਾ ਪਰਿਭਾਸ਼ਿਤ ਕੀਤਾ ਸੀ। ਇੱਥੇ ਅਸੀਂ ਇੱਕ ਖਾਸ **ਸਿਮੂਲੇਸ਼ਨ ਵਾਤਾਵਰਣ** ਦੀ ਵਰਤੋਂ ਕਰਾਂਗੇ, ਜੋ ਬੈਲੰਸਿੰਗ ਪੋਲ ਦੇ ਭੌਤਿਕ ਵਿਗਿਆਨ ਦੀ ਨਕਲ ਕਰੇਗਾ। ਰੀਇਨਫੋਰਸਮੈਂਟ ਲਰਨਿੰਗ ਐਲਗੋਰਿਥਮਾਂ ਨੂੰ ਸਿਖਾਉਣ ਲਈ ਸਭ ਤੋਂ ਪ੍ਰਸਿੱਧ ਸਿਮੂਲੇਸ਼ਨ ਵਾਤਾਵਰਣਾਂ ਵਿੱਚੋਂ ਇੱਕ ਹੈ [Gym](https://gym.openai.com/), ਜਿਸਨੂੰ [OpenAI](https://openai.com/) ਦੁਆਰਾ ਸੰਭਾਲਿਆ ਜਾਂਦਾ ਹੈ। ਇਸ Gym ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਸੀਂ ਵੱਖ-ਵੱਖ **ਵਾਤਾਵਰਣਾਂ** ਨੂੰ ਬਣਾਉਣਗੇ, ਜਿਵੇਂ ਕਿ CartPole ਸਿਮੂਲੇਸ਼ਨ ਤੋਂ Atari ਗੇਮਾਂ ਤੱਕ।

> **Note**: ਤੁਸੀਂ OpenAI Gym ਦੁਆਰਾ ਉਪਲਬਧ ਹੋਰ ਵਾਤਾਵਰਣਾਂ ਨੂੰ [ਇੱਥੇ](https://gym.openai.com/envs/#classic_control) ਵੇਖ ਸਕਦੇ ਹੋ।

ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਆਓ Gym ਨੂੰ ਇੰਸਟਾਲ ਕਰੀਏ ਅਤੇ ਲੋੜੀਂਦੇ ਲਾਇਬ੍ਰੇਰੀਆਂ ਨੂੰ ਇੰਪੋਰਟ ਕਰੀਏ (ਕੋਡ ਬਲਾਕ 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## ਅਭਿਆਸ - CartPole ਵਾਤਾਵਰਣ ਸ਼ੁਰੂ ਕਰੋ

CartPole ਬੈਲੰਸਿੰਗ ਸਮੱਸਿਆ 'ਤੇ ਕੰਮ ਕਰਨ ਲਈ, ਸਾਨੂੰ ਸੰਬੰਧਿਤ ਵਾਤਾਵਰਣ ਸ਼ੁਰੂ ਕਰਨ ਦੀ ਲੋੜ ਹੈ। ਹਰ ਵਾਤਾਵਰਣ ਨਾਲ ਜੁੜਿਆ ਹੁੰਦਾ ਹੈ:

- **Observation space**, ਜੋ ਸਾਨੂੰ ਵਾਤਾਵਰਣ ਤੋਂ ਪ੍ਰਾਪਤ ਜਾਣਕਾਰੀ ਦੀ ਬਣਾਵਟ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦਾ ਹੈ। CartPole ਸਮੱਸਿਆ ਲਈ, ਸਾਨੂੰ ਪੋਲ ਦੀ ਸਥਿਤੀ, ਗਤੀ ਅਤੇ ਕੁਝ ਹੋਰ ਮੁੱਲ ਪ੍ਰਾਪਤ ਹੁੰਦੇ ਹਨ।

- **Action space**, ਜੋ ਸੰਭਾਵਿਤ ਕਿਰਿਆਵਾਂ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦਾ ਹੈ। ਸਾਡੇ ਕੇਸ ਵਿੱਚ, Action space discrete ਹੈ, ਅਤੇ ਇਸ ਵਿੱਚ ਦੋ ਕਿਰਿਆਵਾਂ ਹਨ - **ਖੱਬੇ** ਅਤੇ **ਸੱਜੇ**। (ਕੋਡ ਬਲਾਕ 2)

1. ਸ਼ੁਰੂ ਕਰਨ ਲਈ, ਹੇਠਾਂ ਦਿੱਤਾ ਕੋਡ ਟਾਈਪ ਕਰੋ:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

ਵਾਤਾਵਰਣ ਕਿਵੇਂ ਕੰਮ ਕਰਦਾ ਹੈ ਇਹ ਵੇਖਣ ਲਈ, ਆਓ 100 ਕਦਮਾਂ ਲਈ ਇੱਕ ਛੋਟੀ ਸਿਮੂਲੇਸ਼ਨ ਚਲਾਈਏ। ਹਰ ਕਦਮ 'ਤੇ, ਅਸੀਂ ਇੱਕ ਕਿਰਿਆ ਪ੍ਰਦਾਨ ਕਰਦੇ ਹਾਂ ਜੋ ਕੀਤੀ ਜਾਵੇਗੀ - ਇਸ ਸਿਮੂਲੇਸ਼ਨ ਵਿੱਚ ਅਸੀਂ ਸਿਰਫ਼ `action_space` ਤੋਂ ਰੈਂਡਮ ਕਿਰਿਆ ਚੁਣਦੇ ਹਾਂ।

1. ਹੇਠਾਂ ਦਿੱਤਾ ਕੋਡ ਚਲਾਓ ਅਤੇ ਦੇਖੋ ਕਿ ਇਹ ਕੀ ਨਤੀਜਾ ਦਿੰਦਾ ਹੈ।

    ✅ ਯਾਦ ਰੱਖੋ ਕਿ ਇਹ ਕੋਡ ਸਥਾਨਕ Python ਇੰਸਟਾਲੇਸ਼ਨ 'ਤੇ ਚਲਾਉਣਾ ਪਸੰਦ ਕੀਤਾ ਜਾਂਦਾ ਹੈ! (ਕੋਡ ਬਲਾਕ 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    ਤੁਹਾਨੂੰ ਕੁਝ ਇਸ ਤਰ੍ਹਾਂ ਦੀ ਚਿੱਤਰਕਲਾ ਦੇਖਣੀ ਚਾਹੀਦੀ ਹੈ:

    ![non-balancing cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. ਸਿਮੂਲੇਸ਼ਨ ਦੌਰਾਨ, ਸਾਨੂੰ ਕਿਰਿਆ ਕਰਨ ਲਈ ਅਵਲੋਕਨ ਪ੍ਰਾਪਤ ਕਰਨ ਦੀ ਲੋੜ ਹੈ। ਦਰਅਸਲ, `step` ਫੰਕਸ਼ਨ ਮੌਜੂਦਾ ਅਵਲੋਕਨ, ਇੱਕ ਇਨਾਮ ਫੰਕਸ਼ਨ, ਅਤੇ `done` ਫਲੈਗ ਵਾਪਸ ਕਰਦਾ ਹੈ ਜੋ ਇਹ ਦਰਸਾਉਂਦਾ ਹੈ ਕਿ ਸਿਮੂਲੇਸ਼ਨ ਜਾਰੀ ਰੱਖਣ ਦਾ ਕੋਈ ਮਤਲਬ ਹੈ ਜਾਂ ਨਹੀਂ: (ਕੋਡ ਬਲਾਕ 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    ਤੁਹਾਨੂੰ ਨੋਟਬੁੱਕ ਆਉਟਪੁੱਟ ਵਿੱਚ ਕੁਝ ਇਸ ਤਰ੍ਹਾਂ ਦੇਖਣ ਨੂੰ ਮਿਲੇਗਾ:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    ਸਿਮੂਲੇਸ਼ਨ ਦੇ ਹਰ ਕਦਮ 'ਤੇ ਵਾਪਸ ਕੀਤੇ ਗਏ ਅਵਲੋਕਨ ਵੇਕਟਰ ਵਿੱਚ ਹੇਠਾਂ ਦਿੱਤੇ ਮੁੱਲ ਸ਼ਾਮਲ ਹੁੰਦੇ ਹਨ:
    - ਕਾਰਟ ਦੀ ਸਥਿਤੀ
    - ਕਾਰਟ ਦੀ ਗਤੀ
    - ਪੋਲ ਦਾ ਕੋਣ
    - ਪੋਲ ਦੀ ਘੁੰਮਣ ਦੀ ਦਰ

1. ਉਹਨਾਂ ਨੰਬਰਾਂ ਦੇ ਘੱਟੋ-ਘੱਟ ਅਤੇ ਵੱਧ ਤੋਂ ਵੱਧ ਮੁੱਲ ਪ੍ਰਾਪਤ ਕਰੋ: (ਕੋਡ ਬਲਾਕ 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    ਤੁਸੀਂ ਇਹ ਵੀ ਨੋਟ ਕਰ ਸਕਦੇ ਹੋ ਕਿ ਹਰ ਸਿਮੂਲੇਸ਼ਨ ਕਦਮ 'ਤੇ ਇਨਾਮ ਦਾ ਮੁੱਲ ਹਮੇਸ਼ਾਂ 1 ਹੁੰਦਾ ਹੈ। ਇਹ ਇਸ ਲਈ ਹੈ ਕਿਉਂਕਿ ਸਾਡਾ ਲਕਸ਼ ਸਭ ਤੋਂ ਲੰਬੇ ਸਮੇਂ ਲਈ ਪੋਲ ਨੂੰ ਇੱਕ ਸਮਝਦਾਰ ਖੜੇ ਸਥਿਤੀ ਵਿੱਚ ਰੱਖਣਾ ਹੈ।

    ✅ ਦਰਅਸਲ, ਜੇਕਰ ਅਸੀਂ 100 ਲਗਾਤਾਰ ਪ੍ਰਯੋਗਾਂ ਵਿੱਚ 195 ਦਾ ਔਸਤ ਇਨਾਮ ਪ੍ਰਾਪਤ ਕਰ ਲੈਂਦੇ ਹਾਂ, ਤਾਂ CartPole ਸਿਮੂਲੇਸ਼ਨ ਨੂੰ ਹੱਲ ਕੀਤਾ ਮੰਨਿਆ ਜਾਂਦਾ ਹੈ।

## ਸਥਿਤੀ ਡਿਸਕ੍ਰੀਟਾਈਜ਼ੇਸ਼ਨ

Q-Learning ਵਿੱਚ, ਸਾਨੂੰ Q-Table ਬਣਾਉਣ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ ਜੋ ਹਰ ਸਥਿਤੀ 'ਤੇ ਕੀ ਕਰਨਾ ਹੈ ਇਹ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦਾ ਹੈ। ਇਹ ਕਰਨ ਲਈ, ਸਥਿਤੀ ਨੂੰ **ਡਿਸਕ੍ਰੀਟ** ਹੋਣਾ ਚਾਹੀਦਾ ਹੈ, ਅਰਥਾਤ ਇਸ ਵਿੱਚ ਡਿਸਕ੍ਰੀਟ ਮੁੱਲਾਂ ਦੀ ਸੀਮਿਤ ਗਿਣਤੀ ਹੋਣੀ ਚਾਹੀਦੀ ਹੈ। ਇਸ ਲਈ, ਸਾਨੂੰ ਕਿਸੇ ਤਰੀਕੇ ਨਾਲ ਆਪਣੇ ਅਵਲੋਕਨਾਂ ਨੂੰ ਡਿਸਕ੍ਰੀਟ ਕਰਨਾ ਪਵੇਗਾ, ਅਤੇ ਉਨ੍ਹਾਂ ਨੂੰ ਸਥਿਤੀਆਂ ਦੇ ਸੀਮਿਤ ਸੈੱਟ ਨਾਲ ਨਕਸ਼ਾ ਬਣਾਉਣਾ ਪਵੇਗਾ।

ਇਸ ਨੂੰ ਕਰਨ ਦੇ ਕੁਝ ਤਰੀਕੇ ਹਨ:

- **ਬਿਨਾਂ ਵਿੱਚ ਵੰਡੋ**। ਜੇਕਰ ਸਾਨੂੰ ਕਿਸੇ ਮੁੱਲ ਦੇ ਇੰਟਰਵਲ ਦਾ ਪਤਾ ਹੈ, ਤਾਂ ਅਸੀਂ ਇਸ ਇੰਟਰਵਲ ਨੂੰ ਕੁਝ **ਬਿਨਾਂ** ਵਿੱਚ ਵੰਡ ਸਕਦੇ ਹਾਂ, ਅਤੇ ਫਿਰ ਮੁੱਲ ਨੂੰ ਉਸ ਬਿਨ ਨੰਬਰ ਨਾਲ ਬਦਲ ਸਕਦੇ ਹਾਂ ਜਿਸ ਵਿੱਚ ਇਹ ਸ਼ਾਮਲ ਹੈ। ਇਹ numpy ਦੇ [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) ਵਿਧੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਇਸ ਮਾਮਲੇ ਵਿੱਚ, ਸਾਨੂੰ ਸਥਿਤੀ ਦੇ ਆਕਾਰ ਦਾ ਸਹੀ ਪਤਾ ਹੋਵੇਗਾ, ਕਿਉਂਕਿ ਇਹ ਡਿਜ਼ੀਟਲਾਈਜ਼ੇਸ਼ਨ ਲਈ ਚੁਣੇ ਗਏ ਬਿਨਾਂ ਦੀ ਗਿਣਤੀ 'ਤੇ ਨਿਰਭਰ ਕਰੇਗਾ।

✅ ਅਸੀਂ ਮੁੱਲਾਂ ਨੂੰ ਕਿਸੇ ਸੀਮਿਤ ਇੰਟਰਵਲ (ਜਿਵੇਂ -20 ਤੋਂ 20) ਵਿੱਚ ਲਿਆਉਣ ਲਈ ਰੇਖੀ ਇੰਟਰਪੋਲੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹਾਂ, ਅਤੇ ਫਿਰ ਨੰਬਰਾਂ ਨੂੰ ਗੋਲ ਕਰਕੇ ਪੂਰਨ ਅੰਕਾਂ ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ। ਇਸ ਨਾਲ ਸਾਨੂੰ ਸਥਿਤੀ ਦੇ ਆਕਾਰ 'ਤੇ ਘੱਟ ਨਿਯੰਤਰਣ ਮਿਲਦਾ ਹੈ, ਖਾਸ ਕਰਕੇ ਜੇਕਰ ਸਾਨੂੰ ਇਨਪੁਟ ਮੁੱਲਾਂ ਦੀ ਸਹੀ ਸੀਮਾਵਾਂ ਦਾ ਪਤਾ ਨਹੀਂ ਹੈ। ਉਦਾਹਰਣ ਲਈ, ਸਾਡੇ ਕੇਸ ਵਿੱਚ 4 ਵਿੱਚੋਂ 2 ਮੁੱਲਾਂ ਦੀ ਉੱਪਰ/ਹੇਠਾਂ ਸੀਮਾ ਨਹੀਂ ਹੈ, ਜਿਸ ਨਾਲ ਸਥਿਤੀਆਂ ਦੀ ਅਨੰਤ ਗਿਣਤੀ ਹੋ ਸਕਦੀ ਹੈ।

ਸਾਡੇ ਉਦਾਹਰਣ ਵਿੱਚ, ਅਸੀਂ ਦੂਜੇ ਤਰੀਕੇ ਨਾਲ ਜਾਵਾਂਗੇ। ਜਿਵੇਂ ਤੁਸੀਂ ਬਾਅਦ ਵਿੱਚ ਨੋਟ ਕਰ ਸਕਦੇ ਹੋ, ਅਸਪਸ਼ਟ ਉੱਪਰ/ਹੇਠਾਂ ਸੀਮਾਵਾਂ ਦੇ ਬਾਵਜੂਦ, ਉਹ ਮੁੱਲ ਕਦਰਾਂ ਹੀ ਕੁਝ ਸੀਮਿਤ ਇੰਟਰਵਲਾਂ ਤੋਂ ਬਾਹਰ ਲੈਂਦੇ ਹਨ, ਇਸ ਲਈ ਉਹ ਸਥਿਤੀਆਂ ਜਿਨ੍ਹਾਂ ਦੇ ਮੁੱਲ ਬਹੁਤ ਜ਼ਿਆਦਾ ਹਨ, ਬਹੁਤ ਘੱਟ ਹੋਣਗੀਆਂ।

1. ਇੱਥੇ ਉਹ ਫੰਕਸ਼ਨ ਹੈ ਜੋ ਸਾਡੇ ਮਾਡਲ ਤੋਂ ਅਵਲੋਕਨ ਲਵੇਗਾ ਅਤੇ 4 ਪੂਰਨ ਅੰਕਾਂ ਦੇ ਟਿਊਪਲ ਦਾ ਉਤਪਾਦਨ ਕਰੇਗਾ: (ਕੋਡ ਬਲਾਕ 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. ਆਓ ਬਿਨਾਂ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਹੋਰ ਡਿਸਕ੍ਰੀਟਾਈਜ਼ੇਸ਼ਨ ਵਿਧੀ ਦੀ ਵੀ ਪੜਚੋਲ ਕਰੀਏ: (ਕੋਡ ਬਲਾਕ 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. ਹੁਣ ਆਓ ਇੱਕ ਛੋਟੀ ਸਿਮੂਲੇਸ਼ਨ ਚਲਾਈਏ ਅਤੇ ਉਹਨਾਂ ਡਿਸਕ੍ਰੀਟ ਵਾਤਾਵਰਣ ਮੁੱਲਾਂ ਦਾ ਅਵਲੋਕਨ ਕਰੀਏ। ਦੋਵੇਂ `discretize` ਅਤੇ `discretize_bins` ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ ਅਤੇ ਵੇਖੋ ਕਿ ਕੀ ਕੋਈ ਫਰਕ ਹੈ।

    ✅ `discretize_bins` ਬਿਨ ਨੰਬਰ ਵਾਪਸ ਕਰਦਾ ਹੈ, ਜੋ ਕਿ 0-ਅਧਾਰਿਤ ਹੁੰਦਾ ਹੈ। ਇਸ ਲਈ ਇਨਪੁਟ ਵੇਰੀਏਬਲ ਦੇ ਮੁੱਲਾਂ ਦੇ ਆਸ-ਪਾਸ 0 ਲਈ ਇਹ ਇੰਟਰਵਲ ਦੇ ਵਿਚਕਾਰੋਂ ਨੰਬਰ (10) ਵਾਪਸ ਕਰਦਾ ਹੈ। `discretize` ਵਿੱਚ, ਅਸੀਂ ਆਉਟਪੁੱਟ ਮੁੱਲਾਂ ਦੀ ਰੇਂਜ ਦੀ ਚਿੰਤਾ ਨਹੀਂ ਕੀਤੀ, ਉਨ੍ਹਾਂ ਨੂੰ ਨਕਾਰਾਤਮਕ ਹੋਣ ਦੀ ਆਗਿਆ ਦਿੱਤੀ, ਇਸ ਲਈ ਸਥਿਤੀ ਮੁੱਲ ਸ਼ਿਫਟ ਨਹੀਂ ਹੁੰਦੇ, ਅਤੇ 0 ਦਾ ਅਰਥ 0 ਹੁੰਦਾ ਹੈ। (ਕੋਡ ਬਲਾਕ 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ✅ ਜੇਕਰ ਤੁਸੀਂ ਵਾਤਾਵਰਣ ਨੂੰ ਕਿਵੇਂ ਚਲਦਾ ਹੈ ਇਹ ਦੇਖਣਾ ਚਾਹੁੰਦੇ ਹੋ, ਤਾਂ `env.render` ਨਾਲ ਸ਼ੁਰੂ ਹੋਣ ਵਾਲੀ ਲਾਈਨ ਨੂੰ ਅਨਕਮੈਂਟ ਕਰੋ। ਨਹੀਂ ਤਾਂ ਤੁਸੀਂ ਇਸਨੂੰ ਬੈਕਗ੍ਰਾਊਂਡ ਵਿੱਚ ਚਲਾ ਸਕਦੇ ਹੋ, ਜੋ ਕਿ ਤੇਜ਼ ਹੈ। ਅਸੀਂ ਆਪਣੇ Q-Learning ਪ੍ਰਕਿਰਿਆ ਦੌਰਾਨ ਇਸ "ਅਦ੍ਰਿਸ਼" ਕਾਰਜ ਨੂੰ ਵਰਤਾਂਗੇ।

## Q-Table ਦੀ ਬਣਾਵਟ

ਪਿਛਲੇ ਪਾਠ ਵਿੱਚ, ਸਥਿਤੀ ਸਿਰਫ਼ 0 ਤੋਂ 8 ਦੇ ਨੰਬਰਾਂ ਦੀ ਇੱਕ ਜੋੜੀ ਸੀ, ਅਤੇ ਇਸ ਲਈ Q-Table ਨੂੰ numpy ਟੈਂਸਰ ਦੁਆਰਾ ਪ੍ਰਸਤੁਤ ਕਰਨਾ ਸੁਵਿਧਾਜਨਕ ਸੀ ਜਿਸਦਾ ਆਕਾਰ 8x8x2 ਸੀ। ਜੇਕਰ ਅਸੀਂ ਬਿਨ ਡਿਸਕ੍ਰੀਟਾਈਜ਼ੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਸਾਡੇ ਸਥਿਤੀ ਵੇਕਟਰ ਦਾ ਆਕਾਰ ਵੀ ਪਤਾ ਹੈ, ਇਸ ਲਈ ਅਸੀਂ ਉਹੀ ਤਰੀਕਾ ਵਰਤ ਸਕਦੇ ਹਾਂ ਅਤੇ ਸਥਿਤੀ ਨੂੰ 20x20x10x10x2 ਦੇ ਐਰੇ ਦੁਆਰਾ ਪ੍ਰਸਤੁਤ ਕਰ ਸਕਦੇ ਹਾਂ (ਇੱਥੇ 2 ਐਕਸ਼ਨ ਸਪੇਸ ਦਾ ਆਕਾਰ ਹੈ, ਅਤੇ ਪਹਿਲੇ ਆਕਾਰ ਅਵਲੋਕਨ ਸਪੇਸ ਵਿੱਚ ਪ੍ਰਤੀਕ ਪੈਰਾਮੀਟਰਾਂ ਲਈ ਚੁਣੇ ਗਏ ਬਿਨਾਂ ਦੀ ਗਿਣਤੀ ਨੂੰ ਦਰਸਾਉਂਦੇ ਹਨ)।

ਹਾਲਾਂਕਿ, ਕਈ ਵਾਰ ਅਵਲੋਕਨ ਸਪੇਸ ਦੇ ਸਹੀ ਆਕਾਰ ਪਤਾ ਨਹੀਂ ਹੁੰਦੇ। `discretize` ਫੰਕਸ਼ਨ ਦੇ ਕੇਸ ਵਿੱਚ, ਅਸੀਂ ਕਦੇ ਵੀ ਯਕੀਨੀ ਨਹੀਂ ਹੋ ਸਕਦੇ ਕਿ ਸਾਡੀ ਸਥਿਤੀ ਕੁਝ ਸੀਮਾਵਾਂ ਦੇ ਅੰਦਰ ਰਹਿੰਦੀ ਹੈ, ਕਿਉਂਕਿ ਕੁਝ ਮੁੱਲ ਬਾਊਂਡ ਨਹੀਂ ਹਨ। ਇਸ ਲਈ, ਅਸੀਂ ਥੋੜ੍ਹਾ ਵੱਖਰਾ ਤਰੀਕਾ ਵਰਤਾਂਗੇ ਅਤੇ Q-Table ਨੂੰ ਡਿਕਸ਼ਨਰੀ ਦੁਆਰਾ ਪ੍ਰਸਤੁਤ ਕਰਾਂਗੇ।

1. ਡਿਕਸ਼ਨਰੀ ਕੁੰਜੀ ਵਜੋਂ *(state,action)* ਦੀ ਜੋੜੀ ਦੀ ਵਰਤੋਂ ਕਰੋ, ਅਤੇ ਮੁੱਲ Q-Table ਐਂਟਰੀ ਮੁੱਲ ਨੂੰ ਦਰਸਾਏਗਾ। (ਕੋਡ ਬਲਾਕ 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    ਇੱਥੇ ਅਸੀਂ ਇੱਕ ਫੰਕਸ਼ਨ `qvalues()` ਵੀ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦੇ ਹਾਂ, ਜੋ ਦਿੱਤੀ ਸਥਿਤੀ ਲਈ Q-Table ਮੁੱਲਾਂ ਦੀ ਸੂਚੀ ਵਾਪਸ ਕਰਦਾ ਹੈ ਜੋ ਸਾਰੀਆਂ ਸੰਭਾਵਿਤ ਕਿਰਿਆਵਾਂ ਨਾਲ ਸੰਬੰਧਿਤ ਹੁੰਦੀ ਹੈ। ਜੇਕਰ ਐਂਟਰੀ Q-Table ਵਿੱਚ ਮੌਜੂਦ ਨਹੀਂ ਹੈ, ਤਾਂ ਅਸੀਂ ਡਿਫਾਲਟ ਵਜੋਂ 0 ਵਾਪਸ ਕਰਾਂਗੇ।

## ਆਓ Q-Learning ਸ਼ੁਰੂ ਕਰੀਏ

ਹੁਣ ਅਸੀਂ ਪੀਟਰ ਨੂੰ ਬੈਲੈਂਸ ਸਿਖਾਉਣ ਲਈ ਤਿਆਰ ਹਾਂ!

1. ਸਭ ਤੋਂ ਪਹਿਲਾਂ, ਕੁਝ ਹਾਈਪਰਪੈਰਾਮੀਟਰ ਸੈੱਟ ਕਰੀਏ: (ਕੋਡ ਬਲਾਕ 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    ਇੱਥੇ, `alpha` **ਲਰਨਿੰਗ ਰੇਟ** ਹੈ ਜੋ ਇਹ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦਾ ਹੈ ਕਿ ਸਾਡੇ Q-Table ਦੇ ਮੌਜੂਦਾ ਮੁੱਲਾਂ ਨੂੰ ਹਰ ਕਦਮ 'ਤੇ ਕਿੰਨਾ ਸਮਾਯੋਜਿਤ ਕਰਨਾ ਚਾਹੀਦਾ ਹੈ। ਪਿਛਲੇ ਪਾਠ ਵਿੱਚ ਅਸੀਂ 1 ਨਾਲ ਸ਼ੁਰੂ ਕੀਤਾ ਸੀ, ਅਤੇ ਫਿਰ ਸਿਖਲਾਈ ਦੌਰਾਨ `alpha` ਨੂੰ ਘਟਾਇਆ ਸੀ। ਇਸ ਉਦਾਹਰਣ ਵਿੱਚ ਅਸੀਂ ਸਿਰਫ਼ ਸਧਾਰਨਤਾ ਲਈ ਇਸਨੂੰ ਸਥਿਰ ਰੱਖਾਂਗੇ, ਅਤੇ ਤੁਸੀਂ ਬਾਅਦ ਵਿੱਚ `alpha` ਮੁੱਲਾਂ ਨੂੰ ਸਹੀ ਕਰਨ ਦੇ ਨਾਲ ਪ੍ਰਯੋਗ ਕਰ ਸਕਦੇ ਹੋ।

    `gamma` **ਡਿਸਕਾਊਂਟ ਫੈਕਟਰ** ਹੈ ਜੋ ਇਹ ਦਰਸਾਉਂਦਾ ਹੈ ਕਿ ਸਾਨੂੰ ਭਵਿੱਖ ਦੇ ਇਨਾਮ ਨੂੰ ਮੌਜੂਦਾ ਇਨਾਮ ਦੇ ਮੁਕਾਬਲੇ ਕਿੰਨਾ ਮਹੱਤਵ ਦੇਣਾ ਚਾਹੀਦਾ ਹੈ।

    `epsilon` **ਐਕਸਪਲੋਰੇਸ਼ਨ/ਐਕਸਪਲੋਇਟੇਸ਼ਨ ਫੈਕਟਰ** ਹੈ ਜੋ ਇਹ ਨਿਰਧਾਰਤ ਕਰਦਾ ਹੈ ਕਿ ਸਾਨੂੰ ਐਕਸਪਲੋਰੇਸ਼ਨ ਨੂੰ ਐਕਸਪਲੋਇਟੇਸ਼ਨ ਦੇ ਮੁਕਾਬਲੇ ਕਿੰਨਾ ਤਰਜੀਹ ਦੇਣੀ ਚਾਹੀਦੀ ਹੈ। ਸਾਡੇ ਐਲਗੋਰਿਥਮ ਵਿੱਚ, ਅਸੀਂ `epsilon` ਪ੍ਰਤੀਸ਼ਤ ਮਾਮਲਿਆਂ ਵਿੱਚ Q-Table ਮੁੱਲਾਂ ਦੇ ਅਨੁਸਾਰ ਅਗਲੀ ਕਿਰਿਆ ਚੁਣਾਂਗੇ, ਅਤੇ ਬਾਕੀ ਮਾਮਲਿਆਂ ਵਿੱਚ ਅਸੀਂ ਇੱਕ ਰੈਂਡਮ ਕਿਰਿਆ ਚਲਾਵਾਂਗੇ। ਇਹ ਸਾਨੂੰ ਖੋਜ ਸਪੇਸ ਦੇ ਉਹਨਾਂ ਖੇਤਰਾਂ ਦੀ ਪੜਚੋਲ ਕਰਨ ਦੀ ਆਗਿਆ ਦੇਵੇਗਾ ਜੋ ਅਸੀਂ ਪਹਿਲਾਂ ਕਦੇ ਨਹੀਂ ਵੇਖੇ।

    ✅ ਬੈਲੈਂਸਿੰਗ ਦੇ ਸੰਦਰਭ ਵਿੱਚ - ਰੈਂਡਮ ਕਿਰਿਆ ਚੁਣਨਾ (ਐਕਸਪਲੋਰੇਸ਼ਨ) ਗਲਤ ਦਿਸ਼ਾ ਵਿੱਚ ਇੱਕ ਰੈਂਡਮ ਝਟਕੇ ਵਜੋਂ ਕੰਮ ਕਰੇਗਾ, ਅਤੇ ਪੋਲ ਨੂੰ ਉਹਨਾਂ "ਗਲਤੀਆਂ" ਤੋਂ ਬੈਲੈਂਸ ਮੁੜ ਪ੍ਰਾਪਤ ਕਰਨਾ ਸਿੱਖਣਾ ਪਵੇਗਾ।

### ਐਲਗੋਰਿਥਮ ਵਿੱਚ ਸੁਧਾਰ ਕਰੋ

ਅਸੀਂ ਪਿਛਲੇ ਪਾਠ ਤੋਂ ਆਪਣੇ ਐਲਗੋਰਿਥਮ ਵਿੱਚ ਦੋ ਸੁਧਾਰ ਕਰ ਸਕਦੇ ਹਾਂ:

- **ਔਸਤ ਸੰਚਿਤ ਇਨਾਮ ਦੀ ਗਣਨਾ ਕਰੋ**, ਕਈ ਸਿਮੂਲੇਸ਼ਨਾਂ ਦੇ ਦੌਰਾਨ। ਅਸੀਂ ਹਰ 5000 ਇਟਰੇਸ਼ਨ 'ਤੇ ਪ੍ਰਗਤੀ ਪ੍ਰਿੰਟ ਕਰਾਂਗੇ, ਅਤੇ ਅਸੀਂ ਉਸ ਸਮੇਂ ਦੇ ਦੌਰਾਨ ਆਪਣੇ ਸੰਚਿਤ ਇਨਾਮ ਦਾ ਔਸਤ ਲਵਾਂਗੇ। ਇਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਜੇਕਰ ਅਸੀਂ 195 ਤੋਂ ਵੱਧ ਪਾਇੰਟ ਪ੍ਰਾਪਤ ਕਰਦੇ ਹਾਂ - ਅਸੀਂ ਸਮੱਸਿਆ ਨੂੰ ਹੱਲ ਕੀਤਾ ਮੰਨ ਸਕਦੇ ਹਾਂ, ਜੋ ਲੋੜੀਂਦੇ ਮਿਆਰ ਨਾਲੋਂ ਵੀ ਉੱਚ ਗੁਣਵੱਤਾ ਵਾਲਾ ਹੈ।

- **ਅਧਿਕਤਮ ਔਸਤ ਸੰਚਿਤ ਨਤੀਜੇ ਦੀ ਗਣਨਾ ਕਰੋ**, `Qmax`, ਅਤੇ ਅਸੀਂ ਉਸ ਨ
> **ਕਿਰਿਆ 1**: ਹਾਈਪਰਪੈਰਾਮੀਟਰ ਮੁੱਲਾਂ ਨਾਲ ਖੇਡੋ ਅਤੇ ਵੇਖੋ ਕਿ ਕੀ ਤੁਸੀਂ ਵਧੇਰੇ ਕੁੱਲ ਇਨਾਮ ਪ੍ਰਾਪਤ ਕਰ ਸਕਦੇ ਹੋ। ਕੀ ਤੁਸੀਂ 195 ਤੋਂ ਉੱਪਰ ਪ੍ਰਾਪਤ ਕਰ ਰਹੇ ਹੋ?
> **ਟਾਸਕ 2**: ਸਮੱਸਿਆ ਨੂੰ ਔਪਚਾਰਿਕ ਤੌਰ 'ਤੇ ਹੱਲ ਕਰਨ ਲਈ, ਤੁਹਾਨੂੰ 100 ਲਗਾਤਾਰ ਦੌੜਾਂ ਵਿੱਚ 195 ਔਸਤ ਇਨਾਮ ਪ੍ਰਾਪਤ ਕਰਨਾ ਹੋਵੇਗਾ। ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਇਸਨੂੰ ਮਾਪੋ ਅਤੇ ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਤੁਸੀਂ ਸਮੱਸਿਆ ਨੂੰ ਔਪਚਾਰਿਕ ਤੌਰ 'ਤੇ ਹੱਲ ਕਰ ਲਿਆ ਹੈ!

## ਨਤੀਜੇ ਨੂੰ ਕਾਰਵਾਈ ਵਿੱਚ ਦੇਖਣਾ

ਇਹ ਦਿਲਚਸਪ ਹੋਵੇਗਾ ਕਿ ਸੱਚਮੁੱਚ ਦੇਖਿਆ ਜਾਵੇ ਕਿ ਟ੍ਰੇਨ ਕੀਤੇ ਮਾਡਲ ਦਾ ਵਿਹਾਰ ਕਿਵੇਂ ਹੈ। ਆਓ ਸਿਮੂਲੇਸ਼ਨ ਚਲਾਈਏ ਅਤੇ ਟ੍ਰੇਨਿੰਗ ਦੌਰਾਨ ਵਰਤੀ ਗਈ ਇੱਕੋ ਜਿਹੀ ਐਕਸ਼ਨ ਚੋਣ ਰਣਨੀਤੀ ਦੀ ਪਾਲਣਾ ਕਰੀਏ, Q-ਟੇਬਲ ਵਿੱਚ ਸੰਭਾਵਨਾ ਵੰਡ ਅਨੁਸਾਰ ਸੈਂਪਲਿੰਗ ਕਰਦੇ ਹੋਏ: (ਕੋਡ ਬਲਾਕ 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

ਤੁਹਾਨੂੰ ਕੁਝ ਇਸ ਤਰ੍ਹਾਂ ਦੇਖਣ ਨੂੰ ਮਿਲੇਗਾ:

![a balancing cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## 🚀ਚੁਣੌਤੀ

> **ਟਾਸਕ 3**: ਇੱਥੇ, ਅਸੀਂ Q-ਟੇਬਲ ਦੀ ਅੰਤਮ ਕਾਪੀ ਵਰਤ ਰਹੇ ਸਾਂ, ਜੋ ਸ਼ਾਇਦ ਸਭ ਤੋਂ ਵਧੀਆ ਨਹੀਂ ਹੋ ਸਕਦੀ। ਯਾਦ ਰੱਖੋ ਕਿ ਅਸੀਂ ਸਭ ਤੋਂ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਕਰਨ ਵਾਲੀ Q-ਟੇਬਲ ਨੂੰ `Qbest` ਵੈਰੀਏਬਲ ਵਿੱਚ ਸਟੋਰ ਕੀਤਾ ਹੈ! `Qbest` ਨੂੰ `Q` ਵਿੱਚ ਕਾਪੀ ਕਰਕੇ ਅਤੇ ਇਹ ਦੇਖ ਕੇ ਕਿ ਕੀ ਤੁਸੀਂ ਕੋਈ ਅੰਤਰ ਮਹਿਸੂਸ ਕਰਦੇ ਹੋ, ਇਸੇ ਉਦਾਹਰਨ ਨੂੰ ਸਭ ਤੋਂ ਵਧੀਆ Q-ਟੇਬਲ ਨਾਲ ਅਜ਼ਮਾਓ।

> **ਟਾਸਕ 4**: ਇੱਥੇ ਅਸੀਂ ਹਰ ਕਦਮ 'ਤੇ ਸਭ ਤੋਂ ਵਧੀਆ ਕਾਰਵਾਈ ਨਹੀਂ ਚੁਣ ਰਹੇ ਸਾਂ, ਸਗੋਂ ਸੰਭਾਵਨਾ ਵੰਡ ਦੇ ਅਨੁਸਾਰ ਸੈਂਪਲਿੰਗ ਕਰ ਰਹੇ ਸਾਂ। ਕੀ ਹਮੇਸ਼ਾ ਸਭ ਤੋਂ ਵਧੀਆ ਕਾਰਵਾਈ ਚੁਣਨਾ ਵਧੀਆ ਹੋਵੇਗਾ, ਜਿਸਦਾ Q-ਟੇਬਲ ਮੁੱਲ ਸਭ ਤੋਂ ਉੱਚਾ ਹੈ? ਇਹ `np.argmax` ਫੰਕਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ, ਜੋ ਸਭ ਤੋਂ ਉੱਚੇ Q-ਟੇਬਲ ਮੁੱਲ ਦੇ ਅਨੁਸਾਰ ਕਾਰਵਾਈ ਨੰਬਰ ਦਾ ਪਤਾ ਲਗਾਉਂਦਾ ਹੈ। ਇਸ ਰਣਨੀਤੀ ਨੂੰ ਲਾਗੂ ਕਰੋ ਅਤੇ ਦੇਖੋ ਕਿ ਕੀ ਇਹ ਸੰਤੁਲਨ ਵਿੱਚ ਸੁਧਾਰ ਕਰਦਾ ਹੈ।

## [ਪੋਸਟ-ਲੈਕਚਰ ਕਵਿਜ਼](https://ff-quizzes.netlify.app/en/ml/)

## ਅਸਾਈਨਮੈਂਟ
[ਮਾਊਂਟੇਨ ਕਾਰ ਨੂੰ ਟ੍ਰੇਨ ਕਰੋ](assignment.md)

## ਨਿਸਕਰਸ਼

ਅਸੀਂ ਹੁਣ ਸਿੱਖ ਲਿਆ ਹੈ ਕਿ ਕੇਵਲ ਇੱਕ ਇਨਾਮ ਫੰਕਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਕੇ, ਜੋ ਖੇਡ ਦੀ ਇੱਛਿਤ ਸਥਿਤੀ ਨੂੰ ਪਰਿਭਾਸ਼ਿਤ ਕਰਦਾ ਹੈ, ਅਤੇ ਉਨ੍ਹਾਂ ਨੂੰ ਸਮਰਥਕ ਤਰੀਕੇ ਨਾਲ ਖੋਜ ਸਥਾਨ ਦੀ ਪੜਚੋਲ ਕਰਨ ਦਾ ਮੌਕਾ ਦੇ ਕੇ, ਏਜੰਟਾਂ ਨੂੰ ਚੰਗੇ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ ਕਿਵੇਂ ਟ੍ਰੇਨ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ। ਅਸੀਂ ਸਫਲਤਾਪੂਰਵਕ Q-ਲਰਨਿੰਗ ਐਲਗੋਰਿਥਮ ਨੂੰ ਵਿਸ਼ੇਸ਼ ਅਤੇ ਲਗਾਤਾਰ ਵਾਤਾਵਰਣ ਦੇ ਮਾਮਲਿਆਂ ਵਿੱਚ ਲਾਗੂ ਕੀਤਾ ਹੈ, ਪਰ ਵਿਸ਼ੇਸ਼ ਕਾਰਵਾਈਆਂ ਨਾਲ।

ਇਹ ਵੀ ਅਹਿਮ ਹੈ ਕਿ ਉਹ ਸਥਿਤੀਆਂ ਦਾ ਅਧਿਐਨ ਕੀਤਾ ਜਾਵੇ ਜਿੱਥੇ ਕਾਰਵਾਈ ਸਥਿਤੀ ਵੀ ਲਗਾਤਾਰ ਹੁੰਦੀ ਹੈ, ਅਤੇ ਜਦੋਂ ਅਵਲੋਕਨ ਸਥਾਨ ਕਾਫੀ ਜਟਿਲ ਹੁੰਦਾ ਹੈ, ਜਿਵੇਂ ਕਿ Atari ਗੇਮ ਸਕ੍ਰੀਨ ਤੋਂ ਚਿੱਤਰ। ਉਹਨਾਂ ਸਮੱਸਿਆਵਾਂ ਵਿੱਚ ਅਕਸਰ ਚੰਗੇ ਨਤੀਜੇ ਪ੍ਰਾਪਤ ਕਰਨ ਲਈ ਹੋਰ ਸ਼ਕਤੀਸ਼ਾਲੀ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਤਕਨੀਕਾਂ ਦੀ ਲੋੜ ਹੁੰਦੀ ਹੈ, ਜਿਵੇਂ ਕਿ ਨਿਊਰਲ ਨੈਟਵਰਕ। ਇਹ ਹੋਰ ਅਗਰਸਰ ਵਿਸ਼ੇ ਅਸੀਂ ਆਪਣੇ ਆਉਣ ਵਾਲੇ ਅਗਰਸਰ AI ਕੋਰਸ ਵਿੱਚ ਕਵਰ ਕਰਾਂਗੇ।

---

**ਅਸਵੀਕਾਰਨਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਤਾ ਲਈ ਯਤਨਸ਼ੀਲ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਚਨਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।