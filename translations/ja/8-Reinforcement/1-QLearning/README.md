# å¼·åŒ–å­¦ç¿’ã¨Qå­¦ç¿’ã®ç´¹ä»‹

![æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹å¼·åŒ–ã®æ¦‚è¦ã‚’ã‚¹ã‚±ãƒƒãƒãƒãƒ¼ãƒˆã§è¡¨ç¾](../../../../translated_images/ml-reinforcement.94024374d63348dbb3571c343ca7ddabef72adac0b8086d47164b769ba3a8a1d.ja.png)
> ã‚¹ã‚±ãƒƒãƒãƒãƒ¼ãƒˆ: [Tomomi Imura](https://www.twitter.com/girlie_mac)

å¼·åŒ–å­¦ç¿’ã«ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€çŠ¶æ…‹ã€å„çŠ¶æ…‹ã”ã¨ã®ä¸€é€£ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã„ã†3ã¤ã®é‡è¦ãªæ¦‚å¿µãŒå«ã¾ã‚Œã¾ã™ã€‚æŒ‡å®šã•ã‚ŒãŸçŠ¶æ…‹ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å ±é…¬ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚²ãƒ¼ãƒ ã€Œã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒªã‚ªã€ã‚’æƒ³åƒã—ã¦ã¿ã¦ãã ã•ã„ã€‚ã‚ãªãŸã¯ãƒãƒªã‚ªã§ã€å´–ã®ç«¯ã«ç«‹ã£ã¦ã„ã‚‹ã‚²ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«ã«ã„ã¾ã™ã€‚ä¸Šã«ã¯ã‚³ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚ã‚ãªãŸãŒãƒãƒªã‚ªã§ã€ç‰¹å®šã®ä½ç½®ã«ã„ã‚‹ã‚²ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«...ãã‚ŒãŒã‚ãªãŸã®çŠ¶æ…‹ã§ã™ã€‚å³ã«ä¸€æ­©é€²ã‚€ï¼ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã¨å´–ã‹ã‚‰è½ã¡ã¦ã—ã¾ã„ã€ä½ã„æ•°å€¤ã‚¹ã‚³ã‚¢ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€ã‚¸ãƒ£ãƒ³ãƒ—ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ãƒã‚¤ãƒ³ãƒˆãŒå¾—ã‚‰ã‚Œã€ç”Ÿãæ®‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ãªçµæœã§ã‚ã‚Šã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªæ•°å€¤ã‚¹ã‚³ã‚¢ãŒä¸ãˆã‚‰ã‚Œã‚‹ã¹ãã§ã™ã€‚

å¼·åŒ–å­¦ç¿’ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆã‚²ãƒ¼ãƒ ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã—ã¦å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚å ±é…¬ã¯ç”Ÿãæ®‹ã‚Šã€ã§ãã‚‹ã ã‘å¤šãã®ãƒã‚¤ãƒ³ãƒˆã‚’ç²å¾—ã™ã‚‹ã“ã¨ã§ã™ã€‚

[![å¼·åŒ–å­¦ç¿’ã®ç´¹ä»‹](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ ä¸Šã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€Dmitry ãŒå¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦è©±ã™ã®ã‚’èã„ã¦ã¿ã¾ã—ã‚‡ã†

## [è¬›ç¾©å‰ã®ã‚¯ã‚¤ã‚º](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/)

## å‰ææ¡ä»¶ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã¯ã€Python ã§ã„ãã¤ã‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿé¨“ã—ã¾ã™ã€‚ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã® Jupyter Notebook ã‚³ãƒ¼ãƒ‰ã‚’ã€è‡ªåˆ†ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ä¸Šã¾ãŸã¯ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

[ãƒ¬ãƒƒã‚¹ãƒ³ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb)ã‚’é–‹ã„ã¦ã€ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã‚’é€²ã‚ãªãŒã‚‰æ§‹ç¯‰ã—ã¦ã„ãã“ã¨ãŒã§ãã¾ã™ã€‚

> **Note:** ã‚¯ãƒ©ã‚¦ãƒ‰ã‹ã‚‰ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’é–‹ãå ´åˆã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã•ã‚Œã‚‹ [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å–å¾—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

## ã¯ã˜ã‚ã«

ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã¯ã€ãƒ­ã‚·ã‚¢ã®ä½œæ›²å®¶ [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev) ã«ã‚ˆã‚‹éŸ³æ¥½ç«¥è©±ã«è§¦ç™ºã•ã‚ŒãŸ **[ãƒ”ãƒ¼ã‚¿ãƒ¼ã¨ç‹¼](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)** ã®ä¸–ç•Œã‚’æ¢ã‚Šã¾ã™ã€‚**å¼·åŒ–å­¦ç¿’** ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ”ãƒ¼ã‚¿ãƒ¼ãŒç’°å¢ƒã‚’æ¢ç´¢ã—ã€ç¾å‘³ã—ã„ãƒªãƒ³ã‚´ã‚’é›†ã‚ã€ç‹¼ã«å‡ºä¼šã‚ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚

**å¼·åŒ–å­¦ç¿’** (RL) ã¯ã€**ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ** ãŒã„ãã¤ã‹ã® **ç’°å¢ƒ** ã§æœ€é©ãªè¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã“ã®ç’°å¢ƒã§ **å ±é…¬é–¢æ•°** ã«ã‚ˆã£ã¦å®šç¾©ã•ã‚ŒãŸ **ç›®æ¨™** ã‚’æŒã¤ã¹ãã§ã™ã€‚

## ç’°å¢ƒ

ç°¡å˜ã«ã™ã‚‹ãŸã‚ã«ã€ãƒ”ãƒ¼ã‚¿ãƒ¼ã®ä¸–ç•Œã‚’æ¬¡ã®ã‚ˆã†ãª `width` x `height` ã®ã‚µã‚¤ã‚ºã®æ­£æ–¹å½¢ã®ãƒœãƒ¼ãƒ‰ã¨è€ƒãˆã¾ã™ï¼š

![ãƒ”ãƒ¼ã‚¿ãƒ¼ã®ç’°å¢ƒ](../../../../translated_images/environment.40ba3cb66256c93fa7e92f6f7214e1d1f588aafa97d266c11d108c5c5d101b6c.ja.png)

ã“ã®ãƒœãƒ¼ãƒ‰ã®å„ã‚»ãƒ«ã¯æ¬¡ã®ã„ãšã‚Œã‹ã§ã™ï¼š

* **åœ°é¢**: ãƒ”ãƒ¼ã‚¿ãƒ¼ã‚„ä»–ã®ç”Ÿãç‰©ãŒæ­©ã‘ã‚‹å ´æ‰€ã€‚
* **æ°´**: æ˜ã‚‰ã‹ã«æ­©ã‘ãªã„å ´æ‰€ã€‚
* **æœ¨** ã¾ãŸã¯ **è‰**: ä¼‘ã‚€å ´æ‰€ã€‚
* **ãƒªãƒ³ã‚´**: ãƒ”ãƒ¼ã‚¿ãƒ¼ãŒè¦‹ã¤ã‘ã¦é£Ÿã¹ãŸã„ã‚‚ã®ã€‚
* **ç‹¼**: å±é™ºã§é¿ã‘ã‚‹ã¹ãã‚‚ã®ã€‚

ã“ã®ç’°å¢ƒã§å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚€åˆ¥ã® Python ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã¯æ¦‚å¿µã®ç†è§£ã«ã¯é‡è¦ã§ã¯ãªã„ãŸã‚ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 1ï¼‰ï¼š

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ä¸Šè¨˜ã®ç’°å¢ƒã«ä¼¼ãŸç”»åƒã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

## ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ãƒãƒªã‚·ãƒ¼

ã“ã®ä¾‹ã§ã¯ã€ãƒ”ãƒ¼ã‚¿ãƒ¼ã®ç›®æ¨™ã¯ç‹¼ã‚„ä»–ã®éšœå®³ç‰©ã‚’é¿ã‘ãªãŒã‚‰ãƒªãƒ³ã‚´ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã‚’è¡Œã†ãŸã‚ã«ã€å½¼ã¯ãƒªãƒ³ã‚´ã‚’è¦‹ã¤ã‘ã‚‹ã¾ã§åŸºæœ¬çš„ã«æ­©ãå›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã—ãŸãŒã£ã¦ã€ä»»æ„ã®ä½ç½®ã§ã€å½¼ã¯æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã„ãšã‚Œã‹ã‚’é¸æŠã§ãã¾ã™ï¼šä¸Šã€ä¸‹ã€å·¦ã€å³ã€‚

ã“ã‚Œã‚‰ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¾æ›¸ã¨ã—ã¦å®šç¾©ã—ã€ãã‚Œã‚‰ã‚’å¯¾å¿œã™ã‚‹åº§æ¨™ã®å¤‰åŒ–ã®ãƒšã‚¢ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€å³ã«ç§»å‹•ã™ã‚‹ (`R`) would correspond to a pair `(1,0)` ã¨ã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 2ï¼‰ï¼š

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

ã¾ã¨ã‚ã‚‹ã¨ã€ã“ã®ã‚·ãƒŠãƒªã‚ªã®æˆ¦ç•¥ã¨ç›®æ¨™ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ï¼š

- **æˆ¦ç•¥**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆãƒ”ãƒ¼ã‚¿ãƒ¼ï¼‰ã®æˆ¦ç•¥ã¯ **ãƒãƒªã‚·ãƒ¼** ã¨å‘¼ã°ã‚Œã‚‹é–¢æ•°ã«ã‚ˆã£ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚ãƒãƒªã‚·ãƒ¼ã¯ä»»æ„ã®çŠ¶æ…‹ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿”ã™é–¢æ•°ã§ã™ã€‚ç§ãŸã¡ã®å ´åˆã€å•é¡Œã®çŠ¶æ…‹ã¯ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ç¾åœ¨ä½ç½®ã‚’å«ã‚€ãƒœãƒ¼ãƒ‰ã«ã‚ˆã£ã¦è¡¨ã•ã‚Œã¾ã™ã€‚

- **ç›®æ¨™**: å¼·åŒ–å­¦ç¿’ã®ç›®æ¨™ã¯ã€å•é¡Œã‚’åŠ¹ç‡çš„ã«è§£æ±ºã™ã‚‹ãŸã‚ã®è‰¯ã„ãƒãƒªã‚·ãƒ¼ã‚’æœ€çµ‚çš„ã«å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã™ã€‚ãŸã ã—ã€åŸºæº–ã¨ã—ã¦ã€æœ€ã‚‚å˜ç´”ãªãƒãƒªã‚·ãƒ¼ã§ã‚ã‚‹ **ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯** ã‚’è€ƒãˆã¾ã™ã€‚

## ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯

ã¾ãšã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯æˆ¦ç•¥ã‚’å®Ÿè£…ã—ã¦å•é¡Œã‚’è§£æ±ºã—ã¾ã—ã‚‡ã†ã€‚ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã§ã¯ã€è¨±å¯ã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã—ã€ãƒªãƒ³ã‚´ã«åˆ°é”ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 3ï¼‰ã€‚

1. ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã§ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã‚’å®Ÿè£…ã—ã¾ã™ï¼š

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    `walk` ã®å‘¼ã³å‡ºã—ã¯ã€å¯¾å¿œã™ã‚‹çµŒè·¯ã®é•·ã•ã‚’è¿”ã™ã¹ãã§ã™ã€‚ã“ã‚Œã¯å®Ÿè¡Œã”ã¨ã«ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

1. ã‚¦ã‚©ãƒ¼ã‚¯å®Ÿé¨“ã‚’ä½•åº¦ã‹ï¼ˆä¾‹ãˆã°100å›ï¼‰å®Ÿè¡Œã—ã€çµæœã®çµ±è¨ˆã‚’å‡ºåŠ›ã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 4ï¼‰ï¼š

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    çµŒè·¯ã®å¹³å‡é•·ã•ãŒç´„30ã€œ40ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€æœ€ã‚‚è¿‘ã„ãƒªãƒ³ã‚´ã¾ã§ã®å¹³å‡è·é›¢ãŒç´„5ã€œ6ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€ã‹ãªã‚Šå¤šã„ã§ã™ã€‚

    ã¾ãŸã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ä¸­ã®ãƒ”ãƒ¼ã‚¿ãƒ¼ã®å‹•ããŒã©ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã‹ã‚‚ç¢ºèªã§ãã¾ã™ï¼š

    ![ãƒ”ãƒ¼ã‚¿ãƒ¼ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## å ±é…¬é–¢æ•°

ãƒãƒªã‚·ãƒ¼ã‚’ã‚ˆã‚ŠçŸ¥çš„ã«ã™ã‚‹ãŸã‚ã«ã¯ã€ã©ã®ç§»å‹•ãŒä»–ã®ç§»å‹•ã‚ˆã‚Šã‚‚ã€Œè‰¯ã„ã€ã‹ã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚’è¡Œã†ãŸã‚ã«ã¯ã€ç›®æ¨™ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ç›®æ¨™ã¯ã€å„çŠ¶æ…‹ã«å¯¾ã—ã¦ã„ãã¤ã‹ã®ã‚¹ã‚³ã‚¢å€¤ã‚’è¿”ã™ **å ±é…¬é–¢æ•°** ã®è¦³ç‚¹ã‹ã‚‰å®šç¾©ã§ãã¾ã™ã€‚æ•°å€¤ãŒé«˜ã„ã»ã©ã€å ±é…¬é–¢æ•°ãŒè‰¯ã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 5ï¼‰ã€‚

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

å ±é…¬é–¢æ•°ã«ã¤ã„ã¦èˆˆå‘³æ·±ã„ç‚¹ã¯ã€ã»ã¨ã‚“ã©ã®å ´åˆã€*ã‚²ãƒ¼ãƒ ã®æœ€å¾Œã«ã®ã¿å®Ÿè³ªçš„ãªå ±é…¬ãŒä¸ãˆã‚‰ã‚Œã‚‹* ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒãƒã‚¸ãƒ†ã‚£ãƒ–ãªå ±é…¬ã«ã¤ãªãŒã‚‹ã€Œè‰¯ã„ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨˜æ†¶ã—ã€ãã‚Œã‚‰ã®é‡è¦æ€§ã‚’é«˜ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚åŒæ§˜ã«ã€æ‚ªã„çµæœã«ã¤ãªãŒã‚‹ã™ã¹ã¦ã®ç§»å‹•ã¯æŠ‘åˆ¶ã•ã‚Œã‚‹ã¹ãã§ã™ã€‚

## Qå­¦ç¿’

ã“ã“ã§è­°è«–ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ **Qå­¦ç¿’** ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€ãƒãƒªã‚·ãƒ¼ã¯ **Qãƒ†ãƒ¼ãƒ–ãƒ«** ã¨å‘¼ã°ã‚Œã‚‹é–¢æ•°ï¼ˆã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼‰ã«ã‚ˆã£ã¦å®šç¾©ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ç‰¹å®šã®çŠ¶æ…‹ã§å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã€Œè‰¯ã•ã€ã‚’è¨˜éŒ²ã—ã¾ã™ã€‚

Qãƒ†ãƒ¼ãƒ–ãƒ«ã¨å‘¼ã°ã‚Œã‚‹ã®ã¯ã€ãã‚Œã‚’è¡¨å½¢å¼ã‚„å¤šæ¬¡å…ƒé…åˆ—ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã®ãŒä¾¿åˆ©ãªãŸã‚ã§ã™ã€‚ãƒœãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºãŒ `width` x `height` ã§ã‚ã‚‹ãŸã‚ã€`width` x `height` x `len(actions)` ã®å½¢çŠ¶ã‚’æŒã¤ numpy é…åˆ—ã‚’ä½¿ç”¨ã—ã¦ Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¾ã§ãã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 6ï¼‰ã€‚

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Qãƒ†ãƒ¼ãƒ–ãƒ«ã®ã™ã¹ã¦ã®å€¤ã‚’ç­‰ã—ã„å€¤ï¼ˆã“ã®å ´åˆã¯ 0.25ï¼‰ã§åˆæœŸåŒ–ã™ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯ã€ã™ã¹ã¦ã®çŠ¶æ…‹ã§ã®ã™ã¹ã¦ã®ç§»å‹•ãŒç­‰ã—ãè‰¯ã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€Œãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã€ãƒãƒªã‚·ãƒ¼ã«å¯¾å¿œã—ã¾ã™ã€‚Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ `plot` function in order to visualize the table on the board: `m.plot(Q)`.

![Peter's Environment](../../../../translated_images/env_init.04e8f26d2d60089e128f21d22e5fef57d580e559f0d5937b06c689e5e7cdd438.ja.png)

In the center of each cell there is an "arrow" that indicates the preferred direction of movement. Since all directions are equal, a dot is displayed.

Now we need to run the simulation, explore our environment, and learn a better distribution of Q-Table values, which will allow us to find the path to the apple much faster.

## Essence of Q-Learning: Bellman Equation

Once we start moving, each action will have a corresponding reward, i.e. we can theoretically select the next action based on the highest immediate reward. However, in most states, the move will not achieve our goal of reaching the apple, and thus we cannot immediately decide which direction is better.

> Remember that it is not the immediate result that matters, but rather the final result, which we will obtain at the end of the simulation.

In order to account for this delayed reward, we need to use the principles of **[dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)**, which allow us to think about out problem recursively.

Suppose we are now at the state *s*, and we want to move to the next state *s'*. By doing so, we will receive the immediate reward *r(s,a)*, defined by the reward function, plus some future reward. If we suppose that our Q-Table correctly reflects the "attractiveness" of each action, then at state *s'* we will chose an action *a* that corresponds to maximum value of *Q(s',a')*. Thus, the best possible future reward we could get at state *s* will be defined as `max`<sub>a'</sub>*Q(s',a')* (maximum here is computed over all possible actions *a'* at state *s'*).

This gives the **Bellman formula** for calculating the value of the Q-Table at state *s*, given action *a*:

<img src="images/bellman-equation.png"/>

Here Î³ is the so-called **discount factor** that determines to which extent you should prefer the current reward over the future reward and vice versa.

## Learning Algorithm

Given the equation above, we can now write pseudo-code for our learning algorithm:

* Initialize Q-Table Q with equal numbers for all states and actions
* Set learning rate Î± â† 1
* Repeat simulation many times
   1. Start at random position
   1. Repeat
        1. Select an action *a* at state *s*
        2. Execute action by moving to a new state *s'*
        3. If we encounter end-of-game condition, or total reward is too small - exit simulation  
        4. Compute reward *r* at the new state
        5. Update Q-Function according to Bellman equation: *Q(s,a)* â† *(1-Î±)Q(s,a)+Î±(r+Î³ max<sub>a'</sub>Q(s',a'))*
        6. *s* â† *s'*
        7. Update the total reward and decrease Î±.

## Exploit vs. explore

In the algorithm above, we did not specify how exactly we should choose an action at step 2.1. If we are choosing the action randomly, we will randomly **explore** the environment, and we are quite likely to die often as well as explore areas where we would not normally go. An alternative approach would be to **exploit** the Q-Table values that we already know, and thus to choose the best action (with higher Q-Table value) at state *s*. This, however, will prevent us from exploring other states, and it's likely we might not find the optimal solution.

Thus, the best approach is to strike a balance between exploration and exploitation. This can be done by choosing the action at state *s* with probabilities proportional to values in the Q-Table. In the beginning, when Q-Table values are all the same, it would correspond to a random selection, but as we learn more about our environment, we would be more likely to follow the optimal route while allowing the agent to choose the unexplored path once in a while.

## Python implementation

We are now ready to implement the learning algorithm. Before we do that, we also need some function that will convert arbitrary numbers in the Q-Table into a vector of probabilities for corresponding actions.

1. Create a function `probs()` ã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼š

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    ```

    åˆæœŸçŠ¶æ…‹ã§ãƒ™ã‚¯ãƒˆãƒ«ã®ã™ã¹ã¦ã®æˆåˆ†ãŒåŒä¸€ã§ã‚ã‚‹å ´åˆã« 0 ã§å‰²ã‚‹ã“ã¨ã‚’é¿ã‘ã‚‹ãŸã‚ã«ã€å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã«ã„ãã¤ã‹ã® `eps` ã‚’è¿½åŠ ã—ã¾ã™ã€‚

5000å›ã®å®Ÿé¨“ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰ã‚’é€šã˜ã¦å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 8ï¼‰ã€‚

```python
    for epoch in range(5000):
    
        # Pick initial point
        m.random_start()
        
        # Start travelling
        n=0
        cum_reward = 0
        while True:
            x,y = m.human
            v = probs(Q[x,y])
            a = random.choices(list(actions),weights=v)[0]
            dpos = actions[a]
            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
            r = reward(m)
            cum_reward += r
            if r==end_reward or cum_reward < -1000:
                lpath.append(n)
                break
            alpha = np.exp(-n / 10e5)
            gamma = 0.5
            ai = action_idx[a]
            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
            n+=1
```

ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè¡Œã—ãŸå¾Œã€Qãƒ†ãƒ¼ãƒ–ãƒ«ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ç•°ãªã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é­…åŠ›ã‚’å®šç¾©ã™ã‚‹å€¤ã§æ›´æ–°ã•ã‚Œã¾ã™ã€‚Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¦–è¦šåŒ–ã—ã¦ã€å„ã‚»ãƒ«ã«å°ã•ãªå††ã‚’æãã“ã¨ã§ã€ç§»å‹•ã®å¸Œæœ›æ–¹å‘ã‚’ç¤ºã™ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## ãƒãƒªã‚·ãƒ¼ã®ç¢ºèª

Qãƒ†ãƒ¼ãƒ–ãƒ«ã¯å„çŠ¶æ…‹ã§ã®å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã€Œé­…åŠ›ã€ã‚’ãƒªã‚¹ãƒˆã—ã¦ã„ã‚‹ãŸã‚ã€åŠ¹ç‡çš„ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®šç¾©ã™ã‚‹ã®ã«ç°¡å˜ã«ä½¿ç”¨ã§ãã¾ã™ã€‚æœ€ã‚‚ç°¡å˜ãªå ´åˆã€Qãƒ†ãƒ¼ãƒ–ãƒ«ã®å€¤ãŒæœ€ã‚‚é«˜ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã§ãã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 9ï¼‰ã€‚

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’æ•°å›è©¦ã—ã¦ã¿ã‚‹ã¨ã€æ™‚ã€…ã€Œãƒãƒ³ã‚°ã€ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã® STOP ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ä¸­æ–­ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ°—ä»˜ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã€æœ€é©ãª Qå€¤ã®è¦³ç‚¹ã‹ã‚‰2ã¤ã®çŠ¶æ…‹ãŒäº’ã„ã«ã€ŒæŒ‡ã—ç¤ºã™ã€çŠ¶æ³ãŒã‚ã‚Šã€ãã®å ´åˆã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç„¡é™ã«ãã®çŠ¶æ…‹é–“ã‚’ç§»å‹•ã—ç¶šã‘ã‚‹ãŸã‚ã§ã™ã€‚

## ğŸš€ãƒãƒ£ãƒ¬ãƒ³ã‚¸

> **ã‚¿ã‚¹ã‚¯ 1:** `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.

> **Task 2:** Modify the `walk` function so that it does not go back to the places where it has already been previously. This will prevent `walk` from looping, however, the agent can still end up being "trapped" in a location from which it is unable to escape.

## Navigation

A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that `print_statistics` ã‚’ä¿®æ­£ã—ã¦ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’100å›å®Ÿè¡Œã—ã¾ã™ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ 10ï¼‰ã€‚

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ãŸå¾Œã€ä»¥å‰ã‚ˆã‚Šã‚‚å¹³å‡çµŒè·¯é•·ãŒã¯ã‚‹ã‹ã«çŸ­ããªã‚Šã€3ã€œ6ã®ç¯„å›²ã«ãªã‚Šã¾ã™ã€‚

## å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®èª¿æŸ»

å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€å•é¡Œç©ºé–“ã®æ§‹é€ ã«é–¢ã™ã‚‹ç²å¾—ã—ãŸçŸ¥è­˜ã®æ¢ç´¢ã¨æ¢ç´¢ã®ãƒãƒ©ãƒ³ã‚¹ã§ã™ã€‚å­¦ç¿’ã®çµæœï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç›®æ¨™ã«åˆ°é”ã™ã‚‹ãŸã‚ã®çŸ­ã„çµŒè·¯ã‚’è¦‹ã¤ã‘ã‚‹èƒ½åŠ›ï¼‰ãŒå‘ä¸Šã—ãŸã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸãŒã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®å¹³å‡çµŒè·¯é•·ã®å¤‰åŒ–ã‚’è¦³å¯Ÿã™ã‚‹ã“ã¨ã‚‚èˆˆå‘³æ·±ã„ã§ã™ã€‚

å­¦ç¿’ã®è¦ç‚¹ã‚’ã¾ã¨ã‚ã‚‹ã¨ï¼š

- **å¹³å‡çµŒè·¯é•·ã®å¢—åŠ **ã€‚æœ€åˆã¯å¹³å‡çµŒè·¯é•·ãŒå¢—åŠ ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ç’°å¢ƒã«ã¤ã„ã¦ä½•ã‚‚çŸ¥ã‚‰ãªã„ã¨ãã«ã€æ‚ªã„çŠ¶æ…‹ï¼ˆæ°´ã‚„ç‹¼ï¼‰ã«é–‰ã˜è¾¼ã‚ã‚‰ã‚Œã‚„ã™ã„ã“ã¨ãŒåŸå› ã§ã™ã€‚ã‚ˆã‚Šå¤šãã‚’å­¦ã³ã€ã“ã®çŸ¥è­˜ã‚’ä½¿ã„å§‹ã‚ã‚‹ã¨ã€ç’°å¢ƒã‚’ã‚ˆã‚Šé•·ãæ¢ç´¢ã§ãã¾ã™ãŒã€ãƒªãƒ³ã‚´ã®ä½ç½®ã«ã¤ã„ã¦ã¯ã¾ã ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ã€‚

- **å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã¦çµŒè·¯é•·ãŒæ¸›å°‘**ã€‚ååˆ†ã«å­¦ç¿’ã™ã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç›®æ¨™ã‚’é”æˆã™ã‚‹ã®ãŒç°¡å˜ã«ãªã‚Šã€çµŒè·¯é•·ãŒæ¸›å°‘ã—å§‹ã‚ã¾ã™ã€‚ãŸã ã—ã€æ¢ç´¢ã¯ç¶šã‘ã¦ã„ã‚‹ãŸã‚ã€æœ€é©ãªçµŒè·¯ã‹ã‚‰é€¸ã‚Œã€æ–°ã—ã„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¢ç´¢ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã€çµŒè·¯ãŒæœ€é©ã‚ˆã‚Šé•·ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

- **çªç„¶ã®çµŒè·¯é•·ã®å¢—åŠ **ã€‚ã‚°ãƒ©ãƒ•ã§çµŒè·¯é•·ãŒçªç„¶å¢—åŠ ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºç‡çš„ãªæ€§è³ªã‚’ç¤ºã—ã¦ãŠã‚Šã€æ–°ã—ã„å€¤ã§ Qãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¿‚æ•°ã‚’ä¸Šæ›¸ãã™ã‚‹ã“ã¨ã§ Qãƒ†ãƒ¼ãƒ–ãƒ«ãŒã€Œæãªã‚ã‚Œã‚‹ã€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç†æƒ³çš„ã«ã¯ã€å­¦ç¿’ç‡ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ã§ã“ã‚Œã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã¹ãã§ã™ï¼ˆä¾‹ãˆã°ã€å­¦ç¿’ã®çµ‚ã‚ã‚Šã«å‘ã‹ã£ã¦ã€Qãƒ†ãƒ¼ãƒ–ãƒ«ã®å€¤ã‚’ã‚ãšã‹ã«èª¿æ•´ã™ã‚‹ï¼‰ã€‚

å…¨ä½“ã¨ã—ã¦ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®æˆåŠŸã¨è³ªã¯ã€å­¦ç¿’ç‡ã€å­¦ç¿’ç‡ã®æ¸›è¡°ã€å‰²å¼•ç‡ãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤§ããä¾å­˜ã™ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠãã“ã¨ãŒé‡è¦ã§ã™ã€‚ã“ã‚Œã‚‰ã¯ **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** ã¨å‘¼ã°ã‚Œã€**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** ã¨ã¯åŒºåˆ¥ã•ã‚Œã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å­¦ç¿’ä¸­ã«æœ€é©åŒ–ã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šï¼ˆä¾‹ãˆã°ã€Qãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¿‚æ•°ï¼‰ã€æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã‚’è¦‹ã¤ã‘ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–** ã¨å‘¼ã°ã‚Œã€åˆ¥ã®ãƒˆãƒ”ãƒƒã‚¯ã¨ã—ã¦å–ã‚Šä¸Šã’ã‚‹ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚

## [è¬›ç¾©å¾Œã®ã‚¯ã‚¤ã‚º](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## èª²é¡Œ 
[ã‚ˆã‚Šç¾å®Ÿçš„ãªä¸–ç•Œ](assignment.md)

**å…è²¬äº‹é …**:
ã“ã®æ–‡æ›¸ã¯æ©Ÿæ¢°ç¿»è¨³AIã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºã•ã‚’æœŸã—ã¦ã„ã¾ã™ãŒã€è‡ªå‹•ç¿»è¨³ã«ã¯èª¤ã‚Šã‚„ä¸æ­£ç¢ºã•ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã®ã§ã”æ³¨æ„ãã ã•ã„ã€‚å…ƒã®è¨€èªã§è¨˜è¼‰ã•ã‚ŒãŸæ–‡æ›¸ãŒä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã¨è¦‹ãªã•ã‚Œã‚‹ã¹ãã§ã™ã€‚é‡è¦ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€å°‚é–€ã®äººé–“ã«ã‚ˆã‚‹ç¿»è¨³ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã®ç¿»è¨³ã®ä½¿ç”¨ã«èµ·å› ã™ã‚‹èª¤è§£ã‚„èª¤ã£ãŸè§£é‡ˆã«ã¤ã„ã¦ã€å½“ç¤¾ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚