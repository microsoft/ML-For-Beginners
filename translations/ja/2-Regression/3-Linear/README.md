# Scikit-learnã‚’ä½¿ç”¨ã—ã¦å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹: å›å¸°ã®4ã¤ã®æ–¹æ³•

![ç·šå½¢å›å¸°ã¨å¤šé …å¼å›å¸°ã®ã‚¤ãƒ³ãƒ•ã‚©ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.ja.png)
> ã‚¤ãƒ³ãƒ•ã‚©ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ä½œæˆè€… [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [è¬›ç¾©å‰ã‚¯ã‚¤ã‚º](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã¯Rã§ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ï¼](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### ã¯ã˜ã‚ã«

ã“ã‚Œã¾ã§ã«ã€ã‹ã¼ã¡ã‚ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰åé›†ã—ãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€å›å¸°ãŒä½•ã§ã‚ã‚‹ã‹ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚ã¾ãŸã€Matplotlibã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚’è¦–è¦šåŒ–ã—ã¾ã—ãŸã€‚

ä»Šåº¦ã¯ã€MLã®å›å¸°ã«ã¤ã„ã¦ã•ã‚‰ã«æ·±ãæ˜ã‚Šä¸‹ã’ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸã€‚è¦–è¦šåŒ–ã¯ãƒ‡ãƒ¼ã‚¿ã‚’ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ãŒã€æ©Ÿæ¢°å­¦ç¿’ã®æœ¬å½“ã®åŠ›ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ã‚Šã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯éå»ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã®ä¾å­˜é–¢ä¿‚ã‚’è‡ªå‹•çš„ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒä»¥å‰ã«è¦‹ãŸã“ã¨ã®ãªã„æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®çµæœã‚’äºˆæ¸¬ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã¯ã€åŸºæœ¬çš„ãªç·šå½¢å›å¸°ã¨å¤šé …å¼å›å¸°ã®2ç¨®é¡ã®å›å¸°ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ç•°ãªã‚‹å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã‹ã¼ã¡ã‚ƒã®ä¾¡æ ¼ã‚’äºˆæ¸¬ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

[![åˆå¿ƒè€…å‘ã‘ML - ç·šå½¢å›å¸°ã®ç†è§£](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "åˆå¿ƒè€…å‘ã‘ML - ç·šå½¢å›å¸°ã®ç†è§£")

> ğŸ¥ ä¸Šã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ç·šå½¢å›å¸°ã®æ¦‚è¦ã‚’çŸ­ã„ãƒ“ãƒ‡ã‚ªã§ç¢ºèªã§ãã¾ã™ã€‚

> ã“ã®ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å…¨ä½“ã‚’é€šã˜ã¦ã€æ•°å­¦ã®çŸ¥è­˜ãŒæœ€å°é™ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ãŠã‚Šã€ä»–ã®åˆ†é‡ã‹ã‚‰æ¥ã‚‹å­¦ç”Ÿã«ã‚‚ç†è§£ã—ã‚„ã™ã„ã‚ˆã†ã«ã€ãƒãƒ¼ãƒˆã€ğŸ§® ã‚³ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã€å›³è¡¨ã€ãã®ä»–ã®å­¦ç¿’ãƒ„ãƒ¼ãƒ«ã‚’æ´»ç”¨ã—ã¦ã„ã¾ã™ã€‚

### å‰ææ¡ä»¶

ã“ã‚Œã¾ã§ã«ã€èª¿æŸ»ã—ã¦ã„ã‚‹ã‹ã¼ã¡ã‚ƒãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã«æ…£ã‚Œã¦ã„ã‚‹ã¯ãšã§ã™ã€‚ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã®_notebook.ipynb_ãƒ•ã‚¡ã‚¤ãƒ«ã«ã€ã‚ã‚‰ã‹ã˜ã‚èª­ã¿è¾¼ã¾ã‚Œã€å‰å‡¦ç†ã•ã‚ŒãŸçŠ¶æ…‹ã§ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€ã‹ã¼ã¡ã‚ƒã®ä¾¡æ ¼ãŒæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒ–ãƒƒã‚·ã‚§ãƒ«ã”ã¨ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’Visual Studio Codeã®ã‚«ãƒ¼ãƒãƒ«ã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

### æº–å‚™

ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ç›®çš„ã‚’æ€ã„å‡ºã—ã¦ãã ã•ã„ã€‚

- ã‹ã¼ã¡ã‚ƒã‚’è²·ã†ã®ã«æœ€é©ãªæ™‚æœŸã¯ã„ã¤ã§ã™ã‹ï¼Ÿ
- ãƒŸãƒ‹ãƒãƒ¥ã‚¢ã‹ã¼ã¡ã‚ƒ1ã‚±ãƒ¼ã‚¹ã®ä¾¡æ ¼ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ
- ãã‚Œã‚‰ã‚’åŠãƒ–ãƒƒã‚·ã‚§ãƒ«ãƒã‚¹ã‚±ãƒƒãƒˆã§è²·ã†ã¹ãã§ã™ã‹ã€ãã‚Œã¨ã‚‚1 1/9ãƒ–ãƒƒã‚·ã‚§ãƒ«ãƒœãƒƒã‚¯ã‚¹ã§è²·ã†ã¹ãã§ã™ã‹ï¼Ÿ
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«æ˜ã‚Šä¸‹ã’ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

å‰ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã¯ã€Pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆã—ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸€éƒ¨ã‚’å–ã‚Šè¾¼ã¿ã€ãƒ–ãƒƒã‚·ã‚§ãƒ«å˜ä½ã§ä¾¡æ ¼ã‚’æ¨™æº–åŒ–ã—ã¾ã—ãŸã€‚ã—ã‹ã—ã€ãã®çµæœã€ç´„400ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã—ã‹åé›†ã§ããšã€ç§‹ã®æ•°ã‹æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«äº‹å‰ã«èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãƒ‡ãƒ¼ã‚¿ã¯äº‹å‰ã«èª­ã¿è¾¼ã¾ã‚Œã€æœˆã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¤ºã™åˆæœŸã®æ•£å¸ƒå›³ãŒæã‹ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’ã•ã‚‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªã«ã¤ã„ã¦ã‚‚ã†å°‘ã—è©³ç´°ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

## ç·šå½¢å›å¸°ã®ãƒ©ã‚¤ãƒ³

ãƒ¬ãƒƒã‚¹ãƒ³1ã§å­¦ã‚“ã ã‚ˆã†ã«ã€ç·šå½¢å›å¸°ã®ç›®çš„ã¯æ¬¡ã®ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãƒ©ã‚¤ãƒ³ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã“ã¨ã§ã™ï¼š

- **å¤‰æ•°ã®é–¢ä¿‚ã‚’ç¤ºã™**ã€‚å¤‰æ•°é–“ã®é–¢ä¿‚ã‚’ç¤ºã™
- **äºˆæ¸¬ã‚’è¡Œã†**ã€‚æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒãã®ãƒ©ã‚¤ãƒ³ã«å¯¾ã—ã¦ã©ã“ã«ä½ç½®ã™ã‚‹ã‹ã‚’æ­£ç¢ºã«äºˆæ¸¬ã™ã‚‹

ã“ã®ã‚¿ã‚¤ãƒ—ã®ãƒ©ã‚¤ãƒ³ã‚’æãã®ã¯ã€é€šå¸¸ã€**æœ€å°äºŒä¹—æ³•å›å¸°**ã§ã™ã€‚ã€Œæœ€å°äºŒä¹—ã€ã¨ã¯ã€å›å¸°ç·šã®å‘¨ã‚Šã®ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’äºŒä¹—ã—ã¦ã‹ã‚‰åˆè¨ˆã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ç†æƒ³çš„ã«ã¯ã€ãã®æœ€çµ‚çš„ãªåˆè¨ˆãŒã§ãã‚‹ã ã‘å°ã•ã„ã“ã¨ãŒæœ›ã¾ã‚Œã¾ã™ã€‚ãªãœãªã‚‰ã€ã‚¨ãƒ©ãƒ¼ã®æ•°ã‚’å°‘ãªãã—ãŸã„ã‹ã‚‰ã§ã™ã€‚

ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®ç´¯ç©è·é›¢ãŒæœ€å°ã«ãªã‚‹ã‚ˆã†ã«ãƒ©ã‚¤ãƒ³ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ãŸã„ã‹ã‚‰ã§ã™ã€‚ã¾ãŸã€æ–¹å‘ã‚ˆã‚Šã‚‚å¤§ãã•ã‚’é‡è¦–ã™ã‚‹ãŸã‚ã€é …ã‚’åŠ ç®—ã™ã‚‹å‰ã«äºŒä¹—ã—ã¾ã™ã€‚

> **ğŸ§® æ•°å­¦ã‚’è¦‹ã›ã¦**
> 
> ã“ã®ãƒ©ã‚¤ãƒ³ã€æœ€é©ãƒ•ã‚£ãƒƒãƒˆãƒ©ã‚¤ãƒ³ã¯[æ–¹ç¨‹å¼](https://en.wikipedia.org/wiki/Simple_linear_regression)ã§è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ï¼š
> 
> ```
> Y = a + bX
> ```
>
> `X` is the 'explanatory variable'. `Y` is the 'dependent variable'. The slope of the line is `b` and `a` is the y-intercept, which refers to the value of `Y` when `X = 0`. 
>
>![calculate the slope](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.ja.png)
>
> First, calculate the slope `b`. Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> In other words, and referring to our pumpkin data's original question: "predict the price of a pumpkin per bushel by month", `X` would refer to the price and `Y` would refer to the month of sale. 
>
>![complete the equation](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.ja.png)
>
> Calculate the value of Y. If you're paying around $4, it must be April! Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where `Y` is situated when `X = 0`.
>
> You can observe the method of calculation for these values on the [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) web site. Also visit [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) to watch how the numbers' values impact the line.

## Correlation

One more term to understand is the **Correlation Coefficient** between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.

A good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.

âœ… Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of `Month`, eg. *day of the year* (i.e. number of days since the beginning of the year)?

In the code below, we will assume that we have cleaned up the data, and obtained a data frame called `new_pumpkins`, similar to the following:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> The code to clean the data is available in [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). We have performed the same cleaning steps as in the previous lesson, and have calculated `DayOfYear` ã‚«ãƒ©ãƒ ã‚’æ¬¡ã®å¼ã‚’ä½¿ç”¨ã—ã¦è¨ˆç®—ã—ã¾ã™ï¼š

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

ç·šå½¢å›å¸°ã®æ•°å­¦çš„èƒŒæ™¯ã‚’ç†è§£ã—ãŸã®ã§ã€å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ã€ã©ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‹ã¼ã¡ã‚ƒãŒæœ€ã‚‚è‰¯ã„ä¾¡æ ¼ã‚’æŒã¤ã‹ã‚’äºˆæ¸¬ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãƒ›ãƒªãƒ‡ãƒ¼ãƒ‘ãƒ³ãƒ—ã‚­ãƒ³ãƒ‘ãƒƒãƒã®ãŸã‚ã«ã‹ã¼ã¡ã‚ƒã‚’è³¼å…¥ã™ã‚‹äººã¯ã€ã“ã®æƒ…å ±ã‚’åˆ©ç”¨ã—ã¦ã‹ã¼ã¡ã‚ƒãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è³¼å…¥ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã§ã—ã‚‡ã†ã€‚

## ç›¸é–¢ã‚’æ¢ã™

[![åˆå¿ƒè€…å‘ã‘ML - ç›¸é–¢ã‚’æ¢ã™: ç·šå½¢å›å¸°ã®éµ](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "åˆå¿ƒè€…å‘ã‘ML - ç›¸é–¢ã‚’æ¢ã™: ç·šå½¢å›å¸°ã®éµ")

> ğŸ¥ ä¸Šã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ç›¸é–¢ã®æ¦‚è¦ã‚’çŸ­ã„ãƒ“ãƒ‡ã‚ªã§ç¢ºèªã§ãã¾ã™ã€‚

å‰ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã€ç•°ãªã‚‹æœˆã®å¹³å‡ä¾¡æ ¼ãŒæ¬¡ã®ã‚ˆã†ã«è¦‹ãˆã‚‹ã“ã¨ã«æ°—ã¥ã„ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼š

<img alt="æœˆã”ã¨ã®å¹³å‡ä¾¡æ ¼" src="../2-Data/images/barchart.png" width="50%"/>

ã“ã‚Œã¯ã€ã‚ã‚‹ç¨®ã®ç›¸é–¢ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ãŠã‚Šã€`Month` and `Price`, or between `DayOfYear` and `Price`. Here is the scatter plot that shows the latter relationship:

<img alt="Scatter plot of Price vs. Day of Year" src="images/scatter-dayofyear.png" width="50%" /> 

Let's see if there is a correlation using the `corr` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ç›¸é–¢ã‚’è¨ˆç®—ã—ã¦ã¿ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

ç›¸é–¢ã¯ã‹ãªã‚Šå°ã•ã„ã‚ˆã†ã§ã™ã€`Month` and -0.17 by the `DayOfMonth`, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an `ax` parameter to the `scatter` ãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã™ã¹ã¦ã®ãƒã‚¤ãƒ³ãƒˆã‚’åŒã˜ã‚°ãƒ©ãƒ•ã«ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã™ï¼š

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="å¹´ã®ä¸­ã®æ—¥ä»˜ã¨ä¾¡æ ¼ã®æ•£å¸ƒå›³" src="images/scatter-dayofyear-color.png" width="50%" /> 

ç§ãŸã¡ã®èª¿æŸ»ã¯ã€å®Ÿéš›ã®è²©å£²æ—¥ã‚ˆã‚Šã‚‚å“ç¨®ãŒå…¨ä½“ã®ä¾¡æ ¼ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚’æ£’ã‚°ãƒ©ãƒ•ã§ç¢ºèªã§ãã¾ã™ï¼š

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="å“ç¨®ã”ã¨ã®ä¾¡æ ¼ã®æ£’ã‚°ãƒ©ãƒ•" src="images/price-by-variety.png" width="50%" /> 

ä»Šã®ã¨ã“ã‚ã€'ãƒ‘ã‚¤ã‚¿ã‚¤ãƒ—'ã®ã‹ã¼ã¡ã‚ƒå“ç¨®ã«ã®ã¿ç„¦ç‚¹ã‚’å½“ã¦ã€æ—¥ä»˜ãŒä¾¡æ ¼ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="å¹´ã®ä¸­ã®æ—¥ä»˜ã¨ä¾¡æ ¼ã®æ•£å¸ƒå›³" src="images/pie-pumpkins-scatter.png" width="50%" /> 

`Price` and `DayOfYear` using `corr` function, we will get something like `-0.27` ã®ç›¸é–¢ã‚’è¨ˆç®—ã™ã‚‹ã¨ã€äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ„å‘³ã‚’æŒã¤ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

> ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å‰ã«ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚¯ãƒªãƒ¼ãƒ³ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ç·šå½¢å›å¸°ã¯æ¬ æå€¤ã«å¯¾ã—ã¦ã†ã¾ãæ©Ÿèƒ½ã—ãªã„ãŸã‚ã€ã™ã¹ã¦ã®ç©ºã®ã‚»ãƒ«ã‚’å–ã‚Šé™¤ãã“ã¨ãŒç†ã«ã‹ãªã£ã¦ã„ã¾ã™ï¼š

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

ã‚‚ã†ä¸€ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãã‚Œã‚‰ã®ç©ºã®å€¤ã‚’å¯¾å¿œã™ã‚‹åˆ—ã®å¹³å‡å€¤ã§åŸ‹ã‚ã‚‹ã“ã¨ã§ã™ã€‚

## å˜ç´”ãªç·šå½¢å›å¸°

[![åˆå¿ƒè€…å‘ã‘ML - Scikit-learnã‚’ä½¿ç”¨ã—ãŸç·šå½¢ãŠã‚ˆã³å¤šé …å¼å›å¸°](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "åˆå¿ƒè€…å‘ã‘ML - Scikit-learnã‚’ä½¿ç”¨ã—ãŸç·šå½¢ãŠã‚ˆã³å¤šé …å¼å›å¸°")

> ğŸ¥ ä¸Šã®ç”»åƒã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ç·šå½¢å›å¸°ã¨å¤šé …å¼å›å¸°ã®æ¦‚è¦ã‚’çŸ­ã„ãƒ“ãƒ‡ã‚ªã§ç¢ºèªã§ãã¾ã™ã€‚

ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã€**Scikit-learn**ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ã¾ãšã€å…¥åŠ›å€¤ï¼ˆç‰¹å¾´ï¼‰ã¨äºˆæƒ³å‡ºåŠ›ï¼ˆãƒ©ãƒ™ãƒ«ï¼‰ã‚’åˆ¥ã€…ã®numpyé…åˆ—ã«åˆ†ã‘ã¾ã™ï¼š

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«`reshape`ã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ç·šå½¢å›å¸°ã¯2Dé…åˆ—ã‚’å…¥åŠ›ã¨ã—ã¦æœŸå¾…ã—ã€é…åˆ—ã®å„è¡ŒãŒå…¥åŠ›ç‰¹å¾´ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾å¿œã—ã¾ã™ã€‚ç§ãŸã¡ã®å ´åˆã€å…¥åŠ›ãŒ1ã¤ã—ã‹ãªã„ãŸã‚ã€å½¢çŠ¶ãŒNÃ—1ã®é…åˆ—ãŒå¿…è¦ã§ã™ã€‚Nã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã§ã™ã€‚

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ï¼š

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

æœ€å¾Œã«ã€å®Ÿéš›ã®ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯2è¡Œã®ã‚³ãƒ¼ãƒ‰ã§è¡Œã„ã¾ã™ã€‚`LinearRegression` object, and fit it to our data using the `fit` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®šç¾©ã—ã¾ã™ï¼š

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression` object after `fit`-ting contains all the coefficients of the regression, which can be accessed using `.coef_` property. In our case, there is just one coefficient, which should be around `-0.017`. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using `lin_reg.intercept_` - it will be around `21` ãŒç¤ºã—ã¦ã„ã‚‹ã‚ˆã†ã«ã€å¹´ã®åˆã‚ã®ä¾¡æ ¼ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä¾¡æ ¼ã‚’äºˆæ¸¬ã—ã€äºˆæ¸¬å€¤ã¨æœŸå¾…å€¤ã®é•ã„ã‚’æ¸¬å®šã—ã¾ã™ã€‚ã“ã‚Œã¯ã€æœŸå¾…å€¤ã¨äºˆæ¸¬å€¤ã®ã™ã¹ã¦ã®äºŒä¹—èª¤å·®ã®å¹³å‡ã§ã‚ã‚‹å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰ã‚’ä½¿ç”¨ã—ã¦è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

èª¤å·®ã¯ç´„2ãƒã‚¤ãƒ³ãƒˆã§ã€ç´„17ï¼…ã§ã™ã€‚ã‚ã¾ã‚Šè‰¯ãã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã®ã‚‚ã†ä¸€ã¤ã®æŒ‡æ¨™ã¯**æ±ºå®šä¿‚æ•°**ã§ã‚ã‚Šã€æ¬¡ã®ã‚ˆã†ã«å–å¾—ã§ãã¾ã™ï¼š

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
å€¤ãŒ0ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è€ƒæ…®ã›ãšã€*æœ€æ‚ªã®ç·šå½¢äºˆæ¸¬å™¨*ã¨ã—ã¦æ©Ÿèƒ½ã—ã€å˜ã«çµæœã®å¹³å‡å€¤ã‚’ç¤ºã—ã¾ã™ã€‚å€¤ãŒ1ã®å ´åˆã€ã™ã¹ã¦ã®æœŸå¾…å‡ºåŠ›ã‚’å®Œå…¨ã«äºˆæ¸¬ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ç§ãŸã¡ã®å ´åˆã€æ±ºå®šä¿‚æ•°ã¯ç´„0.06ã§ã€ã‹ãªã‚Šä½ã„ã§ã™ã€‚

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨å›å¸°ãƒ©ã‚¤ãƒ³ã‚’ä¸€ç·’ã«ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã€å›å¸°ãŒã©ã®ã‚ˆã†ã«æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’ã‚ˆã‚Šã‚ˆãè¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="ç·šå½¢å›å¸°" src="images/linear-results.png" width="50%" />

## å¤šé …å¼å›å¸°

ã‚‚ã†ä¸€ã¤ã®ç·šå½¢å›å¸°ã®ã‚¿ã‚¤ãƒ—ã¯å¤šé …å¼å›å¸°ã§ã™ã€‚å¤‰æ•°é–“ã«ç·šå½¢é–¢ä¿‚ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€ä¾‹ãˆã°ã€ã‹ã¼ã¡ã‚ƒã®ä½“ç©ãŒå¤§ãã„ã»ã©ä¾¡æ ¼ãŒé«˜ããªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€ã“ã‚Œã‚‰ã®é–¢ä¿‚ã¯å¹³é¢ã‚„ç›´ç·šã¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã§ããªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

âœ… ã“ã“ã«[ã„ãã¤ã‹ã®ä¾‹](https://online.stat.psu.edu/stat501/lesson/9/9.8)ãŒã‚ã‚Šã¾ã™ã€‚å¤šé …å¼å›å¸°ã‚’ä½¿ç”¨ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã§ã™ã€‚

æ—¥ä»˜ã¨ä¾¡æ ¼ã®é–¢ä¿‚ã‚’ã‚‚ã†ä¸€åº¦è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã®æ•£å¸ƒå›³ã¯ç›´ç·šã§åˆ†æã™ã¹ãã ã¨æ€ã„ã¾ã™ã‹ï¼Ÿä¾¡æ ¼ã¯å¤‰å‹•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã“ã®å ´åˆã€å¤šé …å¼å›å¸°ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

âœ… å¤šé …å¼ã¯ã€1ã¤ä»¥ä¸Šã®å¤‰æ•°ã¨ä¿‚æ•°ã§æ§‹æˆã•ã‚Œã‚‹æ•°å­¦çš„è¡¨ç¾ã§ã™ã€‚

å¤šé …å¼å›å¸°ã¯ã€éç·šå½¢ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Šé©åˆã™ã‚‹æ›²ç·šã‚’ä½œæˆã—ã¾ã™ã€‚ç§ãŸã¡ã®å ´åˆã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«`DayOfYear`ã®äºŒä¹—å¤‰æ•°ã‚’å«ã‚ã‚‹ã¨ã€å¹´ã®ã‚ã‚‹æ™‚ç‚¹ã§æœ€å°å€¤ã‚’æŒã¤æ”¾ç‰©ç·šã‚’ãƒ•ã‚£ãƒƒãƒˆã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

Scikit-learnã«ã¯ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ç•°ãªã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®ä¾¿åˆ©ãª[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline)ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã¯ã€**æ¨å®šå™¨**ã®ãƒã‚§ãƒ¼ãƒ³ã§ã™ã€‚ç§ãŸã¡ã®å ´åˆã€ã¾ãšãƒ¢ãƒ‡ãƒ«ã«å¤šé …å¼ç‰¹å¾´ã‚’è¿½åŠ ã—ã€ãã®å¾Œå›å¸°ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—ã¾ã™ï¼š

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

`PolynomialFeatures(2)` means that we will include all second-degree polynomials from the input data. In our case it will just mean `DayOfYear`<sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.

Pipelines can be used in the same manner as the original `LinearRegression` object, i.e. we can `fit` the pipeline, and then use `predict` to get the prediction results. Here is the graph showing test data, and the approximation curve:

<img alt="Polynomial regression" src="images/poly-results.png" width="50%" />

Using Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!

> You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? 

ğŸƒ Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!

## Categorical Features

In the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the `Variety` column is somewhat different from columns like `Month`, because it contains non-numeric values. Such columns are called **categorical**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> ğŸ¥ Click the image above for a short video overview of using categorical features.

Here you can see how average price depends on variety:

<img alt="Average price by variety" src="images/price-by-variety.png" width="50%" />

To take variety into account, we first need to convert it to numeric form, or **encode** it. There are several way we can do it:

* Simple **numeric encoding** will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.
* **One-hot encoding** will replace the `Variety` column by 4 different columns, one for each variety. Each column will contain `1` if the corresponding row is of a given variety, and `0` ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã¤ã¾ã‚Šã€ç·šå½¢å›å¸°ã«ã¯4ã¤ã®ä¿‚æ•°ãŒã‚ã‚Šã€å„ã‹ã¼ã¡ã‚ƒå“ç¨®ã”ã¨ã«1ã¤ã®ä¿‚æ•°ãŒã‚ã‚Šã€ãã®å“ç¨®ã®ã€Œé–‹å§‹ä¾¡æ ¼ã€ï¼ˆã¾ãŸã¯ã€Œè¿½åŠ ä¾¡æ ¼ã€ï¼‰ã‚’è¡¨ã—ã¾ã™ã€‚

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€å“ç¨®ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼š

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå“ç¨®ã‚’ä½¿ç”¨ã—ã¦ç·šå½¢å›å¸°ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã«ã¯ã€`X` and `y`ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãåˆæœŸåŒ–ã™ã‚‹ã ã‘ã§ã™ï¼š

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

ä»–ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ä¸Šè¨˜ã§ä½¿ç”¨ã—ãŸç·šå½¢å›å¸°ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ã§ã™ã€‚è©¦ã—ã¦ã¿ã‚‹ã¨ã€å¹³å‡äºŒä¹—èª¤å·®ã¯ã»ã¼åŒã˜ã§ã™ãŒã€æ±ºå®šä¿‚æ•°ãŒå¤§å¹…ã«é«˜ããªã‚Šã¾ã™ï¼ˆç´„77ï¼…ï¼‰ã€‚ã•ã‚‰ã«æ­£ç¢ºãªäºˆæ¸¬ã‚’è¡Œã†ãŸã‚ã«ã€ã‚ˆã‚Šå¤šãã®ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´ã‚„æ•°å€¤çš„ç‰¹å¾´ï¼ˆä¾‹ãˆã°`Month` or `DayOfYear`. To get one large array of features, we can use `join`ï¼‰ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

ã“ã“ã§ã¯ã€`City` and `Package`ã‚¿ã‚¤ãƒ—ã‚‚è€ƒæ…®ã—ã¦ãŠã‚Šã€MSEã¯2.84ï¼ˆ10ï¼…ï¼‰ã€æ±ºå®šä¿‚æ•°ã¯0.94ã§ã™ï¼

## ã™ã¹ã¦ã‚’ã¾ã¨ã‚ã‚‹

æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã®ä¾‹ã‹ã‚‰ã®çµåˆãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‹æ•°å€¤ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨å¤šé …å¼å›å¸°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã“ã«å®Œå…¨ãªã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã™ï¼š

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

ã“ã‚Œã«ã‚ˆã‚Šã€æ±ºå®šä¿‚æ•°ãŒã»ã¼97ï¼…ã€MSE=2.23ï¼ˆç´„8ï¼…ã®äºˆæ¸¬èª¤å·®ï¼‰ã¨ãªã‚Šã¾ã™ã€‚

| ãƒ¢ãƒ‡ãƒ« | MSE | æ±ºå®šä¿‚æ•° |
|-------|-----|---------------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |
| `Variety` ç·šå½¢ | 5.24 (19.7%) | 0.77 |
| ã™ã¹ã¦ã®ç‰¹å¾´ ç·šå½¢ | 2.84 (10.5%) | 0.94 |
| ã™ã¹ã¦ã®ç‰¹å¾´ å¤šé …å¼ | 2.23 (8.25%) | 0.97 |

ğŸ† ã‚ˆãã§ãã¾ã—ãŸï¼1ã¤ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§4ã¤ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’97ï¼…ã«å‘ä¸Šã•ã›ã¾ã—ãŸã€‚å›å¸°ã®æœ€çµ‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã«ã¤ã„ã¦å­¦ã³ã€ã‚«ãƒ†ã‚´ãƒªã‚’

**å…è²¬äº‹é …**:
ã“ã®æ–‡æ›¸ã¯æ©Ÿæ¢°ç¿»è¨³ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³ã•ã‚Œã¦ã„ã¾ã™ã€‚æ­£ç¢ºã•ã‚’æœŸã™ã‚ˆã†åŠªã‚ã¦ã„ã¾ã™ãŒã€è‡ªå‹•ç¿»è¨³ã«ã¯èª¤ã‚Šã‚„ä¸æ­£ç¢ºã•ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚åŸæ–‡ã®è¨€èªã«ã‚ˆã‚‹æ–‡æ›¸ãŒæ¨©å¨ã‚ã‚‹æƒ…å ±æºã¨è¦‹ãªã•ã‚Œã‚‹ã¹ãã§ã™ã€‚é‡è¦ãªæƒ…å ±ã«ã¤ã„ã¦ã¯ã€å°‚é–€ã®äººé–“ã«ã‚ˆã‚‹ç¿»è¨³ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã®ç¿»è¨³ã®ä½¿ç”¨ã«èµ·å› ã™ã‚‹èª¤è§£ã‚„èª¤èªã«ã¤ã„ã¦ã€å½“ç¤¾ã¯ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚