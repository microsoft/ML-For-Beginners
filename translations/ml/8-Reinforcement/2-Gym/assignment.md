<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-12-19T15:42:43+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "ml"
}
-->
# ട്രെയിൻ മൗണ്ടൻ കാർ

[OpenAI Gym](http://gym.openai.com) എല്ലാ പരിസ്ഥിതികളും ഒരേ API നൽകുന്ന വിധത്തിൽ രൂപകൽപ്പന ചെയ്തിരിക്കുന്നു - അഥവാ ഒരേ രീതിയിലുള്ള `reset`, `step` , `render` മെത്തഡുകളും **action space** , **observation space** എന്നിവയുടെ ഒരേ ആബ്സ്ട്രാക്ഷനുകളും. അതിനാൽ, കുറഞ്ഞ കോഡ് മാറ്റങ്ങളോടെ വ്യത്യസ്ത പരിസ്ഥിതികളിൽ ഒരേ reinforcement learning ആൽഗോരിതങ്ങൾ ഉപയോഗിക്കാൻ സാധിക്കണം.

## ഒരു മൗണ്ടൻ കാർ പരിസ്ഥിതി

[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) ഒരു താഴ്വരയിൽ കുടുങ്ങിയ ഒരു കാർ ഉൾക്കൊള്ളുന്നു:

<img src="../../../../translated_images/mountaincar.43d56e588ce581c2.ml.png" width="300"/>

ലക്ഷ്യം താഴ്വരയിൽ നിന്ന് പുറത്തുകടക്കുകയും പതാക പിടിക്കുകയും ചെയ്യുക ആണ്, ഓരോ ഘട്ടത്തിലും താഴെപ്പറയുന്ന പ്രവർത്തനങ്ങളിൽ ഒന്നും ചെയ്യുക:

| Value | അർത്ഥം |
|---|---|
| 0 | ഇടത്തേക്ക് വേഗം കൂട്ടുക |
| 1 | വേഗം കൂട്ടരുത് |
| 2 | വലത്തേക്ക് വേഗം കൂട്ടുക |

എന്നാൽ ഈ പ്രശ്നത്തിന്റെ പ്രധാന തന്ത്രം, കാർ എഞ്ചിൻ മൗണ്ടൻ ഒരു തവണയിൽ കയറിയെത്താൻ മതിയായ ശക്തിയുള്ളതല്ല എന്നതാണ്. അതിനാൽ വിജയിക്കാൻ ഏക മാർഗം മുന്നോട്ടും പിന്നോട്ടും ഓടിച്ച് മോമെന്റം സൃഷ്ടിക്കുകയാണ്.

Observation space രണ്ട് മൂല്യങ്ങളടങ്ങിയതാണ്:

| നമ്പർ | നിരീക്ഷണം  | കുറഞ്ഞത് | പരമാവധി |
|-----|--------------|-----|-----|
|  0  | കാർ സ്ഥാനം | -1.2| 0.6 |
|  1  | കാർ വേഗത | -0.07 | 0.07 |

മൗണ്ടൻ കാർക്ക് റിവാർഡ് സിസ്റ്റം വളരെ സങ്കീർണ്ണമാണ്:

 * ഏജന്റ് പതാക (സ്ഥാനം = 0.5) മൗണ്ടന്റെ മുകളിൽ എത്തിച്ചേർന്നാൽ 0 റിവാർഡ് ലഭിക്കും.
 * ഏജന്റിന്റെ സ്ഥാനം 0.5-ൽ കുറവായാൽ -1 റിവാർഡ് ലഭിക്കും.

കാർ സ്ഥാനം 0.5-ൽ കൂടുതലായാൽ അല്ലെങ്കിൽ എപ്പിസോഡ് ദൈർഘ്യം 200-ൽ കൂടുതലായാൽ എപ്പിസോഡ് അവസാനിക്കും.
## നിർദ്ദേശങ്ങൾ

മൗണ്ടൻ കാർ പ്രശ്നം പരിഹരിക്കാൻ നമ്മുടെ reinforcement learning ആൽഗോരിതം അനുയോജ്യമായി മാറ്റുക. നിലവിലുള്ള [notebook.ipynb](notebook.ipynb) കോഡിൽ നിന്നാരംഭിച്ച് പുതിയ പരിസ്ഥിതി ഉപയോഗിക്കുക, സ്റ്റേറ്റ് ഡിസ്ക്രീറ്റൈസേഷൻ ഫംഗ്ഷനുകൾ മാറ്റുക, കുറഞ്ഞ കോഡ് മാറ്റങ്ങളോടെ നിലവിലുള്ള ആൽഗോരിതം ട്രെയിൻ ചെയ്യാൻ ശ്രമിക്കുക. ഹൈപ്പർപാരാമീറ്ററുകൾ ക്രമീകരിച്ച് ഫലം മെച്ചപ്പെടുത്തുക.

> **കുറിപ്പ്**: ആൽഗോരിതം കൺവെർജ് ചെയ്യാൻ ഹൈപ്പർപാരാമീറ്ററുകൾ ക്രമീകരിക്കേണ്ടതുണ്ടാകാം. 
## റൂബ്രിക്

| മാനദണ്ഡം | ഉദാഹരണമായത് | യോജിച്ചത് | മെച്ചപ്പെടുത്തേണ്ടത് |
| -------- | --------- | -------- | ----------------- |
|          | Q-Learning ആൽഗോരിതം CartPole ഉദാഹരണത്തിൽ നിന്നു കുറഞ്ഞ കോഡ് മാറ്റങ്ങളോടെ വിജയകരമായി അനുയോജ്യമായി മാറ്റിയിട്ടുണ്ട്, 200 ഘട്ടത്തിനുള്ളിൽ പതാക പിടിക്കുന്ന പ്രശ്നം പരിഹരിക്കാൻ കഴിയും. | ഇന്റർനെറ്റിൽ നിന്നുള്ള പുതിയ Q-Learning ആൽഗോരിതം സ്വീകരിച്ചിട്ടുണ്ടെങ്കിലും നന്നായി രേഖപ്പെടുത്തിയിട്ടുണ്ട്; അല്ലെങ്കിൽ നിലവിലുള്ള ആൽഗോരിതം സ്വീകരിച്ചിട്ടുണ്ടെങ്കിലും ആഗ്രഹിച്ച ഫലം ലഭിച്ചിട്ടില്ല | വിദ്യാർത്ഥി യാതൊരു ആൽഗോരിതവും വിജയകരമായി സ്വീകരിക്കാൻ കഴിഞ്ഞില്ല, പക്ഷേ പരിഹാരത്തിലേക്ക് ഗണ്യമായ മുന്നേറ്റങ്ങൾ (സ്റ്റേറ്റ് ഡിസ്ക്രീറ്റൈസേഷൻ, Q-ടേബിൾ ഡാറ്റാ ഘടന എന്നിവ നടപ്പിലാക്കൽ) നടത്തിയിട്ടുണ്ട് |

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാപത്രം**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ വ്യാഖ്യാനക്കേടുകൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->