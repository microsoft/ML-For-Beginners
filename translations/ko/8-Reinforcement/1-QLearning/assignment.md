<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-09-04T00:23:54+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "ko"
}
-->
# 더 현실적인 세계

우리의 상황에서 피터는 거의 지치거나 배고프지 않은 상태로 이동할 수 있었습니다. 더 현실적인 세계에서는 피터가 가끔씩 앉아서 쉬어야 하고, 스스로 먹을 것도 챙겨야 합니다. 다음 규칙들을 구현하여 우리의 세계를 더 현실적으로 만들어 봅시다:

1. 한 장소에서 다른 장소로 이동할 때, 피터는 **에너지**를 잃고 약간의 **피로**를 얻게 됩니다.
2. 피터는 사과를 먹음으로써 에너지를 얻을 수 있습니다.
3. 피터는 나무 아래나 풀밭에서 쉬면서 피로를 해소할 수 있습니다 (즉, 나무나 풀이 있는 보드 위치로 이동하면 됨 - 초록색 필드).
4. 피터는 늑대를 찾아내고 처치해야 합니다.
5. 늑대를 처치하려면, 피터는 특정 수준의 에너지와 피로를 유지해야 하며, 그렇지 않으면 전투에서 패배합니다.

## 지침

해결책의 시작점으로 원본 [notebook.ipynb](notebook.ipynb) 노트북을 사용하세요.

게임 규칙에 따라 위의 보상 함수를 수정하고, 강화 학습 알고리즘을 실행하여 게임에서 승리하기 위한 최적의 전략을 학습하세요. 그리고 무작위 이동(random walk)과 알고리즘의 결과를 비교하여 승리 및 패배 횟수를 평가하세요.

> **Note**: 새로운 세계에서는 상태가 더 복잡해지며, 인간의 위치 외에도 피로와 에너지 수준을 포함합니다. 상태를 (Board, energy, fatigue) 형태의 튜플로 표현하거나, 상태를 정의하는 클래스를 만들 수도 있습니다(예: `Board`에서 파생). 또는 [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py) 파일의 기존 `Board` 클래스를 수정할 수도 있습니다.

해결책에서는 무작위 이동 전략에 대한 코드를 유지하고, 알고리즘과 무작위 이동의 결과를 마지막에 비교하세요.

> **Note**: 작동을 위해 하이퍼파라미터를 조정해야 할 수도 있습니다. 특히 에포크 수를 조정해야 합니다. 게임의 성공(늑대와의 전투 승리)은 드문 이벤트이므로, 훨씬 더 긴 학습 시간이 필요할 수 있습니다.

## 평가 기준

| 기준      | 우수                                                                                                                                                                                                 | 적절                                                                                                                                                                                   | 개선 필요                                                                                                                                |
| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
|           | 새로운 세계 규칙 정의, Q-러닝 알고리즘, 텍스트 설명이 포함된 노트북이 제시되며, Q-러닝이 무작위 이동과 비교하여 결과를 크게 개선함.                                                                  | 노트북이 제시되고, Q-러닝이 구현되어 무작위 이동과 비교하여 결과를 개선하지만, 개선 정도가 크지 않거나, 노트북이 제대로 문서화되지 않고 코드 구조가 잘 정리되지 않음.                     | 세계 규칙을 재정의하려는 시도가 있지만, Q-러닝 알고리즘이 작동하지 않거나, 보상 함수가 완전히 정의되지 않음.                                                                  |

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 신뢰할 수 있는 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.