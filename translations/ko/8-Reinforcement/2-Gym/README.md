## ì‚¬ì „ ìš”êµ¬ì‚¬í•­

ì´ ê°•ì˜ì—ì„œëŠ” ë‹¤ì–‘í•œ **í™˜ê²½**ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ **OpenAI Gym**ì´ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ê°•ì˜ì˜ ì½”ë“œë¥¼ ë¡œì»¬ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: Visual Studio Codeì—ì„œ). ì´ ê²½ìš° ì‹œë®¬ë ˆì´ì…˜ì´ ìƒˆ ì°½ì—ì„œ ì—´ë¦½ë‹ˆë‹¤. ì˜¨ë¼ì¸ìœ¼ë¡œ ì½”ë“œë¥¼ ì‹¤í–‰í•  ë•ŒëŠ” [ì—¬ê¸°](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) ì„¤ëª…ëœ ëŒ€ë¡œ ì•½ê°„ì˜ ìˆ˜ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## OpenAI Gym

ì´ì „ ê°•ì˜ì—ì„œëŠ” ìš°ë¦¬ê°€ ì§ì ‘ ì •ì˜í•œ `Board` í´ë˜ìŠ¤ê°€ ê²Œì„ì˜ ê·œì¹™ê³¼ ìƒíƒœë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” **ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½**ì„ ì‚¬ìš©í•˜ì—¬ ê· í˜• ì¡ê¸° ë§‰ëŒ€ì˜ ë¬¼ë¦¬í•™ì„ ì‹œë®¬ë ˆì´ì…˜í•  ê²ƒì…ë‹ˆë‹¤. ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ ì¤‘ í•˜ë‚˜ëŠ” [Gym](https://gym.openai.com/)ìœ¼ë¡œ, [OpenAI](https://openai.com/)ì—ì„œ ìœ ì§€ ê´€ë¦¬í•©ë‹ˆë‹¤. ì´ Gymì„ ì‚¬ìš©í•˜ì—¬ ì¹´íŠ¸í´ ì‹œë®¬ë ˆì´ì…˜ë¶€í„° Atari ê²Œì„ê¹Œì§€ ë‹¤ì–‘í•œ **í™˜ê²½**ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ì°¸ê³ **: OpenAI Gymì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ í™˜ê²½ì€ [ì—¬ê¸°](https://gym.openai.com/envs/#classic_control)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¨¼ì €, gymì„ ì„¤ì¹˜í•˜ê³  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤(ì½”ë“œ ë¸”ë¡ 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## ì—°ìŠµ - ì¹´íŠ¸í´ í™˜ê²½ ì´ˆê¸°í™”

ì¹´íŠ¸í´ ê· í˜• ë¬¸ì œë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ í•´ë‹¹ í™˜ê²½ì„ ì´ˆê¸°í™”í•´ì•¼ í•©ë‹ˆë‹¤. ê° í™˜ê²½ì€ ë‹¤ìŒê³¼ ì—°ê´€ë©ë‹ˆë‹¤:

- **ê´€ì°° ê³µê°„**: í™˜ê²½ìœ¼ë¡œë¶€í„° ë°›ëŠ” ì •ë³´ì˜ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì¹´íŠ¸í´ ë¬¸ì œì˜ ê²½ìš°, ë§‰ëŒ€ì˜ ìœ„ì¹˜, ì†ë„ ë° ê¸°íƒ€ ê°’ì„ ë°›ìŠµë‹ˆë‹¤.

- **ì•¡ì…˜ ê³µê°„**: ê°€ëŠ¥í•œ ë™ì‘ì„ ì •ì˜í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ê²½ìš°, ì•¡ì…˜ ê³µê°„ì€ ì´ì‚°ì ì´ë©°, **ì™¼ìª½**ê³¼ **ì˜¤ë¥¸ìª½**ì˜ ë‘ ê°€ì§€ ë™ì‘ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. (ì½”ë“œ ë¸”ë¡ 2)

1. ì´ˆê¸°í™”í•˜ë ¤ë©´ ë‹¤ìŒ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

í™˜ê²½ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë³´ê¸° ìœ„í•´ 100ë‹¨ê³„ì˜ ì§§ì€ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•´ ë´…ì‹œë‹¤. ê° ë‹¨ê³„ì—ì„œ `action_space`ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ë™ì‘ ì¤‘ í•˜ë‚˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

1. ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.

    âœ… ì´ ì½”ë“œëŠ” ë¡œì»¬ Python ì„¤ì¹˜ì—ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤! (ì½”ë“œ ë¸”ë¡ 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    ë‹¤ìŒê³¼ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

    ![ê· í˜• ì¡íˆì§€ ì•ŠëŠ” ì¹´íŠ¸í´](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. ì‹œë®¬ë ˆì´ì…˜ ì¤‘ì— ì–´ë–»ê²Œ í–‰ë™í• ì§€ ê²°ì •í•˜ê¸° ìœ„í•´ ê´€ì°° ê°’ì„ ì–»ì–´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ì‹¤, step í•¨ìˆ˜ëŠ” í˜„ì¬ì˜ ê´€ì°° ê°’, ë³´ìƒ í•¨ìˆ˜ ë° ì‹œë®¬ë ˆì´ì…˜ì„ ê³„ì†í•  ê°€ì¹˜ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì™„ë£Œ í”Œë˜ê·¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤: (ì½”ë“œ ë¸”ë¡ 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    ë…¸íŠ¸ë¶ ì¶œë ¥ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ë³´ê²Œ ë  ê²ƒì…ë‹ˆë‹¤:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    ì‹œë®¬ë ˆì´ì…˜ì˜ ê° ë‹¨ê³„ì—ì„œ ë°˜í™˜ë˜ëŠ” ê´€ì°° ë²¡í„°ëŠ” ë‹¤ìŒ ê°’ì„ í¬í•¨í•©ë‹ˆë‹¤:
    - ì¹´íŠ¸ì˜ ìœ„ì¹˜
    - ì¹´íŠ¸ì˜ ì†ë„
    - ë§‰ëŒ€ì˜ ê°ë„
    - ë§‰ëŒ€ì˜ íšŒì „ ì†ë„

1. ì´ ìˆ«ìë“¤ì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤: (ì½”ë“œ ë¸”ë¡ 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    ê° ì‹œë®¬ë ˆì´ì…˜ ë‹¨ê³„ì—ì„œ ë³´ìƒ ê°’ì´ í•­ìƒ 1ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìš°ë¦¬ì˜ ëª©í‘œê°€ ê°€ëŠ¥í•œ í•œ ì˜¤ë˜ ìƒì¡´í•˜ëŠ” ê²ƒ, ì¦‰ ë§‰ëŒ€ë¥¼ ê°€ëŠ¥í•œ í•œ ì˜¤ë«ë™ì•ˆ ìˆ˜ì§ì— ê°€ê¹ê²Œ ìœ ì§€í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

    âœ… ì‚¬ì‹¤, ì¹´íŠ¸í´ ì‹œë®¬ë ˆì´ì…˜ì€ 100ë²ˆì˜ ì—°ì†ì ì¸ ì‹œë„ì—ì„œ í‰ê·  ë³´ìƒì´ 195ì— ë„ë‹¬í•˜ë©´ í•´ê²°ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.

## ìƒíƒœ ì´ì‚°í™”

Q-Learningì—ì„œëŠ” ê° ìƒíƒœì—ì„œ ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ì •ì˜í•˜ëŠ” Q-Tableì„ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ìƒíƒœê°€ **ì´ì‚°ì **ì´ì–´ì•¼ í•˜ë©°, ë” ì •í™•í•˜ê²ŒëŠ” ìœ í•œí•œ ìˆ˜ì˜ ì´ì‚° ê°’ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê´€ì°° ê°’ì„ **ì´ì‚°í™”**í•˜ì—¬ ìœ í•œí•œ ìƒíƒœ ì§‘í•©ìœ¼ë¡œ ë§¤í•‘í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì—ëŠ” ëª‡ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤:

- **êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°**. íŠ¹ì • ê°’ì˜ ë²”ìœ„ë¥¼ ì•Œê³  ìˆëŠ” ê²½ìš°, ì´ ë²”ìœ„ë¥¼ ì—¬ëŸ¬ **êµ¬ê°„**ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìœ¼ë©°, ê·¸ëŸ° ë‹¤ìŒ ê°’ì„ í•´ë‹¹í•˜ëŠ” êµ¬ê°„ ë²ˆí˜¸ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ë””ì§€í„¸í™”ì— ì„ íƒí•œ êµ¬ê°„ ìˆ˜ì— ë”°ë¼ ìƒíƒœ í¬ê¸°ë¥¼ ì •í™•íˆ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  
âœ… ê°’ì„ ìœ í•œí•œ ë²”ìœ„(ì˜ˆ: -20ì—ì„œ 20)ë¡œ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì„ í˜• ë³´ê°„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê·¸ëŸ° ë‹¤ìŒ ê°’ì„ ë°˜ì˜¬ë¦¼í•˜ì—¬ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ì…ë ¥ ê°’ì˜ ì •í™•í•œ ë²”ìœ„ë¥¼ ëª¨ë¥´ëŠ” ê²½ìš° ìƒíƒœ í¬ê¸°ì— ëŒ€í•œ ì œì–´ê°€ ëœ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ì˜ ê²½ìš° 4ê°œì˜ ê°’ ì¤‘ 2ê°œëŠ” ìƒí•œ/í•˜í•œ ê°’ì´ ì—†ìœ¼ë©°, ì´ëŠ” ë¬´í•œí•œ ìˆ˜ì˜ ìƒíƒœë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ ì˜ˆì œì—ì„œëŠ” ë‘ ë²ˆì§¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë‚˜ì¤‘ì— ì•Œê²Œ ë˜ê² ì§€ë§Œ, ì •ì˜ë˜ì§€ ì•Šì€ ìƒí•œ/í•˜í•œ ê°’ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ¬í•œ ê°’ë“¤ì€ íŠ¹ì • ìœ í•œí•œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš°ê°€ ë“œë­…ë‹ˆë‹¤. ë”°ë¼ì„œ ê·¹ë‹¨ì ì¸ ê°’ì´ ìˆëŠ” ìƒíƒœëŠ” ë§¤ìš° ë“œë­…ë‹ˆë‹¤.

1. ëª¨ë¸ì˜ ê´€ì°° ê°’ì„ ë°›ì•„ 4ê°œì˜ ì •ìˆ˜ ê°’ íŠœí”Œì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: (ì½”ë“œ ë¸”ë¡ 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. êµ¬ê°„ì„ ì‚¬ìš©í•˜ëŠ” ë‹¤ë¥¸ ì´ì‚°í™” ë°©ë²•ì„ íƒìƒ‰í•´ ë´…ì‹œë‹¤: (ì½”ë“œ ë¸”ë¡ 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. ì§§ì€ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ê³  ì´ëŸ¬í•œ ì´ì‚° í™˜ê²½ ê°’ì„ ê´€ì°°í•´ ë´…ì‹œë‹¤. `discretize` and `discretize_bins` ë‘˜ ë‹¤ ì‹œë„í•´ ë³´ê³  ì°¨ì´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

    âœ… discretize_binsëŠ” 0 ê¸°ë°˜ì˜ êµ¬ê°„ ë²ˆí˜¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì…ë ¥ ë³€ìˆ˜ ê°’ì´ 0ì— ê°€ê¹Œìš´ ê²½ìš° êµ¬ê°„ì˜ ì¤‘ê°„ ê°’(10)ì—ì„œ ë²ˆí˜¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. discretizeì—ì„œëŠ” ì¶œë ¥ ê°’ì˜ ë²”ìœ„ì— ì‹ ê²½ ì“°ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, ê°’ì´ ì´ë™í•˜ì§€ ì•Šìœ¼ë©° 0ì´ 0ì— í•´ë‹¹í•©ë‹ˆë‹¤. (ì½”ë“œ ë¸”ë¡ 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… í™˜ê²½ ì‹¤í–‰ì„ ë³´ê³  ì‹¶ë‹¤ë©´ env.renderë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë” ë¹ ë¦…ë‹ˆë‹¤. Q-Learning ê³¼ì • ë™ì•ˆ ì´ "ë³´ì´ì§€ ì•ŠëŠ”" ì‹¤í–‰ì„ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.

## Q-Table êµ¬ì¡°

ì´ì „ ê°•ì˜ì—ì„œëŠ” ìƒíƒœê°€ 0ì—ì„œ 8ê¹Œì§€ì˜ ê°„ë‹¨í•œ ìˆ«ì ìŒì´ì—ˆê¸° ë•Œë¬¸ì— Q-Tableì„ 8x8x2 ëª¨ì–‘ì˜ numpy í…ì„œë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ í¸ë¦¬í–ˆìŠµë‹ˆë‹¤. êµ¬ê°„ ì´ì‚°í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ìƒíƒœ ë²¡í„°ì˜ í¬ê¸°ë„ ì•Œë ¤ì ¸ ìˆìœ¼ë¯€ë¡œ ë™ì¼í•œ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ìƒíƒœë¥¼ 20x20x10x10x2 ëª¨ì–‘ì˜ ë°°ì—´ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì—¬ê¸°ì„œ 2ëŠ” ì•¡ì…˜ ê³µê°„ì˜ ì°¨ì›ì´ë©°, ì²« ë²ˆì§¸ ì°¨ì›ì€ ê´€ì°° ê³µê°„ì˜ ê° ë§¤ê°œë³€ìˆ˜ì— ì‚¬ìš©í•  êµ¬ê°„ ìˆ˜ì— í•´ë‹¹í•©ë‹ˆë‹¤).

ê·¸ëŸ¬ë‚˜ ê´€ì°° ê³µê°„ì˜ ì •í™•í•œ ì°¨ì›ì´ ì•Œë ¤ì§€ì§€ ì•Šì€ ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. `discretize` í•¨ìˆ˜ì˜ ê²½ìš°, ì¼ë¶€ ì›ë˜ ê°’ì´ ì œí•œë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ìƒíƒœê°€ íŠ¹ì • í•œê³„ ë‚´ì— ë¨¸ë¬´ë¥´ëŠ”ì§€ í™•ì‹ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì•½ê°„ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ Q-Tableì„ ì‚¬ì „ìœ¼ë¡œ í‘œí˜„í•  ê²ƒì…ë‹ˆë‹¤.

1. *(state, action)* ìŒì„ ì‚¬ì „ í‚¤ë¡œ ì‚¬ìš©í•˜ê³  ê°’ì€ Q-Table í•­ëª© ê°’ì— í•´ë‹¹í•©ë‹ˆë‹¤. (ì½”ë“œ ë¸”ë¡ 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    ì—¬ê¸°ì„œ `qvalues()` í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ ì£¼ì–´ì§„ ìƒíƒœì— ëŒ€í•œ Q-Table ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤. Q-Tableì— í•­ëª©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

## Q-Learning ì‹œì‘í•˜ê¸°

ì´ì œ Peterì—ê²Œ ê· í˜•ì„ ì¡ëŠ” ë²•ì„ ê°€ë¥´ì¹  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!

1. ë¨¼ì € ëª‡ ê°€ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ ë´…ì‹œë‹¤: (ì½”ë“œ ë¸”ë¡ 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    ì—¬ê¸°ì„œ `alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.

    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.

    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. 

    âœ… In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those "mistakes"

### Improve the algorithm

We can also make two improvements to our algorithm from the previous lesson:

- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.
  
- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.

1. Collect all cumulative rewards at each simulation at `rewards` ë²¡í„°ë¥¼ ë‚˜ì¤‘ì— í”Œë¡œíŒ…í•˜ê¸° ìœ„í•´ ì •ì˜í•©ë‹ˆë‹¤. (ì½”ë“œ ë¸”ë¡  11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

ì´ ê²°ê³¼ì—ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒ:

- **ëª©í‘œì— ê°€ê¹Œì›Œì§**. 100íšŒ ì—°ì† ì‹œë®¬ë ˆì´ì…˜ì—ì„œ 195 ëˆ„ì  ë³´ìƒì„ ë‹¬ì„±í•˜ëŠ” ëª©í‘œì— ë§¤ìš° ê°€ê¹Œì›Œì¡ŒìŠµë‹ˆë‹¤. ë˜ëŠ” ì‹¤ì œë¡œ ë‹¬ì„±í–ˆì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤! ë” ì‘ì€ ìˆ«ìë¥¼ ì–»ë”ë¼ë„ 5000íšŒ ì‹¤í–‰ì˜ í‰ê· ì„ ë‚´ê³  ìˆê¸° ë•Œë¬¸ì— ê³µì‹ ê¸°ì¤€ì—ì„œëŠ” 100íšŒ ì‹¤í–‰ë§Œ í•„ìš”í•©ë‹ˆë‹¤.
  
- **ë³´ìƒì´ ë–¨ì–´ì§€ê¸° ì‹œì‘í•¨**. ë•Œë•Œë¡œ ë³´ìƒì´ ë–¨ì–´ì§€ê¸° ì‹œì‘í•˜ì—¬ Q-Tableì— ì´ë¯¸ í•™ìŠµëœ ê°’ì„ ìƒí™©ì„ ì•…í™”ì‹œí‚¤ëŠ” ê°’ìœ¼ë¡œ "íŒŒê´´"í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ê´€ì°°ì€ í•™ìŠµ ì§„í–‰ ìƒí™©ì„ í”Œë¡œíŒ…í•  ë•Œ ë” ëª…í™•í•˜ê²Œ ë³´ì…ë‹ˆë‹¤.

## í•™ìŠµ ì§„í–‰ ìƒí™© í”Œë¡œíŒ…

í›ˆë ¨ ì¤‘ì— ê° ë°˜ë³µì—ì„œ ëˆ„ì  ë³´ìƒ ê°’ì„ `rewards` ë²¡í„°ì— ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°˜ë³µ ë²ˆí˜¸ì— ëŒ€í•´ í”Œë¡œíŒ…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```python
plt.plot(rewards)
```

![í•™ìŠµ ì§„í–‰ ìƒí™© ì›ë³¸](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.ko.png)

ì´ ê·¸ë˜í”„ì—ì„œëŠ” ì•„ë¬´ê²ƒë„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™•ë¥ ì  í•™ìŠµ ê³¼ì •ì˜ íŠ¹ì„±ìƒ í›ˆë ¨ ì„¸ì…˜ì˜ ê¸¸ì´ê°€ í¬ê²Œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê·¸ë˜í”„ë¥¼ ë” ì´í•´í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ê¸° ìœ„í•´ 100íšŒ ì‹¤í—˜ì— ëŒ€í•œ **ì´ë™ í‰ê· **ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” `np.convolve`ë¥¼ ì‚¬ìš©í•˜ì—¬ í¸ë¦¬í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: (ì½”ë“œ ë¸”ë¡ 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![í•™ìŠµ ì§„í–‰ ìƒí™©](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.ko.png)

## í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³€ê²½

í•™ìŠµì„ ë” ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ í›ˆë ¨ ì¤‘ì— ì¼ë¶€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. íŠ¹íˆ:

- **í•™ìŠµë¥ **ì˜ ê²½ìš°, `alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.

- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon` ê°’ì„ ê±°ì˜ 1ê¹Œì§€ ì˜¬ë¦½ë‹ˆë‹¤.

> **ê³¼ì œ 1**: í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì¡°ì •í•˜ì—¬ ë” ë†’ì€ ëˆ„ì  ë³´ìƒì„ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. 195ë¥¼ ì´ˆê³¼í•˜ê³  ìˆë‚˜ìš”?

> **ê³¼ì œ 2**: ë¬¸ì œë¥¼ ê³µì‹ì ìœ¼ë¡œ í•´ê²°í•˜ë ¤ë©´ 100íšŒ ì—°ì† ì‹¤í–‰ì—ì„œ í‰ê·  195 ë³´ìƒì„ ë‹¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤. í›ˆë ¨ ì¤‘ì— ì´ë¥¼ ì¸¡ì •í•˜ê³  ë¬¸ì œë¥¼ ê³µì‹ì ìœ¼ë¡œ í•´ê²°í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!

## ê²°ê³¼ë¥¼ ì‹¤ì œë¡œ ë³´ê¸°

í›ˆë ¨ëœ ëª¨ë¸ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‹¤ì œë¡œ ë³´ëŠ” ê²ƒì€ í¥ë¯¸ë¡œìš¸ ê²ƒì…ë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ê³  í›ˆë ¨ ì¤‘ê³¼ ë™ì¼í•œ ë™ì‘ ì„ íƒ ì „ëµì„ ë”°ë¥´ë©°, Q-Tableì˜ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤: (ì½”ë“œ ë¸”ë¡ 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

![ê· í˜• ì¡ëŠ” ì¹´íŠ¸í´](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€ë„ì „

> **ê³¼ì œ 3**: ì—¬ê¸°ì„œëŠ” ìµœì¢… Q-Tableì„ ì‚¬ìš©í–ˆëŠ”ë°, ì´ëŠ” ìµœìƒì˜ ê²ƒì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœê³  ì„±ëŠ¥ì˜ Q-Tableì„ `Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.

> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ë†’ì€ Q-Table ê°’ì— í•´ë‹¹í•˜ëŠ” ë™ì‘ ë²ˆí˜¸ë¥¼ ì°¾ëŠ” ì „ëµì„ êµ¬í˜„í•˜ì„¸ìš”. ì´ ì „ëµì´ ê· í˜•ì„ ê°œì„ í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

## [ê°•ì˜ í›„ í€´ì¦ˆ](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## ê³¼ì œ
[Mountain Car í›ˆë ¨í•˜ê¸°](assignment.md)

## ê²°ë¡ 

ì´ì œ ìš°ë¦¬ëŠ” ë³´ìƒ í•¨ìˆ˜ë¥¼ ì œê³µí•˜ê³ , ì§€ëŠ¥ì ìœ¼ë¡œ íƒìƒ‰í•  ê¸°íšŒë¥¼ ì œê³µí•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì‚° ë° ì—°ì† í™˜ê²½ì—ì„œ Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ ì„±ê³µì ìœ¼ë¡œ ì ìš©í–ˆì§€ë§Œ, ì´ì‚° ë™ì‘ë§Œì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

ë™ì‘ ìƒíƒœë„ ì—°ì†ì ì´ê³ , ê´€ì°° ê³µê°„ì´ Atari ê²Œì„ í™”ë©´ ì´ë¯¸ì§€ì²˜ëŸ¼ í›¨ì”¬ ë” ë³µì¡í•œ ìƒí™©ì„ ì—°êµ¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì—ì„œëŠ” ì‹ ê²½ë§ê³¼ ê°™ì€ ë” ê°•ë ¥í•œ ê¸°ê³„ í•™ìŠµ ê¸°ìˆ ì„ ì‚¬ìš©í•´ì•¼ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë” ê³ ê¸‰ ì£¼ì œëŠ” ìš°ë¦¬ì˜ ë‹¤ê°€ì˜¤ëŠ” ë” ê³ ê¸‰ AI ê³¼ì •ì˜ ì£¼ì œì…ë‹ˆë‹¤.

**ë©´ì±… ì¡°í•­**:
ì´ ë¬¸ì„œëŠ” ê¸°ê³„ ê¸°ë°˜ AI ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ì„ ìœ„í•´ ë…¸ë ¥í•˜ì§€ë§Œ, ìë™ ë²ˆì—­ì—ëŠ” ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•ì„±ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë¬¸ì„œì˜ ëª¨êµ­ì–´ ë²„ì „ì„ ê¶Œìœ„ ìˆëŠ” ì†ŒìŠ¤ë¡œ ê°„ì£¼í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì •ë³´ì˜ ê²½ìš°, ì „ë¬¸ì ì¸ ì¸ê°„ ë²ˆì—­ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ì´ ë²ˆì—­ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤í•´ë‚˜ ì˜¤ì—­ì— ëŒ€í•´ ìš°ë¦¬ëŠ” ì±…ì„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.