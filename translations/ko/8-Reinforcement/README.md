<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-04T00:14:05+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "ko"
}
-->
# 강화 학습 소개

강화 학습(RL)은 지도 학습과 비지도 학습과 함께 기본적인 머신 러닝 패러다임 중 하나로 여겨집니다. RL은 올바른 결정을 내리거나 최소한 그로부터 배우는 데 초점이 맞춰져 있습니다.

주식 시장과 같은 시뮬레이션 환경을 상상해 보세요. 특정 규제를 적용하면 어떤 일이 발생할까요? 긍정적인 효과가 있을까요, 아니면 부정적인 효과가 있을까요? 부정적인 일이 발생하면 이를 _부정적 강화_로 받아들이고, 이를 통해 배우며 방향을 바꿔야 합니다. 긍정적인 결과가 나오면 _긍정적 강화_를 기반으로 더 발전시켜야 합니다.

![peter and the wolf](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.ko.png)

> 피터와 그의 친구들은 배고픈 늑대에게서 도망쳐야 합니다! 이미지 제공: [Jen Looper](https://twitter.com/jenlooper)

## 지역 주제: 피터와 늑대 (러시아)

[피터와 늑대](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)는 러시아 작곡가 [세르게이 프로코피예프](https://en.wikipedia.org/wiki/Sergei_Prokofiev)가 쓴 음악 동화입니다. 이 이야기는 어린 개척자 피터가 늑대를 쫓기 위해 집을 나와 숲으로 나가는 용감한 이야기를 담고 있습니다. 이 섹션에서는 피터를 도울 머신 러닝 알고리즘을 훈련할 것입니다:

- **탐색**: 주변 지역을 탐험하고 최적의 내비게이션 지도를 구축합니다.
- **학습**: 스케이트보드를 사용하는 방법과 균형을 잡는 방법을 배워 더 빠르게 이동할 수 있도록 합니다.

[![피터와 늑대](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 🎥 위 이미지를 클릭하여 프로코피예프의 피터와 늑대를 들어보세요

## 강화 학습

이전 섹션에서는 두 가지 머신 러닝 문제의 예를 보았습니다:

- **지도 학습**: 우리가 해결하려는 문제에 대한 샘플 솔루션을 제안하는 데이터셋이 있는 경우. [분류](../4-Classification/README.md)와 [회귀](../2-Regression/README.md)는 지도 학습 작업입니다.
- **비지도 학습**: 레이블이 지정된 학습 데이터가 없는 경우. 비지도 학습의 주요 예는 [클러스터링](../5-Clustering/README.md)입니다.

이 섹션에서는 레이블이 지정된 학습 데이터가 필요하지 않은 새로운 유형의 학습 문제를 소개합니다. 이러한 문제에는 여러 유형이 있습니다:

- **[반지도 학습](https://wikipedia.org/wiki/Semi-supervised_learning)**: 레이블이 없는 많은 데이터를 사용하여 모델을 사전 훈련할 수 있는 경우.
- **[강화 학습](https://wikipedia.org/wiki/Reinforcement_learning)**: 에이전트가 시뮬레이션 환경에서 실험을 수행하며 행동하는 방법을 배우는 경우.

### 예제 - 컴퓨터 게임

컴퓨터가 체스나 [슈퍼 마리오](https://wikipedia.org/wiki/Super_Mario)와 같은 게임을 하도록 가르치고 싶다고 가정해 보세요. 컴퓨터가 게임을 하려면 각 게임 상태에서 어떤 움직임을 취할지 예측해야 합니다. 이는 분류 문제처럼 보일 수 있지만, 실제로는 그렇지 않습니다. 왜냐하면 상태와 해당 행동이 포함된 데이터셋이 없기 때문입니다. 체스 경기 기록이나 슈퍼 마리오 플레이어의 녹화 데이터와 같은 일부 데이터를 가지고 있을 수 있지만, 이러한 데이터가 가능한 상태를 충분히 포괄하지 못할 가능성이 높습니다.

기존 게임 데이터를 찾는 대신, **강화 학습**(RL)은 컴퓨터가 여러 번 게임을 하게 하고 결과를 관찰하는 아이디어에 기반합니다. 따라서 강화 학습을 적용하려면 두 가지가 필요합니다:

- **환경**과 **시뮬레이터**: 게임을 여러 번 플레이할 수 있도록 하는 환경. 이 시뮬레이터는 모든 게임 규칙과 가능한 상태 및 행동을 정의합니다.

- **보상 함수**: 각 움직임이나 게임 동안 얼마나 잘했는지 알려주는 함수.

다른 유형의 머신 러닝과 RL의 주요 차이점은 RL에서는 게임이 끝날 때까지 승패를 알 수 없다는 점입니다. 따라서 특정 움직임이 단독으로 좋은지 아닌지 알 수 없으며, 게임이 끝난 후에야 보상을 받습니다. 우리의 목표는 불확실한 조건에서 모델을 훈련할 수 있는 알고리즘을 설계하는 것입니다. 우리는 **Q-러닝**이라는 RL 알고리즘에 대해 배울 것입니다.

## 학습 내용

1. [강화 학습과 Q-러닝 소개](1-QLearning/README.md)
2. [Gym 시뮬레이션 환경 사용](2-Gym/README.md)

## 크레딧

"강화 학습 소개"는 [Dmitry Soshnikov](http://soshnikov.com)가 ♥️를 담아 작성했습니다.

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.  