<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T00:16:00+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "da"
}
-->
# Postscript: Model Debugging i Maskinl√¶ring ved hj√¶lp af komponenter fra Responsible AI-dashboardet

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Introduktion

Maskinl√¶ring p√•virker vores daglige liv. AI finder vej ind i nogle af de mest betydningsfulde systemer, der ber√∏rer os som individer og som samfund, fra sundhedspleje, finans, uddannelse og besk√¶ftigelse. For eksempel er systemer og modeller involveret i daglige beslutningsprocesser, s√•som sundhedsdiagnoser eller afsl√∏ring af svindel. Som f√∏lge heraf m√∏des fremskridt inden for AI og den accelererede adoption med udviklende samfundsm√¶ssige forventninger og stigende regulering. Vi ser konstant omr√•der, hvor AI-systemer ikke lever op til forventningerne; de afsl√∏rer nye udfordringer; og regeringer begynder at regulere AI-l√∏sninger. Derfor er det vigtigt, at disse modeller analyseres for at sikre retf√¶rdige, p√•lidelige, inkluderende, transparente og ansvarlige resultater for alle.

I dette pensum vil vi se p√• praktiske v√¶rkt√∏jer, der kan bruges til at vurdere, om en model har problemer med ansvarlig AI. Traditionelle debugging-teknikker inden for maskinl√¶ring er ofte baseret p√• kvantitative beregninger s√•som samlet n√∏jagtighed eller gennemsnitligt fejl-tab. Forestil dig, hvad der kan ske, n√•r de data, du bruger til at bygge disse modeller, mangler visse demografiske grupper, s√•som race, k√∏n, politisk holdning, religion, eller uforholdsm√¶ssigt repr√¶senterer s√•danne grupper. Hvad med n√•r modellens output tolkes til at favorisere en bestemt demografisk gruppe? Dette kan f√∏re til over- eller underrepr√¶sentation af disse f√∏lsomme egenskabsgrupper, hvilket resulterer i retf√¶rdigheds-, inklusions- eller p√•lidelighedsproblemer fra modellen. En anden faktor er, at maskinl√¶ringsmodeller ofte betragtes som "black boxes", hvilket g√∏r det sv√¶rt at forst√• og forklare, hvad der driver modellens forudsigelser. Alle disse er udfordringer, som dataforskere og AI-udviklere st√•r over for, n√•r de ikke har tilstr√¶kkelige v√¶rkt√∏jer til at debugge og vurdere en models retf√¶rdighed eller trov√¶rdighed.

I denne lektion vil du l√¶re at debugge dine modeller ved hj√¶lp af:

- **Fejlanalyse**: Identificer, hvor i din datadistribution modellen har h√∏je fejlrater.
- **Modeloversigt**: Udf√∏r sammenlignende analyser p√• tv√¶rs af forskellige datakohorter for at opdage uligheder i modellens pr√¶stationsm√•linger.
- **Dataanalyse**: Unders√∏g, hvor der kan v√¶re over- eller underrepr√¶sentation af dine data, som kan sk√¶vvride din model til at favorisere √©n demografisk gruppe frem for en anden.
- **Feature Importance**: Forst√• hvilke egenskaber der driver modellens forudsigelser p√• et globalt eller lokalt niveau.

## Foruds√¶tning

Som foruds√¶tning bedes du gennemg√• [Responsible AI tools for developers](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif om Responsible AI Tools](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Fejlanalyse

Traditionelle pr√¶stationsm√•linger for modeller, der bruges til at m√•le n√∏jagtighed, er ofte beregninger baseret p√• korrekte vs. forkerte forudsigelser. For eksempel kan det at fastsl√•, at en model er n√∏jagtig 89% af tiden med et fejl-tab p√• 0,001, betragtes som en god pr√¶station. Fejl er dog ofte ikke j√¶vnt fordelt i det underliggende datas√¶t. Du kan f√• en modeln√∏jagtighed p√• 89%, men opdage, at der er forskellige omr√•der i dine data, hvor modellen fejler 42% af tiden. Konsekvensen af disse fejlm√∏nstre med visse datagrupper kan f√∏re til retf√¶rdigheds- eller p√•lidelighedsproblemer. Det er afg√∏rende at forst√• omr√•der, hvor modellen klarer sig godt eller d√•rligt. De dataomr√•der, hvor der er et h√∏jt antal un√∏jagtigheder i din model, kan vise sig at v√¶re en vigtig demografisk gruppe.

![Analyser og debug model-fejl](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

Fejlanalyse-komponenten p√• RAI-dashboardet viser, hvordan model-fejl er fordelt p√• tv√¶rs af forskellige kohorter med en tr√¶visualisering. Dette er nyttigt til at identificere egenskaber eller omr√•der, hvor der er en h√∏j fejlrater i dit datas√¶t. Ved at se, hvor de fleste af modellens un√∏jagtigheder kommer fra, kan du begynde at unders√∏ge √•rsagen. Du kan ogs√• oprette datakohorter til at udf√∏re analyser p√•. Disse datakohorter hj√¶lper i debugging-processen med at afg√∏re, hvorfor modelpr√¶stationen er god i √©n kohorte, men fejlagtig i en anden.

![Fejlanalyse](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

De visuelle indikatorer p√• tr√¶diagrammet hj√¶lper med at lokalisere problemomr√•derne hurtigere. For eksempel, jo m√∏rkere r√∏d farve en tr√¶knude har, jo h√∏jere er fejlraten.

Heatmap er en anden visualiseringsfunktion, som brugere kan anvende til at unders√∏ge fejlraten ved hj√¶lp af √©n eller to egenskaber for at finde bidragende faktorer til modellens fejl p√• tv√¶rs af hele datas√¶ttet eller kohorter.

![Fejlanalyse Heatmap](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Brug fejlanalyse, n√•r du har brug for at:

* F√• en dyb forst√•else af, hvordan model-fejl er fordelt p√• tv√¶rs af et datas√¶t og p√• tv√¶rs af flere input- og egenskabsdimensioner.
* Bryde de samlede pr√¶stationsm√•linger ned for automatisk at opdage fejlagtige kohorter og informere dine m√•lrettede afhj√¶lpningstrin.

## Modeloversigt

Evaluering af en maskinl√¶ringsmodels pr√¶station kr√¶ver en holistisk forst√•else af dens adf√¶rd. Dette kan opn√•s ved at gennemg√• mere end √©n m√•ling, s√•som fejlrater, n√∏jagtighed, recall, pr√¶cision eller MAE (Mean Absolute Error) for at finde uligheder blandt pr√¶stationsm√•linger. √ân pr√¶stationsm√•ling kan se godt ud, men un√∏jagtigheder kan afsl√∏res i en anden m√•ling. Derudover hj√¶lper sammenligning af m√•linger for uligheder p√• tv√¶rs af hele datas√¶ttet eller kohorter med at belyse, hvor modellen klarer sig godt eller d√•rligt. Dette er is√¶r vigtigt for at se modellens pr√¶station blandt f√∏lsomme vs. uf√∏lsomme egenskaber (f.eks. patientens race, k√∏n eller alder) for at afd√¶kke potentiel uretf√¶rdighed, modellen m√•tte have. For eksempel kan det at opdage, at modellen er mere fejlagtig i en kohorte med f√∏lsomme egenskaber, afsl√∏re potentiel uretf√¶rdighed.

Modeloversigt-komponenten p√• RAI-dashboardet hj√¶lper ikke kun med at analysere pr√¶stationsm√•linger for datarepr√¶sentationen i en kohorte, men giver ogs√• brugerne mulighed for at sammenligne modellens adf√¶rd p√• tv√¶rs af forskellige kohorter.

![Datas√¶t-kohorter - modeloversigt i RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

Komponentens egenskabsbaserede analysefunktionalitet giver brugerne mulighed for at indsn√¶vre datasubgrupper inden for en bestemt egenskab for at identificere anomalier p√• et detaljeret niveau. For eksempel har dashboardet indbygget intelligens til automatisk at generere kohorter for en bruger-valgt egenskab (f.eks. *"time_in_hospital < 3"* eller *"time_in_hospital >= 7"*). Dette g√∏r det muligt for en bruger at isolere en bestemt egenskab fra en st√∏rre datagruppe for at se, om den er en n√∏glefaktor for modellens fejlagtige resultater.

![Egenskabskohorter - modeloversigt i RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

Modeloversigt-komponenten underst√∏tter to klasser af ulighedsm√•linger:

**Ulighed i modelpr√¶station**: Disse s√¶t af m√•linger beregner uligheden (forskellen) i v√¶rdierne for den valgte pr√¶stationsm√•ling p√• tv√¶rs af data-subgrupper. Her er nogle eksempler:

* Ulighed i n√∏jagtighedsrate
* Ulighed i fejlrater
* Ulighed i pr√¶cision
* Ulighed i recall
* Ulighed i Mean Absolute Error (MAE)

**Ulighed i udv√¶lgelsesrate**: Denne m√•ling indeholder forskellen i udv√¶lgelsesrate (gunstig forudsigelse) blandt subgrupper. Et eksempel p√• dette er ulighed i l√•negodkendelsesrater. Udv√¶lgelsesrate betyder andelen af datapunkter i hver klasse klassificeret som 1 (i bin√¶r klassifikation) eller fordelingen af forudsigelsesv√¶rdier (i regression).

## Dataanalyse

> "Hvis du torturerer data l√¶nge nok, vil de tilst√• hvad som helst" - Ronald Coase

Denne udtalelse lyder ekstrem, men det er sandt, at data kan manipuleres til at underst√∏tte enhver konklusion. S√•dan manipulation kan nogle gange ske utilsigtet. Som mennesker har vi alle bias, og det er ofte sv√¶rt at bevidst vide, hvorn√•r man introducerer bias i data. At garantere retf√¶rdighed i AI og maskinl√¶ring forbliver en kompleks udfordring.

Data er et stort blindt punkt for traditionelle modelpr√¶stationsm√•linger. Du kan have h√∏je n√∏jagtighedsscorer, men dette afspejler ikke altid den underliggende databias, der kunne v√¶re i dit datas√¶t. For eksempel, hvis et datas√¶t af medarbejdere har 27% kvinder i lederstillinger i en virksomhed og 73% m√¶nd p√• samme niveau, kan en jobannoncerings-AI-model, der er tr√¶net p√• disse data, m√•lrette sig mest mod en mandlig m√•lgruppe for seniorjobstillinger. Denne ubalance i data sk√¶vvred modellens forudsigelse til at favorisere √©t k√∏n. Dette afsl√∏rer et retf√¶rdighedsproblem, hvor der er k√∏nsbias i AI-modellen.

Dataanalyse-komponenten p√• RAI-dashboardet hj√¶lper med at identificere omr√•der, hvor der er over- og underrepr√¶sentation i datas√¶ttet. Den hj√¶lper brugere med at diagnosticere √•rsagen til fejl og retf√¶rdighedsproblemer, der introduceres fra dataubalancer eller manglende repr√¶sentation af en bestemt datagruppe. Dette giver brugerne mulighed for at visualisere datas√¶t baseret p√• forudsagte og faktiske resultater, fejlgrupper og specifikke egenskaber. Nogle gange kan det at opdage en underrepr√¶senteret datagruppe ogs√• afsl√∏re, at modellen ikke l√¶rer godt, hvilket resulterer i h√∏je un√∏jagtigheder. At have en model med databias er ikke kun et retf√¶rdighedsproblem, men viser ogs√•, at modellen ikke er inkluderende eller p√•lidelig.

![Dataanalyse-komponent p√• RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Brug dataanalyse, n√•r du har brug for at:

* Udforske dine datas√¶tstatistikker ved at v√¶lge forskellige filtre for at opdele dine data i forskellige dimensioner (ogs√• kendt som kohorter).
* Forst√• fordelingen af dit datas√¶t p√• tv√¶rs af forskellige kohorter og egenskabsgrupper.
* Afg√∏re, om dine fund relateret til retf√¶rdighed, fejlanalyse og kausalitet (afledt fra andre dashboard-komponenter) skyldes dit datas√¶ts fordeling.
* Beslutte, i hvilke omr√•der du skal indsamle flere data for at afhj√¶lpe fejl, der kommer fra repr√¶sentationsproblemer, label-st√∏j, egenskabsst√∏j, label-bias og lignende faktorer.

## Modelfortolkning

Maskinl√¶ringsmodeller har en tendens til at v√¶re "black boxes". Det kan v√¶re udfordrende at forst√•, hvilke n√∏gleegenskaber der driver en models forudsigelse. Det er vigtigt at give gennemsigtighed i forhold til, hvorfor en model laver en bestemt forudsigelse. For eksempel, hvis et AI-system forudsiger, at en diabetisk patient er i risiko for at blive genindlagt p√• et hospital inden for mindre end 30 dage, b√∏r det kunne give underst√∏ttende data, der f√∏rte til denne forudsigelse. At have underst√∏ttende dataindikatorer skaber gennemsigtighed, der hj√¶lper klinikere eller hospitaler med at tr√¶ffe velinformerede beslutninger. Derudover g√∏r det muligt at forklare, hvorfor en model lavede en forudsigelse for en individuel patient, at man kan opfylde ansvarlighed med sundhedsreguleringer. N√•r du bruger maskinl√¶ringsmodeller p√• m√•der, der p√•virker menneskers liv, er det afg√∏rende at forst√• og forklare, hvad der p√•virker en models adf√¶rd. Model-forklarbarhed og fortolkning hj√¶lper med at besvare sp√∏rgsm√•l i scenarier s√•som:

* Model-debugging: Hvorfor lavede min model denne fejl? Hvordan kan jeg forbedre min model?
* Menneske-AI-samarbejde: Hvordan kan jeg forst√• og stole p√• modellens beslutninger?
* Regulatorisk overholdelse: Opfylder min model lovkrav?

Feature Importance-komponenten p√• RAI-dashboardet hj√¶lper dig med at debugge og f√• en omfattende forst√•else af, hvordan en model laver forudsigelser. Det er ogs√• et nyttigt v√¶rkt√∏j for maskinl√¶ringsprofessionelle og beslutningstagere til at forklare og vise beviser for egenskaber, der p√•virker en models adf√¶rd for regulatorisk overholdelse. Brugere kan derefter udforske b√•de globale og lokale forklaringer for at validere, hvilke egenskaber der driver en models forudsigelse. Globale forklaringer viser de vigtigste egenskaber, der p√•virkede en models samlede forudsigelse. Lokale forklaringer viser, hvilke egenskaber der f√∏rte til en models forudsigelse for en individuel sag. Muligheden for at evaluere lokale forklaringer er ogs√• nyttig i debugging eller revision af en specifik sag for bedre at forst√• og fortolke, hvorfor en model lavede en korrekt eller ukorrekt forudsigelse.

![Feature Importance-komponent p√• RAI-dashboardet](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Globale forklaringer: For eksempel, hvilke egenskaber p√•virker den samlede adf√¶rd af en diabetes-hospital-genindl√¶sningsmodel?
* Lokale forklaringer: For eksempel, hvorfor blev en diabetisk patient over 60 √•r med tidligere indl√¶ggelser forudsagt til at blive genindlagt eller ikke genindlagt inden for 30 dage p√• et hospital?

I debugging-processen med at unders√∏ge en models pr√¶station p√• tv√¶rs af forskellige kohorter viser Feature Importance, hvilken grad af indflydelse en egenskab har p√• tv√¶rs af kohorter. Det hj√¶lper med at afsl√∏re anomalier, n√•r man sammenligner niveauet af indflydelse, egenskaben har p√• at drive en models fejlagtige forudsigelser. Feature Importance-komponenten kan vise, hvilke v√¶rdier i en egenskab der positivt eller negativt p√•virkede modellens resultat. For eksempel, hvis en model lavede en ukorrekt forudsigelse, giver komponenten dig mulighed for at bore ned og identificere, hvilke egenskaber eller egenskabsv√¶rdier der drev forudsigelsen. Dette detaljeringsniveau hj√¶lper ikke kun med debugging, men skaber gennemsigtighed og ansvarlighed i revisionssituationer. Endelig kan komponenten hj√¶lpe dig med at identificere retf√¶rdighedsproblemer. For at illustrere, hvis en f√∏lsom egenskab s√•som etnicitet eller k√∏n har stor indflydelse p√• at drive en models forudsigelse, kan dette v√¶re et tegn p√• race- eller k√∏nsbias i modellen.

![Feature Importance](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Brug fortolkning, n√•r du har brug for at:

* Afg√∏re, hvor trov√¶rdige din AI-models forudsigelser er ved at forst√•, hvilke egenskaber der er mest vigtige for forudsigelserne.
* Tilg√• debugging af din model ved f√∏rst at forst√• den og identificere, om modellen bruger sunde egenskaber eller blot falske korrelationer.
* Afsl√∏re potentielle kilder til uretf√¶rdighed ved at forst√•, om modellen baserer forudsigelser p√• f√∏lsomme egenskaber eller p√• egenskaber, der er st√¶rkt korreleret med dem.
* Opbygge brugerens tillid til modellens beslutninger ved at generere lokale forklaringer for at illustrere deres resultater.
* Fuldf√∏re en regulatorisk revision af et AI-system for at validere modeller og overv√•ge modellens beslutningers indvirkning p√• mennesker.

## Konklusion

Alle komponenterne i RAI-dashboardet er praktiske v√¶rkt√∏jer, der hj√¶lper dig med at bygge maskinl√¶ringsmodeller, der er mindre skadelige og mere trov√¶rdige for samfundet. Det forbedrer forebyggelsen af trusler mod menneskerettigheder; diskrimination eller eksklusion af visse grupper fra livsmuligheder; og risikoen for fysisk eller psykologisk skade. Det hj√¶lper ogs√• med at opbygge tillid til modellens beslutninger ved at generere lokale forklaringer for at illustrere deres resultater. Nogle af de potentielle skader kan klassificeres som:

- **Allokering**, hvis et k√∏n eller en etnicitet for eksempel favoriseres frem for en anden.
- **Kvalitet af service**. Hvis du tr√¶ner data til et specifikt scenarie, men virkeligheden er langt mere kompleks, f√∏rer det til en d√•rligt fungerende service.
- **Stereotyper**. At associere en given gruppe med forudbestemte attributter.
- **Nedvurdering**. At kritisere og m√¶rke noget eller nogen uretf√¶rd
- **Over- eller underrepr√¶sentation**. Ideen er, at en bestemt gruppe ikke er repr√¶senteret i et bestemt erhverv, og enhver tjeneste eller funktion, der fortsat fremmer dette, bidrager til skade.

### Azure RAI-dashboard

[Azure RAI-dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) er bygget p√• open source-v√¶rkt√∏jer udviklet af f√∏rende akademiske institutioner og organisationer, herunder Microsoft. Disse v√¶rkt√∏jer er afg√∏rende for dataforskere og AI-udviklere til bedre at forst√• modeladf√¶rd, opdage og afhj√¶lpe u√∏nskede problemer i AI-modeller.

- L√¶r, hvordan du bruger de forskellige komponenter, ved at tjekke RAI-dashboardets [dokumentation.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Se nogle RAI-dashboard [eksempelsnotebooks](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) til fejlfinding af mere ansvarlige AI-scenarier i Azure Machine Learning.

---
## üöÄ Udfordring

For at forhindre, at statistiske eller datam√¶ssige sk√¶vheder opst√•r fra starten, b√∏r vi:

- sikre en mangfoldighed af baggrunde og perspektiver blandt de personer, der arbejder p√• systemerne
- investere i datas√¶t, der afspejler mangfoldigheden i vores samfund
- udvikle bedre metoder til at opdage og rette sk√¶vheder, n√•r de opst√•r

T√¶nk p√• virkelige scenarier, hvor uretf√¶rdighed er tydelig i modeludvikling og brug. Hvad b√∏r vi ellers overveje?

## [Quiz efter forel√¶sningen](https://ff-quizzes.netlify.app/en/ml/)
## Gennemgang & Selvstudie

I denne lektion har du l√¶rt nogle af de praktiske v√¶rkt√∏jer til at integrere ansvarlig AI i maskinl√¶ring.

Se denne workshop for at dykke dybere ned i emnerne:

- Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice af Besmira Nushi og Mehrnoosh Sameki

[![Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice")

> üé• Klik p√• billedet ovenfor for at se videoen: Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice af Besmira Nushi og Mehrnoosh Sameki

Refer√©r til f√∏lgende materialer for at l√¶re mere om ansvarlig AI og hvordan man bygger mere p√•lidelige modeller:

- Microsofts RAI-dashboardv√¶rkt√∏jer til fejlfinding af ML-modeller: [Ressourcer til ansvarlige AI-v√¶rkt√∏jer](https://aka.ms/rai-dashboard)

- Udforsk Responsible AI-v√¶rkt√∏jskassen: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsofts RAI-ressourcecenter: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE-forskningsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Opgave

[Udforsk RAI-dashboardet](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• at opn√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.