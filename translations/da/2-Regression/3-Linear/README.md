<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-04T23:21:44+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "da"
}
-->
# Byg en regressionsmodel med Scikit-learn: regression p√• fire m√•der

![Infografik om line√¶r vs. polynomisk regression](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografik af [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denne lektion er ogs√• tilg√¶ngelig i R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introduktion 

Indtil nu har du udforsket, hvad regression er, med eksempeldata fra gr√¶skarpris-datas√¶ttet, som vi vil bruge gennem hele denne lektion. Du har ogs√• visualiseret det med Matplotlib.

Nu er du klar til at dykke dybere ned i regression for maskinl√¶ring. Mens visualisering hj√¶lper dig med at forst√• data, ligger den virkelige styrke i maskinl√¶ring i _tr√¶ning af modeller_. Modeller tr√¶nes p√• historiske data for automatisk at fange dataafh√¶ngigheder, og de giver dig mulighed for at forudsige resultater for nye data, som modellen ikke har set f√∏r.

I denne lektion vil du l√¶re mere om to typer regression: _grundl√¶ggende line√¶r regression_ og _polynomisk regression_, sammen med noget af den matematik, der ligger bag disse teknikker. Disse modeller vil give os mulighed for at forudsige gr√¶skarpriser baseret p√• forskellige inputdata. 

[![ML for begyndere - Forst√•else af line√¶r regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML for begyndere - Forst√•else af line√¶r regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over line√¶r regression.

> Gennem hele dette pensum antager vi minimal viden om matematik og s√∏ger at g√∏re det tilg√¶ngeligt for studerende fra andre omr√•der, s√• hold √∏je med noter, üßÆ callouts, diagrammer og andre l√¶ringsv√¶rkt√∏jer, der kan hj√¶lpe med forst√•elsen.

### Foruds√¶tninger

Du b√∏r nu v√¶re bekendt med strukturen af gr√¶skardataene, som vi unders√∏ger. Du kan finde det forudindl√¶st og forudrenset i denne lektions _notebook.ipynb_-fil. I filen vises gr√¶skarprisen pr. bushel i en ny data frame. S√∏rg for, at du kan k√∏re disse notebooks i kerner i Visual Studio Code.

### Forberedelse

Som en p√•mindelse indl√¶ser du disse data for at stille sp√∏rgsm√•l til dem. 

- Hvorn√•r er det bedste tidspunkt at k√∏be gr√¶skar? 
- Hvilken pris kan jeg forvente for en kasse med miniaturegr√¶skar?
- Skal jeg k√∏be dem i halv-bushel kurve eller i 1 1/9 bushel kasser?
Lad os forts√¶tte med at grave i disse data.

I den forrige lektion oprettede du en Pandas data frame og fyldte den med en del af det originale datas√¶t, hvor du standardiserede prisen pr. bushel. Ved at g√∏re det var du dog kun i stand til at samle omkring 400 datapunkter og kun for efter√•rsm√•nederne. 

Tag et kig p√• de data, der er forudindl√¶st i denne lektions tilh√∏rende notebook. Dataene er forudindl√¶st, og et indledende scatterplot er oprettet for at vise m√•nedsdata. M√•ske kan vi f√• lidt mere detaljeret information om dataenes natur ved at rense dem yderligere.

## En line√¶r regressionslinje

Som du l√¶rte i Lektion 1, er m√•let med en line√¶r regressions√∏velse at kunne plotte en linje for at:

- **Vis variableforhold**. Vis forholdet mellem variabler
- **Lav forudsigelser**. Lav pr√¶cise forudsigelser om, hvor et nyt datapunkt vil falde i forhold til den linje. 
 
Det er typisk for **Least-Squares Regression** at tegne denne type linje. Udtrykket 'least-squares' betyder, at alle datapunkterne omkring regressionslinjen kvadreres og derefter l√¶gges sammen. Ideelt set er den endelige sum s√• lille som muligt, fordi vi √∏nsker et lavt antal fejl, eller `least-squares`. 

Vi g√∏r dette, fordi vi √∏nsker at modellere en linje, der har den mindste kumulative afstand fra alle vores datapunkter. Vi kvadrerer ogs√• termerne, f√∏r vi l√¶gger dem sammen, da vi er interesserede i deres st√∏rrelse snarere end deres retning.

> **üßÆ Vis mig matematikken** 
> 
> Denne linje, kaldet _linjen for bedste pasform_, kan udtrykkes ved [en ligning](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` er den 'forklarende variabel'. `Y` er den 'afh√¶ngige variabel'. H√¶ldningen af linjen er `b`, og `a` er y-sk√¶ringen, som refererer til v√¶rdien af `Y`, n√•r `X = 0`. 
>
>![beregn h√¶ldningen](../../../../2-Regression/3-Linear/images/slope.png)
>
> F√∏rst beregnes h√¶ldningen `b`. Infografik af [Jen Looper](https://twitter.com/jenlooper)
>
> Med andre ord, og med henvisning til det oprindelige sp√∏rgsm√•l om gr√¶skardata: "forudsige prisen p√• et gr√¶skar pr. bushel efter m√•ned", ville `X` referere til prisen, og `Y` ville referere til salgsdatoen. 
>
>![fuldf√∏r ligningen](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Beregn v√¶rdien af Y. Hvis du betaler omkring $4, m√• det v√¶re april! Infografik af [Jen Looper](https://twitter.com/jenlooper)
>
> Matematikken, der beregner linjen, skal demonstrere h√¶ldningen af linjen, som ogs√• afh√¶nger af sk√¶ringspunktet, eller hvor `Y` er placeret, n√•r `X = 0`.
>
> Du kan se metoden til beregning af disse v√¶rdier p√• [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) webstedet. Bes√∏g ogs√• [denne Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) for at se, hvordan talv√¶rdierne p√•virker linjen.

## Korrelation

Et andet begreb, du skal forst√•, er **korrelationskoefficienten** mellem givne X- og Y-variabler. Ved hj√¶lp af et scatterplot kan du hurtigt visualisere denne koefficient. Et plot med datapunkter spredt i en p√¶n linje har h√∏j korrelation, men et plot med datapunkter spredt overalt mellem X og Y har lav korrelation.

En god line√¶r regressionsmodel vil v√¶re en, der har en h√∏j (n√¶rmere 1 end 0) korrelationskoefficient ved hj√¶lp af Least-Squares Regression-metoden med en regressionslinje.

‚úÖ K√∏r notebooken, der ledsager denne lektion, og kig p√• scatterplottet for m√•ned til pris. Ser dataene, der forbinder m√•ned til pris for gr√¶skarsalg, ud til at have h√∏j eller lav korrelation if√∏lge din visuelle fortolkning af scatterplottet? √Ündrer det sig, hvis du bruger en mere detaljeret m√•ling i stedet for `Month`, fx *dag i √•ret* (dvs. antal dage siden √•rets begyndelse)?

I koden nedenfor antager vi, at vi har renset dataene og opn√•et en data frame kaldet `new_pumpkins`, der ligner f√∏lgende:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Koden til at rense dataene er tilg√¶ngelig i [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Vi har udf√∏rt de samme reng√∏ringsskridt som i den forrige lektion og har beregnet `DayOfYear`-kolonnen ved hj√¶lp af f√∏lgende udtryk: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Nu hvor du har en forst√•else af matematikken bag line√¶r regression, lad os oprette en regressionsmodel for at se, om vi kan forudsige, hvilken pakke gr√¶skar der vil have de bedste gr√¶skarpriser. En person, der k√∏ber gr√¶skar til en feriegr√¶skarplads, vil m√•ske have denne information for at optimere sine k√∏b af gr√¶skarpakker til pladsen.

## S√∏ger efter korrelation

[![ML for begyndere - S√∏ger efter korrelation: N√∏glen til line√¶r regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML for begyndere - S√∏ger efter korrelation: N√∏glen til line√¶r regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over korrelation.

Fra den forrige lektion har du sandsynligvis set, at gennemsnitsprisen for forskellige m√•neder ser s√•dan ud:

<img alt="Gennemsnitspris pr. m√•ned" src="../2-Data/images/barchart.png" width="50%"/>

Dette antyder, at der b√∏r v√¶re en vis korrelation, og vi kan pr√∏ve at tr√¶ne en line√¶r regressionsmodel til at forudsige forholdet mellem `Month` og `Price`, eller mellem `DayOfYear` og `Price`. Her er scatterplottet, der viser sidstn√¶vnte forhold:

<img alt="Scatterplot af pris vs. dag i √•ret" src="images/scatter-dayofyear.png" width="50%" /> 

Lad os se, om der er en korrelation ved hj√¶lp af funktionen `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Det ser ud til, at korrelationen er ret lille, -0.15 for `Month` og -0.17 for `DayOfMonth`, men der kunne v√¶re et andet vigtigt forhold. Det ser ud til, at der er forskellige prisgrupper, der svarer til forskellige gr√¶skartyper. For at bekr√¶fte denne hypotese, lad os plotte hver gr√¶skarkategori med en anden farve. Ved at sende en `ax`-parameter til scatter-plotfunktionen kan vi plotte alle punkter p√• samme graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Scatterplot af pris vs. dag i √•ret" src="images/scatter-dayofyear-color.png" width="50%" /> 

Vores unders√∏gelse antyder, at sorten har st√∏rre effekt p√• den samlede pris end den faktiske salgsdato. Vi kan se dette med et s√∏jlediagram:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="S√∏jlediagram af pris vs. sort" src="images/price-by-variety.png" width="50%" /> 

Lad os fokusere for √∏jeblikket kun p√• √©n gr√¶skarsort, 'pie type', og se, hvilken effekt datoen har p√• prisen:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Scatterplot af pris vs. dag i √•ret" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Hvis vi nu beregner korrelationen mellem `Price` og `DayOfYear` ved hj√¶lp af funktionen `corr`, f√•r vi noget som `-0.27` - hvilket betyder, at det giver mening at tr√¶ne en forudsigelsesmodel.

> F√∏r du tr√¶ner en line√¶r regressionsmodel, er det vigtigt at sikre, at vores data er rene. Line√¶r regression fungerer ikke godt med manglende v√¶rdier, s√• det giver mening at fjerne alle tomme celler:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

En anden tilgang ville v√¶re at udfylde de tomme v√¶rdier med gennemsnitsv√¶rdier fra den tilsvarende kolonne.

## Simpel line√¶r regression

[![ML for begyndere - Line√¶r og polynomisk regression med Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML for begyndere - Line√¶r og polynomisk regression med Scikit-learn")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over line√¶r og polynomisk regression.

For at tr√¶ne vores line√¶re regressionsmodel vil vi bruge **Scikit-learn**-biblioteket.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Vi starter med at adskille inputv√¶rdier (features) og det forventede output (label) i separate numpy-arrays:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Bem√¶rk, at vi var n√∏dt til at udf√∏re `reshape` p√• inputdataene, for at Linear Regression-pakken kunne forst√• det korrekt. Line√¶r regression forventer et 2D-array som input, hvor hver r√¶kke i arrayet svarer til en vektor af inputfeatures. I vores tilf√¶lde, da vi kun har √©n input, har vi brug for et array med formen N√ó1, hvor N er datas√¶ttets st√∏rrelse.

Derefter skal vi opdele dataene i tr√¶nings- og testdatas√¶t, s√• vi kan validere vores model efter tr√¶ning:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Endelig tager det kun to linjer kode at tr√¶ne den faktiske line√¶re regressionsmodel. Vi definerer `LinearRegression`-objektet og tilpasser det til vores data ved hj√¶lp af metoden `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objektet indeholder efter `fit`-processen alle koefficienterne for regressionen, som kan tilg√•s ved hj√¶lp af `.coef_`-egenskaben. I vores tilf√¶lde er der kun √©n koefficient, som b√∏r v√¶re omkring `-0.017`. Det betyder, at priserne ser ud til at falde lidt med tiden, men ikke meget, omkring 2 cent pr. dag. Vi kan ogs√• tilg√• sk√¶ringspunktet for regressionen med Y-aksen ved hj√¶lp af `lin_reg.intercept_` - det vil v√¶re omkring `21` i vores tilf√¶lde, hvilket indikerer prisen i begyndelsen af √•ret.

For at se, hvor pr√¶cis vores model er, kan vi forudsige priser p√• et testdatas√¶t og derefter m√•le, hvor t√¶t vores forudsigelser er p√• de forventede v√¶rdier. Dette kan g√∏res ved hj√¶lp af mean square error (MSE)-metrikken, som er gennemsnittet af alle kvadrerede forskelle mellem forventet og forudsagt v√¶rdi.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Vores fejl ser ud til at ligge omkring 2 punkter, hvilket svarer til ~17%. Ikke s√• godt. En anden indikator for modelkvalitet er **bestemmelseskoefficienten**, som kan beregnes s√•dan her:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Hvis v√¶rdien er 0, betyder det, at modellen ikke tager inputdata i betragtning og fungerer som den *d√•rligste line√¶re forudsigelse*, hvilket simpelthen er gennemsnitsv√¶rdien af resultatet. V√¶rdien 1 betyder, at vi kan forudsige alle forventede output perfekt. I vores tilf√¶lde er koefficienten omkring 0,06, hvilket er ret lavt.

Vi kan ogs√• plotte testdata sammen med regressionslinjen for bedre at se, hvordan regression fungerer i vores tilf√¶lde:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Line√¶r regression" src="images/linear-results.png" width="50%" />

## Polynomisk Regression

En anden type line√¶r regression er polynomisk regression. Mens der nogle gange er en line√¶r sammenh√¶ng mellem variabler - jo st√∏rre gr√¶skar i volumen, jo h√∏jere pris - kan disse sammenh√¶nge nogle gange ikke plottes som et plan eller en lige linje.

‚úÖ Her er [nogle flere eksempler](https://online.stat.psu.edu/stat501/lesson/9/9.8) p√• data, der kunne bruge polynomisk regression.

Tag et nyt kig p√• sammenh√¶ngen mellem dato og pris. Ser dette scatterplot ud som om det n√∏dvendigvis skal analyseres med en lige linje? Kan priser ikke svinge? I dette tilf√¶lde kan du pr√∏ve polynomisk regression.

‚úÖ Polynomier er matematiske udtryk, der kan best√• af en eller flere variabler og koefficienter.

Polynomisk regression skaber en buet linje for bedre at tilpasse sig ikke-line√¶re data. I vores tilf√¶lde, hvis vi inkluderer en kvadreret `DayOfYear`-variabel i inputdata, burde vi kunne tilpasse vores data med en parabolsk kurve, som vil have et minimum p√• et bestemt tidspunkt i l√∏bet af √•ret.

Scikit-learn inkluderer en nyttig [pipeline-API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) til at kombinere forskellige trin i databehandlingen. En **pipeline** er en k√¶de af **estimators**. I vores tilf√¶lde vil vi oprette en pipeline, der f√∏rst tilf√∏jer polynomiske funktioner til vores model og derefter tr√¶ner regressionen:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Ved at bruge `PolynomialFeatures(2)` betyder det, at vi vil inkludere alle anden-gradspolynomier fra inputdata. I vores tilf√¶lde vil det blot betyde `DayOfYear`<sup>2</sup>, men givet to inputvariabler X og Y, vil dette tilf√∏je X<sup>2</sup>, XY og Y<sup>2</sup>. Vi kan ogs√• bruge polynomier af h√∏jere grad, hvis vi √∏nsker det.

Pipelines kan bruges p√• samme m√•de som det originale `LinearRegression`-objekt, dvs. vi kan `fit` pipelinen og derefter bruge `predict` til at f√• forudsigelsesresultater. Her er grafen, der viser testdata og tiln√¶rmningskurven:

<img alt="Polynomisk regression" src="images/poly-results.png" width="50%" />

Ved at bruge polynomisk regression kan vi f√• en lidt lavere MSE og h√∏jere bestemmelseskoefficient, men ikke markant. Vi skal tage andre funktioner i betragtning!

> Du kan se, at de laveste gr√¶skarpriser observeres omkring Halloween. Hvordan kan du forklare dette?

üéÉ Tillykke, du har lige oprettet en model, der kan hj√¶lpe med at forudsige prisen p√• t√¶rtegr√¶skar. Du kan sandsynligvis gentage den samme procedure for alle gr√¶skartyper, men det ville v√¶re tidskr√¶vende. Lad os nu l√¶re, hvordan man tager gr√¶skarsort i betragtning i vores model!

## Kategoriske Funktioner

I en ideel verden √∏nsker vi at kunne forudsige priser for forskellige gr√¶skarsorter ved hj√¶lp af den samme model. Dog er kolonnen `Variety` noget anderledes end kolonner som `Month`, fordi den indeholder ikke-numeriske v√¶rdier. S√•danne kolonner kaldes **kategoriske**.

[![ML for begyndere - Kategoriske funktioner med line√¶r regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for begyndere - Kategoriske funktioner med line√¶r regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt om brug af kategoriske funktioner.

Her kan du se, hvordan gennemsnitsprisen afh√¶nger af sorten:

<img alt="Gennemsnitspris efter sort" src="images/price-by-variety.png" width="50%" />

For at tage sorten i betragtning skal vi f√∏rst konvertere den til numerisk form, eller **kode** den. Der er flere m√•der, vi kan g√∏re det p√•:

* Enkel **numerisk kodning** vil oprette en tabel over forskellige sorter og derefter erstatte sortsnavnet med et indeks i den tabel. Dette er ikke den bedste id√© for line√¶r regression, fordi line√¶r regression tager den faktiske numeriske v√¶rdi af indekset og tilf√∏jer det til resultatet, multipliceret med en koefficient. I vores tilf√¶lde er forholdet mellem indeksnummeret og prisen klart ikke-line√¶rt, selv hvis vi s√∏rger for, at indeksene er ordnet p√• en bestemt m√•de.
* **One-hot kodning** vil erstatte kolonnen `Variety` med 4 forskellige kolonner, √©n for hver sort. Hver kolonne vil indeholde `1`, hvis den tilsvarende r√¶kke er af en given sort, og `0` ellers. Dette betyder, at der vil v√¶re fire koefficienter i line√¶r regression, √©n for hver gr√¶skarsort, ansvarlig for "startpris" (eller rettere "ekstrapris") for den p√•g√¶ldende sort.

Koden nedenfor viser, hvordan vi kan one-hot kode en sort:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

For at tr√¶ne line√¶r regression ved hj√¶lp af one-hot kodet sort som input skal vi blot initialisere `X` og `y` data korrekt:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Resten af koden er den samme som den, vi brugte ovenfor til at tr√¶ne line√¶r regression. Hvis du pr√∏ver det, vil du se, at den gennemsnitlige kvadratiske fejl er omtrent den samme, men vi f√•r en meget h√∏jere bestemmelseskoefficient (~77%). For at f√• endnu mere pr√¶cise forudsigelser kan vi tage flere kategoriske funktioner i betragtning samt numeriske funktioner som `Month` eller `DayOfYear`. For at f√• √©n stor array af funktioner kan vi bruge `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Her tager vi ogs√• `City` og `Package` type i betragtning, hvilket giver os MSE 2.84 (10%) og bestemmelse 0.94!

## Samlet set

For at lave den bedste model kan vi bruge kombinerede (one-hot kodede kategoriske + numeriske) data fra ovenst√•ende eksempel sammen med polynomisk regression. Her er den komplette kode for din bekvemmelighed:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Dette burde give os den bedste bestemmelseskoefficient p√• n√¶sten 97% og MSE=2.23 (~8% forudsigelsesfejl).

| Model | MSE | Bestemmelse |
|-------|-----|-------------|
| `DayOfYear` Line√¶r | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynomisk | 2.73 (17.0%) | 0.08 |
| `Variety` Line√¶r | 5.24 (19.7%) | 0.77 |
| Alle funktioner Line√¶r | 2.84 (10.5%) | 0.94 |
| Alle funktioner Polynomisk | 2.23 (8.25%) | 0.97 |

üèÜ Godt g√•et! Du har oprettet fire regressionsmodeller i √©n lektion og forbedret modelkvaliteten til 97%. I den sidste sektion om regression vil du l√¶re om logistisk regression til at bestemme kategorier.

---
## üöÄUdfordring

Test flere forskellige variabler i denne notebook for at se, hvordan korrelation svarer til modeln√∏jagtighed.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

I denne lektion l√¶rte vi om line√¶r regression. Der er andre vigtige typer af regression. L√¶s om Stepwise, Ridge, Lasso og Elasticnet teknikker. Et godt kursus at studere for at l√¶re mere er [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Opgave 

[Byg en model](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.