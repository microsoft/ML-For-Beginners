<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-04T23:29:27+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "da"
}
-->
# Logistisk regression til at forudsige kategorier

![Infografik om logistisk vs. line√¶r regression](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denne lektion er tilg√¶ngelig i R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Introduktion

I denne sidste lektion om regression, en af de grundl√¶ggende _klassiske_ ML-teknikker, vil vi se n√¶rmere p√• logistisk regression. Du kan bruge denne teknik til at finde m√∏nstre og forudsige bin√¶re kategorier. Er dette slik chokolade eller ej? Er denne sygdom smitsom eller ej? Vil denne kunde v√¶lge dette produkt eller ej?

I denne lektion vil du l√¶re:

- Et nyt bibliotek til datavisualisering
- Teknikker til logistisk regression

‚úÖ Uddyb din forst√•else af at arbejde med denne type regression i dette [Learn-modul](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Foruds√¶tning

Efter at have arbejdet med gr√¶skardataene er vi nu tilstr√¶kkeligt bekendte med dem til at indse, at der er √©n bin√¶r kategori, vi kan arbejde med: `Color`.

Lad os bygge en logistisk regressionsmodel for at forudsige, givet nogle variabler, _hvilken farve et givet gr√¶skar sandsynligvis har_ (orange üéÉ eller hvid üëª).

> Hvorfor taler vi om bin√¶r klassifikation i en lektion om regression? Kun af sproglig bekvemmelighed, da logistisk regression [faktisk er en klassifikationsmetode](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), omend en line√¶r-baseret en. L√¶r om andre m√•der at klassificere data p√• i den n√¶ste lektion.

## Definer sp√∏rgsm√•let

For vores form√•l vil vi udtrykke dette som en bin√¶r: 'Hvid' eller 'Ikke hvid'. Der er ogs√• en 'stribet' kategori i vores datas√¶t, men der er f√• forekomster af den, s√• vi vil ikke bruge den. Den forsvinder alligevel, n√•r vi fjerner null-v√¶rdier fra datas√¶ttet.

> üéÉ Sjov fakta: Vi kalder nogle gange hvide gr√¶skar for 'sp√∏gelsesgr√¶skar'. De er ikke s√¶rlig nemme at sk√¶re i, s√• de er ikke lige s√• popul√¶re som de orange, men de ser seje ud! S√• vi kunne ogs√• omformulere vores sp√∏rgsm√•l som: 'Sp√∏gelse' eller 'Ikke sp√∏gelse'. üëª

## Om logistisk regression

Logistisk regression adskiller sig fra line√¶r regression, som du tidligere har l√¶rt om, p√• nogle vigtige m√•der.

[![ML for begyndere - Forst√•else af logistisk regression til klassifikation i maskinl√¶ring](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML for begyndere - Forst√•else af logistisk regression til klassifikation i maskinl√¶ring")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over logistisk regression.

### Bin√¶r klassifikation

Logistisk regression tilbyder ikke de samme funktioner som line√¶r regression. Den f√∏rstn√¶vnte giver en forudsigelse om en bin√¶r kategori ("hvid eller ikke hvid"), mens den sidstn√¶vnte er i stand til at forudsige kontinuerlige v√¶rdier, for eksempel givet oprindelsen af et gr√¶skar og tidspunktet for h√∏sten, _hvor meget prisen vil stige_.

![Gr√¶skar klassifikationsmodel](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografik af [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Andre klassifikationer

Der findes andre typer logistisk regression, herunder multinomial og ordinal:

- **Multinomial**, som involverer mere end √©n kategori - "Orange, Hvid og Stribet".
- **Ordinal**, som involverer ordnede kategorier, nyttigt hvis vi √∏nskede at ordne vores resultater logisk, som vores gr√¶skar, der er ordnet efter et begr√¶nset antal st√∏rrelser (mini, sm, med, lg, xl, xxl).

![Multinomial vs ordinal regression](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Variabler beh√∏ver IKKE at korrelere

Kan du huske, hvordan line√¶r regression fungerede bedre med mere korrelerede variabler? Logistisk regression er det modsatte - variablerne beh√∏ver ikke at v√¶re i overensstemmelse. Det fungerer for disse data, som har noget svage korrelationer.

### Du har brug for mange rene data

Logistisk regression giver mere pr√¶cise resultater, hvis du bruger flere data; vores lille datas√¶t er ikke optimalt til denne opgave, s√• husk det.

[![ML for begyndere - Dataanalyse og forberedelse til logistisk regression](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML for begyndere - Dataanalyse og forberedelse til logistisk regression")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over forberedelse af data til line√¶r regression

‚úÖ T√¶nk over, hvilke typer data der egner sig godt til logistisk regression

## √òvelse - ryd op i dataene

F√∏rst skal du rydde lidt op i dataene ved at fjerne null-v√¶rdier og v√¶lge kun nogle af kolonnerne:

1. Tilf√∏j f√∏lgende kode:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Du kan altid tage et kig p√• din nye dataframe:

    ```python
    pumpkins.info
    ```

### Visualisering - kategorisk plot

Nu har du indl√¶st [start-notebooken](../../../../2-Regression/4-Logistic/notebook.ipynb) med gr√¶skardata igen og ryddet op i den, s√• du har et datas√¶t, der indeholder nogle f√• variabler, inklusive `Color`. Lad os visualisere dataframen i notebooken ved hj√¶lp af et andet bibliotek: [Seaborn](https://seaborn.pydata.org/index.html), som er bygget p√• Matplotlib, som vi brugte tidligere.

Seaborn tilbyder nogle smarte m√•der at visualisere dine data p√•. For eksempel kan du sammenligne fordelingen af dataene for hver `Variety` og `Color` i et kategorisk plot.

1. Opret et s√•dant plot ved hj√¶lp af funktionen `catplot`, brug vores gr√¶skardata `pumpkins`, og angiv en farvekodning for hver gr√¶skarkategori (orange eller hvid):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Et gitter af visualiserede data](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Ved at observere dataene kan du se, hvordan `Color` relaterer sig til `Variety`.

    ‚úÖ Givet dette kategoriske plot, hvilke interessante unders√∏gelser kan du forestille dig?

### Databehandling: feature- og labelkodning

Vores gr√¶skardatas√¶t indeholder strengv√¶rdier for alle dets kolonner. At arbejde med kategoriske data er intuitivt for mennesker, men ikke for maskiner. Maskinl√¶ringsalgoritmer fungerer godt med tal. Derfor er kodning et meget vigtigt trin i databehandlingsfasen, da det g√∏r det muligt for os at omdanne kategoriske data til numeriske data uden at miste nogen information. God kodning f√∏rer til opbygning af en god model.

For feature-kodning er der to hovedtyper af kodere:

1. Ordinal encoder: Den passer godt til ordinale variabler, som er kategoriske variabler, hvor deres data f√∏lger en logisk r√¶kkef√∏lge, som kolonnen `Item Size` i vores datas√¶t. Den opretter en mapping, s√• hver kategori repr√¶senteres af et tal, som er r√¶kkef√∏lgen af kategorien i kolonnen.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorisk encoder: Den passer godt til nominelle variabler, som er kategoriske variabler, hvor deres data ikke f√∏lger en logisk r√¶kkef√∏lge, som alle funktionerne bortset fra `Item Size` i vores datas√¶t. Det er en one-hot encoding, hvilket betyder, at hver kategori repr√¶senteres af en bin√¶r kolonne: den kodede variabel er lig med 1, hvis gr√¶skarret tilh√∏rer den `Variety` og 0 ellers.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```

Derefter bruges `ColumnTransformer` til at kombinere flere kodere i et enkelt trin og anvende dem p√• de relevante kolonner.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```

For at kode labelen bruger vi scikit-learn-klassen `LabelEncoder`, som er en hj√¶lpeklasse til at normalisere labels, s√• de kun indeholder v√¶rdier mellem 0 og n_classes-1 (her, 0 og 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

N√•r vi har kodet funktionerne og labelen, kan vi flette dem til en ny dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

‚úÖ Hvad er fordelene ved at bruge en ordinal encoder til kolonnen `Item Size`?

### Analyser forholdet mellem variabler

Nu hvor vi har forbehandlet vores data, kan vi analysere forholdet mellem funktionerne og labelen for at f√• en id√© om, hvor godt modellen vil kunne forudsige labelen givet funktionerne. Den bedste m√•de at udf√∏re denne type analyse p√• er at plotte dataene. Vi bruger igen Seaborn-funktionen `catplot` til at visualisere forholdet mellem `Item Size`, `Variety` og `Color` i et kategorisk plot. For bedre at plotte dataene bruger vi den kodede `Item Size`-kolonne og den ukodede `Variety`-kolonne.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

![Et kategorisk plot af visualiserede data](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Brug et swarm plot

Da `Color` er en bin√¶r kategori (Hvid eller Ikke hvid), kr√¶ver det 'en [specialiseret tilgang](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) til visualisering'. Der er andre m√•der at visualisere forholdet mellem denne kategori og andre variabler.

Du kan visualisere variabler side om side med Seaborn-plots.

1. Pr√∏v et 'swarm'-plot for at vise fordelingen af v√¶rdier:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Et swarm af visualiserede data](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Pas p√•**: Koden ovenfor kan generere en advarsel, da Seaborn har sv√¶rt ved at repr√¶sentere s√• mange datapunkter i et swarm-plot. En mulig l√∏sning er at reducere st√∏rrelsen p√• mark√∏ren ved hj√¶lp af parameteren 'size'. V√¶r dog opm√¶rksom p√•, at dette p√•virker l√¶sbarheden af plottet.

> **üßÆ Vis mig matematikken**
>
> Logistisk regression bygger p√• konceptet 'maksimal sandsynlighed' ved hj√¶lp af [sigmoid-funktioner](https://wikipedia.org/wiki/Sigmoid_function). En 'Sigmoid-funktion' p√• et plot ligner en 'S'-form. Den tager en v√¶rdi og mapper den til et sted mellem 0 og 1. Dens kurve kaldes ogs√• en 'logistisk kurve'. Dens formel ser s√•dan ud:
>
> ![logistisk funktion](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> hvor sigmoids midtpunkt findes ved x's 0-punkt, L er kurvens maksimale v√¶rdi, og k er kurvens stejlhed. Hvis resultatet af funktionen er mere end 0,5, vil den p√•g√¶ldende label blive givet klassen '1' af det bin√¶re valg. Hvis ikke, vil den blive klassificeret som '0'.

## Byg din model

At bygge en model til at finde disse bin√¶re klassifikationer er overraskende ligetil i Scikit-learn.

[![ML for begyndere - Logistisk regression til klassifikation af data](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML for begyndere - Logistisk regression til klassifikation af data")

> üé• Klik p√• billedet ovenfor for en kort videooversigt over opbygning af en line√¶r regressionsmodel

1. V√¶lg de variabler, du vil bruge i din klassifikationsmodel, og opdel tr√¶nings- og testdatas√¶t ved at kalde `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Nu kan du tr√¶ne din model ved at kalde `fit()` med dine tr√¶ningsdata og udskrive resultatet:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Tag et kig p√• din models score. Det er ikke d√•rligt, taget i betragtning at du kun har omkring 1000 r√¶kker data:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Bedre forst√•else via en forvirringsmatrix

Mens du kan f√• en score-rapport [termer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) ved at udskrive ovenst√•ende elementer, kan du muligvis forst√• din model lettere ved at bruge en [forvirringsmatrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) til at hj√¶lpe os med at forst√•, hvordan modellen klarer sig.

> üéì En '[forvirringsmatrix](https://wikipedia.org/wiki/Confusion_matrix)' (eller 'fejlmatrix') er en tabel, der udtrykker din models sande vs. falske positive og negative, og dermed vurderer n√∏jagtigheden af forudsigelserne.

1. For at bruge en forvirringsmatrix skal du kalde `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Tag et kig p√• din models forvirringsmatrix:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

I Scikit-learn er r√¶kker (akse 0) faktiske labels, og kolonner (akse 1) er forudsagte labels.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Hvad sker der her? Lad os sige, at vores model bliver bedt om at klassificere gr√¶skar mellem to bin√¶re kategorier, kategori 'hvid' og kategori 'ikke-hvid'.

- Hvis din model forudsiger et gr√¶skar som ikke hvidt, og det faktisk tilh√∏rer kategorien 'ikke-hvid', kalder vi det en sand negativ, vist ved det √∏verste venstre tal.
- Hvis din model forudsiger et gr√¶skar som hvidt, og det faktisk tilh√∏rer kategorien 'ikke-hvid', kalder vi det en falsk negativ, vist ved det nederste venstre tal.
- Hvis din model forudsiger et gr√¶skar som ikke hvidt, og det faktisk tilh√∏rer kategorien 'hvid', kalder vi det en falsk positiv, vist ved det √∏verste h√∏jre tal.
- Hvis din model forudsiger et gr√¶skar som hvidt, og det faktisk tilh√∏rer kategorien 'hvid', kalder vi det en sand positiv, vist ved det nederste h√∏jre tal.

Som du m√•ske har g√¶ttet, er det at foretr√¶kke at have et st√∏rre antal sande positive og sande negative og et lavere antal falske positive og falske negative, hvilket indeb√¶rer, at modellen klarer sig bedre.
Hvordan relaterer forvirringsmatricen sig til pr√¶cision og recall? Husk, at klassifikationsrapporten, der blev printet ovenfor, viste pr√¶cision (0.85) og recall (0.67).

Pr√¶cision = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461

Recall = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666

‚úÖ Q: If√∏lge forvirringsmatricen, hvordan klarede modellen sig? A: Ikke d√•rligt; der er et godt antal sande negative, men ogs√• nogle f√• falske negative.

Lad os genbes√∏ge de begreber, vi s√• tidligere, ved hj√¶lp af forvirringsmatricens kortl√¶gning af TP/TN og FP/FN:

üéì Pr√¶cision: TP/(TP + FP) Andelen af relevante instanser blandt de hentede instanser (f.eks. hvilke labels blev korrekt m√¶rket)

üéì Recall: TP/(TP + FN) Andelen af relevante instanser, der blev hentet, uanset om de blev korrekt m√¶rket eller ej

üéì f1-score: (2 * pr√¶cision * recall)/(pr√¶cision + recall) Et v√¶gtet gennemsnit af pr√¶cision og recall, hvor det bedste er 1 og det v√¶rste er 0

üéì Support: Antallet af forekomster af hver hentet label

üéì N√∏jagtighed: (TP + TN)/(TP + TN + FP + FN) Procentdelen af labels, der blev korrekt forudsagt for en pr√∏ve.

üéì Macro Avg: Beregningen af de uv√¶gtede gennemsnitlige metrikker for hver label, uden at tage h√∏jde for label-ubalance.

üéì Weighted Avg: Beregningen af de gennemsnitlige metrikker for hver label, hvor der tages h√∏jde for label-ubalance ved at v√¶gte dem efter deres support (antallet af sande instanser for hver label).

‚úÖ Kan du t√¶nke p√•, hvilken metrik du b√∏r holde √∏je med, hvis du vil have din model til at reducere antallet af falske negative?

## Visualiser ROC-kurven for denne model

[![ML for begyndere - Analyse af logistisk regression med ROC-kurver](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML for begyndere - Analyse af logistisk regression med ROC-kurver")


> üé• Klik p√• billedet ovenfor for en kort videooversigt over ROC-kurver

Lad os lave endnu en visualisering for at se den s√•kaldte 'ROC'-kurve:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Ved hj√¶lp af Matplotlib kan du plotte modellens [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) eller ROC. ROC-kurver bruges ofte til at f√• et overblik over en klassifikators output i forhold til dens sande vs. falske positive. "ROC-kurver har typisk den sande positive rate p√• Y-aksen og den falske positive rate p√• X-aksen." Derfor betyder kurvens stejlhed og afstanden mellem midtlinjen og kurven noget: du vil have en kurve, der hurtigt bev√¶ger sig op og over linjen. I vores tilf√¶lde er der falske positive til at starte med, og derefter bev√¶ger linjen sig op og over korrekt:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Til sidst kan du bruge Scikit-learns [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) til at beregne den faktiske 'Area Under the Curve' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Resultatet er `0.9749908725812341`. Da AUC sp√¶nder fra 0 til 1, vil du have en h√∏j score, da en model, der er 100% korrekt i sine forudsigelser, vil have en AUC p√• 1; i dette tilf√¶lde er modellen _ret god_. 

I fremtidige lektioner om klassifikationer vil du l√¶re, hvordan du kan iterere for at forbedre modellens scores. Men for nu, tillykke! Du har gennemf√∏rt disse regression-lektioner!

---
## üöÄUdfordring

Der er meget mere at udforske omkring logistisk regression! Men den bedste m√•de at l√¶re p√• er at eksperimentere. Find et datas√¶t, der egner sig til denne type analyse, og byg en model med det. Hvad l√¶rer du? Tip: pr√∏v [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) for interessante datas√¶t.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

L√¶s de f√∏rste par sider af [denne artikel fra Stanford](https://web.stanford.edu/~jurafsky/slp3/5.pdf) om nogle praktiske anvendelser af logistisk regression. T√¶nk over opgaver, der er bedre egnet til den ene eller den anden type regression, som vi har studeret indtil nu. Hvad ville fungere bedst?

## Opgave 

[Pr√∏v denne regression igen](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.