<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T00:22:19+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "da"
}
-->
# Bygge maskinl√¶ringsl√∏sninger med ansvarlig AI

![Oversigt over ansvarlig AI i maskinl√¶ring i en sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote af [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Introduktion

I dette pensum vil du begynde at opdage, hvordan maskinl√¶ring kan og allerede p√•virker vores daglige liv. Selv nu er systemer og modeller involveret i daglige beslutningsopgaver, s√•som sundhedsdiagnoser, l√•neans√∏gninger eller afsl√∏ring af svindel. Derfor er det vigtigt, at disse modeller fungerer godt for at levere resultater, der er trov√¶rdige. Ligesom enhver softwareapplikation vil AI-systemer kunne fejle eller have u√∏nskede resultater. Derfor er det afg√∏rende at kunne forst√• og forklare adf√¶rden af en AI-model.

Forestil dig, hvad der kan ske, n√•r de data, du bruger til at bygge disse modeller, mangler visse demografiske grupper, s√•som race, k√∏n, politisk holdning, religion, eller uforholdsm√¶ssigt repr√¶senterer s√•danne grupper. Hvad med n√•r modellens output tolkes til at favorisere en bestemt demografisk gruppe? Hvad er konsekvensen for applikationen? Derudover, hvad sker der, n√•r modellen har et negativt resultat og skader mennesker? Hvem er ansvarlig for AI-systemets adf√¶rd? Dette er nogle af de sp√∏rgsm√•l, vi vil udforske i dette pensum.

I denne lektion vil du:

- √òge din bevidsthed om vigtigheden af retf√¶rdighed i maskinl√¶ring og skader relateret til retf√¶rdighed.
- Blive bekendt med praksis for at udforske outliers og us√¶dvanlige scenarier for at sikre p√•lidelighed og sikkerhed.
- F√• forst√•else for behovet for at styrke alle ved at designe inkluderende systemer.
- Udforske, hvor vigtigt det er at beskytte privatliv og sikkerhed for data og mennesker.
- Se vigtigheden af en "glasboks"-tilgang til at forklare adf√¶rden af AI-modeller.
- V√¶re opm√¶rksom p√•, hvordan ansvarlighed er afg√∏rende for at opbygge tillid til AI-systemer.

## Foruds√¶tning

Som foruds√¶tning bedes du tage "Principper for ansvarlig AI"-l√¶ringsstien og se videoen nedenfor om emnet:

L√¶r mere om ansvarlig AI ved at f√∏lge denne [l√¶ringssti](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Microsofts tilgang til ansvarlig AI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsofts tilgang til ansvarlig AI")

> üé• Klik p√• billedet ovenfor for en video: Microsofts tilgang til ansvarlig AI

## Retf√¶rdighed

AI-systemer b√∏r behandle alle retf√¶rdigt og undg√• at p√•virke lignende grupper af mennesker p√• forskellige m√•der. For eksempel, n√•r AI-systemer giver vejledning om medicinsk behandling, l√•neans√∏gninger eller ans√¶ttelse, b√∏r de give de samme anbefalinger til alle med lignende symptomer, √∏konomiske forhold eller faglige kvalifikationer. Hver af os som mennesker b√¶rer med os arvede fordomme, der p√•virker vores beslutninger og handlinger. Disse fordomme kan v√¶re tydelige i de data, vi bruger til at tr√¶ne AI-systemer. S√•dan manipulation kan nogle gange ske utilsigtet. Det er ofte sv√¶rt bevidst at vide, hvorn√•r man introducerer bias i data.

**"Uretf√¶rdighed"** omfatter negative konsekvenser eller "skader" for en gruppe mennesker, s√•som dem defineret ud fra race, k√∏n, alder eller handicapstatus. De vigtigste skader relateret til retf√¶rdighed kan klassificeres som:

- **Allokering**, hvis et k√∏n eller en etnicitet for eksempel favoriseres frem for en anden.
- **Kvalitet af service**. Hvis du tr√¶ner data til et specifikt scenarie, men virkeligheden er meget mere kompleks, f√∏rer det til en d√•rligt fungerende service. For eksempel en h√•nds√¶bedispenser, der ikke kunne registrere personer med m√∏rk hud. [Reference](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Nedg√∏relse**. At kritisere og m√¶rke noget eller nogen uretf√¶rdigt. For eksempel mislabeled en billedm√¶rkningsteknologi ber√∏mt billeder af m√∏rkhudede mennesker som gorillaer.
- **Over- eller underrepr√¶sentation**. Ideen er, at en bestemt gruppe ikke ses i en bestemt profession, og enhver service eller funktion, der fortsat fremmer dette, bidrager til skade.
- **Stereotyper**. At associere en given gruppe med forudbestemte attributter. For eksempel kan et sprogovers√¶ttelsessystem mellem engelsk og tyrkisk have un√∏jagtigheder p√• grund af ord med stereotype associationer til k√∏n.

![overs√¶ttelse til tyrkisk](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> overs√¶ttelse til tyrkisk

![overs√¶ttelse tilbage til engelsk](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> overs√¶ttelse tilbage til engelsk

N√•r vi designer og tester AI-systemer, skal vi sikre, at AI er retf√¶rdig og ikke programmeret til at tr√¶ffe biased eller diskriminerende beslutninger, som mennesker ogs√• er forbudt mod at tr√¶ffe. At garantere retf√¶rdighed i AI og maskinl√¶ring forbliver en kompleks socioteknisk udfordring.

### P√•lidelighed og sikkerhed

For at opbygge tillid skal AI-systemer v√¶re p√•lidelige, sikre og konsistente under normale og uventede forhold. Det er vigtigt at vide, hvordan AI-systemer vil opf√∏re sig i en r√¶kke forskellige situationer, is√¶r n√•r de er outliers. N√•r man bygger AI-l√∏sninger, skal der v√¶re betydelig fokus p√•, hvordan man h√•ndterer en bred vifte af omst√¶ndigheder, som AI-l√∏sningerne ville st√∏de p√•. For eksempel skal en selvk√∏rende bil prioritere menneskers sikkerhed h√∏jt. Som et resultat skal AI, der driver bilen, tage h√∏jde for alle de mulige scenarier, bilen kunne st√∏de p√•, s√•som nat, tordenvejr eller snestorme, b√∏rn der l√∏ber over gaden, k√¶ledyr, vejarbejder osv. Hvor godt et AI-system kan h√•ndtere en bred vifte af forhold p√•lideligt og sikkert afspejler niveauet af forudseenhed, som dataforskeren eller AI-udvikleren har overvejet under design eller test af systemet.

> [üé• Klik her for en video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inklusion

AI-systemer b√∏r designes til at engagere og styrke alle. N√•r dataforskere og AI-udviklere designer og implementerer AI-systemer, identificerer og adresserer de potentielle barrierer i systemet, der utilsigtet kunne ekskludere mennesker. For eksempel er der 1 milliard mennesker med handicap verden over. Med fremskridt inden for AI kan de f√• adgang til en bred vifte af information og muligheder lettere i deres daglige liv. Ved at adressere barriererne skabes der muligheder for at innovere og udvikle AI-produkter med bedre oplevelser, der gavner alle.

> [üé• Klik her for en video: inklusion i AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sikkerhed og privatliv

AI-systemer b√∏r v√¶re sikre og respektere folks privatliv. Folk har mindre tillid til systemer, der s√¶tter deres privatliv, information eller liv i fare. N√•r vi tr√¶ner maskinl√¶ringsmodeller, stoler vi p√• data for at producere de bedste resultater. I den proces skal dataenes oprindelse og integritet overvejes. For eksempel, blev dataene indsendt af brugere eller offentligt tilg√¶ngelige? Dern√¶st, mens vi arbejder med dataene, er det afg√∏rende at udvikle AI-systemer, der kan beskytte fortrolige oplysninger og modst√• angreb. Efterh√•nden som AI bliver mere udbredt, bliver beskyttelse af privatliv og sikring af vigtige personlige og forretningsm√¶ssige oplysninger mere kritisk og komplekst. Privatlivs- og datasikkerhedsproblemer kr√¶ver s√¶rlig opm√¶rksomhed for AI, fordi adgang til data er afg√∏rende for, at AI-systemer kan lave pr√¶cise og informerede forudsigelser og beslutninger om mennesker.

> [üé• Klik her for en video: sikkerhed i AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Som industri har vi gjort betydelige fremskridt inden for privatliv og sikkerhed, drevet v√¶sentligt af reguleringer som GDPR (General Data Protection Regulation).
- Alligevel m√• vi med AI-systemer erkende sp√¶ndingen mellem behovet for mere personlige data for at g√∏re systemer mere personlige og effektive ‚Äì og privatliv.
- Ligesom med internettets f√∏dsel ser vi ogs√• en stor stigning i antallet af sikkerhedsproblemer relateret til AI.
- Samtidig har vi set AI blive brugt til at forbedre sikkerheden. For eksempel drives de fleste moderne antivirus-scannere i dag af AI-heuristikker.
- Vi skal sikre, at vores dataforskningsprocesser harmonisk integreres med de nyeste privatlivs- og sikkerhedspraksisser.

### Transparens

AI-systemer b√∏r v√¶re forst√•elige. En afg√∏rende del af transparens er at forklare adf√¶rden af AI-systemer og deres komponenter. At forbedre forst√•elsen af AI-systemer kr√¶ver, at interessenter forst√•r, hvordan og hvorfor de fungerer, s√• de kan identificere potentielle pr√¶stationsproblemer, sikkerheds- og privatlivsbekymringer, bias, ekskluderende praksis eller utilsigtede resultater. Vi mener ogs√•, at de, der bruger AI-systemer, b√∏r v√¶re √¶rlige og √•bne om, hvorn√•r, hvorfor og hvordan de v√¶lger at implementere dem. Samt begr√¶nsningerne af de systemer, de bruger. For eksempel, hvis en bank bruger et AI-system til at underst√∏tte sine forbrugerl√•nsbeslutninger, er det vigtigt at unders√∏ge resultaterne og forst√•, hvilke data der p√•virker systemets anbefalinger. Regeringer begynder at regulere AI p√• tv√¶rs af industrier, s√• dataforskere og organisationer skal forklare, om et AI-system opfylder lovgivningsm√¶ssige krav, is√¶r n√•r der er et u√∏nsket resultat.

> [üé• Klik her for en video: transparens i AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Fordi AI-systemer er s√• komplekse, er det sv√¶rt at forst√•, hvordan de fungerer og tolke resultaterne.
- Denne manglende forst√•else p√•virker, hvordan disse systemer administreres, operationaliseres og dokumenteres.
- Denne manglende forst√•else p√•virker endnu vigtigere de beslutninger, der tr√¶ffes ved hj√¶lp af de resultater, disse systemer producerer.

### Ansvarlighed

De mennesker, der designer og implementerer AI-systemer, skal v√¶re ansvarlige for, hvordan deres systemer fungerer. Behovet for ansvarlighed er s√¶rligt vigtigt med f√∏lsomme teknologier som ansigtsgenkendelse. For nylig har der v√¶ret en stigende eftersp√∏rgsel efter ansigtsgenkendelsesteknologi, is√¶r fra retsh√•ndh√¶vende organisationer, der ser potentialet i teknologien til anvendelser som at finde forsvundne b√∏rn. Men disse teknologier kunne potentielt bruges af en regering til at s√¶tte borgernes grundl√¶ggende friheder i fare ved for eksempel at muligg√∏re kontinuerlig overv√•gning af specifikke individer. Derfor skal dataforskere og organisationer v√¶re ansvarlige for, hvordan deres AI-system p√•virker individer eller samfundet.

[![Ledende AI-forsker advarer om masseoverv√•gning gennem ansigtsgenkendelse](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsofts tilgang til ansvarlig AI")

> üé• Klik p√• billedet ovenfor for en video: Advarsler om masseoverv√•gning gennem ansigtsgenkendelse

I sidste ende er et af de st√∏rste sp√∏rgsm√•l for vores generation, som den f√∏rste generation, der bringer AI til samfundet, hvordan vi sikrer, at computere forbliver ansvarlige over for mennesker, og hvordan vi sikrer, at de mennesker, der designer computere, forbliver ansvarlige over for alle andre.

## Vurdering af p√•virkning

F√∏r du tr√¶ner en maskinl√¶ringsmodel, er det vigtigt at gennemf√∏re en vurdering af p√•virkning for at forst√• form√•let med AI-systemet; hvad den tilsigtede anvendelse er; hvor det vil blive implementeret; og hvem der vil interagere med systemet. Disse er nyttige for anmelder(e) eller testere, der evaluerer systemet, for at vide, hvilke faktorer der skal tages i betragtning, n√•r man identificerer potentielle risici og forventede konsekvenser.

F√∏lgende er fokusomr√•der, n√•r man gennemf√∏rer en vurdering af p√•virkning:

* **Negative konsekvenser for individer**. At v√¶re opm√¶rksom p√• eventuelle begr√¶nsninger eller krav, ikke-underst√∏ttet brug eller kendte begr√¶nsninger, der hindrer systemets ydeevne, er afg√∏rende for at sikre, at systemet ikke bruges p√• en m√•de, der kan skade individer.
* **Data krav**. At f√• en forst√•else af, hvordan og hvor systemet vil bruge data, g√∏r det muligt for anmeldere at udforske eventuelle datakrav, du skal v√¶re opm√¶rksom p√• (f.eks. GDPR eller HIPPA dataregler). Derudover skal du unders√∏ge, om kilden eller m√¶ngden af data er tilstr√¶kkelig til tr√¶ning.
* **Opsummering af p√•virkning**. Saml en liste over potentielle skader, der kunne opst√• ved brug af systemet. Gennem hele ML-livscyklussen skal du gennemg√•, om de identificerede problemer er afhjulpet eller adresseret.
* **Anvendelige m√•l** for hver af de seks kerneprincipper. Vurder, om m√•lene fra hvert af principperne er opfyldt, og om der er nogen mangler.

## Fejlfinding med ansvarlig AI

Ligesom fejlfinding af en softwareapplikation er fejlfinding af et AI-system en n√∏dvendig proces for at identificere og l√∏se problemer i systemet. Der er mange faktorer, der kan p√•virke en models ydeevne eller ansvarlighed. De fleste traditionelle modelpr√¶stationsm√•linger er kvantitative aggregater af en models ydeevne, hvilket ikke er tilstr√¶kkeligt til at analysere, hvordan en model overtr√¶der principperne for ansvarlig AI. Desuden er en maskinl√¶ringsmodel en "black box", der g√∏r det sv√¶rt at forst√•, hvad der driver dens resultater eller give forklaringer, n√•r den beg√•r fejl. Senere i dette kursus vil vi l√¶re, hvordan man bruger Responsible AI-dashboardet til at hj√¶lpe med fejlfinding af AI-systemer. Dashboardet giver et holistisk v√¶rkt√∏j til dataforskere og AI-udviklere til at udf√∏re:

* **Fejlanalyse**. For at identificere fejlfordelingen i modellen, der kan p√•virke systemets retf√¶rdighed eller p√•lidelighed.
* **Modeloversigt**. For at opdage, hvor der er forskelle i modellens ydeevne p√• tv√¶rs af datakohorter.
* **Dataanalyse**. For at forst√• datadistributionen og identificere eventuel bias i dataene, der kunne f√∏re til problemer med retf√¶rdighed, inklusion og p√•lidelighed.
* **Modelfortolkning**. For at forst√•, hvad der p√•virker eller influerer modellens forudsigelser. Dette hj√¶lper med at forklare modellens adf√¶rd, hvilket er vigtigt for transparens og ansvarlighed.

## üöÄ Udfordring

For at forhindre skader i at blive introduceret i f√∏rste omgang b√∏r vi:

- have en mangfoldighed af baggrunde og perspektiver blandt de mennesker, der arbejder p√• systemer
- investere i datas√¶t, der afspejler mangfoldigheden i vores samfund
- udvikle bedre metoder gennem hele maskinl√¶ringslivscyklussen til at opdage og rette ansvarlig AI, n√•r det opst√•r

T√¶nk p√• virkelige scenarier, hvor en models utrov√¶rdighed er tydelig i modelbygning og brug. Hvad b√∏r vi ellers overveje?

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

I denne lektion har du l√¶rt nogle grundl√¶ggende begreber om retf√¶rdighed og uretf√¶rdighed i maskinl√¶ring.
Se denne workshop for at dykke dybere ned i emnerne:

- P√• jagt efter ansvarlig AI: Fra principper til praksis af Besmira Nushi, Mehrnoosh Sameki og Amit Sharma

[![Responsible AI Toolbox: En open-source ramme for at bygge ansvarlig AI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: En open-source ramme for at bygge ansvarlig AI")

> üé• Klik p√• billedet ovenfor for en video: RAI Toolbox: En open-source ramme for at bygge ansvarlig AI af Besmira Nushi, Mehrnoosh Sameki og Amit Sharma

L√¶s ogs√•:

- Microsofts RAI ressourcecenter: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE forskningsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Responsible AI Toolbox GitHub repository](https://github.com/microsoft/responsible-ai-toolbox)

L√¶s om Azure Machine Learnings v√¶rkt√∏jer til at sikre retf√¶rdighed:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Opgave

[Udforsk RAI Toolbox](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.