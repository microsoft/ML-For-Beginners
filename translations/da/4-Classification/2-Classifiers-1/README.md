<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T00:43:18+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "da"
}
-->
# Klassifikatorer for k√∏kkener 1

I denne lektion vil du bruge det datas√¶t, du gemte fra den sidste lektion, fyldt med balancerede og rene data om k√∏kkener.

Du vil bruge dette datas√¶t med en r√¶kke klassifikatorer til _at forudsige et givet nationalt k√∏kken baseret p√• en gruppe ingredienser_. Mens du g√∏r dette, vil du l√¶re mere om nogle af de m√•der, algoritmer kan bruges til klassifikationsopgaver.

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ml/)
# Forberedelse

Forudsat at du har gennemf√∏rt [Lektion 1](../1-Introduction/README.md), skal du sikre dig, at en _cleaned_cuisines.csv_-fil findes i rodmappen `/data` for disse fire lektioner.

## √òvelse - forudsige et nationalt k√∏kken

1. Arbejd i denne lektions _notebook.ipynb_-mappe, og importer filen sammen med Pandas-biblioteket:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Dataene ser s√•dan ud:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Importer nu flere biblioteker:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Del X- og y-koordinaterne op i to dataframes til tr√¶ning. `cuisine` kan v√¶re labels-dataframen:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Det vil se s√•dan ud:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. Fjern kolonnen `Unnamed: 0` og kolonnen `cuisine` ved at kalde `drop()`. Gem resten af dataene som tr√¶ningsfunktioner:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    Dine funktioner ser s√•dan ud:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Nu er du klar til at tr√¶ne din model!

## Valg af klassifikator

Nu hvor dine data er rene og klar til tr√¶ning, skal du beslutte, hvilken algoritme du vil bruge til opgaven. 

Scikit-learn grupperer klassifikation under Supervised Learning, og i den kategori finder du mange m√•der at klassificere p√•. [Udvalget](https://scikit-learn.org/stable/supervised_learning.html) kan virke overv√¶ldende ved f√∏rste √∏jekast. F√∏lgende metoder inkluderer alle klassifikationsteknikker:

- Line√¶re modeller
- Support Vector Machines
- Stokastisk gradientnedstigning
- N√¶rmeste naboer
- Gaussian-processer
- Beslutningstr√¶er
- Ensemble-metoder (voting Classifier)
- Multiclass- og multioutput-algoritmer (multiclass- og multilabel-klassifikation, multiclass-multioutput-klassifikation)

> Du kan ogs√• bruge [neural netv√¶rk til at klassificere data](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), men det ligger uden for denne lektions omfang.

### Hvilken klassifikator skal du v√¶lge?

S√•, hvilken klassifikator skal du v√¶lge? Ofte kan det v√¶re en god id√© at pr√∏ve flere og se, hvilken der giver det bedste resultat. Scikit-learn tilbyder en [side-by-side sammenligning](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) p√• et oprettet datas√¶t, hvor KNeighbors, SVC p√• to m√•der, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB og QuadraticDiscriminationAnalysis sammenlignes og visualiseres:

![sammenligning af klassifikatorer](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Diagrammer genereret fra Scikit-learns dokumentation

> AutoML l√∏ser dette problem elegant ved at k√∏re disse sammenligninger i skyen, s√• du kan v√¶lge den bedste algoritme til dine data. Pr√∏v det [her](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### En bedre tilgang

En bedre tilgang end blot at g√¶tte er at f√∏lge ideerne p√• dette downloadbare [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). Her opdager vi, at vi for vores multiclass-problem har nogle valgmuligheder:

![cheatsheet for multiclass-problemer](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> En sektion af Microsofts Algorithm Cheat Sheet, der beskriver muligheder for multiclass-klassifikation

‚úÖ Download dette cheat sheet, print det ud, og h√¶ng det op p√• din v√¶g!

### Overvejelser

Lad os se, om vi kan r√¶sonnere os frem til forskellige tilgange givet de begr√¶nsninger, vi har:

- **Neural netv√¶rk er for tunge**. Givet vores rene, men minimale datas√¶t, og det faktum at vi k√∏rer tr√¶ning lokalt via notebooks, er neural netv√¶rk for tunge til denne opgave.
- **Ingen to-klassifikator**. Vi bruger ikke en to-klassifikator, s√• det udelukker one-vs-all.
- **Beslutningstr√¶ eller logistisk regression kunne fungere**. Et beslutningstr√¶ kunne fungere, eller logistisk regression for multiclass-data.
- **Multiclass Boosted Decision Trees l√∏ser et andet problem**. Multiclass Boosted Decision Tree er mest egnet til ikke-parametriske opgaver, f.eks. opgaver designet til at opbygge rangeringer, s√• det er ikke nyttigt for os.

### Brug af Scikit-learn 

Vi vil bruge Scikit-learn til at analysere vores data. Der er dog mange m√•der at bruge logistisk regression i Scikit-learn. Se p√• [parametrene, der kan angives](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

Grundl√¶ggende er der to vigtige parametre - `multi_class` og `solver` - som vi skal angive, n√•r vi beder Scikit-learn om at udf√∏re en logistisk regression. `multi_class`-v√¶rdien anvender en bestemt adf√¶rd. V√¶rdien af solver angiver, hvilken algoritme der skal bruges. Ikke alle solvers kan kombineres med alle `multi_class`-v√¶rdier.

If√∏lge dokumentationen, i multiclass-tilf√¶ldet, tr√¶ningsalgoritmen:

- **Bruger one-vs-rest (OvR)-skemaet**, hvis `multi_class`-indstillingen er sat til `ovr`
- **Bruger krydsentropitab**, hvis `multi_class`-indstillingen er sat til `multinomial`. (I √∏jeblikket underst√∏ttes `multinomial`-indstillingen kun af ‚Äòlbfgs‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô og ‚Äònewton-cg‚Äô-solvers.)

> üéì 'Skemaet' her kan enten v√¶re 'ovr' (one-vs-rest) eller 'multinomial'. Da logistisk regression egentlig er designet til at underst√∏tte bin√¶r klassifikation, giver disse skemaer den mulighed for bedre at h√•ndtere multiclass-klassifikationsopgaver. [kilde](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> üéì 'Solveren' defineres som "den algoritme, der skal bruges i optimeringsproblemet". [kilde](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn tilbyder denne tabel til at forklare, hvordan solvers h√•ndterer forskellige udfordringer pr√¶senteret af forskellige typer datastrukturer:

![solvers](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## √òvelse - del dataene

Vi kan fokusere p√• logistisk regression til vores f√∏rste tr√¶ningsfors√∏g, da du for nylig har l√¶rt om sidstn√¶vnte i en tidligere lektion.
Del dine data i tr√¶nings- og testgrupper ved at kalde `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## √òvelse - anvend logistisk regression

Da du bruger multiclass-tilf√¶ldet, skal du v√¶lge, hvilket _skema_ du vil bruge, og hvilken _solver_ du vil angive. Brug LogisticRegression med en multiclass-indstilling og **liblinear**-solveren til at tr√¶ne.

1. Opret en logistisk regression med multi_class sat til `ovr` og solver sat til `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    ‚úÖ Pr√∏v en anden solver som `lbfgs`, som ofte er angivet som standard
> Bem√¶rk, brug Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html)-funktionen til at flade dine data ud, n√•r det er n√∏dvendigt.
N√∏jagtigheden er god ved over **80%**!

1. Du kan se denne model i aktion ved at teste en r√¶kke data (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Resultatet bliver udskrevet:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   ‚úÖ Pr√∏v et andet r√¶kkenummer og tjek resultaterne

1. G√• dybere, og unders√∏g n√∏jagtigheden af denne forudsigelse:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Resultatet bliver udskrevet - indisk k√∏kken er dens bedste g√¶t, med god sandsynlighed:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    ‚úÖ Kan du forklare, hvorfor modellen er ret sikker p√•, at dette er et indisk k√∏kken?

1. F√• flere detaljer ved at udskrive en klassifikationsrapport, som du gjorde i regression-lektionerne:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | precision | recall | f1-score | support |
    | ------------ | --------- | ------ | -------- | ------- |
    | chinese      | 0.73      | 0.71   | 0.72     | 229     |
    | indian       | 0.91      | 0.93   | 0.92     | 254     |
    | japanese     | 0.70      | 0.75   | 0.72     | 220     |
    | korean       | 0.86      | 0.76   | 0.81     | 242     |
    | thai         | 0.79      | 0.85   | 0.82     | 254     |
    | accuracy     | 0.80      | 1199   |          |         |
    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |
    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |

## üöÄUdfordring

I denne lektion brugte du dine rensede data til at bygge en maskinl√¶ringsmodel, der kan forudsige en national k√∏kkenstil baseret p√• en r√¶kke ingredienser. Tag dig tid til at l√¶se om de mange muligheder, Scikit-learn tilbyder til at klassificere data. G√• dybere ned i konceptet 'solver' for at forst√•, hvad der sker bag kulisserne.

## [Quiz efter lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

Unders√∏g lidt mere om matematikken bag logistisk regression i [denne lektion](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## Opgave 

[Unders√∏g solvers](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.