<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T01:03:59+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "da"
}
-->
# Introduktion til forst칝rkningsl칝ring

Forst칝rkningsl칝ring, RL, betragtes som en af de grundl칝ggende paradigmer inden for maskinl칝ring, ved siden af superviseret l칝ring og usuperviseret l칝ring. RL handler om beslutninger: at tr칝ffe de rigtige beslutninger eller i det mindste l칝re af dem.

Forestil dig, at du har et simuleret milj칮, som f.eks. aktiemarkedet. Hvad sker der, hvis du indf칮rer en given regulering? Har det en positiv eller negativ effekt? Hvis noget negativt sker, skal du tage denne _negative forst칝rkning_, l칝re af den og 칝ndre kurs. Hvis det er et positivt resultat, skal du bygge videre p친 den _positive forst칝rkning_.

![Peter og ulven](../../../8-Reinforcement/images/peter.png)

> Peter og hans venner skal undslippe den sultne ulv! Billede af [Jen Looper](https://twitter.com/jenlooper)

## Regionalt emne: Peter og ulven (Rusland)

[Peter og ulven](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) er et musikalsk eventyr skrevet af den russiske komponist [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Det er en historie om den unge pioner Peter, der modigt g친r ud af sit hus til lysningen i skoven for at jage ulven. I denne sektion vil vi tr칝ne maskinl칝ringsalgoritmer, der kan hj칝lpe Peter:

- **Udforske** det omkringliggende omr친de og opbygge et optimalt navigationskort.
- **L칝re** at bruge et skateboard og balancere p친 det for at bev칝ge sig hurtigere rundt.

[![Peter og ulven](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 游꿘 Klik p친 billedet ovenfor for at lytte til Peter og ulven af Prokofiev

## Forst칝rkningsl칝ring

I tidligere sektioner har du set to eksempler p친 maskinl칝ringsproblemer:

- **Superviseret**, hvor vi har datas칝t, der foresl친r eksempler p친 l칮sninger til det problem, vi 칮nsker at l칮se. [Klassifikation](../4-Classification/README.md) og [regression](../2-Regression/README.md) er superviserede l칝ringsopgaver.
- **Usuperviseret**, hvor vi ikke har m칝rkede tr칝ningsdata. Det prim칝re eksempel p친 usuperviseret l칝ring er [Clustering](../5-Clustering/README.md).

I denne sektion vil vi introducere dig til en ny type l칝ringsproblem, der ikke kr칝ver m칝rkede tr칝ningsdata. Der er flere typer af s친danne problemer:

- **[Semi-superviseret l칝ring](https://wikipedia.org/wiki/Semi-supervised_learning)**, hvor vi har en masse um칝rkede data, der kan bruges til at fortr칝ne modellen.
- **[Forst칝rkningsl칝ring](https://wikipedia.org/wiki/Reinforcement_learning)**, hvor en agent l칝rer at opf칮re sig ved at udf칮re eksperimenter i et simuleret milj칮.

### Eksempel - computerspil

Forestil dig, at du vil l칝re en computer at spille et spil, som f.eks. skak eller [Super Mario](https://wikipedia.org/wiki/Super_Mario). For at computeren kan spille et spil, skal den kunne forudsige, hvilket tr칝k den skal foretage i hver af spillets tilstande. Selvom dette kan virke som et klassifikationsproblem, er det det ikke - fordi vi ikke har et datas칝t med tilstande og tilsvarende handlinger. Selvom vi m친ske har nogle data som eksisterende skakpartier eller optagelser af spillere, der spiller Super Mario, er det sandsynligt, at disse data ikke tilstr칝kkeligt d칝kker et stort nok antal mulige tilstande.

I stedet for at lede efter eksisterende spildata er **Forst칝rkningsl칝ring** (RL) baseret p친 ideen om *at lade computeren spille* mange gange og observere resultatet. For at anvende Forst칝rkningsl칝ring har vi brug for to ting:

- **Et milj칮** og **en simulator**, der giver os mulighed for at spille et spil mange gange. Denne simulator skal definere alle spillets regler samt mulige tilstande og handlinger.

- **En bel칮nningsfunktion**, der fort칝ller os, hvor godt vi klarede os under hvert tr칝k eller spil.

Den st칮rste forskel mellem andre typer maskinl칝ring og RL er, at vi i RL typisk ikke ved, om vi vinder eller taber, f칮r vi har afsluttet spillet. Derfor kan vi ikke sige, om et bestemt tr칝k alene er godt eller ej - vi modtager kun en bel칮nning ved slutningen af spillet. Vores m친l er at designe algoritmer, der g칮r det muligt for os at tr칝ne en model under usikre forhold. Vi vil l칝re om en RL-algoritme kaldet **Q-learning**.

## Lektioner

1. [Introduktion til forst칝rkningsl칝ring og Q-Learning](1-QLearning/README.md)
2. [Brug af et gym-simuleringsmilj칮](2-Gym/README.md)

## Credits

"Introduktion til Forst칝rkningsl칝ring" er skrevet med 鮫봺잺 af [Dmitry Soshnikov](http://soshnikov.com)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.