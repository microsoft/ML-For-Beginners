<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T01:15:52+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "da"
}
-->
# CartPole Skating

Problemet, vi har arbejdet med i den tidligere lektion, kan virke som et leget√∏jsproblem, der ikke rigtig har relevans for virkelige scenarier. Dette er dog ikke tilf√¶ldet, da mange virkelige problemer deler samme karakteristika ‚Äì herunder at spille skak eller Go. De er ens, fordi vi ogs√• har et br√¶t med givne regler og en **diskret tilstand**.

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Introduktion

I denne lektion vil vi anvende de samme principper fra Q-Learning p√• et problem med en **kontinuerlig tilstand**, dvs. en tilstand, der er givet ved en eller flere reelle tal. Vi vil arbejde med f√∏lgende problem:

> **Problem**: Hvis Peter vil undslippe ulven, skal han kunne bev√¶ge sig hurtigere. Vi vil se, hvordan Peter kan l√¶re at sk√∏jte, is√¶r hvordan han kan holde balancen, ved hj√¶lp af Q-Learning.

![Den store flugt!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> Peter og hans venner bliver kreative for at undslippe ulven! Billede af [Jen Looper](https://twitter.com/jenlooper)

Vi vil bruge en forenklet version af balancering kendt som **CartPole**-problemet. I CartPole-verdenen har vi en horisontal slider, der kan bev√¶ge sig til venstre eller h√∏jre, og m√•let er at balancere en lodret stang oven p√• slideren.

## Foruds√¶tninger

I denne lektion vil vi bruge et bibliotek kaldet **OpenAI Gym** til at simulere forskellige **milj√∏er**. Du kan k√∏re lektionens kode lokalt (f.eks. fra Visual Studio Code), hvor simuleringen √•bnes i et nyt vindue. Hvis du k√∏rer koden online, skal du muligvis lave nogle justeringer, som beskrevet [her](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

I den tidligere lektion blev spillets regler og tilstand givet af klassen `Board`, som vi selv definerede. Her vil vi bruge et specielt **simuleringsmilj√∏**, der simulerer fysikken bag den balancerende stang. Et af de mest popul√¶re simuleringsmilj√∏er til tr√¶ning af reinforcement learning-algoritmer kaldes [Gym](https://gym.openai.com/), som vedligeholdes af [OpenAI](https://openai.com/). Ved at bruge dette Gym kan vi skabe forskellige **milj√∏er**, fra CartPole-simuleringer til Atari-spil.

> **Note**: Du kan se andre milj√∏er, der er tilg√¶ngelige fra OpenAI Gym [her](https://gym.openai.com/envs/#classic_control).

F√∏rst skal vi installere Gym og importere de n√∏dvendige biblioteker (kodeblok 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## √òvelse - initialiser et CartPole-milj√∏

For at arbejde med CartPole-balanceringsproblemet skal vi initialisere det tilsvarende milj√∏. Hvert milj√∏ er forbundet med:

- **Observation space**, der definerer strukturen af den information, vi modtager fra milj√∏et. For CartPole-problemet modtager vi positionen af stangen, hastighed og nogle andre v√¶rdier.

- **Action space**, der definerer mulige handlinger. I vores tilf√¶lde er action space diskret og best√•r af to handlinger - **venstre** og **h√∏jre**. (kodeblok 2)

1. For at initialisere skal du skrive f√∏lgende kode:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

For at se, hvordan milj√∏et fungerer, lad os k√∏re en kort simulering i 100 trin. Ved hvert trin giver vi en af de handlinger, der skal udf√∏res ‚Äì i denne simulering v√¶lger vi bare tilf√¶ldigt en handling fra `action_space`.

1. K√∏r koden nedenfor og se, hvad det f√∏rer til.

    ‚úÖ Husk, at det er bedst at k√∏re denne kode p√• en lokal Python-installation! (kodeblok 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Du b√∏r se noget, der ligner dette billede:

    ![ikke-balancerende CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Under simuleringen skal vi f√• observationer for at beslutte, hvordan vi skal handle. Faktisk returnerer step-funktionen aktuelle observationer, en bel√∏nningsfunktion og en "done"-flag, der angiver, om det giver mening at forts√¶tte simuleringen eller ej: (kodeblok 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Du vil ende med at se noget som dette i notebook-outputtet:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Observationsvektoren, der returneres ved hvert trin i simuleringen, indeholder f√∏lgende v√¶rdier:
    - Position af vognen
    - Hastighed af vognen
    - Vinkel af stangen
    - Rotationshastighed af stangen

1. F√• minimums- og maksimumsv√¶rdier for disse tal: (kodeblok 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Du vil ogs√• bem√¶rke, at bel√∏nningsv√¶rdien ved hvert simuleringstrin altid er 1. Dette skyldes, at vores m√•l er at overleve s√• l√¶nge som muligt, dvs. holde stangen i en rimelig lodret position i l√¶ngst mulig tid.

    ‚úÖ Faktisk anses CartPole-simuleringen for at v√¶re l√∏st, hvis vi form√•r at opn√• en gennemsnitlig bel√∏nning p√• 195 over 100 p√• hinanden f√∏lgende fors√∏g.

## Diskretisering af tilstand

I Q-Learning skal vi opbygge en Q-Table, der definerer, hvad vi skal g√∏re i hver tilstand. For at kunne g√∏re dette skal tilstanden v√¶re **diskret**, mere pr√¶cist skal den indeholde et begr√¶nset antal diskrete v√¶rdier. Derfor skal vi p√• en eller anden m√•de **diskretisere** vores observationer og kortl√¶gge dem til et begr√¶nset s√¶t af tilstande.

Der er et par m√•der, vi kan g√∏re dette p√•:

- **Opdel i intervaller**. Hvis vi kender intervallet for en bestemt v√¶rdi, kan vi opdele dette interval i et antal **intervaller** og derefter erstatte v√¶rdien med nummeret p√• det interval, den tilh√∏rer. Dette kan g√∏res ved hj√¶lp af numpy-metoden [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). I dette tilf√¶lde vil vi pr√¶cist kende st√∏rrelsen p√• tilstanden, da den vil afh√¶nge af antallet af intervaller, vi v√¶lger til digitalisering.

‚úÖ Vi kan bruge line√¶r interpolation til at bringe v√¶rdier til et begr√¶nset interval (f.eks. fra -20 til 20) og derefter konvertere tal til heltal ved at runde dem. Dette giver os lidt mindre kontrol over st√∏rrelsen af tilstanden, is√¶r hvis vi ikke kender de n√∏jagtige intervaller for inputv√¶rdierne. For eksempel har 2 ud af 4 v√¶rdier i vores tilf√¶lde ikke √∏vre/nedre gr√¶nser for deres v√¶rdier, hvilket kan resultere i et uendeligt antal tilstande.

I vores eksempel vil vi g√• med den anden tilgang. Som du m√•ske bem√¶rker senere, p√• trods af udefinerede √∏vre/nedre gr√¶nser, tager disse v√¶rdier sj√¶ldent ekstreme v√¶rdier uden for visse begr√¶nsede intervaller, s√• tilstande med ekstreme v√¶rdier vil v√¶re meget sj√¶ldne.

1. Her er funktionen, der tager observationen fra vores model og producerer en tuple med 4 heltalsv√¶rdier: (kodeblok 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Lad os ogs√• udforske en anden diskretiseringsmetode ved hj√¶lp af intervaller: (kodeblok 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Lad os nu k√∏re en kort simulering og observere disse diskrete milj√∏v√¶rdier. Pr√∏v gerne b√•de `discretize` og `discretize_bins` og se, om der er en forskel.

    ‚úÖ `discretize_bins` returnerer intervalnummeret, som er 0-baseret. For v√¶rdier af inputvariablen omkring 0 returnerer den nummeret fra midten af intervallet (10). I `discretize` bekymrede vi os ikke om outputv√¶rdiernes interval, hvilket tillod dem at v√¶re negative, s√• tilstandsv√¶rdierne er ikke forskudt, og 0 svarer til 0. (kodeblok 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Fjern kommentaren fra linjen, der starter med `env.render`, hvis du vil se, hvordan milj√∏et udf√∏res. Ellers kan du udf√∏re det i baggrunden, hvilket er hurtigere. Vi vil bruge denne "usynlige" udf√∏relse under vores Q-Learning-proces.

## Q-Tabellens struktur

I vores tidligere lektion var tilstanden et simpelt par af tal fra 0 til 8, og det var derfor bekvemt at repr√¶sentere Q-Table med en numpy-tensor med en form p√• 8x8x2. Hvis vi bruger intervaller til diskretisering, er st√∏rrelsen af vores tilstandsvektor ogs√• kendt, s√• vi kan bruge samme tilgang og repr√¶sentere tilstanden med en array med formen 20x20x10x10x2 (her er 2 dimensionen af action space, og de f√∏rste dimensioner svarer til antallet af intervaller, vi har valgt at bruge for hver af parametrene i observationsrummet).

Men nogle gange er de pr√¶cise dimensioner af observationsrummet ikke kendt. I tilf√¶lde af funktionen `discretize` kan vi aldrig v√¶re sikre p√•, at vores tilstand holder sig inden for visse gr√¶nser, fordi nogle af de oprindelige v√¶rdier ikke er begr√¶nsede. Derfor vil vi bruge en lidt anderledes tilgang og repr√¶sentere Q-Table med en ordbog.

1. Brug parret *(state,action)* som ordbogs-n√∏gle, og v√¶rdien vil svare til Q-Table-indgangsv√¶rdien. (kodeblok 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Her definerer vi ogs√• en funktion `qvalues()`, som returnerer en liste over Q-Table-v√¶rdier for en given tilstand, der svarer til alle mulige handlinger. Hvis indgangen ikke er til stede i Q-Table, returnerer vi 0 som standard.

## Lad os starte Q-Learning

Nu er vi klar til at l√¶re Peter at balancere!

1. F√∏rst skal vi s√¶tte nogle hyperparametre: (kodeblok 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Her er `alpha` **l√¶ringsraten**, der definerer, i hvilken grad vi skal justere de aktuelle v√¶rdier i Q-Table ved hvert trin. I den tidligere lektion startede vi med 1 og reducerede derefter `alpha` til lavere v√¶rdier under tr√¶ningen. I dette eksempel vil vi holde den konstant for enkelhedens skyld, og du kan eksperimentere med at justere `alpha`-v√¶rdier senere.

    `gamma` er **diskonteringsfaktoren**, der viser, i hvilken grad vi skal prioritere fremtidig bel√∏nning over nuv√¶rende bel√∏nning.

    `epsilon` er **udforsknings-/udnyttelsesfaktoren**, der bestemmer, om vi skal foretr√¶kke udforskning frem for udnyttelse eller omvendt. I vores algoritme vil vi i `epsilon` procent af tilf√¶ldene v√¶lge den n√¶ste handling baseret p√• Q-Table-v√¶rdier, og i de resterende tilf√¶lde vil vi udf√∏re en tilf√¶ldig handling. Dette vil give os mulighed for at udforske omr√•der af s√∏gefeltet, som vi aldrig har set f√∏r.

    ‚úÖ N√•r det g√¶lder balancering ‚Äì at v√¶lge en tilf√¶ldig handling (udforskning) vil fungere som et tilf√¶ldigt skub i den forkerte retning, og stangen skal l√¶re at genvinde balancen fra disse "fejl".

### Forbedr algoritmen

Vi kan ogs√• lave to forbedringer af vores algoritme fra den tidligere lektion:

- **Beregn gennemsnitlig kumulativ bel√∏nning** over et antal simuleringer. Vi vil udskrive fremskridtet hver 5000 iterationer og gennemsnitligg√∏re vores kumulative bel√∏nning over denne periode. Det betyder, at hvis vi f√•r mere end 195 point, kan vi betragte problemet som l√∏st, med endnu h√∏jere kvalitet end kr√¶vet.

- **Beregn maksimal gennemsnitlig kumulativ bel√∏nning**, `Qmax`, og vi vil gemme Q-Table, der svarer til dette resultat. N√•r du k√∏rer tr√¶ningen, vil du bem√¶rke, at den gennemsnitlige kumulative bel√∏nning nogle gange begynder at falde, og vi √∏nsker at bevare v√¶rdierne i Q-Table, der svarer til den bedste model, der er observeret under tr√¶ningen.

1. Saml alle kumulative bel√∏nninger ved hver simulering i `rewards`-vektoren til senere plotning. (kodeblok 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Hvad du m√•ske bem√¶rker fra disse resultater:

- **T√¶t p√• vores m√•l**. Vi er meget t√¶t p√• at opn√• m√•let om at f√• 195 kumulative bel√∏nninger over 100+ p√• hinanden f√∏lgende simuleringer, eller vi har m√•ske faktisk opn√•et det! Selv hvis vi f√•r mindre tal, ved vi stadig ikke, fordi vi gennemsnitligg√∏r over 5000 k√∏rsel, og kun 100 k√∏rsel er kr√¶vet i de formelle kriterier.

- **Bel√∏nning begynder at falde**. Nogle gange begynder bel√∏nningen at falde, hvilket betyder, at vi kan "√∏del√¶gge" allerede l√¶rte v√¶rdier i Q-Table med dem, der g√∏r situationen v√¶rre.

Denne observation er mere tydelig, hvis vi plotter tr√¶ningsfremskridtet.

## Plotning af tr√¶ningsfremskridt

Under tr√¶ningen har vi samlet den kumulative bel√∏nningsv√¶rdi ved hver af iterationerne i `rewards`-vektoren. Her er, hvordan det ser ud, n√•r vi plotter det mod iterationsnummeret:

```python
plt.plot(rewards)
```

![r√•t fremskridt](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Fra denne graf er det ikke muligt at sige noget, fordi l√¶ngden af tr√¶ningssessionerne varierer meget p√• grund af den stokastiske tr√¶ningsproces. For at give mere mening til denne graf kan vi beregne **l√∏bende gennemsnit** over en r√¶kke eksperimenter, lad os sige 100. Dette kan g√∏res bekvemt ved hj√¶lp af `np.convolve`: (kodeblok 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![tr√¶ningsfremskridt](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Justering af hyperparametre

For at g√∏re l√¶ringen mere stabil giver det mening at justere nogle af vores hyperparametre under tr√¶ningen. Is√¶r:

- **For l√¶ringsraten**, `alpha`, kan vi starte med v√¶rdier t√¶t p√• 1 og derefter gradvist reducere parameteren. Med tiden vil vi f√• gode sandsynlighedsv√¶rdier i Q-Table, og derfor b√∏r vi justere dem lidt og ikke overskrive dem fuldst√¶ndigt med nye v√¶rdier.

- **√òg epsilon**. Vi kan langsomt √∏ge `epsilon` for at udforske mindre og udnytte mere. Det giver sandsynligvis mening at starte med en lav v√¶rdi af `epsilon` og gradvist √∏ge den til n√¶sten 1.
> **Opgave 1**: Pr√∏v at √¶ndre v√¶rdierne for hyperparametrene og se, om du kan opn√• en h√∏jere samlet bel√∏nning. Kommer du over 195?
> **Opgave 2**: For at l√∏se problemet formelt, skal du opn√• en gennemsnitlig bel√∏nning p√• 195 over 100 p√• hinanden f√∏lgende k√∏rsler. M√•l dette under tr√¶ningen og s√∏rg for, at du har l√∏st problemet formelt!

## Se resultatet i aktion

Det kunne v√¶re interessant at se, hvordan den tr√¶nede model opf√∏rer sig. Lad os k√∏re simuleringen og f√∏lge den samme strategi for valg af handlinger som under tr√¶ningen, hvor vi sampler i henhold til sandsynlighedsfordelingen i Q-Tabellen: (kodeblok 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Du b√∏r se noget lignende dette:

![en balancerende cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄUdfordring

> **Opgave 3**: Her brugte vi den endelige kopi af Q-Tabellen, som m√•ske ikke er den bedste. Husk, at vi har gemt den bedst pr√¶sterende Q-Tabel i variablen `Qbest`! Pr√∏v det samme eksempel med den bedst pr√¶sterende Q-Tabel ved at kopiere `Qbest` over til `Q` og se, om du bem√¶rker en forskel.

> **Opgave 4**: Her valgte vi ikke den bedste handling ved hvert trin, men samplede i stedet med den tilsvarende sandsynlighedsfordeling. Ville det give mere mening altid at v√¶lge den bedste handling med den h√∏jeste v√¶rdi i Q-Tabellen? Dette kan g√∏res ved at bruge funktionen `np.argmax` til at finde handlingsnummeret, der svarer til den h√∏jeste v√¶rdi i Q-Tabellen. Implementer denne strategi og se, om det forbedrer balanceringen.

## [Quiz efter forel√¶sning](https://ff-quizzes.netlify.app/en/ml/)

## Opgave
[Tr√¶n en Mountain Car](assignment.md)

## Konklusion

Vi har nu l√¶rt, hvordan man tr√¶ner agenter til at opn√• gode resultater blot ved at give dem en bel√∏nningsfunktion, der definerer den √∏nskede tilstand i spillet, og ved at give dem mulighed for intelligent at udforske s√∏geomr√•det. Vi har med succes anvendt Q-Learning-algoritmen i tilf√¶lde af diskrete og kontinuerlige milj√∏er, men med diskrete handlinger.

Det er ogs√• vigtigt at studere situationer, hvor handlingsrummet ogs√• er kontinuerligt, og hvor observationsrummet er meget mere komplekst, s√•som billedet fra sk√¶rmen i Atari-spillet. I disse problemer har vi ofte brug for mere kraftfulde maskinl√¶ringsteknikker, s√•som neurale netv√¶rk, for at opn√• gode resultater. Disse mere avancerede emner er genstand for vores kommende mere avancerede AI-kursus.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal du v√¶re opm√¶rksom p√•, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi er ikke ansvarlige for eventuelle misforst√•elser eller fejltolkninger, der opst√•r som f√∏lge af brugen af denne overs√¶ttelse.