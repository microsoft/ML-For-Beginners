<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T01:08:08+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "da"
}
-->
# Introduktion til Forst칝rkningsl칝ring og Q-Learning

![Oversigt over forst칝rkning i maskinl칝ring i en sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote af [Tomomi Imura](https://www.twitter.com/girlie_mac)

Forst칝rkningsl칝ring involverer tre vigtige begreber: agenten, nogle tilstande og et s칝t handlinger pr. tilstand. Ved at udf칮re en handling i en specifik tilstand f친r agenten en bel칮nning. Forestil dig igen computerspillet Super Mario. Du er Mario, du befinder dig i et spilniveau, st친ende ved kanten af en klippe. Over dig er der en m칮nt. Du som Mario, i et spilniveau, p친 en specifik position ... det er din tilstand. Hvis du tager et skridt til h칮jre (en handling), falder du ud over kanten, hvilket giver dig en lav numerisk score. Men hvis du trykker p친 hop-knappen, scorer du et point og forbliver i live. Det er et positivt resultat, og det b칮r give dig en positiv numerisk score.

Ved at bruge forst칝rkningsl칝ring og en simulator (spillet) kan du l칝re at spille spillet for at maksimere bel칮nningen, som er at forblive i live og score s친 mange point som muligt.

[![Introduktion til Forst칝rkningsl칝ring](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 游꿘 Klik p친 billedet ovenfor for at h칮re Dmitry diskutere Forst칝rkningsl칝ring

## [Quiz f칮r lektionen](https://ff-quizzes.netlify.app/en/ml/)

## Foruds칝tninger og Ops칝tning

I denne lektion vil vi eksperimentere med noget kode i Python. Du skal kunne k칮re Jupyter Notebook-koden fra denne lektion, enten p친 din computer eller i skyen.

Du kan 친bne [lektionens notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) og gennemg친 denne lektion for at bygge.

> **Bem칝rk:** Hvis du 친bner denne kode fra skyen, skal du ogs친 hente filen [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), som bruges i notebook-koden. Tilf칮j den til samme mappe som notebooken.

## Introduktion

I denne lektion vil vi udforske verdenen af **[Peter og Ulven](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspireret af et musikalsk eventyr af den russiske komponist [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Vi vil bruge **Forst칝rkningsl칝ring** til at lade Peter udforske sit milj칮, samle l칝kre 칝bler og undg친 at m칮de ulven.

**Forst칝rkningsl칝ring** (RL) er en l칝ringsteknik, der giver os mulighed for at l칝re en optimal adf칝rd for en **agent** i et **milj칮** ved at udf칮re mange eksperimenter. En agent i dette milj칮 skal have et **m친l**, defineret af en **bel칮nningsfunktion**.

## Milj칮et

For enkelhedens skyld lad os antage, at Peters verden er en kvadratisk spilleplade med st칮rrelsen `bredde` x `h칮jde`, som denne:

![Peters Milj칮](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Hver celle p친 denne spilleplade kan enten v칝re:

* **jord**, som Peter og andre v칝sener kan g친 p친.
* **vand**, som man selvf칮lgelig ikke kan g친 p친.
* et **tr칝** eller **gr칝s**, et sted hvor man kan hvile sig.
* et **칝ble**, som repr칝senterer noget, Peter ville v칝re glad for at finde for at spise.
* en **ulv**, som er farlig og b칮r undg친s.

Der er et separat Python-modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), som indeholder koden til at arbejde med dette milj칮. Fordi denne kode ikke er vigtig for at forst친 vores begreber, vil vi importere modulet og bruge det til at oprette spillepladen (kodeblok 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Denne kode b칮r udskrive et billede af milj칮et, der ligner det ovenfor.

## Handlinger og strategi

I vores eksempel vil Peters m친l v칝re at finde et 칝ble, mens han undg친r ulven og andre forhindringer. For at g칮re dette kan han i princippet g친 rundt, indtil han finder et 칝ble.

Derfor kan han p친 enhver position v칝lge mellem f칮lgende handlinger: op, ned, venstre og h칮jre.

Vi vil definere disse handlinger som en ordbog og kortl칝gge dem til par af tilsvarende koordinat칝ndringer. For eksempel vil bev칝gelse til h칮jre (`R`) svare til et par `(1,0)`. (kodeblok 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

For at opsummere er strategien og m친let for dette scenarie som f칮lger:

- **Strategien**, for vores agent (Peter) er defineret af en s친kaldt **politik**. En politik er en funktion, der returnerer handlingen i en given tilstand. I vores tilf칝lde er problemet repr칝senteret af spillepladen, inklusive spillerens aktuelle position.

- **M친let**, med forst칝rkningsl칝ring er at l칝re en god politik, der g칮r det muligt for os at l칮se problemet effektivt. Som en baseline lad os overveje den enkleste politik kaldet **tilf칝ldig gang**.

## Tilf칝ldig gang

Lad os f칮rst l칮se vores problem ved at implementere en strategi med tilf칝ldig gang. Med tilf칝ldig gang vil vi tilf칝ldigt v칝lge den n칝ste handling fra de tilladte handlinger, indtil vi n친r 칝blet (kodeblok 3).

1. Implementer den tilf칝ldige gang med nedenst친ende kode:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Kaldet til `walk` b칮r returnere l칝ngden af den tilsvarende sti, som kan variere fra den ene k칮rsel til den anden. 

1. K칮r gangeksperimentet et antal gange (f.eks. 100), og udskriv de resulterende statistikker (kodeblok 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Bem칝rk, at den gennemsnitlige l칝ngde af en sti er omkring 30-40 trin, hvilket er ret meget, i betragtning af at den gennemsnitlige afstand til det n칝rmeste 칝ble er omkring 5-6 trin.

    Du kan ogs친 se, hvordan Peters bev칝gelse ser ud under den tilf칝ldige gang:

    ![Peters Tilf칝ldige Gang](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Bel칮nningsfunktion

For at g칮re vores politik mere intelligent skal vi forst친, hvilke bev칝gelser der er "bedre" end andre. For at g칮re dette skal vi definere vores m친l.

M친let kan defineres i form af en **bel칮nningsfunktion**, som vil returnere en scorev칝rdi for hver tilstand. Jo h칮jere tallet er, desto bedre er bel칮nningsfunktionen. (kodeblok 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

En interessant ting ved bel칮nningsfunktioner er, at i de fleste tilf칝lde *f친r vi kun en v칝sentlig bel칮nning ved slutningen af spillet*. Det betyder, at vores algoritme p친 en eller anden m친de skal huske "gode" trin, der f칮rer til en positiv bel칮nning til sidst, og 칮ge deres betydning. Tilsvarende b칮r alle bev칝gelser, der f칮rer til d친rlige resultater, frar친des.

## Q-Learning

En algoritme, som vi vil diskutere her, kaldes **Q-Learning**. I denne algoritme defineres politikken af en funktion (eller en datastruktur) kaldet en **Q-Table**. Den registrerer "godheden" af hver af handlingerne i en given tilstand.

Den kaldes en Q-Table, fordi det ofte er praktisk at repr칝sentere den som en tabel eller en multidimensionel array. Da vores spilleplade har dimensionerne `bredde` x `h칮jde`, kan vi repr칝sentere Q-Table ved hj칝lp af en numpy-array med formen `bredde` x `h칮jde` x `len(actions)`: (kodeblok 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Bem칝rk, at vi initialiserer alle v칝rdierne i Q-Table med en ens v칝rdi, i vores tilf칝lde - 0.25. Dette svarer til politikken "tilf칝ldig gang", fordi alle bev칝gelser i hver tilstand er lige gode. Vi kan sende Q-Table til `plot`-funktionen for at visualisere tabellen p친 spillepladen: `m.plot(Q)`.

![Peters Milj칮](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

I midten af hver celle er der en "pil", der angiver den foretrukne bev칝gelsesretning. Da alle retninger er ens, vises en prik.

Nu skal vi k칮re simuleringen, udforske vores milj칮 og l칝re en bedre fordeling af Q-Table-v칝rdier, som g칮r det muligt for os at finde vejen til 칝blet meget hurtigere.

## Essensen af Q-Learning: Bellman-ligningen

N친r vi begynder at bev칝ge os, vil hver handling have en tilsvarende bel칮nning, dvs. vi kan teoretisk v칝lge den n칝ste handling baseret p친 den h칮jeste umiddelbare bel칮nning. Men i de fleste tilstande vil bev칝gelsen ikke opn친 vores m친l om at n친 칝blet, og vi kan derfor ikke straks beslutte, hvilken retning der er bedre.

> Husk, at det ikke er det umiddelbare resultat, der betyder noget, men snarere det endelige resultat, som vi vil opn친 ved slutningen af simuleringen.

For at tage h칮jde for denne forsinkede bel칮nning skal vi bruge principperne for **[dynamisk programmering](https://en.wikipedia.org/wiki/Dynamic_programming)**, som giver os mulighed for at t칝nke p친 vores problem rekursivt.

Antag, at vi nu er i tilstanden *s*, og vi 칮nsker at bev칝ge os til den n칝ste tilstand *s'*. Ved at g칮re det vil vi modtage den umiddelbare bel칮nning *r(s,a)*, defineret af bel칮nningsfunktionen, plus en fremtidig bel칮nning. Hvis vi antager, at vores Q-Table korrekt afspejler "attraktiviteten" af hver handling, vil vi i tilstanden *s'* v칝lge en handling *a*, der svarer til den maksimale v칝rdi af *Q(s',a')*. S친ledes vil den bedste mulige fremtidige bel칮nning, vi kunne f친 i tilstanden *s*, v칝re defineret som `max`

## Kontrol af politikken

Da Q-Tabellen viser "attraktiviteten" af hver handling i hver tilstand, er det ret nemt at bruge den til at definere effektiv navigation i vores verden. I det simpleste tilf칝lde kan vi v칝lge den handling, der svarer til den h칮jeste Q-Table-v칝rdi: (kodeblok 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Hvis du pr칮ver koden ovenfor flere gange, vil du m친ske bem칝rke, at den nogle gange "h칝nger", og du skal trykke p친 STOP-knappen i notebooken for at afbryde den. Dette sker, fordi der kan v칝re situationer, hvor to tilstande "peger" p친 hinanden i forhold til optimal Q-v칝rdi, hvilket f친r agenten til at bev칝ge sig mellem disse tilstande uendeligt.

## 游Udfordring

> **Opgave 1:** Modificer `walk`-funktionen til at begr칝nse den maksimale l칝ngde af stien til et bestemt antal trin (f.eks. 100), og se koden ovenfor returnere denne v칝rdi fra tid til anden.

> **Opgave 2:** Modificer `walk`-funktionen, s친 den ikke vender tilbage til steder, hvor den allerede har v칝ret tidligere. Dette vil forhindre `walk` i at gentage sig selv, men agenten kan stadig ende med at v칝re "fanget" p친 et sted, hvorfra den ikke kan undslippe.

## Navigation

En bedre navigationspolitik ville v칝re den, vi brugte under tr칝ningen, som kombinerer udnyttelse og udforskning. I denne politik vil vi v칝lge hver handling med en vis sandsynlighed, proportional med v칝rdierne i Q-Tabellen. Denne strategi kan stadig resultere i, at agenten vender tilbage til en position, den allerede har udforsket, men som du kan se fra koden nedenfor, resulterer den i en meget kort gennemsnitlig sti til den 칮nskede placering (husk, at `print_statistics` k칮rer simuleringen 100 gange): (kodeblok 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Efter at have k칮rt denne kode, b칮r du f친 en meget mindre gennemsnitlig stiens l칝ngde end f칮r, i omr친det 3-6.

## Unders칮gelse af l칝ringsprocessen

Som vi har n칝vnt, er l칝ringsprocessen en balance mellem udforskning og udnyttelse af den viden, der er opn친et om problemrummets struktur. Vi har set, at resultaterne af l칝ringen (evnen til at hj칝lpe en agent med at finde en kort sti til m친let) er forbedret, men det er ogs친 interessant at observere, hvordan den gennemsnitlige stiens l칝ngde opf칮rer sig under l칝ringsprocessen:

L칝ringen kan opsummeres som:

- **Gennemsnitlig stiens l칝ngde stiger**. Det, vi ser her, er, at i starten stiger den gennemsnitlige stiens l칝ngde. Dette skyldes sandsynligvis, at n친r vi intet ved om milj칮et, er vi tilb칮jelige til at blive fanget i d친rlige tilstande, vand eller ulv. N친r vi l칝rer mere og begynder at bruge denne viden, kan vi udforske milj칮et l칝ngere, men vi ved stadig ikke s칝rlig godt, hvor 칝blerne er.

- **Stiens l칝ngde falder, efterh친nden som vi l칝rer mere**. N친r vi l칝rer nok, bliver det lettere for agenten at n친 m친let, og stiens l칝ngde begynder at falde. Vi er dog stadig 친bne for udforskning, s친 vi afviger ofte fra den bedste sti og udforsker nye muligheder, hvilket g칮r stien l칝ngere end optimal.

- **L칝ngden stiger pludseligt**. Det, vi ogs친 observerer p친 denne graf, er, at l칝ngden p친 et tidspunkt steg pludseligt. Dette indikerer den stokastiske natur af processen, og at vi p친 et tidspunkt kan "칮del칝gge" Q-Table-koefficienterne ved at overskrive dem med nye v칝rdier. Dette b칮r ideelt set minimeres ved at reducere l칝ringsraten (for eksempel mod slutningen af tr칝ningen justerer vi kun Q-Table-v칝rdierne med en lille v칝rdi).

Samlet set er det vigtigt at huske, at succes og kvaliteten af l칝ringsprocessen afh칝nger betydeligt af parametre som l칝ringsrate, l칝ringsrate-nedgang og diskonteringsfaktor. Disse kaldes ofte **hyperparametre** for at skelne dem fra **parametre**, som vi optimerer under tr칝ningen (for eksempel Q-Table-koefficienter). Processen med at finde de bedste hyperparameterv칝rdier kaldes **hyperparameteroptimering**, og det fortjener et separat emne.

## [Quiz efter forel칝sning](https://ff-quizzes.netlify.app/en/ml/)

## Opgave 
[En Mere Realistisk Verden](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af AI-overs칝ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi er ikke ansvarlige for eventuelle misforst친elser eller fejltolkninger, der opst친r som f칮lge af brugen af denne overs칝ttelse.