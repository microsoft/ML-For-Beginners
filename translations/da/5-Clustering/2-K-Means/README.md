<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T00:05:35+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "da"
}
-->
# K-Means clustering

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

I denne lektion vil du l√¶re, hvordan du opretter klynger ved hj√¶lp af Scikit-learn og det nigerianske musikdatas√¶t, du importerede tidligere. Vi vil d√¶kke det grundl√¶ggende i K-Means til klyngedannelse. Husk, som du l√¶rte i den tidligere lektion, at der er mange m√•der at arbejde med klynger p√•, og den metode, du bruger, afh√¶nger af dine data. Vi vil pr√∏ve K-Means, da det er den mest almindelige teknik til klyngedannelse. Lad os komme i gang!

Begreber, du vil l√¶re om:

- Silhouettescore
- Albue-metoden
- Inerti
- Varians

## Introduktion

[K-Means Clustering](https://wikipedia.org/wiki/K-means_clustering) er en metode, der stammer fra signalbehandlingsomr√•det. Den bruges til at opdele og partitionere grupper af data i 'k' klynger ved hj√¶lp af en r√¶kke observationer. Hver observation arbejder p√• at gruppere et givet datapunkt t√¶ttest p√• dets n√¶rmeste 'gennemsnit', eller midtpunktet af en klynge.

Klyngerne kan visualiseres som [Voronoi-diagrammer](https://wikipedia.org/wiki/Voronoi_diagram), som inkluderer et punkt (eller 'fr√∏') og dets tilsvarende omr√•de.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografik af [Jen Looper](https://twitter.com/jenlooper)

K-Means klyngedannelsesprocessen [udf√∏res i en tretrinsproces](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritmen v√¶lger k-antal midtpunkter ved at tage stikpr√∏ver fra datas√¶ttet. Derefter gentager den:
    1. Den tildeler hver pr√∏ve til det n√¶rmeste midtpunkt.
    2. Den skaber nye midtpunkter ved at tage gennemsnitsv√¶rdien af alle pr√∏ver, der er tildelt de tidligere midtpunkter.
    3. Derefter beregner den forskellen mellem de nye og gamle midtpunkter og gentager, indtil midtpunkterne stabiliseres.

En ulempe ved at bruge K-Means er, at du skal fasts√¶tte 'k', alts√• antallet af midtpunkter. Heldigvis hj√¶lper 'albue-metoden' med at estimere en god startv√¶rdi for 'k'. Du vil pr√∏ve det om lidt.

## Foruds√¶tning

Du vil arbejde i denne lektions [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) fil, som inkluderer dataimporten og den indledende reng√∏ring, du lavede i den sidste lektion.

## √òvelse - forberedelse

Start med at tage et nyt kig p√• sangdataene.

1. Opret et boxplot ved at kalde `boxplot()` for hver kolonne:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Disse data er lidt st√∏jende: ved at observere hver kolonne som et boxplot kan du se outliers.

    ![outliers](../../../../5-Clustering/2-K-Means/images/boxplots.png)

Du kunne gennemg√• datas√¶ttet og fjerne disse outliers, men det ville g√∏re dataene ret minimale.

1. V√¶lg for nu, hvilke kolonner du vil bruge til din klyngedannelses√∏velse. V√¶lg dem med lignende intervaller og kod kolonnen `artist_top_genre` som numeriske data:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Nu skal du v√¶lge, hvor mange klynger du vil m√•lrette. Du ved, at der er 3 sanggenrer, som vi har udskilt fra datas√¶ttet, s√• lad os pr√∏ve med 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Du ser en array udskrevet med forudsagte klynger (0, 1 eller 2) for hver r√¶kke i dataframen.

1. Brug denne array til at beregne en 'silhouettescore':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhouettescore

S√∏g efter en silhouettescore t√¶ttere p√• 1. Denne score varierer fra -1 til 1, og hvis scoren er 1, er klyngen t√¶t og godt adskilt fra andre klynger. En v√¶rdi n√¶r 0 repr√¶senterer overlappende klynger med pr√∏ver meget t√¶t p√• beslutningsgr√¶nsen for de n√¶rliggende klynger. [(Kilde)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Vores score er **.53**, alts√• midt i mellem. Dette indikerer, at vores data ikke er s√¶rligt velegnede til denne type klyngedannelse, men lad os forts√¶tte.

### √òvelse - byg en model

1. Import√©r `KMeans` og start klyngedannelsesprocessen.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Der er nogle dele her, der kr√¶ver forklaring.

    > üéì range: Dette er iterationerne af klyngedannelsesprocessen.

    > üéì random_state: "Bestemmer tilf√¶ldig talgenerering til initialisering af midtpunkter." [Kilde](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "within-cluster sums of squares" m√•ler den kvadrerede gennemsnitlige afstand af alle punkter inden for en klynge til klyngens midtpunkt. [Kilde](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì Inerti: K-Means algoritmer fors√∏ger at v√¶lge midtpunkter for at minimere 'inerti', "et m√•l for, hvor internt sammenh√¶ngende klynger er." [Kilde](https://scikit-learn.org/stable/modules/clustering.html). V√¶rdien tilf√∏jes til wcss-variablen ved hver iteration.

    > üéì k-means++: I [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) kan du bruge 'k-means++'-optimeringen, som "initialiserer midtpunkterne til at v√¶re (generelt) fjernt fra hinanden, hvilket sandsynligvis f√∏rer til bedre resultater end tilf√¶ldig initialisering."

### Albue-metoden

Tidligere antog du, at fordi du har m√•lrettet 3 sanggenrer, b√∏r du v√¶lge 3 klynger. Men er det tilf√¶ldet?

1. Brug 'albue-metoden' for at v√¶re sikker.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Brug den `wcss`-variabel, du byggede i det foreg√•ende trin, til at oprette et diagram, der viser, hvor 'kn√¶kket' i albuen er, hvilket indikerer det optimale antal klynger. M√•ske er det **faktisk** 3!

    ![elbow method](../../../../5-Clustering/2-K-Means/images/elbow.png)

## √òvelse - vis klyngerne

1. Pr√∏v processen igen, denne gang med tre klynger, og vis klyngerne som et scatterplot:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Tjek modellens n√∏jagtighed:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    Denne models n√∏jagtighed er ikke s√¶rlig god, og formen p√• klyngerne giver dig et hint om hvorfor.

    ![clusters](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Disse data er for ubalancerede, for lidt korrelerede, og der er for meget varians mellem kolonnev√¶rdierne til at danne gode klynger. Faktisk er de klynger, der dannes, sandsynligvis st√¶rkt p√•virket eller sk√¶vvredet af de tre genrekategorier, vi definerede ovenfor. Det var en l√¶ringsproces!

    I Scikit-learns dokumentation kan du se, at en model som denne, med klynger, der ikke er s√¶rlig godt afgr√¶nsede, har et 'varians'-problem:

    ![problem models](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografik fra Scikit-learn

## Varians

Varians defineres som "gennemsnittet af de kvadrerede forskelle fra gennemsnittet" [(Kilde)](https://www.mathsisfun.com/data/standard-deviation.html). I konteksten af dette klyngedannelsesproblem refererer det til data, hvor tallene i vores datas√¶t har en tendens til at afvige lidt for meget fra gennemsnittet.

‚úÖ Dette er et godt tidspunkt at t√¶nke over alle de m√•der, du kunne rette dette problem p√•. Justere dataene lidt mere? Bruge andre kolonner? Bruge en anden algoritme? Tip: Pr√∏v at [skalere dine data](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) for at normalisere dem og teste andre kolonner.

> Pr√∏v denne '[variansberegner](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' for at forst√• konceptet lidt bedre.

---

## üöÄUdfordring

Brug lidt tid med denne notebook og juster parametrene. Kan du forbedre modellens n√∏jagtighed ved at rense dataene mere (fjerne outliers, for eksempel)? Du kan bruge v√¶gte til at give mere v√¶gt til bestemte datapunkter. Hvad kan du ellers g√∏re for at skabe bedre klynger?

Tip: Pr√∏v at skalere dine data. Der er kommenteret kode i notebooken, der tilf√∏jer standard skalering for at f√• datakolonnerne til at ligne hinanden mere i forhold til interval. Du vil opdage, at mens silhouettescoren g√•r ned, udj√¶vnes 'kn√¶kket' i albuegrafen. Dette skyldes, at hvis dataene ikke skaleres, f√•r data med mindre varians mere v√¶gt. L√¶s lidt mere om dette problem [her](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Gennemgang & Selvstudie

Tag et kig p√• en K-Means Simulator [som denne](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Du kan bruge dette v√¶rkt√∏j til at visualisere pr√∏vepunkter og bestemme deres midtpunkter. Du kan redigere dataenes tilf√¶ldighed, antal klynger og antal midtpunkter. Hj√¶lper dette dig med at f√• en id√© om, hvordan dataene kan grupperes?

Tag ogs√• et kig p√• [dette handout om K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) fra Stanford.

## Opgave

[Pr√∏v forskellige klyngedannelsesmetoder](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• at sikre n√∏jagtighed, skal det bem√¶rkes, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.