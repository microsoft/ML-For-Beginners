<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-05T22:10:28+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "th"
}
-->
# ฝึก Mountain Car

[OpenAI Gym](http://gym.openai.com) ถูกออกแบบมาให้ทุกสภาพแวดล้อมมี API เดียวกัน นั่นคือมีเมธอด `reset`, `step` และ `render` เหมือนกัน รวมถึงมีการจัดการ **action space** และ **observation space** ในรูปแบบเดียวกัน ดังนั้นจึงควรสามารถปรับอัลกอริทึม reinforcement learning เดียวกันให้เข้ากับสภาพแวดล้อมต่าง ๆ ได้โดยมีการเปลี่ยนแปลงโค้ดเพียงเล็กน้อย

## สภาพแวดล้อม Mountain Car

[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) มีรถที่ติดอยู่ในหุบเขา:

เป้าหมายคือการออกจากหุบเขาและจับธง โดยในแต่ละขั้นตอนสามารถทำหนึ่งในสามการกระทำต่อไปนี้:

| ค่า | ความหมาย |
|---|---|
| 0 | เร่งไปทางซ้าย |
| 1 | ไม่เร่ง |
| 2 | เร่งไปทางขวา |

อย่างไรก็ตาม เคล็ดลับสำคัญของปัญหานี้คือเครื่องยนต์ของรถไม่แรงพอที่จะขึ้นภูเขาในครั้งเดียว ดังนั้นวิธีเดียวที่จะสำเร็จคือการขับรถไปมาเพื่อสร้างแรงเฉื่อย

**Observation space** ประกอบด้วยค่าทั้งหมดสองค่า:

| หมายเลข | การสังเกตการณ์ | ต่ำสุด | สูงสุด |
|-----|--------------|-----|-----|
|  0  | ตำแหน่งของรถ | -1.2| 0.6 |
|  1  | ความเร็วของรถ | -0.07 | 0.07 |

ระบบรางวัลสำหรับ Mountain Car ค่อนข้างซับซ้อน:

 * รางวัล 0 จะได้รับเมื่อ agent ไปถึงธง (ตำแหน่ง = 0.5) บนยอดภูเขา
 * รางวัล -1 จะได้รับเมื่อ agent มีตำแหน่งน้อยกว่า 0.5

**Episode** จะสิ้นสุดลงเมื่อรถมีตำแหน่งมากกว่า 0.5 หรือความยาวของ episode มากกว่า 200

## คำแนะนำ

ปรับอัลกอริทึม reinforcement learning ของเราเพื่อแก้ปัญหา Mountain Car เริ่มต้นด้วยโค้ดใน [notebook.ipynb](../../../../8-Reinforcement/2-Gym/notebook.ipynb) เปลี่ยนสภาพแวดล้อมใหม่ ปรับฟังก์ชันการแยกสถานะ และพยายามทำให้อัลกอริทึมที่มีอยู่สามารถฝึกได้โดยมีการเปลี่ยนแปลงโค้ดน้อยที่สุด ปรับผลลัพธ์ให้เหมาะสมโดยการปรับ hyperparameters

> **Note**: อาจจำเป็นต้องปรับ hyperparameters เพื่อให้อัลกอริทึมสามารถบรรลุผลสำเร็จได้

## เกณฑ์การประเมิน

| เกณฑ์ | ดีเยี่ยม | พอใช้ | ต้องปรับปรุง |
| -------- | --------- | -------- | ----------------- |
|          | อัลกอริทึม Q-Learning ถูกปรับจากตัวอย่าง CartPole ได้สำเร็จ โดยมีการเปลี่ยนแปลงโค้ดน้อยที่สุด และสามารถแก้ปัญหาการจับธงได้ภายใน 200 ขั้นตอน | อัลกอริทึม Q-Learning ใหม่ถูกนำมาใช้จากอินเทอร์เน็ต แต่มีการอธิบายอย่างดี; หรืออัลกอริทึมที่มีอยู่ถูกนำมาใช้ แต่ไม่สามารถบรรลุผลตามที่ต้องการ | นักเรียนไม่สามารถนำอัลกอริทึมใด ๆ มาใช้ได้สำเร็จ แต่ได้ทำขั้นตอนสำคัญไปสู่การแก้ปัญหา (เช่น การแยกสถานะ, โครงสร้างข้อมูล Q-Table ฯลฯ) |

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้