<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T21:34:30+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "th"
}
-->
# ภาคผนวก: การดีบักโมเดลใน Machine Learning ด้วยส่วนประกอบของ Responsible AI Dashboard

## [แบบทดสอบก่อนการบรรยาย](https://ff-quizzes.netlify.app/en/ml/)

## บทนำ

Machine learning มีผลกระทบต่อชีวิตประจำวันของเรา AI กำลังเข้ามามีบทบาทในระบบที่สำคัญที่สุดที่ส่งผลต่อเราในฐานะบุคคลและสังคม เช่น ด้านสุขภาพ การเงิน การศึกษา และการจ้างงาน ตัวอย่างเช่น ระบบและโมเดลถูกนำมาใช้ในงานตัดสินใจประจำวัน เช่น การวินิจฉัยโรคหรือการตรวจจับการฉ้อโกง ด้วยเหตุนี้ ความก้าวหน้าของ AI พร้อมกับการนำไปใช้ที่รวดเร็วขึ้นจึงถูกตอบสนองด้วยความคาดหวังของสังคมที่เปลี่ยนแปลงไปและกฎระเบียบที่เพิ่มขึ้น เรามักเห็นกรณีที่ระบบ AI ไม่สามารถตอบสนองความคาดหวังได้ เปิดเผยความท้าทายใหม่ ๆ และรัฐบาลเริ่มควบคุมโซลูชัน AI ดังนั้นจึงเป็นสิ่งสำคัญที่ต้องวิเคราะห์โมเดลเหล่านี้เพื่อให้ได้ผลลัพธ์ที่ยุติธรรม เชื่อถือได้ ครอบคลุม โปร่งใส และมีความรับผิดชอบสำหรับทุกคน

ในหลักสูตรนี้ เราจะศึกษาวิธีการและเครื่องมือที่สามารถใช้ในการประเมินว่าโมเดลมีปัญหาเกี่ยวกับ Responsible AI หรือไม่ เทคนิคการดีบักโมเดลแบบดั้งเดิมมักอิงกับการคำนวณเชิงปริมาณ เช่น ความแม่นยำรวม หรือค่าเฉลี่ยของการสูญเสียข้อผิดพลาด ลองจินตนาการถึงสิ่งที่อาจเกิดขึ้นเมื่อข้อมูลที่คุณใช้สร้างโมเดลเหล่านี้ขาดกลุ่มประชากรบางประเภท เช่น เชื้อชาติ เพศ มุมมองทางการเมือง ศาสนา หรือมีการแสดงกลุ่มประชากรเหล่านี้อย่างไม่สมดุล หรือเมื่อผลลัพธ์ของโมเดลถูกตีความว่าให้ความสำคัญกับกลุ่มประชากรบางกลุ่ม สิ่งนี้อาจนำไปสู่การแสดงผลที่มากเกินไปหรือน้อยเกินไปของกลุ่มคุณลักษณะที่ละเอียดอ่อน ส่งผลให้เกิดปัญหาด้านความยุติธรรม ความครอบคลุม หรือความน่าเชื่อถือจากโมเดล อีกปัจจัยหนึ่งคือ โมเดล Machine Learning มักถูกมองว่าเป็นกล่องดำ ซึ่งทำให้ยากต่อการเข้าใจและอธิบายว่าอะไรเป็นตัวขับเคลื่อนการคาดการณ์ของโมเดล ทั้งหมดนี้เป็นความท้าทายที่นักวิทยาศาสตร์ข้อมูลและนักพัฒนา AI เผชิญเมื่อไม่มีเครื่องมือที่เพียงพอในการดีบักและประเมินความยุติธรรมหรือความน่าเชื่อถือของโมเดล

ในบทเรียนนี้ คุณจะได้เรียนรู้เกี่ยวกับการดีบักโมเดลของคุณโดยใช้:

- **Error Analysis**: ระบุว่าบริเวณใดในการกระจายข้อมูลที่โมเดลมีอัตราความผิดพลาดสูง
- **Model Overview**: วิเคราะห์เปรียบเทียบระหว่างกลุ่มข้อมูลต่าง ๆ เพื่อค้นหาความแตกต่างในเมตริกประสิทธิภาพของโมเดล
- **Data Analysis**: ตรวจสอบว่ามีการแสดงข้อมูลมากเกินไปหรือน้อยเกินไปที่อาจทำให้โมเดลของคุณเอนเอียงไปยังกลุ่มประชากรหนึ่งมากกว่ากลุ่มอื่น
- **Feature Importance**: เข้าใจว่าคุณลักษณะใดเป็นตัวขับเคลื่อนการคาดการณ์ของโมเดลในระดับโลกหรือระดับเฉพาะกรณี

## ความต้องการเบื้องต้น

ก่อนเริ่มต้น โปรดทบทวน [เครื่องมือ Responsible AI สำหรับนักพัฒนา](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif เกี่ยวกับเครื่องมือ Responsible AI](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## การวิเคราะห์ข้อผิดพลาด (Error Analysis)

เมตริกประสิทธิภาพของโมเดลแบบดั้งเดิมที่ใช้วัดความแม่นยำมักเป็นการคำนวณที่อิงกับการคาดการณ์ที่ถูกต้องและผิดพลาด ตัวอย่างเช่น การพิจารณาว่าโมเดลมีความแม่นยำ 89% และมีการสูญเสียข้อผิดพลาด 0.001 อาจถือว่าเป็นประสิทธิภาพที่ดี อย่างไรก็ตาม ข้อผิดพลาดมักไม่ได้กระจายอย่างสม่ำเสมอในชุดข้อมูลพื้นฐาน คุณอาจได้คะแนนความแม่นยำของโมเดล 89% แต่พบว่ามีบางส่วนของข้อมูลที่โมเดลล้มเหลวถึง 42% ผลกระทบของรูปแบบความล้มเหลวเหล่านี้ในกลุ่มข้อมูลบางกลุ่มอาจนำไปสู่ปัญหาด้านความยุติธรรมหรือความน่าเชื่อถือ การเข้าใจพื้นที่ที่โมเดลทำงานได้ดีหรือไม่ดีจึงเป็นสิ่งสำคัญ บริเวณข้อมูลที่มีความไม่ถูกต้องสูงในโมเดลของคุณอาจกลายเป็นกลุ่มประชากรข้อมูลที่สำคัญ

![วิเคราะห์และดีบักข้อผิดพลาดของโมเดล](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

ส่วนประกอบ Error Analysis บน RAI Dashboard แสดงให้เห็นว่าความล้มเหลวของโมเดลกระจายอยู่ในกลุ่มต่าง ๆ อย่างไรผ่านการแสดงผลแบบต้นไม้ สิ่งนี้มีประโยชน์ในการระบุคุณลักษณะหรือพื้นที่ที่มีอัตราความผิดพลาดสูงในชุดข้อมูลของคุณ โดยการดูว่าความไม่ถูกต้องของโมเดลส่วนใหญ่มาจากที่ใด คุณสามารถเริ่มต้นการตรวจสอบสาเหตุที่แท้จริงได้ คุณยังสามารถสร้างกลุ่มข้อมูลเพื่อทำการวิเคราะห์ได้ กลุ่มข้อมูลเหล่านี้ช่วยในกระบวนการดีบักเพื่อพิจารณาว่าทำไมประสิทธิภาพของโมเดลจึงดีในกลุ่มหนึ่ง แต่ผิดพลาดในอีกกลุ่มหนึ่ง

![Error Analysis](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

ตัวบ่งชี้ภาพบนแผนที่ต้นไม้ช่วยให้ค้นหาพื้นที่ปัญหาได้เร็วขึ้น ตัวอย่างเช่น โหนดต้นไม้ที่มีสีแดงเข้มแสดงถึงอัตราความผิดพลาดที่สูงขึ้น

Heat map เป็นอีกหนึ่งฟังก์ชันการแสดงผลที่ผู้ใช้สามารถใช้ในการตรวจสอบอัตราความผิดพลาดโดยใช้หนึ่งหรือสองคุณลักษณะเพื่อค้นหาปัจจัยที่ส่งผลต่อข้อผิดพลาดของโมเดลในชุดข้อมูลทั้งหมดหรือกลุ่มข้อมูล

![Error Analysis Heatmap](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

ใช้การวิเคราะห์ข้อผิดพลาดเมื่อคุณต้องการ:

* เข้าใจอย่างลึกซึ้งว่าความล้มเหลวของโมเดลกระจายอยู่ในชุดข้อมูลและมิติของคุณลักษณะต่าง ๆ อย่างไร
* แยกเมตริกประสิทธิภาพรวมเพื่อค้นพบกลุ่มข้อมูลที่มีข้อผิดพลาดโดยอัตโนมัติเพื่อแจ้งขั้นตอนการแก้ไขเป้าหมายของคุณ

## ภาพรวมของโมเดล (Model Overview)

การประเมินประสิทธิภาพของโมเดล Machine Learning ต้องการความเข้าใจที่ครอบคลุมเกี่ยวกับพฤติกรรมของมัน สิ่งนี้สามารถทำได้โดยการตรวจสอบเมตริกมากกว่าหนึ่งตัว เช่น อัตราความผิดพลาด ความแม่นยำ การเรียกคืน ความแม่นยำ หรือ MAE (Mean Absolute Error) เพื่อค้นหาความแตกต่างระหว่างเมตริกประสิทธิภาพ เมตริกประสิทธิภาพหนึ่งอาจดูดี แต่ความไม่ถูกต้องอาจถูกเปิดเผยในเมตริกอื่น นอกจากนี้ การเปรียบเทียบเมตริกเพื่อค้นหาความแตกต่างในชุดข้อมูลทั้งหมดหรือกลุ่มข้อมูลช่วยให้เห็นว่าโมเดลทำงานได้ดีหรือไม่ดีในพื้นที่ใด สิ่งนี้มีความสำคัญอย่างยิ่งในการดูประสิทธิภาพของโมเดลในคุณลักษณะที่ละเอียดอ่อนเทียบกับคุณลักษณะที่ไม่ละเอียดอ่อน (เช่น เชื้อชาติ เพศ หรืออายุของผู้ป่วย) เพื่อเปิดเผยความไม่ยุติธรรมที่โมเดลอาจมี ตัวอย่างเช่น การค้นพบว่าโมเดลมีข้อผิดพลาดมากกว่าในกลุ่มที่มีคุณลักษณะละเอียดอ่อนสามารถเปิดเผยความไม่ยุติธรรมที่โมเดลอาจมี

ส่วนประกอบ Model Overview บน RAI Dashboard ช่วยไม่เพียงแต่ในการวิเคราะห์เมตริกประสิทธิภาพของการแสดงข้อมูลในกลุ่ม แต่ยังให้ความสามารถแก่ผู้ใช้ในการเปรียบเทียบพฤติกรรมของโมเดลในกลุ่มต่าง ๆ

![กลุ่มข้อมูล - ภาพรวมโมเดลใน RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

ฟังก์ชันการวิเคราะห์ตามคุณลักษณะของส่วนประกอบช่วยให้ผู้ใช้สามารถเจาะลึกกลุ่มข้อมูลย่อยภายในคุณลักษณะเฉพาะเพื่อระบุความผิดปกติในระดับละเอียด ตัวอย่างเช่น Dashboard มีความฉลาดในตัวเพื่อสร้างกลุ่มข้อมูลโดยอัตโนมัติสำหรับคุณลักษณะที่ผู้ใช้เลือก (เช่น *"time_in_hospital < 3"* หรือ *"time_in_hospital >= 7"*) สิ่งนี้ช่วยให้ผู้ใช้สามารถแยกคุณลักษณะเฉพาะออกจากกลุ่มข้อมูลขนาดใหญ่เพื่อดูว่ามันเป็นตัวขับเคลื่อนสำคัญของผลลัพธ์ที่ผิดพลาดของโมเดลหรือไม่

![กลุ่มคุณลักษณะ - ภาพรวมโมเดลใน RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

ส่วนประกอบ Model Overview รองรับเมตริกความแตกต่างสองประเภท:

**ความแตกต่างในประสิทธิภาพของโมเดล**: เมตริกเหล่านี้คำนวณความแตกต่าง (difference) ในค่าของเมตริกประสิทธิภาพที่เลือกในกลุ่มข้อมูลย่อย ตัวอย่างเช่น:

* ความแตกต่างในอัตราความแม่นยำ
* ความแตกต่างในอัตราความผิดพลาด
* ความแตกต่างในความแม่นยำ
* ความแตกต่างในการเรียกคืน
* ความแตกต่างในค่า MAE (Mean Absolute Error)

**ความแตกต่างในอัตราการเลือก**: เมตริกนี้แสดงความแตกต่างในอัตราการเลือก (การคาดการณ์ที่เป็นผลดี) ในกลุ่มข้อมูลย่อย ตัวอย่างเช่น ความแตกต่างในอัตราการอนุมัติสินเชื่อ อัตราการเลือกหมายถึงเศษส่วนของจุดข้อมูลในแต่ละคลาสที่ถูกจัดประเภทเป็น 1 (ในการจัดประเภทแบบไบนารี) หรือการกระจายค่าการคาดการณ์ (ในการถดถอย)

## การวิเคราะห์ข้อมูล (Data Analysis)

> "ถ้าคุณทรมานข้อมูลนานพอ มันจะสารภาพทุกอย่าง" - Ronald Coase

คำกล่าวนี้อาจฟังดูรุนแรง แต่เป็นความจริงที่ว่าข้อมูลสามารถถูกจัดการเพื่อสนับสนุนข้อสรุปใด ๆ การจัดการดังกล่าวบางครั้งอาจเกิดขึ้นโดยไม่ตั้งใจ ในฐานะมนุษย์ เราทุกคนมีอคติ และมักยากที่จะรู้ตัวเมื่อเราแนะนำอคติในข้อมูล การรับประกันความยุติธรรมใน AI และ Machine Learning ยังคงเป็นความท้าทายที่ซับซ้อน

ข้อมูลเป็นจุดบอดขนาดใหญ่สำหรับเมตริกประสิทธิภาพของโมเดลแบบดั้งเดิม คุณอาจมีคะแนนความแม่นยำสูง แต่สิ่งนี้ไม่ได้สะท้อนถึงอคติในข้อมูลพื้นฐานที่อาจอยู่ในชุดข้อมูลของคุณ ตัวอย่างเช่น หากชุดข้อมูลของพนักงานมีผู้หญิง 27% ในตำแหน่งผู้บริหารในบริษัท และผู้ชาย 73% ในระดับเดียวกัน โมเดล AI ที่โฆษณางานซึ่งถูกฝึกด้วยข้อมูลนี้อาจมุ่งเป้าหมายไปยังผู้ชายเป็นส่วนใหญ่สำหรับตำแหน่งงานระดับสูง การมีความไม่สมดุลในข้อมูลนี้ทำให้การคาดการณ์ของโมเดลเอนเอียงไปยังเพศหนึ่งมากกว่าอีกเพศหนึ่ง สิ่งนี้เผยให้เห็นปัญหาความยุติธรรมที่มีอคติทางเพศในโมเดล AI

ส่วนประกอบ Data Analysis บน RAI Dashboard ช่วยระบุพื้นที่ที่มีการแสดงข้อมูลมากเกินไปหรือน้อยเกินไปในชุดข้อมูล มันช่วยให้ผู้ใช้วินิจฉัยสาเหตุของข้อผิดพลาดและปัญหาความยุติธรรมที่เกิดจากความไม่สมดุลของข้อมูลหรือการขาดการแสดงผลของกลุ่มข้อมูลเฉพาะ สิ่งนี้ช่วยให้ผู้ใช้สามารถแสดงภาพชุดข้อมูลตามผลลัพธ์ที่คาดการณ์และผลลัพธ์จริง กลุ่มข้อผิดพลาด และคุณลักษณะเฉพาะ บางครั้งการค้นพบกลุ่มข้อมูลที่มีการแสดงผลน้อยเกินไปยังสามารถเปิดเผยว่าโมเดลไม่ได้เรียนรู้ได้ดี ส่งผลให้เกิดความไม่ถูกต้องสูง การมีโมเดลที่มีอคติในข้อมูลไม่เพียงแต่เป็นปัญหาความยุติธรรม แต่ยังแสดงว่าโมเดลไม่ครอบคลุมหรือเชื่อถือได้

![ส่วนประกอบ Data Analysis บน RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

ใช้การวิเคราะห์ข้อมูลเมื่อคุณต้องการ:

* สำรวจสถิติของชุดข้อมูลของคุณโดยเลือกตัวกรองต่าง ๆ เพื่อแบ่งข้อมูลของคุณออกเป็นมิติที่แตกต่างกัน (หรือที่เรียกว่ากลุ่มข้อมูล)
* เข้าใจการกระจายของชุดข้อมูลของคุณในกลุ่มข้อมูลและกลุ่มคุณลักษณะต่าง ๆ
* พิจารณาว่าการค้นพบของคุณที่เกี่ยวข้องกับความยุติธรรม การวิเคราะห์ข้อผิดพลาด และความเป็นเหตุเป็นผล (ที่ได้จากส่วนประกอบอื่น ๆ ของ Dashboard) เป็นผลมาจากการกระจายของชุดข้อมูลของคุณหรือไม่
* ตัดสินใจว่าจะรวบรวมข้อมูลเพิ่มเติมในพื้นที่ใดเพื่อแก้ไขข้อผิดพลาดที่เกิดจากปัญหาการแสดงผล การรบกวนของป้ายกำกับ การรบกวนของคุณลักษณะ อคติของป้ายกำกับ และปัจจัยที่คล้ายกัน

## การตีความโมเดล (Model Interpretability)

โมเดล Machine Learning มักถูกมองว่าเป็นกล่องดำ การเข้าใจว่าคุณลักษณะข้อมูลสำคัญใดเป็นตัวขับเคลื่อนการคาดการณ์ของโมเดลอาจเป็นเรื่องท้าทาย สิ่งสำคัญคือต้องให้ความโปร่งใสว่าเหตุใดโมเดลจึงทำการคาดการณ์บางอย่าง ตัวอย่างเช่น หากระบบ AI คาดการณ์ว่าผู้ป่วยเบาหวานมีความเสี่ยงที่จะกลับเข้ารักษาในโรงพยาบาลภายใน 30 วัน ควรสามารถให้ข้อมูลสนับสนุนที่นำไปสู่การคาดการณ์นั้นได้ การมีตัวบ่งชี้ข้อมูลสนับสนุนช่วยให้เกิดความโปร่งใสเพื่อช่วยให้แพทย์หรือโรงพยาบาลสามารถตัดสินใจได้อย่างมีข้อมูล นอกจากนี้ การสามารถอธิบายว่าเหตุใดโมเดลจึงทำการคาดการณ์สำหรับผู้ป่วยรายบุคคลช่วยให้เกิดความรับผิดชอบต่อกฎระเบียบด้านสุขภาพ เมื่อคุณใช้โมเดล Machine Learning ในวิธีที่ส่งผลต่อชีวิตของผู้คน การเข้าใจและอธิบายว่าอะไรเป็นตัวขับเคลื่อนพฤติกรรมของโมเดลจึงเป็นสิ่งสำคัญ การอธิบายและตีความโมเดลช่วยตอบคำถามในสถานการณ์ เช่น:

* การดีบักโมเดล: ทำไมโมเดลของฉันถึงทำผิดพลาด? ฉันจะปรับปรุงโมเดลของฉันได้อย่างไร?
* การทำงานร่วมกันระหว่างมนุษย์และ AI: ฉันจะเข้าใจและเชื่อมั่นในการตัดสินใจของโมเดลได้อย่างไร?
* การปฏิบัติตามกฎระเบียบ: โมเดลของฉันเป็นไปตามข้อกำหนดทางกฎหมายหรือไม่?

ส่วนประกอบ Feature Importance บน RAI Dashboard ช่วยให้คุณดีบักและเข้าใจอย่างครอบคลุมว่าโมเดลทำการคาดการณ์อย่างไร นอกจากนี้ยังเป็นเครื่องมือที่มีประโยชน์สำหรับมืออาชีพด้าน Machine Learning และผู้ตัดสินใจในการอธิบายและแสดงหลักฐานของคุณลักษณะที่มีอิทธิพลต่อพฤติกรรมของโมเดลเพื่อการปฏิบัติตามกฎระเบียบ ผู้ใช้สามารถสำรวจคำอธิบายทั้งในระดับโลกและระดับเฉพาะกรณีเพื่อยืนยันว่าคุณลักษณะใดเป็นตัวขับเคลื่อนการคาดการณ์ของโมเดล คำอธิบายระดับโลกแสดงรายการคุณลักษณะสำคัญที่ส่งผลต่อการคาดการณ์โดยรวมของโมเดล คำอธิบายระดับเฉพาะกรณีแสดงว่าคุณลักษณะใดนำไปสู่การคาดการณ์ของโมเดลสำหรับกรณีเฉพาะ ความสามารถในการประเมินคำอธิบายระดับเฉพาะกรณีมีประโยชน์ในการดีบักหรือการตรวจสอบกรณีเฉพาะเพื่อเข้าใจและตีความว่าเหตุใดโมเดลจึงทำการคาดการณ์ที่ถูกต้องหรือผิดพลาด

![ส่วนประกอบ Feature Importance บน RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* คำอธิบายระดับโลก: ตัวอย่างเช่น คุณลักษณะใดส่งผลต่อพฤติกรรมโดยรวมของโมเดลการกลับเข้ารักษาในโรงพยาบาลของผู้ป่วยเบาหวาน?
* คำอ
- **การแสดงผลที่มากเกินไปหรือน้อยเกินไป** แนวคิดคือกลุ่มคนบางกลุ่มไม่ได้รับการมองเห็นในอาชีพบางประเภท และบริการหรือฟังก์ชันใด ๆ ที่ยังคงส่งเสริมสิ่งนี้ถือเป็นการสร้างความเสียหาย

### Azure RAI dashboard

[Azure RAI dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) ถูกสร้างขึ้นจากเครื่องมือโอเพ่นซอร์สที่พัฒนาโดยสถาบันการศึกษาและองค์กรชั้นนำ รวมถึง Microsoft ซึ่งมีบทบาทสำคัญสำหรับนักวิทยาศาสตร์ข้อมูลและนักพัฒนา AI ในการทำความเข้าใจพฤติกรรมของโมเดล ค้นหาและลดปัญหาที่ไม่พึงประสงค์จากโมเดล AI

- เรียนรู้วิธีการใช้ส่วนประกอบต่าง ๆ โดยดูเอกสารของ RAI dashboard [docs.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- ดูตัวอย่างโน้ตบุ๊กของ RAI dashboard [sample notebooks](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) สำหรับการแก้ไขปัญหา AI ที่มีความรับผิดชอบมากขึ้นใน Azure Machine Learning

---
## 🚀 ความท้าทาย

เพื่อป้องกันไม่ให้เกิดอคติทางสถิติหรือข้อมูลตั้งแต่แรก เราควร:

- มีความหลากหลายของพื้นหลังและมุมมองในกลุ่มคนที่ทำงานเกี่ยวกับระบบ
- ลงทุนในชุดข้อมูลที่สะท้อนถึงความหลากหลายของสังคมเรา
- พัฒนาวิธีการที่ดีกว่าในการตรวจจับและแก้ไขอคติเมื่อมันเกิดขึ้น

ลองคิดถึงสถานการณ์ในชีวิตจริงที่ความไม่เป็นธรรมปรากฏชัดในกระบวนการสร้างและใช้งานโมเดล เราควรพิจารณาอะไรเพิ่มเติมอีก?

## [แบบทดสอบหลังการบรรยาย](https://ff-quizzes.netlify.app/en/ml/)
## ทบทวนและศึกษาด้วยตนเอง

ในบทเรียนนี้ คุณได้เรียนรู้เครื่องมือที่ใช้งานได้จริงบางส่วนในการนำ AI ที่มีความรับผิดชอบมารวมเข้ากับการเรียนรู้ของเครื่อง

ดูเวิร์กช็อปนี้เพื่อเจาะลึกในหัวข้อเพิ่มเติม:

- Responsible AI Dashboard: ศูนย์รวมสำหรับการนำ RAI ไปใช้ในทางปฏิบัติ โดย Besmira Nushi และ Mehrnoosh Sameki

[![Responsible AI Dashboard: ศูนย์รวมสำหรับการนำ RAI ไปใช้ในทางปฏิบัติ](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: ศูนย์รวมสำหรับการนำ RAI ไปใช้ในทางปฏิบัติ")

> 🎥 คลิกที่ภาพด้านบนเพื่อดูวิดีโอ: Responsible AI Dashboard: ศูนย์รวมสำหรับการนำ RAI ไปใช้ในทางปฏิบัติ โดย Besmira Nushi และ Mehrnoosh Sameki

อ้างอิงเอกสารต่อไปนี้เพื่อเรียนรู้เพิ่มเติมเกี่ยวกับ AI ที่มีความรับผิดชอบและวิธีสร้างโมเดลที่น่าเชื่อถือมากขึ้น:

- เครื่องมือ RAI dashboard ของ Microsoft สำหรับการแก้ไขข้อบกพร่องในโมเดล ML: [Responsible AI tools resources](https://aka.ms/rai-dashboard)

- สำรวจชุดเครื่องมือ Responsible AI: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- ศูนย์ทรัพยากร RAI ของ Microsoft: [Responsible AI Resources – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- กลุ่มวิจัย FATE ของ Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## งานที่ได้รับมอบหมาย

[สำรวจ RAI dashboard](assignment.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้