<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-09-05T13:43:50+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "my"
}
-->
# အကောင်းမွန်သော အပြည့်အစုံသော ကမ္ဘာ

ကျွန်တော်တို့ရဲ့ အခြေအနေမှာ Peter ဟာ မိမိကိုယ်ကို မပင်ပန်းဘဲ၊ မဆာဘဲ နေရာတစ်ခုမှ နေရာတစ်ခုကို လွယ်ကူစွာ ရွှေ့လျားနိုင်ခဲ့ပါတယ်။ အကောင်းမွန်သော အပြည့်အစုံသော ကမ္ဘာတစ်ခုမှာတော့ Peter ဟာ အချိန်အခါတစ်ချို့မှာ ထိုင်ပြီး အနားယူဖို့လိုအပ်တယ်၊ နောက်ပြီး သူ့ကိုယ်သူ အစာစားဖို့လည်း လိုအပ်ပါတယ်။ ကမ္ဘာကို ပိုမိုအပြည့်အစုံဖြစ်အောင် လုပ်ဆောင်ဖို့အတွက် အောက်ပါ စည်းကမ်းများကို အကောင်အထည်ဖော်ကြရအောင်။

1. နေရာတစ်ခုမှ နေရာတစ်ခုကို ရွှေ့လျားတဲ့အခါ Peter ဟာ **စွမ်းအင်** ဆုံးရှုံးပြီး **ပင်ပန်းမှု** တိုးလာမယ်။
2. Peter ဟာ ပန်းသီးတွေ စားခြင်းအားဖြင့် စွမ်းအင်ကို ပြန်လည်ရရှိနိုင်မယ်။
3. Peter ဟာ သစ်ပင်အောက်မှာ ဒါမှမဟုတ် မြက်ခင်းပေါ်မှာ အနားယူခြင်းအားဖြင့် ပင်ပန်းမှုကို လျှော့ချနိုင်မယ် (ဥပမာ - သစ်ပင်နဲ့ မြက်ခင်းရှိတဲ့ နေရာကို လမ်းလျှောက်ဝင်ရောက်ခြင်း)။
4. Peter ဟာ ဝက်ဝံကို ရှာဖွေပြီး သတ်ဖို့ လိုအပ်တယ်။
5. ဝက်ဝံကို သတ်ဖို့အတွက် Peter ဟာ သတ်ပွဲမှာ အနိုင်ရဖို့ လိုအပ်တဲ့ စွမ်းအင်နဲ့ ပင်ပန်းမှု အဆင့်ကို ရှိထားဖို့ လိုအပ်တယ်။ မဟုတ်ရင် သူဟာ သတ်ပွဲမှာ ရှုံးမယ်။

## လမ်းညွှန်ချက်များ

မူရင်း [notebook.ipynb](../../../../8-Reinforcement/1-QLearning/notebook.ipynb) ကို သင့်ရဲ့ ဖြေရှင်းချက်အတွက် အခြေခံအနေဖြင့် အသုံးပြုပါ။

အပေါ်မှာ ဖော်ပြထားတဲ့ ဆုချီးမြှင့်မှု ဖန်တီးမှုကို ပြင်ဆင်ပြီး၊ reinforcement learning algorithm ကို အသုံးပြု၍ အနိုင်ရဖို့ strategy အကောင်းဆုံးကို သင်ယူပါ၊ random walk နဲ့ သင့် algorithm ရဲ့ ရလဒ်ကို အနိုင်ရမှုနဲ့ ရှုံးမှု အရေအတွက်အပေါ်မှာ နှိုင်းယှဉ်ပါ။

> **Note**: သင့်ရဲ့ ကမ္ဘာအသစ်မှာ state ဟာ ပိုမိုရှုပ်ထွေးလာပြီး လူ့နေရာအပြင် ပင်ပန်းမှုနဲ့ စွမ်းအင်အဆင့်တွေကိုလည်း ပါဝင်ပါတယ်။ state ကို (Board, energy, fatigue) အဖြစ် tuple အနေနဲ့ ကိုယ်တိုင်ဖော်ပြနိုင်ပါတယ်၊ ဒါမှမဟုတ် state အတွက် class တစ်ခုကို သတ်မှတ်နိုင်ပါတယ် (သို့မဟုတ် `Board` မှ ဆင်းသက်ထားတဲ့ class ကို ဖန်တီးနိုင်ပါတယ်)၊ ဒါမှမဟုတ် မူရင်း `Board` class ကို [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py) မှာ ပြင်ဆင်နိုင်ပါတယ်။

သင့်ရဲ့ ဖြေရှင်းချက်မှာ random walk strategy အတွက် code ကို ထားရှိပါ၊ နောက်ဆုံးမှာ သင့် algorithm နဲ့ random walk ရဲ့ ရလဒ်ကို နှိုင်းယှဉ်ပါ။

> **Note**: သင့် algorithm ကို အလုပ်လုပ်စေဖို့ hyperparameters တွေကို ပြင်ဆင်ဖို့ လိုအပ်နိုင်ပါတယ်၊ အထူးသဖြင့် epochs အရေအတွက်ကို။ ကမ္ဘာရဲ့ အောင်မြင်မှု (ဝက်ဝံနဲ့ တိုက်ခိုက်မှု) ဟာ ရှားပါးတဲ့ အဖြစ်အပျက်ဖြစ်တဲ့အတွက် သင့်ရဲ့ training time ဟာ ပိုကြာမြင့်နိုင်ပါတယ်။

## အဆင့်သတ်မှတ်ချက်

| စံနှုန်း | အထူးကောင်းမွန်                                                                                                                                                                                             | လုံလောက်သော                                                                                                                                                                                | တိုးတက်မှုလိုအပ်သည်                                                                                                                          |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
|          | notebook တစ်ခုကို အသစ်သော ကမ္ဘာစည်းကမ်းများ၊ Q-Learning algorithm နဲ့ အချို့သော စာသားရှင်းလင်းချက်များနဲ့တင်ပြထားသည်။ Q-Learning ဟာ random walk နဲ့ နှိုင်းယှဉ်ပြီး ရလဒ်ကို အထူးကောင်းမွန်စွာ တိုးတက်စေသည်။ | notebook တစ်ခုကို တင်ပြထားပြီး၊ Q-Learning ကို အကောင်အထည်ဖော်ထားပြီး random walk နဲ့ နှိုင်းယှဉ်ပြီး ရလဒ်ကို တိုးတက်စေသည်၊ သို့သော် အထူးကောင်းမွန်မှုမရှိပါ။ ဒါမှမဟုတ် notebook ဟာ documentation မလုံလောက်ပါ၊ code ဟာ အဆင့်မပြေပါ။ | ကမ္ဘာစည်းကမ်းအသစ်များကို ပြန်လည်သတ်မှတ်ရန် ကြိုးစားမှုများပြုလုပ်ထားသော်လည်း Q-Learning algorithm ဟာ အလုပ်မလုပ်ပါ၊ ဒါမှမဟုတ် reward function ဟာ အပြည့်အစုံမသတ်မှတ်ထားပါ။ |

---

**ဝက်ဘ်ဆိုက်မှတ်ချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားနေပါသော်လည်း၊ အလိုအလျောက်ဘာသာပြန်ဆိုမှုများတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါဝင်နိုင်သည်ကို ကျေးဇူးပြု၍ သတိပြုပါ။ မူရင်းစာရွက်စာတမ်းကို ၎င်း၏ မူလဘာသာစကားဖြင့် အာဏာတည်သောရင်းမြစ်အဖြစ် သတ်မှတ်ရန် လိုအပ်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပြန်ဆိုမှုကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော နားလည်မှုမှားများ သို့မဟုတ် အဓိပ္ပါယ်မှားများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။