<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-05T13:51:35+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "my"
}
-->
# တောင်တက်ကားကို လေ့ကျင့်ပါ

[OpenAI Gym](http://gym.openai.com) ကို အားလုံးတူညီသော API - `reset`, `step` နှင့် `render` method များနှင့် **action space** နှင့် **observation space** အတွက် တူညီသော အခြေခံအဆင့်များကို ပံ့ပိုးပေးသော ပုံစံဖြင့် ဖန်တီးထားသည်။ ထို့ကြောင့် reinforcement learning algorithm များကို အခြေခံကုဒ်ပြောင်းလဲမှု အနည်းငယ်ဖြင့် အခြားသော environment များတွင် လွယ်ကူစွာ အသုံးပြုနိုင်သည်။

## တောင်တက်ကား Environment

[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) တွင် ကားတစ်စီးသည် တောင်ကြားတွင် ပိတ်မိနေသည်။

အဓိကရည်ရွယ်ချက်မှာ တောင်ကြားမှ ထွက်ပြီး အလံကို ဖမ်းယူရန်ဖြစ်ပြီး၊ အောက်ပါ လုပ်ဆောင်ချက်များကို တစ်ဆင့်ချင်းလုပ်ဆောင်ရမည်။

| တန်ဖိုး | အဓိပ္ပါယ် |
|---|---|
| 0 | ဘယ်ဘက်သို့ အရှိန်မြှင့်ပါ |
| 1 | အရှိန်မမြှင့်ပါနှင့် |
| 2 | ညာဘက်သို့ အရှိန်မြှင့်ပါ |

ဒီပြဿနာရဲ့ အဓိကလှည့်ကွက်ကတော့ ကားရဲ့ engine က တစ်ကြိမ်တည်းနဲ့ တောင်ကို တက်နိုင်အောင် အားကောင်းမနေပါဘူး။ ထို့ကြောင့် အောင်မြင်ရန် တစ်ခုတည်းသော နည်းလမ်းမှာ အရှိန်တိုးဖို့အတွက် နောက်ပြန်နှင့်ရှေ့ပြန် မောင်းရမည်ဖြစ်သည်။

Observation space တွင် တန်ဖိုးနှစ်ခုသာ ပါဝင်သည်။

| နံပါတ် | Observation  | အနည်းဆုံး | အများဆုံး |
|-----|--------------|-----|-----|
|  0  | ကားတည်နေရာ | -1.2| 0.6 |
|  1  | ကားအရှိန် | -0.07 | 0.07 |

တောင်တက်ကားအတွက် Reward system က အတော်လှည့်ကွက်များပါဝင်သည်။

 * အကယ်၍ agent သည် တောင်ထိပ်ရှိ အလံ (position = 0.5) ကို ရောက်ရှိနိုင်ပါက Reward 0 ကို ချီးမြှင့်သည်။
 * အကယ်၍ agent ၏ တည်နေရာသည် 0.5 ထက် နည်းပါက Reward -1 ကို ချီးမြှင့်သည်။

Episode သည် ကားတည်နေရာသည် 0.5 ထက် များသောအခါ၊ သို့မဟုတ် episode အရှည်သည် 200 ထက် များသောအခါ အဆုံးသတ်သည်။

## လမ်းညွှန်ချက်များ

တောင်တက်ကားပြဿနာကို ဖြေရှင်းရန် reinforcement learning algorithm ကို ပြောင်းလဲအသုံးပြုပါ။ ရှိပြီးသား [notebook.ipynb](../../../../8-Reinforcement/2-Gym/notebook.ipynb) code ကို စတင်ပြီး၊ environment အသစ်ကို အစားထိုးပါ၊ state discretization function များကို ပြောင်းလဲပါ၊ ရှိပြီးသား algorithm ကို အနည်းငယ်သော code ပြောင်းလဲမှုဖြင့် လေ့ကျင့်နိုင်ရန် ကြိုးစားပါ။ Hyperparameter များကို ပြင်ဆင်ခြင်းဖြင့် ရလဒ်ကို အကောင်းဆုံးဖြစ်အောင် လုပ်ဆောင်ပါ။

> **Note**: Algorithm ကို convergence ဖြစ်အောင် Hyperparameter များကို ပြင်ဆင်ရန် လိုအပ်နိုင်သည်။

## အဆင့်သတ်မှတ်ချက်

| အဆင့်သတ်မှတ်ချက် | ထူးချွန် | လုံလောက် | တိုးတက်မှုလိုအပ် |
| -------- | --------- | -------- | ----------------- |
|          | Q-Learning algorithm ကို CartPole ဥပမာမှ အနည်းငယ်သော code ပြောင်းလဲမှုဖြင့် အောင်မြင်စွာ ပြောင်းလဲအသုံးပြုပြီး၊ တောင်ထိပ်တွင် အလံကို 200 ဆင့်အတွင်း ဖမ်းယူနိုင်သော ပြဿနာကို ဖြေရှင်းနိုင်သည်။ | အင်တာနက်မှ Q-Learning algorithm အသစ်ကို ယူပြီး documentation ကောင်းစွာရေးသားထားသည်။ သို့မဟုတ် ရှိပြီးသား algorithm ကို အသုံးပြုထားပြီး၊ ရလဒ်လိုအပ်ချက်များကို မရောက်ရှိနိုင်ပါ။ | ကျောင်းသားသည် algorithm တစ်ခုကို အောင်မြင်စွာ အသုံးပြုနိုင်ခြင်း မရှိသော်လည်း၊ ဖြေရှင်းရန် အရေးကြီးသော အဆင့်များ (state discretization, Q-Table data structure စသည်တို့) ကို အကောင်အထည်ဖော်ထားသည်။ |

---

**ဝက်ဘ်ဆိုက်မှတ်ချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားနေပါသော်လည်း၊ အလိုအလျောက်ဘာသာပြန်ဆိုမှုများတွင် အမှားများ သို့မဟုတ် မမှန်ကန်မှုများ ပါဝင်နိုင်သည်ကို ကျေးဇူးပြု၍ သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတည်သော ရင်းမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် ပရော်ဖက်ရှင်နယ် လူသားဘာသာပြန်ကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော နားလည်မှုမှားများ သို့မဟုတ် အဓိပ္ပါယ်မှားများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။