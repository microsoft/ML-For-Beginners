<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T16:45:21+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "hu"
}
-->
# CartPole G√∂rdeszk√°z√°s

Az el≈ëz≈ë leck√©ben megoldott probl√©ma tal√°n j√°t√©kszer≈±nek t≈±nhet, √©s nem igaz√°n alkalmazhat√≥nak a val√≥s √©letben. Ez azonban nem √≠gy van, mivel sok val√≥s probl√©ma is hasonl√≥ helyzetet mutat ‚Äì p√©ld√°ul sakk vagy Go j√°t√©k k√∂zben. Ezek hasonl√≥ak, mert van egy t√°bl√°nk adott szab√°lyokkal √©s egy **diszkr√©t √°llapot**.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## Bevezet√©s

Ebben a leck√©ben ugyanazokat az elveket alkalmazzuk a Q-Learning sor√°n egy **folytonos √°llapot√∫** probl√©m√°ra, azaz egy olyan √°llapotra, amelyet egy vagy t√∂bb val√≥s sz√°m hat√°roz meg. A k√∂vetkez≈ë probl√©m√°val foglalkozunk:

> **Probl√©ma**: Ha P√©ter el akar menek√ºlni a farkas el≈ël, gyorsabban kell tudnia mozogni. Megn√©zz√ºk, hogyan tanulhat meg P√©ter g√∂rdeszk√°zni, k√ºl√∂n√∂sen egyens√∫lyozni, a Q-Learning seg√≠ts√©g√©vel.

![A nagy sz√∂k√©s!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> P√©ter √©s bar√°tai kreat√≠vak lesznek, hogy elmenek√ºljenek a farkas el≈ël! K√©p: [Jen Looper](https://twitter.com/jenlooper)

Egy egyszer≈±s√≠tett egyens√∫lyoz√°si probl√©m√°t fogunk haszn√°lni, amelyet **CartPole** probl√©m√°nak neveznek. A CartPole vil√°g√°ban van egy v√≠zszintes cs√∫szka, amely balra vagy jobbra mozoghat, √©s a c√©l az, hogy egy f√ºgg≈ëleges rudat egyens√∫lyban tartsunk a cs√∫szka tetej√©n.

## El≈ëfelt√©telek

Ebben a leck√©ben az **OpenAI Gym** nev≈± k√∂nyvt√°rat fogjuk haszn√°lni k√ºl√∂nb√∂z≈ë **k√∂rnyezetek** szimul√°l√°s√°ra. A lecke k√≥dj√°t futtathatod helyben (p√©ld√°ul Visual Studio Code-ban), ebben az esetben a szimul√°ci√≥ egy √∫j ablakban ny√≠lik meg. Ha online futtatod a k√≥dot, n√©h√°ny m√≥dos√≠t√°st kell v√©gezned, ahogy azt [itt](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7) le√≠rt√°k.

## OpenAI Gym

Az el≈ëz≈ë leck√©ben a j√°t√©k szab√°lyait √©s az √°llapotot a saj√°t magunk √°ltal defini√°lt `Board` oszt√°ly hat√°rozta meg. Itt egy speci√°lis **szimul√°ci√≥s k√∂rnyezetet** fogunk haszn√°lni, amely szimul√°lja az egyens√∫lyoz√≥ r√∫d m√∂g√∂tti fizik√°t. Az egyik legn√©pszer≈±bb szimul√°ci√≥s k√∂rnyezet a meger≈ës√≠t√©ses tanul√°si algoritmusokhoz a [Gym](https://gym.openai.com/), amelyet az [OpenAI](https://openai.com/) tart fenn. Ezzel a Gymmel k√ºl√∂nb√∂z≈ë **k√∂rnyezeteket** hozhatunk l√©tre, a CartPole szimul√°ci√≥t√≥l kezdve az Atari j√°t√©kokig.

> **Megjegyz√©s**: Az OpenAI Gym √°ltal el√©rhet≈ë egy√©b k√∂rnyezeteket [itt](https://gym.openai.com/envs/#classic_control) tal√°lod.

El≈ësz√∂r telep√≠ts√ºk a Gymet √©s import√°ljuk a sz√ºks√©ges k√∂nyvt√°rakat (k√≥dblokk 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Gyakorlat - CartPole k√∂rnyezet inicializ√°l√°sa

Ahhoz, hogy a CartPole egyens√∫lyoz√°si probl√©m√°val dolgozzunk, inicializ√°lnunk kell a megfelel≈ë k√∂rnyezetet. Minden k√∂rnyezethez tartozik:

- **Megfigyel√©si t√©r**, amely meghat√°rozza az inform√°ci√≥ szerkezet√©t, amelyet a k√∂rnyezett≈ël kapunk. A CartPole probl√©m√°ban a r√∫d helyzet√©t, sebess√©g√©t √©s n√©h√°ny m√°s √©rt√©ket kapunk.

- **Akci√≥t√©r**, amely meghat√°rozza a lehets√©ges akci√≥kat. Eset√ºnkben az akci√≥t√©r diszkr√©t, √©s k√©t akci√≥b√≥l √°ll - **balra** √©s **jobbra**. (k√≥dblokk 2)

1. Az inicializ√°l√°shoz √≠rd be a k√∂vetkez≈ë k√≥dot:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Ahhoz, hogy l√°ssuk, hogyan m≈±k√∂dik a k√∂rnyezet, futtassunk egy r√∂vid szimul√°ci√≥t 100 l√©p√©sig. Minden l√©p√©sn√©l megadunk egy akci√≥t, amelyet v√©gre kell hajtani ‚Äì ebben a szimul√°ci√≥ban v√©letlenszer≈±en v√°lasztunk egy akci√≥t az `action_space`-b≈ël.

1. Futtasd az al√°bbi k√≥dot, √©s n√©zd meg, mi t√∂rt√©nik.

    ‚úÖ Ne feledd, hogy ezt a k√≥dot helyi Python telep√≠t√©sen futtatni el≈ëny√∂sebb! (k√≥dblokk 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Valami hasonl√≥t kell l√°tnod, mint ez a k√©p:

    ![nem egyens√∫lyoz√≥ CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. A szimul√°ci√≥ sor√°n megfigyel√©seket kell kapnunk, hogy eld√∂nthess√ºk, hogyan cselekedj√ºnk. Val√≥j√°ban a step f√ºggv√©ny visszaadja az aktu√°lis megfigyel√©seket, egy jutalomf√ºggv√©nyt √©s egy done jelz≈ët, amely jelzi, hogy van-e √©rtelme folytatni a szimul√°ci√≥t: (k√≥dblokk 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    A notebook kimenet√©ben valami ilyesmit fogsz l√°tni:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    A szimul√°ci√≥ minden l√©p√©s√©n√©l visszaadott megfigyel√©si vektor a k√∂vetkez≈ë √©rt√©keket tartalmazza:
    - A kocsi helyzete
    - A kocsi sebess√©ge
    - A r√∫d sz√∂ge
    - A r√∫d forg√°si sebess√©ge

1. Szerezd meg ezeknek a sz√°moknak a minimum √©s maximum √©rt√©k√©t: (k√≥dblokk 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Azt is √©szreveheted, hogy a jutalom√©rt√©k minden szimul√°ci√≥s l√©p√©sn√©l mindig 1. Ennek oka, hogy c√©lunk min√©l tov√°bb √©letben maradni, azaz a rudat √©sszer≈±en f√ºgg≈ëleges helyzetben tartani a lehet≈ë leghosszabb ideig.

    ‚úÖ Val√≥j√°ban a CartPole szimul√°ci√≥t akkor tekintj√ºk megoldottnak, ha siker√ºl 195 √°tlagos jutalmat el√©rni 100 egym√°st k√∂vet≈ë pr√≥b√°lkoz√°s sor√°n.

## √Ållapot diszkretiz√°l√°sa

A Q-Learning sor√°n l√©tre kell hoznunk egy Q-t√°bl√°t, amely meghat√°rozza, mit kell tenni minden √°llapotban. Ehhez az √°llapotnak **diszkr√©tnek** kell lennie, pontosabban v√©ges sz√°m√∫ diszkr√©t √©rt√©ket kell tartalmaznia. Ez√©rt valahogy **diszkretiz√°lnunk** kell a megfigyel√©seinket, √©s azokat egy v√©ges √°llapothalmazhoz kell hozz√°rendelni.

N√©h√°ny m√≥don megtehetj√ºk ezt:

- **Feloszt√°s bin-ekre**. Ha ismerj√ºk egy adott √©rt√©k intervallum√°t, feloszthatjuk ezt az intervallumot egy bizonyos sz√°m√∫ **binre**, majd az √©rt√©ket lecser√©lhetj√ºk arra a bin sz√°mra, amelyhez tartozik. Ez a numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) met√≥dus√°val v√©gezhet≈ë el. Ebben az esetben pontosan ismerni fogjuk az √°llapot m√©ret√©t, mivel az a digitaliz√°l√°shoz kiv√°lasztott bin-ek sz√°m√°t√≥l f√ºgg.

‚úÖ Haszn√°lhatunk line√°ris interpol√°ci√≥t, hogy az √©rt√©keket egy v√©ges intervallumra (p√©ld√°ul -20-t√≥l 20-ig) hozzuk, majd a sz√°mokat kerek√≠t√©ssel eg√©sz sz√°mokk√° alak√≠thatjuk. Ez valamivel kevesebb kontrollt ad az √°llapot m√©ret√©re, k√ºl√∂n√∂sen, ha nem ismerj√ºk a bemeneti √©rt√©kek pontos tartom√°nyait. P√©ld√°ul eset√ºnkben 4 √©rt√©kb≈ël 2-nek nincs fels≈ë/als√≥ hat√°ra, ami v√©gtelen sz√°m√∫ √°llapotot eredm√©nyezhet.

P√©ld√°nkban a m√°sodik megk√∂zel√≠t√©st fogjuk alkalmazni. Ahogy k√©s≈ëbb √©szreveheted, a meghat√°rozatlan fels≈ë/als√≥ hat√°rok ellen√©re ezek az √©rt√©kek ritk√°n vesznek fel bizonyos v√©ges intervallumokon k√≠v√ºli √©rt√©keket, √≠gy azok az √°llapotok, amelyek sz√©ls≈ës√©ges √©rt√©keket tartalmaznak, nagyon ritk√°k lesznek.

1. √çme egy f√ºggv√©ny, amely a modell√ºnk megfigyel√©s√©t veszi, √©s egy 4 eg√©sz √©rt√©k≈± tuple-t √°ll√≠t el≈ë: (k√≥dblokk 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. N√©zz√ºk meg egy m√°sik diszkretiz√°l√°si m√≥dszert bin-ek haszn√°lat√°val: (k√≥dblokk 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Most futtassunk egy r√∂vid szimul√°ci√≥t, √©s figyelj√ºk meg ezeket a diszkr√©t k√∂rnyezeti √©rt√©keket. Nyugodtan pr√≥b√°ld ki a `discretize` √©s `discretize_bins` f√ºggv√©nyeket, √©s n√©zd meg, van-e k√ºl√∂nbs√©g.

    ‚úÖ A `discretize_bins` a bin sz√°m√°t adja vissza, amely 0-alap√∫. √çgy a bemeneti v√°ltoz√≥ k√∂r√ºli √©rt√©kek eset√©n 0 k√∂r√ºl az intervallum k√∂zep√©b≈ël (10) ad vissza sz√°mot. A `discretize` eset√©ben nem t√∂r≈ëdt√ºnk a kimeneti √©rt√©kek tartom√°ny√°val, lehet≈ëv√© t√©ve, hogy negat√≠vak legyenek, √≠gy az √°llapot√©rt√©kek nem tol√≥dnak el, √©s 0 megfelel 0-nak. (k√≥dblokk 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Kommenteld ki az env.render-rel kezd≈ëd≈ë sort, ha l√°tni szeretn√©d, hogyan hajtja v√©gre a k√∂rnyezet. Ellenkez≈ë esetben a h√°tt√©rben is futtathatod, ami gyorsabb. Ezt a "l√°thatatlan" v√©grehajt√°st fogjuk haszn√°lni a Q-Learning folyamat sor√°n.

## A Q-t√°bla szerkezete

Az el≈ëz≈ë leck√©ben az √°llapot egy egyszer≈± sz√°mp√°r volt 0-t√≥l 8-ig, √≠gy k√©nyelmes volt a Q-t√°bl√°t egy 8x8x2 alak√∫ numpy tensorral √°br√°zolni. Ha bin-ek diszkretiz√°l√°s√°t haszn√°ljuk, az √°llapotvektor m√©rete is ismert, √≠gy ugyanazt a megk√∂zel√≠t√©st alkalmazhatjuk, √©s az √°llapotot egy 20x20x10x10x2 alak√∫ t√∂mbbel √°br√°zolhatjuk (itt 2 az akci√≥t√©r dimenzi√≥ja, az els≈ë dimenzi√≥k pedig az egyes param√©terekhez kiv√°lasztott bin-ek sz√°m√°t jel√∂lik a megfigyel√©si t√©rben).

Azonban n√©ha a megfigyel√©si t√©r pontos dimenzi√≥i nem ismertek. A `discretize` f√ºggv√©ny eset√©ben soha nem lehet√ºnk biztosak abban, hogy az √°llapot bizonyos hat√°rokon bel√ºl marad, mivel n√©h√°ny eredeti √©rt√©k nincs korl√°tozva. Ez√©rt kiss√© elt√©r≈ë megk√∂zel√≠t√©st alkalmazunk, √©s a Q-t√°bl√°t sz√≥t√°rk√©nt √°br√°zoljuk.

1. Haszn√°ljuk az *(√°llapot, akci√≥)* p√°rost a sz√≥t√°r kulcsak√©nt, √©s az √©rt√©k a Q-t√°bla bejegyz√©s√©nek √©rt√©k√©t jel√∂li. (k√≥dblokk 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Itt defini√°lunk egy `qvalues()` f√ºggv√©nyt is, amely visszaadja a Q-t√°bla √©rt√©keinek list√°j√°t egy adott √°llapothoz, amely az √∂sszes lehets√©ges akci√≥hoz tartozik. Ha a bejegyz√©s nem szerepel a Q-t√°bl√°ban, alap√©rtelmez√©s szerint 0-t adunk vissza.

## Kezdj√ºk a Q-Learninget

Most k√©szen √°llunk arra, hogy megtan√≠tsuk P√©tert egyens√∫lyozni!

1. El≈ësz√∂r √°ll√≠tsunk be n√©h√°ny hiperparam√©tert: (k√≥dblokk 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Itt az `alpha` a **tanul√°si r√°ta**, amely meghat√°rozza, hogy milyen m√©rt√©kben kell m√≥dos√≠tanunk a Q-t√°bla aktu√°lis √©rt√©keit minden l√©p√©sn√©l. Az el≈ëz≈ë leck√©ben 1-r≈ël indultunk, majd a tanul√°s sor√°n cs√∂kkentett√ºk az `alpha` √©rt√©k√©t. Ebben a p√©ld√°ban egyszer≈±s√©g kedv√©√©rt √°lland√≥ √©rt√©ken tartjuk, de k√©s≈ëbb k√≠s√©rletezhetsz az `alpha` √©rt√©kek m√≥dos√≠t√°s√°val.

    A `gamma` a **diszkontfaktor**, amely megmutatja, hogy milyen m√©rt√©kben kell el≈ënyben r√©szes√≠ten√ºnk a j√∂v≈ëbeli jutalmat a jelenlegi jutalommal szemben.

    Az `epsilon` az **explor√°ci√≥/hasznos√≠t√°s t√©nyez≈ë**, amely meghat√°rozza, hogy az explor√°ci√≥t vagy a hasznos√≠t√°st kell-e el≈ënyben r√©szes√≠ten√ºnk. Algoritmusunkban az esetek `epsilon` sz√°zal√©k√°ban a k√∂vetkez≈ë akci√≥t a Q-t√°bla √©rt√©kei alapj√°n v√°lasztjuk ki, a fennmarad√≥ esetekben pedig v√©letlenszer≈± akci√≥t hajtunk v√©gre. Ez lehet≈ëv√© teszi sz√°munkra, hogy felfedezz√ºk a keres√©si t√©r olyan ter√ºleteit, amelyeket kor√°bban nem l√°ttunk.

    ‚úÖ Az egyens√∫lyoz√°s szempontj√°b√≥l ‚Äì v√©letlenszer≈± akci√≥ v√°laszt√°sa (explor√°ci√≥) olyan, mintha v√©letlenszer≈± √ºt√©st kapn√°nk rossz ir√°nyba, √©s a r√∫dnak meg kell tanulnia, hogyan √°ll√≠tsa vissza az egyens√∫lyt ezekb≈ël a "hib√°kb√≥l".

### Az algoritmus fejleszt√©se

K√©t fejleszt√©st is v√©gezhet√ºnk az el≈ëz≈ë lecke algoritmus√°n:

- **√Åtlagos kumulat√≠v jutalom kisz√°m√≠t√°sa** t√∂bb szimul√°ci√≥ sor√°n. 5000 iter√°ci√≥nk√©nt kinyomtatjuk az el≈ërehalad√°st, √©s az √°tlagos kumulat√≠v jutalmat sz√°m√≠tjuk ki ezen id≈ëszak alatt. Ez azt jelenti, hogy ha t√∂bb mint 195 pontot √©r√ºnk el, akkor a probl√©m√°t megoldottnak tekinthetj√ºk, m√©g a sz√ºks√©gesn√©l is jobb min≈ës√©gben.

- **Maxim√°lis √°tlagos kumulat√≠v eredm√©ny kisz√°m√≠t√°sa**, `Qmax`, √©s elmentj√ºk a Q-t√°bl√°t, amely ehhez az eredm√©nyhez tartozik. Amikor futtatod a tanul√°st, √©szre fogod venni, hogy n√©ha az √°tlagos kumulat√≠v eredm√©ny cs√∂kkenni kezd, √©s meg akarjuk ≈ërizni a Q-t√°bla azon √©rt√©keit, amelyek a legjobb modellhez tartoznak a tanul√°s sor√°n.

1. Gy≈±jtsd √∂ssze az √∂sszes kumulat√≠v jutalmat minden szimul√°ci√≥n√°l a `rewards` vektorban tov√°bbi √°br√°zol√°shoz. (k√≥dblokk 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

A k√∂vetkez≈ëket figyelheted meg az eredm√©nyekb≈ël:

- **K√∂zel a c√©lhoz**. Nagyon k√∂zel vagyunk ahhoz, hogy el√©rj√ºk a 195 kumulat√≠v jutalmat 100+ egym√°st k√∂vet≈ë szimul√°ci√≥ futtat√°sa sor√°n, vagy ak√°r el is √©rhett√ºk! M√©g ha kisebb sz√°mokat kapunk is, nem tudhatjuk biztosan, mert 5000 futtat√°s √°tlag√°t sz√°m√≠tjuk, √©s a hivatalos krit√©riumhoz csak 100 futtat√°s sz√ºks√©ges.

- **A jutalom cs√∂kkenni kezd**. N√©ha a jutalom cs√∂kkenni kezd, ami azt jelenti, hogy "t√∂nkretehetj√ºk" a Q-t√°bl√°ban m√°r megtanult √©rt√©keket olyanokkal, amelyek rosszabb√° teszik a helyzetet.

Ez a megfigyel√©s egy√©rtelm≈±bben l√°that√≥, ha √°br√°zoljuk a tanul√°si folyamatot.

## A tanul√°si folyamat √°br√°zol√°sa

A tanul√°s sor√°n az iter√°ci√≥k sor√°n √∂sszegy≈±jt√∂tt√ºk a kumulat√≠v jutalom √©rt√©k√©t a `rewards` vektorba. √çgy n√©z ki, amikor √°br√°zoljuk az iter√°ci√≥k sz√°ma ellen√©ben:

```python
plt.plot(rewards)
```

![nyers el≈ërehalad√°s](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Ebb≈ël a grafikonb√≥l nem lehet semmit meg√°llap√≠tani, mivel a sztochasztikus tanul√°si folyamat term√©szet√©b≈ël ad√≥d√≥an a tanul√°si szakaszok hossza nagyon v√°ltoz√≥. Hogy √©rtelmesebb√© tegy√ºk ezt a grafikont, kisz√°m√≠thatjuk a **fut√≥ √°tlagot** egy sor k√≠s√©rlet sor√°n, mondjuk 100. Ezt k√©nyelmesen elv√©gezhetj√ºk az `np.convolve` seg√≠ts√©g√©vel: (k√≥dblokk 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![tanul√°si folyamat](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Hiperparam√©terek v√°ltoztat√°sa

A tanul√°s stabilabb√° t√©tele √©rdek√©ben √©rdemes
> **Feladat 1**: J√°tssz a hiperparam√©terek √©rt√©keivel, √©s n√©zd meg, hogy el tudsz-e √©rni magasabb kumulat√≠v jutalmat. Siker√ºl 195 f√∂l√© jutnod?
> **2. feladat**: Ahhoz, hogy hivatalosan megoldjuk a probl√©m√°t, 195-√∂s √°tlagos jutalmat kell el√©rni 100 egym√°st k√∂vet≈ë futtat√°s sor√°n. M√©rd ezt az edz√©s alatt, √©s gy≈ëz≈ëdj meg r√≥la, hogy hivatalosan megoldottad a probl√©m√°t!

## Az eredm√©ny megtekint√©se m≈±k√∂d√©s k√∂zben

√ârdekes lenne l√°tni, hogyan viselkedik a betan√≠tott modell. Futtassuk le a szimul√°ci√≥t, √©s k√∂vess√ºk ugyanazt az akci√≥v√°laszt√°si strat√©gi√°t, mint az edz√©s sor√°n, a Q-t√°bl√°ban l√©v≈ë val√≥sz√≠n≈±s√©gi eloszl√°s alapj√°n mint√°zva: (k√≥d blokk 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Valami ilyesmit kellene l√°tnod:

![egy egyens√∫lyoz√≥ cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄKih√≠v√°s

> **3. feladat**: Itt a Q-t√°bla v√©gs≈ë verzi√≥j√°t haszn√°ltuk, ami lehet, hogy nem a legjobb. Ne feledd, hogy a legjobban teljes√≠t≈ë Q-t√°bl√°t elmentett√ºk a `Qbest` v√°ltoz√≥ba! Pr√≥b√°ld ki ugyanazt a p√©ld√°t a legjobban teljes√≠t≈ë Q-t√°bl√°val, √∫gy, hogy √°tm√°solod a `Qbest`-et a `Q`-ba, √©s n√©zd meg, √©szreveszel-e k√ºl√∂nbs√©get.

> **4. feladat**: Itt nem a legjobb akci√≥t v√°lasztottuk minden l√©p√©sn√©l, hanem a megfelel≈ë val√≥sz√≠n≈±s√©gi eloszl√°s alapj√°n mint√°ztunk. √âsszer≈±bb lenne mindig a legjobb akci√≥t v√°lasztani, amelynek a legmagasabb Q-t√°bla √©rt√©ke van? Ezt megteheted az `np.argmax` f√ºggv√©ny haszn√°lat√°val, amely megadja a legmagasabb Q-t√°bla √©rt√©khez tartoz√≥ akci√≥ sz√°m√°t. Val√≥s√≠tsd meg ezt a strat√©gi√°t, √©s n√©zd meg, jav√≠tja-e az egyens√∫lyoz√°st.

## [Ut√≥-el≈ëad√°s kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## Feladat
[Edz egy Mountain Car modellt](assignment.md)

## √ñsszegz√©s

Mostanra megtanultuk, hogyan lehet √ºgyn√∂k√∂ket betan√≠tani arra, hogy j√≥ eredm√©nyeket √©rjenek el puszt√°n az√°ltal, hogy egy jutalomf√ºggv√©nyt biztos√≠tunk sz√°mukra, amely meghat√°rozza a j√°t√©k k√≠v√°nt √°llapot√°t, √©s lehet≈ës√©get adunk nekik arra, hogy intelligensen felt√©rk√©pezz√©k a keres√©si teret. Sikeresen alkalmaztuk a Q-Learning algoritmust diszkr√©t √©s folytonos k√∂rnyezetek eset√©ben, de diszkr√©t akci√≥kkal.

Fontos tanulm√°nyozni azokat a helyzeteket is, ahol az akci√≥√°llapot szint√©n folytonos, √©s amikor a megfigyel√©si t√©r sokkal √∂sszetettebb, p√©ld√°ul az Atari j√°t√©k k√©perny≈ëj√©nek k√©pe. Ezekben a probl√©m√°kban gyakran er≈ësebb g√©pi tanul√°si technik√°kra, p√©ld√°ul neur√°lis h√°l√≥kra van sz√ºks√©g ahhoz, hogy j√≥ eredm√©nyeket √©rj√ºnk el. Ezek a fejlettebb t√©m√°k a k√∂vetkez≈ë, halad√≥ AI kurzusunk t√°rgy√°t k√©pezik.

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.