<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T16:38:33+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "hu"
}
-->
# Bevezet√©s a meger≈ës√≠t√©ses tanul√°sba √©s a Q-tanul√°sba

![A g√©pi tanul√°s meger≈ës√≠t√©s√©nek √∂sszefoglal√°sa egy sketchnote-ban](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote: [Tomomi Imura](https://www.twitter.com/girlie_mac)

A meger≈ës√≠t√©ses tanul√°s h√°rom fontos fogalmat foglal mag√°ban: az √ºgyn√∂k√∂t, az √°llapotokat √©s az √°llapotonk√©nti cselekv√©sek halmaz√°t. Egy adott √°llapotban v√©grehajtott cselekv√©s√©rt az √ºgyn√∂k jutalmat kap. K√©pzelj√ºk el √∫jra a Super Mario sz√°m√≠t√≥g√©pes j√°t√©kot. Te vagy Mario, egy p√°ly√°n √°llsz egy szakad√©k sz√©l√©n. F√∂l√∂tted egy √©rme van. Az, hogy te Mario vagy, egy adott p√°ly√°n, egy adott poz√≠ci√≥ban... ez az √°llapotod. Ha egy l√©p√©st jobbra l√©psz (egy cselekv√©s), leesel a szakad√©kba, √©s alacsony pontsz√°mot kapsz. Ha viszont megnyomod az ugr√°s gombot, pontot szerzel, √©s √©letben maradsz. Ez egy pozit√≠v kimenetel, ami√©rt pozit√≠v pontsz√°mot kell kapnod.

A meger≈ës√≠t√©ses tanul√°s √©s egy szimul√°tor (a j√°t√©k) seg√≠ts√©g√©vel megtanulhatod, hogyan j√°tszd a j√°t√©kot √∫gy, hogy maximaliz√°ld a jutalmat, vagyis √©letben maradj, √©s min√©l t√∂bb pontot szerezz.

[![Bevezet√©s a meger≈ës√≠t√©ses tanul√°sba](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Kattints a fenti k√©pre, hogy meghallgasd Dmitry el≈ëad√°s√°t a meger≈ës√≠t√©ses tanul√°sr√≥l.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## El≈ëfelt√©telek √©s be√°ll√≠t√°s

Ebben a leck√©ben Pythonban fogunk k√≥dot kipr√≥b√°lni. K√©pesnek kell lenned futtatni a Jupyter Notebook k√≥dj√°t, ak√°r a saj√°t sz√°m√≠t√≥g√©peden, ak√°r a felh≈ëben.

Megnyithatod [a lecke notebookj√°t](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb), √©s v√©gigmehetsz a leck√©n, hogy fel√©p√≠tsd a p√©ld√°t.

> **Megjegyz√©s:** Ha a k√≥dot a felh≈ëb≈ël nyitod meg, le kell t√∂ltened az [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) f√°jlt is, amelyet a notebook k√≥d haszn√°l. Helyezd el ugyanabban a k√∂nyvt√°rban, ahol a notebook tal√°lhat√≥.

## Bevezet√©s

Ebben a leck√©ben **[P√©ter √©s a farkas](https://hu.wikipedia.org/wiki/P%C3%A9ter_%C3%A9s_a_farkas)** vil√°g√°t fogjuk felfedezni, amelyet egy orosz zeneszerz≈ë, [Szergej Prokofjev](https://hu.wikipedia.org/wiki/Szergej_Prokofjev) zen√©s mes√©je ihletett. A **meger≈ës√≠t√©ses tanul√°s** seg√≠ts√©g√©vel P√©tert ir√°ny√≠tjuk, hogy felfedezze a k√∂rnyezet√©t, √≠zletes alm√°kat gy≈±jts√∂n, √©s elker√ºlje a farkassal val√≥ tal√°lkoz√°st.

A **meger≈ës√≠t√©ses tanul√°s** (RL) egy olyan tanul√°si technika, amely lehet≈ëv√© teszi sz√°munkra, hogy egy **√ºgyn√∂k** optim√°lis viselked√©s√©t tanuljuk meg egy adott **k√∂rnyezetben**, sz√°mos k√≠s√©rlet lefuttat√°s√°val. Az √ºgyn√∂knek ebben a k√∂rnyezetben van egy **c√©lja**, amelyet egy **jutalomf√ºggv√©ny** hat√°roz meg.

## A k√∂rnyezet

Egyszer≈±s√©g kedv√©√©rt tekints√ºk P√©ter vil√°g√°t egy `sz√©less√©g` x `magass√°g` m√©ret≈± n√©gyzet alak√∫ t√°bl√°nak, p√©ld√°ul √≠gy:

![P√©ter k√∂rnyezete](../../../../8-Reinforcement/1-QLearning/images/environment.png)

A t√°bla minden cell√°ja lehet:

* **talaj**, amin P√©ter √©s m√°s l√©nyek j√°rhatnak.
* **v√≠z**, amin nyilv√°nval√≥an nem lehet j√°rni.
* **fa** vagy **f≈±**, ahol pihenni lehet.
* egy **alma**, amit P√©ter √∂r√∂mmel tal√°lna meg, hogy t√°pl√°lkozzon.
* egy **farkas**, ami vesz√©lyes, √©s el kell ker√ºlni.

Van egy k√ºl√∂n Python modul, az [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), amely tartalmazza a k√≥dot a k√∂rnyezettel val√≥ munk√°hoz. Mivel ez a k√≥d nem fontos a fogalmak meg√©rt√©s√©hez, import√°ljuk a modult, √©s haszn√°ljuk a minta t√°bla l√©trehoz√°s√°hoz (k√≥dblokk 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Ez a k√≥d egy, a fentiekhez hasonl√≥ k√∂rnyezetet fog megjelen√≠teni.

## Cselekv√©sek √©s strat√©gia

P√©ld√°nkban P√©ter c√©lja az alma megtal√°l√°sa, mik√∂zben elker√ºli a farkast √©s m√°s akad√°lyokat. Ehhez l√©nyeg√©ben addig s√©t√°lhat, am√≠g meg nem tal√°lja az alm√°t.

Ez√©rt b√°rmely poz√≠ci√≥ban v√°laszthat a k√∂vetkez≈ë cselekv√©sek k√∂z√ºl: fel, le, balra √©s jobbra.

Ezeket a cselekv√©seket egy sz√≥t√°rk√©nt defini√°ljuk, √©s a megfelel≈ë koordin√°tav√°ltoz√°sokhoz rendelj√ºk. P√©ld√°ul a jobbra mozg√°s (`R`) egy `(1,0)` p√°rnak felel meg. (k√≥dblokk 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

√ñsszefoglalva, a forgat√≥k√∂nyv strat√©gi√°ja √©s c√©lja a k√∂vetkez≈ë:

- **A strat√©gia**, az √ºgyn√∂k√ºnk (P√©ter) strat√©gi√°j√°t egy √∫gynevezett **politika** hat√°rozza meg. A politika egy olyan f√ºggv√©ny, amely b√°rmely adott √°llapotban visszaadja a cselekv√©st. Eset√ºnkben a probl√©ma √°llapot√°t a t√°bla √©s a j√°t√©kos aktu√°lis poz√≠ci√≥ja k√©pviseli.

- **A c√©l**, a meger≈ës√≠t√©ses tanul√°s c√©lja egy j√≥ politika megtanul√°sa, amely lehet≈ëv√© teszi a probl√©ma hat√©kony megold√°s√°t. Az alapk√©nt vegy√ºk figyelembe a legegyszer≈±bb politik√°t, az √∫gynevezett **v√©letlenszer≈± s√©t√°t**.

## V√©letlenszer≈± s√©ta

El≈ësz√∂r oldjuk meg a probl√©m√°t egy v√©letlenszer≈± s√©ta strat√©gi√°val. A v√©letlenszer≈± s√©ta sor√°n v√©letlenszer≈±en v√°lasztjuk ki a k√∂vetkez≈ë cselekv√©st az enged√©lyezett cselekv√©sek k√∂z√ºl, am√≠g el nem √©rj√ºk az alm√°t (k√≥dblokk 3).

1. Val√≥s√≠tsd meg a v√©letlenszer≈± s√©t√°t az al√°bbi k√≥ddal:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    A `walk` h√≠v√°s visszaadja a megfelel≈ë √∫tvonal hossz√°t, amely futtat√°sonk√©nt v√°ltozhat.

1. Futtasd le a s√©ta k√≠s√©rletet t√∂bbsz√∂r (p√©ld√°ul 100-szor), √©s nyomtasd ki az eredm√©nyeket (k√≥dblokk 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Figyeld meg, hogy az √∫tvonal √°tlagos hossza k√∂r√ºlbel√ºl 30-40 l√©p√©s, ami el√©g sok, tekintve, hogy az √°tlagos t√°vols√°g a legk√∂zelebbi alm√°ig k√∂r√ºlbel√ºl 5-6 l√©p√©s.

    Azt is l√°thatod, hogyan mozog P√©ter a v√©letlenszer≈± s√©ta sor√°n:

    ![P√©ter v√©letlenszer≈± s√©t√°ja](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Jutalomf√ºggv√©ny

Ahhoz, hogy a politik√°nk intelligensebb legyen, meg kell √©rten√ºnk, mely l√©p√©sek "jobbak", mint m√°sok. Ehhez meg kell hat√°roznunk a c√©lunkat.

A c√©l egy **jutalomf√ºggv√©ny** seg√≠ts√©g√©vel hat√°rozhat√≥ meg, amely minden √°llapothoz visszaad egy pontsz√°mot. Min√©l magasabb a sz√°m, ann√°l jobb a jutalomf√ºggv√©ny. (k√≥dblokk 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

A jutalomf√ºggv√©nyek √©rdekess√©ge, hogy a legt√∂bb esetben *csak a j√°t√©k v√©g√©n kapunk jelent≈ës jutalmat*. Ez azt jelenti, hogy az algoritmusunknak valahogy eml√©keznie kell a "j√≥" l√©p√©sekre, amelyek pozit√≠v jutalomhoz vezettek a v√©g√©n, √©s n√∂velnie kell azok fontoss√°g√°t. Hasonl√≥k√©ppen, minden olyan l√©p√©st, amely rossz eredm√©nyhez vezet, el kell ker√ºlni.

## Q-tanul√°s

Az algoritmus, amelyet itt t√°rgyalunk, a **Q-tanul√°s**. Ebben az algoritmusban a politika egy **Q-t√°bla** nev≈± f√ºggv√©nnyel (vagy adatszerkezettel) van meghat√°rozva. Ez r√∂gz√≠ti az egyes cselekv√©sek "j√≥s√°g√°t" egy adott √°llapotban.

Q-t√°bl√°nak h√≠vj√°k, mert gyakran k√©nyelmes t√°bl√°zatk√©nt vagy t√∂bbdimenzi√≥s t√∂mbk√©nt √°br√°zolni. Mivel a t√°bl√°nk m√©rete `sz√©less√©g` x `magass√°g`, a Q-t√°bl√°t egy numpy t√∂mbk√©nt √°br√°zolhatjuk, amelynek alakja `sz√©less√©g` x `magass√°g` x `len(cselekv√©sek)`: (k√≥dblokk 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Figyeld meg, hogy a Q-t√°bla √∂sszes √©rt√©k√©t egyenl≈ë √©rt√©kkel inicializ√°ljuk, eset√ºnkben 0,25-tel. Ez megfelel a "v√©letlenszer≈± s√©ta" politik√°nak, mert minden l√©p√©s minden √°llapotban egyform√°n j√≥. A Q-t√°bl√°t √°tadhatjuk a `plot` f√ºggv√©nynek, hogy vizualiz√°ljuk a t√°bl√°t a t√°bl√°n: `m.plot(Q)`.

![P√©ter k√∂rnyezete](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

A cell√°k k√∂zep√©n egy "ny√≠l" l√°that√≥, amely a mozg√°s prefer√°lt ir√°ny√°t jelzi. Mivel minden ir√°ny egyenl≈ë, egy pont jelenik meg.

Most el kell ind√≠tanunk a szimul√°ci√≥t, felfedezn√ºnk a k√∂rnyezetet, √©s meg kell tanulnunk a Q-t√°bla √©rt√©keinek jobb eloszl√°s√°t, amely lehet≈ëv√© teszi sz√°munkra, hogy sokkal gyorsabban megtal√°ljuk az utat az alm√°hoz.

## A Q-tanul√°s l√©nyege: Bellman-egyenlet

Amint elkezd√ºnk mozogni, minden cselekv√©shez tartozik egy megfelel≈ë jutalom, azaz elm√©letileg kiv√°laszthatjuk a k√∂vetkez≈ë cselekv√©st a legmagasabb azonnali jutalom alapj√°n. Azonban a legt√∂bb √°llapotban a l√©p√©s nem √©ri el a c√©lunkat, hogy el√©rj√ºk az alm√°t, √≠gy nem tudjuk azonnal eld√∂nteni, melyik ir√°ny a jobb.

> Ne feledd, hogy nem az azonnali eredm√©ny sz√°m√≠t, hanem a v√©gs≈ë eredm√©ny, amelyet a szimul√°ci√≥ v√©g√©n kapunk.

Ahhoz, hogy figyelembe vegy√ºk ezt a k√©sleltetett jutalmat, a **[dinamikus programoz√°s](https://hu.wikipedia.org/wiki/Dinamikus_programoz%C3%A1s)** elveit kell alkalmaznunk, amelyek lehet≈ëv√© teszik, hogy rekurz√≠van gondolkodjunk a probl√©m√°nkr√≥l.

Tegy√ºk fel, hogy most az *s* √°llapotban vagyunk, √©s a k√∂vetkez≈ë √°llapotba, *s'*-be akarunk l√©pni. Ezzel megkapjuk az azonnali jutalmat, *r(s,a)*, amelyet a jutalomf√ºggv√©ny hat√°roz meg, plusz n√©mi j√∂v≈ëbeli jutalmat. Ha felt√©telezz√ºk, hogy a Q-t√°bl√°nk helyesen t√ºkr√∂zi az egyes cselekv√©sek "vonzerej√©t", akkor az *s'* √°llapotban egy olyan *a* cselekv√©st v√°lasztunk, amely a *Q(s',a')* maxim√°lis √©rt√©k√©nek felel meg. √çgy az *s* √°llapotban el√©rhet≈ë legjobb j√∂v≈ëbeli jutalom a k√∂vetkez≈ëk√©ppen lesz meghat√°rozva: `max`

## A szab√°ly ellen≈ërz√©se

Mivel a Q-t√°bla felsorolja az egyes √°llapotokban v√©grehajthat√≥ cselekv√©sek "vonzerej√©t", k√∂nnyen haszn√°lhat√≥ hat√©kony navig√°ci√≥ meghat√°roz√°s√°ra a vil√°gunkban. A legegyszer≈±bb esetben kiv√°laszthatjuk azt a cselekv√©st, amely a legmagasabb Q-t√°bla √©rt√©khez tartozik: (k√≥d blokk 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Ha t√∂bbsz√∂r kipr√≥b√°lod a fenti k√≥dot, √©szreveheted, hogy n√©ha "elakad", √©s a jegyzetf√ºzetben a STOP gombot kell megnyomnod a megszak√≠t√°shoz. Ez az√©rt t√∂rt√©nik, mert el≈ëfordulhatnak olyan helyzetek, amikor k√©t √°llapot "mutat" egym√°sra az optim√°lis Q-√©rt√©kek alapj√°n, ilyenkor az √ºgyn√∂k v√©gtelen√ºl mozoghat ezek k√∂z√∂tt az √°llapotok k√∂z√∂tt.

## üöÄKih√≠v√°s

> **Feladat 1:** M√≥dos√≠tsd a `walk` f√ºggv√©nyt √∫gy, hogy korl√°tozza az √∫t maxim√°lis hossz√°t egy bizonyos l√©p√©ssz√°mra (p√©ld√°ul 100), √©s figyeld meg, hogy a fenti k√≥d id≈ënk√©nt visszaadja ezt az √©rt√©ket.

> **Feladat 2:** M√≥dos√≠tsd a `walk` f√ºggv√©nyt √∫gy, hogy ne t√©rjen vissza olyan helyekre, ahol kor√°bban m√°r j√°rt. Ez megakad√°lyozza, hogy a `walk` ciklusba ker√ºlj√∂n, azonban az √ºgyn√∂k m√©g mindig "csapd√°ba" eshet egy olyan helyen, ahonnan nem tud kijutni.

## Navig√°ci√≥

Egy jobb navig√°ci√≥s szab√°ly az lenne, amit a tanul√°s sor√°n haszn√°ltunk, amely kombin√°lja a kihaszn√°l√°st √©s a felfedez√©st. Ebben a szab√°lyban minden cselekv√©st egy bizonyos val√≥sz√≠n≈±s√©ggel v√°lasztunk ki, amely ar√°nyos a Q-t√°bla √©rt√©keivel. Ez a strat√©gia m√©g mindig eredm√©nyezheti, hogy az √ºgyn√∂k visszat√©r egy m√°r felfedezett helyre, de ahogy az al√°bbi k√≥db√≥l l√°that√≥, ez nagyon r√∂vid √°tlagos √∫tvonalat eredm√©nyez a k√≠v√°nt helyre (ne feledd, hogy a `print_statistics` 100-szor futtatja a szimul√°ci√≥t): (k√≥d blokk 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

A k√≥d futtat√°sa ut√°n sokkal r√∂videbb √°tlagos √∫tvonalhosszt kell kapnod, 3-6 l√©p√©s k√∂r√ºl.

## A tanul√°si folyamat vizsg√°lata

Ahogy eml√≠tett√ºk, a tanul√°si folyamat egyens√∫lyoz√°s a felfedez√©s √©s a probl√©mat√©r szerkezet√©r≈ël szerzett tud√°s kihaszn√°l√°sa k√∂z√∂tt. L√°ttuk, hogy a tanul√°s eredm√©nyei (az √ºgyn√∂k k√©pess√©ge, hogy r√∂vid utat tal√°ljon a c√©lhoz) javultak, de az is √©rdekes, hogy megfigyelj√ºk, hogyan viselkedik az √°tlagos √∫tvonalhossz a tanul√°si folyamat sor√°n:

## A tanuls√°gok √∂sszefoglal√°sa:

- **Az √°tlagos √∫tvonalhossz n√∂vekszik**. Amit itt l√°tunk, az az, hogy eleinte az √°tlagos √∫tvonalhossz n√∂vekszik. Ez val√≥sz√≠n≈±leg az√©rt van, mert amikor semmit sem tudunk a k√∂rnyezetr≈ël, hajlamosak vagyunk rossz √°llapotokba, v√≠zbe vagy farkasok k√∂z√© ker√ºlni. Ahogy t√∂bbet tanulunk √©s elkezdj√ºk haszn√°lni ezt a tud√°st, hosszabb ideig tudjuk felfedezni a k√∂rnyezetet, de m√©g mindig nem tudjuk pontosan, hol vannak az alm√°k.

- **Az √∫tvonalhossz cs√∂kken, ahogy t√∂bbet tanulunk**. Miut√°n eleget tanultunk, az √ºgyn√∂k sz√°m√°ra k√∂nnyebb√© v√°lik a c√©l el√©r√©se, √©s az √∫tvonalhossz cs√∂kkenni kezd. Azonban m√©g mindig nyitottak vagyunk a felfedez√©sre, √≠gy gyakran elt√©r√ºnk az optim√°lis √∫tt√≥l, √©s √∫j lehet≈ës√©geket fedez√ºnk fel, ami hosszabb utat eredm√©nyez.

- **A hossz hirtelen megn≈ë**. Amit ezen a grafikonon m√©g megfigyelhet√ºnk, az az, hogy egy ponton az √∫tvonalhossz hirtelen megn≈ëtt. Ez a folyamat sztochasztikus term√©szet√©t jelzi, √©s azt, hogy bizonyos pontokon "elronthatjuk" a Q-t√°bla egy√ºtthat√≥it azzal, hogy √∫j √©rt√©kekkel fel√ºl√≠rjuk ≈ëket. Ezt ide√°lisan minimaliz√°lni kellene a tanul√°si r√°ta cs√∂kkent√©s√©vel (p√©ld√°ul a tanul√°s v√©g√©hez k√∂zeledve csak kis √©rt√©kkel m√≥dos√≠tjuk a Q-t√°bla √©rt√©keit).

√ñsszess√©g√©ben fontos megjegyezni, hogy a tanul√°si folyamat sikere √©s min≈ës√©ge jelent≈ësen f√ºgg a param√©terekt≈ël, mint p√©ld√°ul a tanul√°si r√°ta, a tanul√°si r√°ta cs√∂kken√©se √©s a diszkontfaktor. Ezeket gyakran **hiperparam√©tereknek** nevezik, hogy megk√ºl√∂nb√∂ztess√©k ≈ëket a **param√©terekt≈ël**, amelyeket a tanul√°s sor√°n optimaliz√°lunk (p√©ld√°ul a Q-t√°bla egy√ºtthat√≥i). A legjobb hiperparam√©ter √©rt√©kek megtal√°l√°s√°nak folyamat√°t **hiperparam√©ter optimaliz√°ci√≥nak** nevezz√ºk, √©s ez egy k√ºl√∂n t√©m√°t √©rdemel.

## [Ut√≥-el≈ëad√°s kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## Feladat 
[Egy re√°lisabb vil√°g](assignment.md)

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.