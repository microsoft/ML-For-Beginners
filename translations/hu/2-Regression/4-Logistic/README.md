<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T15:15:21+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "hu"
}
-->
# Logisztikus regresszi√≥ kateg√≥ri√°k el≈ërejelz√©s√©re

![Logisztikus vs. line√°ris regresszi√≥ infografika](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ez a lecke el√©rhet≈ë R-ben is!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Bevezet√©s

Ebben az utols√≥ leck√©ben a regresszi√≥r√≥l, amely az egyik alapvet≈ë _klasszikus_ g√©pi tanul√°si technika, megvizsg√°ljuk a logisztikus regresszi√≥t. Ezt a technik√°t arra haszn√°lhatjuk, hogy mint√°zatokat fedezz√ºnk fel bin√°ris kateg√≥ri√°k el≈ërejelz√©s√©re. Ez a cukorka csokol√°d√© vagy sem? Ez a betegs√©g fert≈ëz≈ë vagy sem? Ez az √ºgyf√©l v√°lasztja-e ezt a term√©ket vagy sem?

Ebben a leck√©ben megtanulod:

- Egy √∫j k√∂nyvt√°r haszn√°lat√°t az adatok vizualiz√°l√°s√°hoz
- Logisztikus regresszi√≥ technik√°it

‚úÖ M√©ly√≠tsd el a logisztikus regresszi√≥val kapcsolatos tud√°sodat ebben a [Learn modulban](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## El≈ëfelt√©tel

A t√∂kadatokkal val√≥ munka sor√°n m√°r el√©g j√≥l megismerkedt√ºnk ahhoz, hogy felismerj√ºk, van egy bin√°ris kateg√≥ria, amellyel dolgozhatunk: `Color`.

√âp√≠ts√ºnk egy logisztikus regresszi√≥s modellt, hogy el≈ëre jelezz√ºk, adott v√°ltoz√≥k alapj√°n _milyen sz√≠n≈± lesz egy adott t√∂k_ (narancs üéÉ vagy feh√©r üëª).

> Mi√©rt besz√©l√ºnk bin√°ris oszt√°lyoz√°sr√≥l egy regresszi√≥val kapcsolatos leck√©ben? Csak nyelvi k√©nyelmi okokb√≥l, mivel a logisztikus regresszi√≥ [val√≥j√°ban egy oszt√°lyoz√°si m√≥dszer](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), b√°r line√°ris alap√∫. Az adatok oszt√°lyoz√°s√°nak m√°s m√≥djair√≥l a k√∂vetkez≈ë leckecsoportban tanulhatsz.

## Fogalmazzuk meg a k√©rd√©st

A mi c√©ljaink √©rdek√©ben ezt bin√°risk√©nt fogalmazzuk meg: 'Feh√©r' vagy 'Nem feh√©r'. Az adatainkban van egy 'cs√≠kos' kateg√≥ria is, de kev√©s el≈ëfordul√°sa van, √≠gy nem fogjuk haszn√°lni. Ez am√∫gy is elt≈±nik, amikor elt√°vol√≠tjuk az adat√°llom√°nyb√≥l a null √©rt√©keket.

> üéÉ √ârdekess√©g: n√©ha a feh√©r t√∂k√∂ket 'szellem' t√∂k√∂knek h√≠vjuk. Nem t√∫l k√∂nny≈± ≈ëket faragni, ez√©rt nem olyan n√©pszer≈±ek, mint a narancss√°rg√°k, de nagyon j√≥l n√©znek ki! √çgy a k√©rd√©s√ºnket √∫gy is megfogalmazhatn√°nk: 'Szellem' vagy 'Nem szellem'. üëª

## A logisztikus regresszi√≥r√≥l

A logisztikus regresszi√≥ n√©h√°ny fontos szempontb√≥l k√ºl√∂nb√∂zik a kor√°bban tanult line√°ris regresszi√≥t√≥l.

[![ML kezd≈ëknek - A logisztikus regresszi√≥ meg√©rt√©se g√©pi tanul√°si oszt√°lyoz√°shoz](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML kezd≈ëknek - A logisztikus regresszi√≥ meg√©rt√©se g√©pi tanul√°si oszt√°lyoz√°shoz")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a logisztikus regresszi√≥r√≥l.

### Bin√°ris oszt√°lyoz√°s

A logisztikus regresszi√≥ nem k√≠n√°lja ugyanazokat a funkci√≥kat, mint a line√°ris regresszi√≥. Az el≈ëbbi bin√°ris kateg√≥ri√°r√≥l ("feh√©r vagy nem feh√©r") ad el≈ërejelz√©st, m√≠g az ut√≥bbi folyamatos √©rt√©keket k√©pes el≈ëre jelezni, p√©ld√°ul a t√∂k sz√°rmaz√°si helye √©s betakar√≠t√°si ideje alapj√°n, _mennyivel fog emelkedni az √°ra_.

![T√∂k oszt√°lyoz√°si modell](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika: [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Egy√©b oszt√°lyoz√°sok

A logisztikus regresszi√≥nak vannak m√°s t√≠pusai is, p√©ld√°ul multinomi√°lis √©s ordin√°lis:

- **Multinomi√°lis**, amely t√∂bb kateg√≥ri√°t foglal mag√°ban - "Narancs, Feh√©r √©s Cs√≠kos".
- **Ordin√°lis**, amely rendezett kateg√≥ri√°kat foglal mag√°ban, hasznos, ha logikusan szeretn√©nk rendezni az eredm√©nyeket, p√©ld√°ul a t√∂k√∂ket, amelyek egy v√©ges sz√°m√∫ m√©ret szerint vannak rendezve (mini, kicsi, k√∂zepes, nagy, XL, XXL).

![Multinomi√°lis vs ordin√°lis regresszi√≥](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### A v√°ltoz√≥knak NEM kell korrel√°lniuk

Eml√©kszel, hogy a line√°ris regresszi√≥ jobban m≈±k√∂d√∂tt, ha a v√°ltoz√≥k korrel√°ltak? A logisztikus regresszi√≥ ennek az ellenkez≈ëje - a v√°ltoz√≥knak nem kell egym√°shoz igazodniuk. Ez j√≥l m≈±k√∂dik az olyan adatokkal, amelyeknek viszonylag gyenge korrel√°ci√≥i vannak.

### Sok tiszta adatra van sz√ºks√©g

A logisztikus regresszi√≥ pontosabb eredm√©nyeket ad, ha t√∂bb adatot haszn√°lunk; a mi kis adat√°llom√°nyunk nem optim√°lis erre a feladatra, ez√©rt ezt tartsd szem el≈ëtt.

[![ML kezd≈ëknek - Adatok elemz√©se √©s el≈ëk√©sz√≠t√©se logisztikus regresszi√≥hoz](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML kezd≈ëknek - Adatok elemz√©se √©s el≈ëk√©sz√≠t√©se logisztikus regresszi√≥hoz")

‚úÖ Gondold √°t, milyen t√≠pus√∫ adatok alkalmasak j√≥l a logisztikus regresszi√≥hoz

## Gyakorlat - adatok tiszt√≠t√°sa

El≈ësz√∂r tiszt√≠tsd meg az adatokat egy kicsit, t√°vol√≠tsd el a null √©rt√©keket, √©s v√°lassz ki csak n√©h√°ny oszlopot:

1. Add hozz√° a k√∂vetkez≈ë k√≥dot:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Mindig megtekintheted az √∫j adatkeretedet:

    ```python
    pumpkins.info
    ```

### Vizualiz√°ci√≥ - kateg√≥ri√°lis diagram

Mostanra bet√∂lt√∂tted a [kezd≈ë notebookot](../../../../2-Regression/4-Logistic/notebook.ipynb) a t√∂kadatokkal, √©s megtiszt√≠tottad √∫gy, hogy megmaradjon egy adat√°llom√°ny n√©h√°ny v√°ltoz√≥val, bele√©rtve a `Color`-t. Vizualiz√°ljuk az adatkeretet a notebookban egy m√°sik k√∂nyvt√°r seg√≠ts√©g√©vel: [Seaborn](https://seaborn.pydata.org/index.html), amely a kor√°bban haszn√°lt Matplotlibre √©p√ºl.

A Seaborn n√©h√°ny remek m√≥dot k√≠n√°l az adatok vizualiz√°l√°s√°ra. P√©ld√°ul √∂sszehasonl√≠thatod az adatok eloszl√°s√°t a `Variety` √©s `Color` kateg√≥ri√°k szerint egy kateg√≥ri√°lis diagramon.

1. Hozz l√©tre egy ilyen diagramot a `catplot` f√ºggv√©ny haszn√°lat√°val, a t√∂kadatainkat (`pumpkins`) haszn√°lva, √©s sz√≠nk√≥dol√°st megadva az egyes t√∂kkateg√≥ri√°khoz (narancs vagy feh√©r):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Vizualiz√°lt adatok r√°csa](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Az adatok megfigyel√©s√©vel l√°thatod, hogyan kapcsol√≥dik a `Color` adat a `Variety`-hez.

    ‚úÖ Ezen kateg√≥ri√°lis diagram alapj√°n milyen √©rdekes vizsg√°latokat tudsz elk√©pzelni?

### Adatok el≈ëfeldolgoz√°sa: jellemz≈ëk √©s c√≠mk√©k k√≥dol√°sa

A t√∂kadataink minden oszlop√°ban sz√∂veges √©rt√©kek tal√°lhat√≥k. A kateg√≥ri√°lis adatokkal val√≥ munka intuit√≠v az emberek sz√°m√°ra, de nem a g√©pek sz√°m√°ra. A g√©pi tanul√°si algoritmusok j√≥l m≈±k√∂dnek sz√°mokkal. Ez√©rt a k√≥dol√°s nagyon fontos l√©p√©s az adatok el≈ëfeldolgoz√°si f√°zis√°ban, mivel lehet≈ëv√© teszi, hogy a kateg√≥ri√°lis adatokat numerikus adatokk√° alak√≠tsuk, an√©lk√ºl, hogy b√°rmilyen inform√°ci√≥t elvesz√≠ten√©nk. A j√≥ k√≥dol√°s j√≥ modell √©p√≠t√©s√©hez vezet.

A jellemz≈ëk k√≥dol√°s√°hoz k√©t f≈ë t√≠pus√∫ k√≥dol√≥ l√©tezik:

1. Ordin√°lis k√≥dol√≥: j√≥l illeszkedik az ordin√°lis v√°ltoz√≥khoz, amelyek kateg√≥ri√°lis v√°ltoz√≥k, ahol az adatok logikai sorrendet k√∂vetnek, mint p√©ld√°ul az `Item Size` oszlop az adat√°llom√°nyunkban. Olyan lek√©pez√©st hoz l√©tre, amelyben minden kateg√≥ri√°t egy sz√°m k√©pvisel, amely az oszlopban l√©v≈ë kateg√≥ria sorrendje.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kateg√≥ri√°lis k√≥dol√≥: j√≥l illeszkedik a nomin√°lis v√°ltoz√≥khoz, amelyek kateg√≥ri√°lis v√°ltoz√≥k, ahol az adatok nem k√∂vetnek logikai sorrendet, mint p√©ld√°ul az adat√°llom√°nyunkban az `Item Size`-t≈ël elt√©r≈ë √∂sszes jellemz≈ë. Ez egy one-hot k√≥dol√°s, ami azt jelenti, hogy minden kateg√≥ri√°t egy bin√°ris oszlop k√©pvisel: a k√≥dolt v√°ltoz√≥ √©rt√©ke 1, ha a t√∂k az adott `Variety`-hez tartozik, √©s 0, ha nem.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```

Ezut√°n a `ColumnTransformer`-t haszn√°ljuk, hogy t√∂bb k√≥dol√≥t egyetlen l√©p√©sben kombin√°ljunk, √©s alkalmazzuk ≈ëket a megfelel≈ë oszlopokra.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```

M√°sr√©szt a c√≠mke k√≥dol√°s√°hoz a scikit-learn `LabelEncoder` oszt√°ly√°t haszn√°ljuk, amely egy seg√©doszt√°ly, amely seg√≠t normaliz√°lni a c√≠mk√©ket √∫gy, hogy csak 0 √©s n_classes-1 k√∂z√∂tti √©rt√©keket tartalmazzanak (itt 0 √©s 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

Miut√°n k√≥doltuk a jellemz≈ëket √©s a c√≠mk√©t, egy √∫j adatkeretbe (`encoded_pumpkins`) egyes√≠thetj√ºk ≈ëket.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

‚úÖ Milyen el≈ënyei vannak az ordin√°lis k√≥dol√≥ haszn√°lat√°nak az `Item Size` oszlop eset√©ben?

### V√°ltoz√≥k k√∂z√∂tti kapcsolatok elemz√©se

Most, hogy el≈ëfeldolgoztuk az adatokat, elemezhetj√ºk a jellemz≈ëk √©s a c√≠mke k√∂z√∂tti kapcsolatokat, hogy meg√©rts√ºk, mennyire lesz k√©pes a modell el≈ëre jelezni a c√≠mk√©t a jellemz≈ëk alapj√°n. Az ilyen t√≠pus√∫ elemz√©s legjobb m√≥dja az adatok √°br√°zol√°sa. Ism√©t a Seaborn `catplot` f√ºggv√©ny√©t fogjuk haszn√°lni, hogy vizualiz√°ljuk az `Item Size`, `Variety` √©s `Color` k√∂z√∂tti kapcsolatokat egy kateg√≥ri√°lis diagramon. Az adatok jobb √°br√°zol√°sa √©rdek√©ben az `Item Size` k√≥dolt oszlop√°t √©s a nem k√≥dolt `Variety` oszlopot fogjuk haszn√°lni.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

![Vizualiz√°lt adatok kateg√≥ri√°lis diagramja](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Swarm diagram haszn√°lata

Mivel a `Color` egy bin√°ris kateg√≥ria (Feh√©r vagy Nem), 'egy [speci√°lis megk√∂zel√≠t√©st](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) ig√©nyel a vizualiz√°ci√≥hoz'. Vannak m√°s m√≥dok is, hogy vizualiz√°ljuk ennek a kateg√≥ri√°nak a kapcsolat√°t m√°s v√°ltoz√≥kkal.

A v√°ltoz√≥kat egym√°s mellett √°br√°zolhatod Seaborn diagramokkal.

1. Pr√≥b√°lj ki egy 'swarm' diagramot az √©rt√©kek eloszl√°s√°nak megjelen√≠t√©s√©re:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Vizualiz√°lt adatok swarm diagramja](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Figyelem**: a fenti k√≥d figyelmeztet√©st gener√°lhat, mivel a Seaborn nem tudja megfelel≈ëen √°br√°zolni ilyen mennyis√©g≈± adatpontot egy swarm diagramon. Egy lehets√©ges megold√°s a marker m√©ret√©nek cs√∂kkent√©se a 'size' param√©ter haszn√°lat√°val. Azonban l√©gy tudat√°ban annak, hogy ez befoly√°solja a diagram olvashat√≥s√°g√°t.

> **üßÆ Mutasd a matematik√°t**
>
> A logisztikus regresszi√≥ a 'maximum likelihood' koncepci√≥j√°n alapul, [szigmoid f√ºggv√©nyek](https://wikipedia.org/wiki/Sigmoid_function) haszn√°lat√°val. Egy 'szigmoid f√ºggv√©ny' egy grafikonon 'S' alak√∫ g√∂rb√©nek t≈±nik. Egy √©rt√©ket vesz, √©s 0 √©s 1 k√∂z√© t√©rk√©pezi. A g√∂rb√©j√©t 'logisztikus g√∂rb√©nek' is nevezik. A k√©plete √≠gy n√©z ki:
>
> ![logisztikus f√ºggv√©ny](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> ahol a szigmoid k√∂z√©ppontja az x 0 pontj√°n tal√°lhat√≥, L a g√∂rbe maxim√°lis √©rt√©ke, √©s k a g√∂rbe meredeks√©ge. Ha a f√ºggv√©ny eredm√©nye nagyobb, mint 0.5, az adott c√≠mk√©t a bin√°ris v√°laszt√°s '1' oszt√°ly√°ba sorolj√°k. Ha nem, akkor '0'-k√©nt oszt√°lyozz√°k.

## √âp√≠tsd fel a modelledet

Egy modell √©p√≠t√©se ezeknek a bin√°ris oszt√°lyoz√°soknak a megtal√°l√°s√°ra meglep≈ëen egyszer≈± a Scikit-learn seg√≠ts√©g√©vel.

[![ML kezd≈ëknek - Logisztikus regresszi√≥ az adatok oszt√°lyoz√°s√°hoz](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML kezd≈ëknek - Logisztikus regresszi√≥ az adatok oszt√°lyoz√°s√°hoz")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a line√°ris regresszi√≥s modell √©p√≠t√©s√©r≈ël

1. V√°laszd ki azokat a v√°ltoz√≥kat, amelyeket az oszt√°lyoz√°si modellben haszn√°lni szeretn√©l, √©s oszd fel a tanul√°si √©s tesztk√©szleteket a `train_test_split()` h√≠v√°s√°val:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Most m√°r betan√≠thatod a modelledet a `fit()` h√≠v√°s√°val a tanul√°si adatokkal, √©s ki√≠rhatod az eredm√©ny√©t:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    N√©zd meg a modelled eredm√©nyt√°bl√°j√°t. Nem rossz, tekintve, hogy csak k√∂r√ºlbel√ºl 1000 sor adatod van:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Jobb meg√©rt√©s egy zavar√≥ m√°trix seg√≠ts√©g√©vel

B√°r az eredm√©nyt√°bl√°t [kifejez√©sekkel](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) is ki√≠rhatod az el≈ëz≈ë elemek nyomtat√°s√°val, k√∂nnyebben meg√©rtheted a modelledet egy [zavar√≥ m√°trix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) haszn√°lat√°val, amely seg√≠t meg√©rteni, hogyan teljes√≠t a modell.

> üéì A '[zavar√≥ m√°trix](https://wikipedia.org/wiki/Confusion_matrix)' (vagy 'hibam√°trix') egy t√°bl√°zat, amely kifejezi a modelled val√≥di vs. hamis pozit√≠v √©s negat√≠v √©rt√©keit, √≠gy m√©rve az el≈ërejelz√©sek pontoss√°g√°t.

1. A zavar√≥ m√°trix haszn√°lat√°hoz h√≠vd meg a `confusion_matrix()` f√ºggv√©nyt:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    N√©zd meg a modelled zavar√≥ m√°trix√°t:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

A Scikit-learnben a zavar√≥ m√°trix sorai (0. tengely) a val√≥s c√≠mk√©k, m√≠g az oszlopai (1. tengely) az el≈ërejelzett c√≠mk√©k.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Mi t√∂rt√©nik itt? Tegy√ºk fel, hogy a modelledet arra k√©rik, hogy oszt√°lyozza a t√∂k√∂ket k√©t bin√°ris kateg√≥ria k√∂z√∂tt: 'feh√©r' √©s 'nem feh√©r'.

- Ha a modelled nem feh√©rk√©nt j√≥solja meg a t√∂k√∂t, √©s az val√≥j√°ban a 'nem feh√©r' kateg√≥ri√°ba tartozik, akkor ezt val√≥di negat√≠vnak nevezz√ºk, amelyet a bal fels≈ë sz√°m mut
Hogyan kapcsol√≥dik az √∂sszezavarod√°si m√°trix a precizit√°shoz √©s a visszah√≠v√°shoz? Ne feledd, a fentebb kinyomtatott oszt√°lyoz√°si jelent√©s megmutatta a precizit√°st (0.85) √©s a visszah√≠v√°st (0.67).

Precizit√°s = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461

Visszah√≠v√°s = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666

‚úÖ K: Az √∂sszezavarod√°si m√°trix alapj√°n hogyan teljes√≠tett a modell? V: Nem rossz; van egy j√≥ sz√°m√∫ val√≥di negat√≠v, de n√©h√°ny hamis negat√≠v is.

N√©zz√ºk meg √∫jra azokat a fogalmakat, amelyeket kor√°bban l√°ttunk, az √∂sszezavarod√°si m√°trix TP/TN √©s FP/FN lek√©pez√©s√©nek seg√≠ts√©g√©vel:

üéì Precizit√°s: TP/(TP + FP) Azoknak a relev√°ns p√©ld√°nyoknak az ar√°nya, amelyek a visszakeresett p√©ld√°nyok k√∂z√∂tt vannak (pl. mely c√≠mk√©k lettek j√≥l c√≠mk√©zve).

üéì Visszah√≠v√°s: TP/(TP + FN) Azoknak a relev√°ns p√©ld√°nyoknak az ar√°nya, amelyek visszakeres√©sre ker√ºltek, ak√°r j√≥l c√≠mk√©zve, ak√°r nem.

üéì f1-pontsz√°m: (2 * precizit√°s * visszah√≠v√°s)/(precizit√°s + visszah√≠v√°s) A precizit√°s √©s visszah√≠v√°s s√∫lyozott √°tlaga, ahol a legjobb √©rt√©k 1, a legrosszabb pedig 0.

üéì T√°mogat√°s: Az egyes visszakeresett c√≠mk√©k el≈ëfordul√°sainak sz√°ma.

üéì Pontoss√°g: (TP + TN)/(TP + TN + FP + FN) Azoknak a c√≠mk√©knek a sz√°zal√©ka, amelyeket egy mint√°ban pontosan el≈ëre jeleztek.

üéì Makro √Åtlag: Az egyes c√≠mk√©k s√∫lyozatlan √°tlagos metrik√°inak kisz√°m√≠t√°sa, figyelmen k√≠v√ºl hagyva a c√≠mk√©k egyens√∫lyhi√°ny√°t.

üéì S√∫lyozott √Åtlag: Az egyes c√≠mk√©k √°tlagos metrik√°inak kisz√°m√≠t√°sa, figyelembe v√©ve a c√≠mk√©k egyens√∫lyhi√°ny√°t, azokat a t√°mogat√°sukkal (az egyes c√≠mk√©k val√≥di p√©ld√°nyainak sz√°ma) s√∫lyozva.

‚úÖ Gondolod, hogy melyik metrik√°t kell figyelned, ha cs√∂kkenteni szeretn√©d a hamis negat√≠vok sz√°m√°t?

## Vizualiz√°ljuk a modell ROC g√∂rb√©j√©t

[![ML kezd≈ëknek - A logisztikus regresszi√≥ teljes√≠tm√©ny√©nek elemz√©se ROC g√∂rb√©kkel](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML kezd≈ëknek - A logisztikus regresszi√≥ teljes√≠tm√©ny√©nek elemz√©se ROC g√∂rb√©kkel")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a ROC g√∂rb√©kr≈ël

K√©sz√≠ts√ºnk m√©g egy vizualiz√°ci√≥t, hogy l√°ssuk az √∫gynevezett 'ROC' g√∂rb√©t:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Haszn√°ljuk a Matplotlibet a modell [Receiver Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) vagy ROC g√∂rb√©j√©nek √°br√°zol√°s√°ra. A ROC g√∂rb√©ket gyakran haszn√°lj√°k arra, hogy megtekints√©k egy oszt√°lyoz√≥ kimenet√©t a val√≥di √©s hamis pozit√≠vok szempontj√°b√≥l. "A ROC g√∂rb√©k jellemz≈ëen a val√≥di pozit√≠v ar√°nyt √°br√°zolj√°k az Y tengelyen, √©s a hamis pozit√≠v ar√°nyt az X tengelyen." Ez√©rt a g√∂rbe meredeks√©ge √©s a k√∂z√©pvonal √©s a g√∂rbe k√∂z√∂tti t√©r sz√°m√≠t: olyan g√∂rb√©t szeretn√©l, amely gyorsan felfel√© √©s a vonal f√∂l√© halad. Ebben az esetben vannak kezdeti hamis pozit√≠vok, majd a vonal megfelel≈ëen felfel√© √©s f√∂l√© halad:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

V√©g√ºl haszn√°ljuk a Scikit-learn [`roc_auc_score` API-j√°t](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) az √∫gynevezett 'G√∂rbe Alatti Ter√ºlet' (AUC) t√©nyleges kisz√°m√≠t√°s√°hoz:

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Az eredm√©ny `0.9749908725812341`. Mivel az AUC 0 √©s 1 k√∂z√∂tt mozog, magas pontsz√°mot szeretn√©l, mivel egy modell, amely 100%-ban helyes el≈ërejelz√©seket ad, AUC √©rt√©ke 1 lesz; ebben az esetben a modell _el√©g j√≥_.

A j√∂v≈ëbeli oszt√°lyoz√°si leck√©kben megtanulod, hogyan iter√°lj a modell pontsz√°mainak jav√≠t√°sa √©rdek√©ben. De most gratul√°lok! Befejezted ezeket a regresszi√≥s leck√©ket!

---
## üöÄKih√≠v√°s

M√©g sok mindent lehet kibontani a logisztikus regresszi√≥val kapcsolatban! De a legjobb m√≥dja a tanul√°snak az, ha k√≠s√©rletezel. Keress egy adat√°llom√°nyt, amely alkalmas erre az elemz√©sre, √©s √©p√≠ts egy modellt vele. Mit tanulsz? Tipp: pr√≥b√°ld ki a [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) oldalt √©rdekes adat√°llom√°nyok√©rt.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Olvasd el [ennek a Stanford-i tanulm√°nynak](https://web.stanford.edu/~jurafsky/slp3/5.pdf) az els≈ë n√©h√°ny oldal√°t a logisztikus regresszi√≥ gyakorlati alkalmaz√°sair√≥l. Gondolj olyan feladatokra, amelyek jobban illenek az egyik vagy m√°sik t√≠pus√∫ regresszi√≥s feladathoz, amelyeket eddig tanulm√°nyoztunk. Mi m≈±k√∂dne a legjobban?

## Feladat

[Pr√≥b√°ld √∫jra ezt a regresszi√≥t](assignment.md)

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.