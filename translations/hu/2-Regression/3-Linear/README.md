<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T15:09:53+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "hu"
}
-->
# K√©sz√≠ts√ºnk regresszi√≥s modellt Scikit-learn seg√≠ts√©g√©vel: n√©gyf√©le regresszi√≥

![Line√°ris vs polinomi√°lis regresszi√≥ infografika](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika: [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ez a lecke el√©rhet≈ë R-ben is!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Bevezet√©s 

Eddig megismerkedt√©l azzal, hogy mi is az a regresszi√≥, p√©ld√°ul a t√∂k√°rak adatain kereszt√ºl, amelyeket ebben a leck√©ben fogunk haszn√°lni. A Matplotlib seg√≠ts√©g√©vel vizualiz√°ltad is az adatokat.

Most k√©szen √°llsz arra, hogy m√©lyebben belemer√ºlj a g√©pi tanul√°s regresszi√≥s technik√°iba. B√°r a vizualiz√°ci√≥ seg√≠t az adatok meg√©rt√©s√©ben, a g√©pi tanul√°s val√≥di ereje a _modellek tan√≠t√°s√°ban_ rejlik. A modelleket t√∂rt√©nelmi adatokon tan√≠tjuk, hogy automatikusan felismerj√©k az adatok k√∂z√∂tti √∂sszef√ºgg√©seket, √©s lehet≈ëv√© tegy√©k az el≈ërejelz√©st olyan √∫j adatokra, amelyeket a modell kor√°bban nem l√°tott.

Ebben a leck√©ben t√∂bbet fogsz megtudni k√©tf√©le regresszi√≥r√≥l: _egyszer≈± line√°ris regresszi√≥r√≥l_ √©s _polinomi√°lis regresszi√≥r√≥l_, valamint az ezek m√∂g√∂tt √°ll√≥ matematik√°r√≥l. Ezek a modellek lehet≈ëv√© teszik sz√°munkra, hogy k√ºl√∂nb√∂z≈ë bemeneti adatok alapj√°n el≈ëre jelezz√ºk a t√∂k√°rakat.

[![G√©pi tanul√°s kezd≈ëknek - A line√°ris regresszi√≥ meg√©rt√©se](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "G√©pi tanul√°s kezd≈ëknek - A line√°ris regresszi√≥ meg√©rt√©se")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √∂sszefoglal√≥√©rt a line√°ris regresszi√≥r√≥l.

> Ebben a tananyagban minim√°lis matematikai ismereteket felt√©telez√ºnk, √©s igyeksz√ºnk hozz√°f√©rhet≈ëv√© tenni a k√ºl√∂nb√∂z≈ë ter√ºletekr≈ël √©rkez≈ë di√°kok sz√°m√°ra. Figyelj a jegyzetekre, üßÆ matematikai h√≠v√°sokra, diagramokra √©s m√°s tanul√°si eszk√∂z√∂kre, amelyek seg√≠tik a meg√©rt√©st.

### El≈ëfelt√©tel

Mostanra m√°r ismerned kell a t√∂kadatok szerkezet√©t, amelyeket vizsg√°lunk. Ezek az adatok el≈ëre bet√∂ltve √©s megtiszt√≠tva tal√°lhat√≥k meg ennek a leck√©nek a _notebook.ipynb_ f√°jlj√°ban. A f√°jlban a t√∂k√°rak bushelre vet√≠tve jelennek meg egy √∫j adatkeretben. Gy≈ëz≈ëdj meg r√≥la, hogy ezeket a notebookokat futtatni tudod a Visual Studio Code kerneljeiben.

### Felk√©sz√ºl√©s

Eml√©keztet≈ë√ºl: ezeket az adatokat az√©rt t√∂lt√∂d be, hogy k√©rd√©seket tegy√©l fel vel√ºk kapcsolatban.

- Mikor √©rdemes t√∂k√∂t v√°s√°rolni?
- Milyen √°rat v√°rhatok egy doboz miniat≈±r t√∂k eset√©ben?
- √ârdemes f√©l bushel kos√°rban vagy 1 1/9 bushel dobozban v√°s√°rolni ≈ëket?
Folytassuk az adatok vizsg√°lat√°t.

Az el≈ëz≈ë leck√©ben l√©trehozt√°l egy Pandas adatkeretet, √©s felt√∂lt√∂tted az eredeti adatk√©szlet egy r√©sz√©vel, standardiz√°lva az √°rakat bushelre vet√≠tve. Ezzel azonban csak k√∂r√ºlbel√ºl 400 adatpontot tudt√°l √∂sszegy≈±jteni, √©s csak az ≈ëszi h√≥napokra vonatkoz√≥an.

N√©zd meg az adatokat, amelyeket el≈ëre bet√∂lt√∂tt√ºnk ennek a leck√©nek a notebookj√°ban. Az adatok el≈ëre bet√∂ltve vannak, √©s egy kezdeti sz√≥r√°sdiagramot is k√©sz√≠tett√ºnk, amely h√≥nap adatokat mutat. Tal√°n egy kicsit r√©szletesebb k√©pet kaphatunk az adatok term√©szet√©r≈ël, ha tov√°bb tiszt√≠tjuk ≈ëket.

## Egy line√°ris regresszi√≥s vonal

Ahogy az 1. leck√©ben megtanultad, a line√°ris regresszi√≥ c√©lja, hogy egy vonalat rajzoljunk, amely:

- **Megmutatja a v√°ltoz√≥k k√∂z√∂tti kapcsolatot**. Megmutatja a v√°ltoz√≥k k√∂z√∂tti √∂sszef√ºgg√©st.
- **El≈ërejelz√©seket k√©sz√≠t**. Pontos el≈ërejelz√©seket k√©sz√≠t arr√≥l, hogy egy √∫j adatpont hol helyezkedne el a vonalhoz k√©pest.

A **Legkisebb n√©gyzetek regresszi√≥ja** √°ltal√°ban ezt a fajta vonalat rajzolja. A "legkisebb n√©gyzetek" kifejez√©s azt jelenti, hogy a regresszi√≥s vonal k√∂r√ºli √∂sszes adatpontot n√©gyzetre emelj√ºk, majd √∂sszeadjuk. Ide√°lis esetben ez az √∂sszeg a lehet≈ë legkisebb, mivel alacsony hibasz√°mot, vagyis `legkisebb n√©gyzeteket` szeretn√©nk.

Ezt az√©rt tessz√ºk, mert olyan vonalat szeretn√©nk modellezni, amelynek a legkisebb kumulat√≠v t√°vols√°ga van az √∂sszes adatpontt√≥l. Az √©rt√©keket n√©gyzetre emelj√ºk, miel≈ëtt √∂sszeadn√°nk ≈ëket, mivel az ir√°ny helyett a nagys√°guk √©rdekel minket.

> **üßÆ Mutasd a matematik√°t** 
> 
> Ez a vonal, amelyet _legjobb illeszked√©s vonal√°nak_ nevez√ºnk, [egy egyenlettel](https://en.wikipedia.org/wiki/Simple_linear_regression) fejezhet≈ë ki: 
> 
> ```
> Y = a + bX
> ```
>
> `X` az 'magyar√°z√≥ v√°ltoz√≥'. `Y` a 'f√ºgg≈ë v√°ltoz√≥'. A vonal meredeks√©ge `b`, √©s `a` az y-metszet, amely arra utal, hogy `Y` √©rt√©ke mennyi, amikor `X = 0`. 
>
>![a meredeks√©g kisz√°m√≠t√°sa](../../../../2-Regression/3-Linear/images/slope.png)
>
> El≈ësz√∂r sz√°m√≠tsd ki a meredeks√©get `b`. Infografika: [Jen Looper](https://twitter.com/jenlooper)
>
> M√°s szavakkal, √©s utalva a t√∂kadatok eredeti k√©rd√©s√©re: "j√≥soljuk meg a t√∂k √°r√°t bushelre vet√≠tve h√≥nap szerint", `X` az √°rra, `Y` pedig az elad√°si h√≥napra utalna. 
>
>![az egyenlet kieg√©sz√≠t√©se](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Sz√°m√≠tsd ki `Y` √©rt√©k√©t. Ha k√∂r√ºlbel√ºl 4 doll√°rt fizetsz, akkor √°prilis van! Infografika: [Jen Looper](https://twitter.com/jenlooper)
>
> Az egyenlet kisz√°m√≠t√°s√°hoz sz√ºks√©ges matematika megmutatja a vonal meredeks√©g√©t, amely az y-metszett≈ël is f√ºgg, vagyis att√≥l, hogy `Y` hol helyezkedik el, amikor `X = 0`.
>
> Megfigyelheted az √©rt√©kek kisz√°m√≠t√°s√°nak m√≥dszer√©t a [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) weboldalon. L√°togasd meg a [Legkisebb n√©gyzetek kalkul√°tort](https://www.mathsisfun.com/data/least-squares-calculator.html), hogy l√°thasd, hogyan befoly√°solj√°k a sz√°mok √©rt√©kei a vonalat.

## Korrel√°ci√≥

Egy m√°sik fontos fogalom a **Korrel√°ci√≥s egy√ºtthat√≥** az adott X √©s Y v√°ltoz√≥k k√∂z√∂tt. Egy sz√≥r√°sdiagram seg√≠ts√©g√©vel gyorsan vizualiz√°lhatod ezt az egy√ºtthat√≥t. Ha az adatpontok sz√©pen egy vonalban helyezkednek el, akkor magas korrel√°ci√≥r√≥l besz√©l√ºnk, de ha az adatpontok sz√©tsz√≥rtan helyezkednek el X √©s Y k√∂z√∂tt, akkor alacsony korrel√°ci√≥r√≥l van sz√≥.

Egy j√≥ line√°ris regresszi√≥s modell olyan, amelynek magas (1-hez k√∂zelebb √°ll√≥, mint 0-hoz) Korrel√°ci√≥s egy√ºtthat√≥ja van a Legkisebb n√©gyzetek regresszi√≥s m√≥dszer√©vel √©s egy regresszi√≥s vonallal.

‚úÖ Futtasd a leck√©hez tartoz√≥ notebookot, √©s n√©zd meg a H√≥nap √©s √År sz√≥r√°sdiagramot. Az adatok, amelyek a H√≥nap √©s √År k√∂z√∂tti kapcsolatot mutatj√°k a t√∂kelad√°sok eset√©ben, vizu√°lis √©rtelmez√©sed szerint magas vagy alacsony korrel√°ci√≥val rendelkeznek? V√°ltozik ez, ha finomabb m√©rt√©ket haszn√°lsz a `H√≥nap` helyett, p√©ld√°ul *az √©v napj√°t* (azaz az √©v eleje √≥ta eltelt napok sz√°ma)?

Az al√°bbi k√≥dban felt√©telezz√ºk, hogy megtiszt√≠tottuk az adatokat, √©s egy `new_pumpkins` nev≈± adatkeretet kaptunk, amely hasonl√≥ az al√°bbihoz:

ID | H√≥nap | √âvNapja | Fajta | V√°ros | Csomagol√°s | Alacsony √°r | Magas √°r | √År
---|-------|---------|-------|-------|------------|-------------|----------|-----
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel kartonok | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel kartonok | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel kartonok | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel kartonok | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel kartonok | 15.0 | 15.0 | 13.636364

> Az adatok tiszt√≠t√°s√°hoz sz√ºks√©ges k√≥d megtal√°lhat√≥ a [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb) f√°jlban. Ugyanazokat a tiszt√≠t√°si l√©p√©seket hajtottuk v√©gre, mint az el≈ëz≈ë leck√©ben, √©s kisz√°m√≠tottuk az `√âvNapja` oszlopot az al√°bbi kifejez√©s seg√≠ts√©g√©vel:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Most, hogy meg√©rtetted a line√°ris regresszi√≥ m√∂g√∂tti matematik√°t, hozzunk l√©tre egy regresszi√≥s modellt, hogy megn√©zz√ºk, meg tudjuk-e j√≥solni, melyik t√∂kcsomagol√°s k√≠n√°lja a legjobb √°rakat. Valaki, aki t√∂k√∂t v√°s√°rol egy √ºnnepi t√∂kfolt sz√°m√°ra, szeretn√© optimaliz√°lni a t√∂kcsomagok v√°s√°rl√°s√°t.

## Korrel√°ci√≥ keres√©se

[![G√©pi tanul√°s kezd≈ëknek - Korrel√°ci√≥ keres√©se: A line√°ris regresszi√≥ kulcsa](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "G√©pi tanul√°s kezd≈ëknek - Korrel√°ci√≥ keres√©se: A line√°ris regresszi√≥ kulcsa")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √∂sszefoglal√≥√©rt a korrel√°ci√≥r√≥l.

Az el≈ëz≈ë leck√©b≈ël val√≥sz√≠n≈±leg l√°ttad, hogy az √°tlag√°r k√ºl√∂nb√∂z≈ë h√≥napokra √≠gy n√©z ki:

<img alt="√Åtlag√°r h√≥naponk√©nt" src="../2-Data/images/barchart.png" width="50%"/>

Ez arra utal, hogy lehet n√©mi korrel√°ci√≥, √©s megpr√≥b√°lhatunk egy line√°ris regresszi√≥s modellt tan√≠tani, hogy megj√≥soljuk a `H√≥nap` √©s `√År`, vagy az `√âvNapja` √©s `√År` k√∂z√∂tti kapcsolatot. √çme egy sz√≥r√°sdiagram, amely az ut√≥bbi kapcsolatot mutatja:

<img alt="Sz√≥r√°sdiagram az √År √©s az √âvNapja k√∂z√∂tt" src="images/scatter-dayofyear.png" width="50%" /> 

N√©zz√ºk meg, van-e korrel√°ci√≥ a `corr` f√ºggv√©ny seg√≠ts√©g√©vel:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

√ögy t≈±nik, hogy a korrel√°ci√≥ el√©g kicsi, -0.15 a `H√≥nap` √©s -0.17 az `√âvNapja` eset√©ben, de lehet, hogy van egy m√°sik fontos kapcsolat. √ögy t≈±nik, hogy az √°rak k√ºl√∂nb√∂z≈ë csoportjai a t√∂kfajt√°khoz kapcsol√≥dnak. Ennek a hipot√©zisnek a meger≈ës√≠t√©s√©hez √°br√°zoljuk minden t√∂kkateg√≥ri√°t k√ºl√∂nb√∂z≈ë sz√≠nnel. Az `ax` param√©ter √°tad√°s√°val a `scatter` √°br√°zol√°si f√ºggv√©nynek az √∂sszes pontot ugyanazon a grafikonon √°br√°zolhatjuk:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Sz√≥r√°sdiagram az √År √©s az √âvNapja k√∂z√∂tt" src="images/scatter-dayofyear-color.png" width="50%" /> 

Vizsg√°latunk azt sugallja, hogy a fajta nagyobb hat√°ssal van az √°rakra, mint az elad√°si d√°tum. Ezt egy oszlopdiagramon is l√°thatjuk:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Oszlopdiagram az √°r √©s a fajta k√∂z√∂tt" src="images/price-by-variety.png" width="50%" /> 

Most koncentr√°ljunk egyetlen t√∂kfajt√°ra, a 'pie type'-ra, √©s n√©zz√ºk meg, milyen hat√°ssal van a d√°tum az √°rra:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Sz√≥r√°sdiagram az √År √©s az √âvNapja k√∂z√∂tt" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Ha most kisz√°m√≠tjuk az `√År` √©s az `√âvNapja` k√∂z√∂tti korrel√°ci√≥t a `corr` f√ºggv√©ny seg√≠ts√©g√©vel, k√∂r√ºlbel√ºl `-0.27` √©rt√©ket kapunk - ami azt jelenti, hogy √©rdemes egy predikt√≠v modellt tan√≠tani.

> Miel≈ëtt line√°ris regresszi√≥s modellt tan√≠tan√°nk, fontos megbizonyosodni arr√≥l, hogy az adataink tiszt√°k. A line√°ris regresszi√≥ nem m≈±k√∂dik j√≥l hi√°nyz√≥ √©rt√©kekkel, ez√©rt √©rdemes megszabadulni az √ºres cell√°kt√≥l:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Egy m√°sik megk√∂zel√≠t√©s az lenne, hogy az √ºres √©rt√©keket az adott oszlop √°tlag√©rt√©keivel t√∂ltj√ºk ki.

## Egyszer≈± line√°ris regresszi√≥

[![G√©pi tanul√°s kezd≈ëknek - Line√°ris √©s polinomi√°lis regresszi√≥ Scikit-learn seg√≠ts√©g√©vel](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "G√©pi tanul√°s kezd≈ëknek - Line√°ris √©s polinomi√°lis regresszi√≥ Scikit-learn seg√≠ts√©g√©vel")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √∂sszefoglal√≥√©rt a line√°ris √©s polinomi√°lis regresszi√≥r√≥l.

A line√°ris regresszi√≥s modell√ºnk tan√≠t√°s√°hoz a **Scikit-learn** k√∂nyvt√°rat fogjuk haszn√°lni.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

El≈ësz√∂r sz√©tv√°lasztjuk a bemeneti √©rt√©keket (jellemz≈ëk) √©s a v√°rt kimenetet (c√≠mke) k√ºl√∂n numpy t√∂mb√∂kbe:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Figyeld meg, hogy a bemeneti adatokat `reshape`-el kellett √°talak√≠tanunk, hogy a Line√°ris Regresszi√≥ csomag helyesen √©rtelmezze. A Line√°ris Regresszi√≥ 2D-t√∂mb√∂t v√°r bemenetk√©nt, ahol a t√∂mb minden sora a bemeneti jellemz≈ëk vektor√°nak felel meg. Ebben az esetben, mivel csak egy bemenet√ºnk van, egy N√ó1 alak√∫ t√∂mbre van sz√ºks√©g√ºnk, ahol N az adatk√©szlet m√©rete.

Ezut√°n az adatokat sz√©t kell osztanunk tan√≠t√≥ √©s teszt adatk√©szletekre, hogy a modell√ºnket valid√°lni tudjuk a tan√≠t√°s ut√°n:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

V√©g√ºl a t√©nyleges Line√°ris Regresszi√≥s modell tan√≠t√°sa mind√∂ssze k√©t k√≥dsort ig√©nyel. Meghat√°rozzuk a `LinearRegression` objektumot, √©s az adatainkra illesztj√ºk a `fit` met√≥dus seg√≠ts√©g√©vel:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

A `LinearRegression` objektum a `fit`-el√©s ut√°n tartalmazza a regresszi√≥ √∂sszes egy√ºtthat√≥j√°t, amelyeket a `.coef_` tulajdons√°gon kereszt√ºl √©rhet√ºnk el. Ebben az esetben csak egy egy√ºtthat√≥ van, amelynek √©rt√©ke
Hib√°nk k√∂r√ºlbel√ºl 2 pontn√°l van, ami ~17%. Nem t√∫l j√≥. Egy m√°sik mutat√≥ a modell min≈ës√©g√©re a **determiniz√°ci√≥s egy√ºtthat√≥**, amelyet √≠gy lehet kisz√°m√≠tani:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Ha az √©rt√©k 0, az azt jelenti, hogy a modell nem veszi figyelembe a bemeneti adatokat, √©s √∫gy m≈±k√∂dik, mint a *legrosszabb line√°ris el≈ërejelz≈ë*, amely egyszer≈±en az eredm√©ny √°tlag√©rt√©ke. Az 1-es √©rt√©k azt jelenti, hogy t√∂k√©letesen meg tudjuk j√≥solni az √∂sszes v√°rt kimenetet. Eset√ºnkben az egy√ºtthat√≥ k√∂r√ºlbel√ºl 0.06, ami el√©g alacsony.

A tesztadatokat a regresszi√≥s vonallal egy√ºtt is √°br√°zolhatjuk, hogy jobban l√°ssuk, hogyan m≈±k√∂dik a regresszi√≥ a mi eset√ºnkben:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Line√°ris regresszi√≥" src="images/linear-results.png" width="50%" />

## Polinomi√°lis regresszi√≥

A line√°ris regresszi√≥ egy m√°sik t√≠pusa a polinomi√°lis regresszi√≥. B√°r n√©ha van line√°ris kapcsolat a v√°ltoz√≥k k√∂z√∂tt ‚Äì p√©ld√°ul min√©l nagyobb a t√∂k t√©rfogata, ann√°l magasabb az √°ra ‚Äì, n√©ha ezek a kapcsolatok nem √°br√°zolhat√≥k s√≠kk√©nt vagy egyenes vonalk√©nt.

‚úÖ Itt van [n√©h√°ny p√©lda](https://online.stat.psu.edu/stat501/lesson/9/9.8) olyan adatokra, amelyekhez polinomi√°lis regresszi√≥t lehet haszn√°lni.

N√©zd meg √∫jra a d√°tum √©s az √°r k√∂z√∂tti kapcsolatot. √ögy t≈±nik, hogy ezt a sz√≥r√°sdiagramot felt√©tlen√ºl egy egyenes vonallal kellene elemezni? Nem ingadozhatnak az √°rak? Ebben az esetben megpr√≥b√°lhatod a polinomi√°lis regresszi√≥t.

‚úÖ A polinomok olyan matematikai kifejez√©sek, amelyek egy vagy t√∂bb v√°ltoz√≥t √©s egy√ºtthat√≥t tartalmazhatnak.

A polinomi√°lis regresszi√≥ egy g√∂rbe vonalat hoz l√©tre, amely jobban illeszkedik a nemline√°ris adatokhoz. Eset√ºnkben, ha egy n√©gyzetes `DayOfYear` v√°ltoz√≥t is hozz√°adunk a bemeneti adatokhoz, akkor egy parabola g√∂rb√©vel tudjuk illeszteni az adatainkat, amelynek minimuma az √©v egy bizonyos pontj√°n lesz.

A Scikit-learn tartalmaz egy hasznos [pipeline API-t](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), amely lehet≈ëv√© teszi az adatfeldolgoz√°s k√ºl√∂nb√∂z≈ë l√©p√©seinek √∂sszekapcsol√°s√°t. A **pipeline** egy **becsl≈ëk** l√°ncolata. Eset√ºnkben egy olyan pipeline-t hozunk l√©tre, amely el≈ësz√∂r polinomi√°lis jellemz≈ëket ad a modellhez, majd elv√©gzi a regresszi√≥t:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

A `PolynomialFeatures(2)` haszn√°lata azt jelenti, hogy a bemeneti adatokb√≥l minden m√°sodfok√∫ polinomot beilleszt√ºnk. Eset√ºnkben ez csak a `DayOfYear`<sup>2</sup>-t jelenti, de ha k√©t bemeneti v√°ltoz√≥nk van, X √©s Y, akkor ez hozz√°adja X<sup>2</sup>-t, XY-t √©s Y<sup>2</sup>-t. Magasabb fok√∫ polinomokat is haszn√°lhatunk, ha szeretn√©nk.

A pipeline-t ugyan√∫gy haszn√°lhatjuk, mint az eredeti `LinearRegression` objektumot, azaz `fit`-elhetj√ºk a pipeline-t, majd a `predict` seg√≠ts√©g√©vel megkaphatjuk az el≈ërejelz√©si eredm√©nyeket. √çme a grafikon, amely a tesztadatokat √©s az approxim√°ci√≥s g√∂rb√©t mutatja:

<img alt="Polinomi√°lis regresszi√≥" src="images/poly-results.png" width="50%" />

A polinomi√°lis regresszi√≥ haszn√°lat√°val kiss√© alacsonyabb MSE-t √©s magasabb determiniz√°ci√≥s egy√ºtthat√≥t √©rhet√ºnk el, de nem jelent≈ësen. Figyelembe kell venn√ºnk m√°s jellemz≈ëket is!

> L√°thatod, hogy a t√∂k√∂k legalacsonyabb √°rai valahol Halloween k√∂rny√©k√©n figyelhet≈ëk meg. Hogyan magyar√°zn√°d ezt?

üéÉ Gratul√°lok, most l√©trehozt√°l egy modellt, amely seg√≠thet a pite t√∂k√∂k √°r√°nak el≈ërejelz√©s√©ben. Val√≥sz√≠n≈±leg ugyanezt az elj√°r√°st megism√©telheted az √∂sszes t√∂kfajt√°ra, de ez el√©g f√°raszt√≥ lenne. Most tanuljuk meg, hogyan vegy√ºk figyelembe a t√∂kfajt√°kat a modell√ºnkben!

## Kategorikus jellemz≈ëk

Az ide√°lis vil√°gban szeretn√©nk k√©pesek lenni el≈ëre jelezni az √°rakat k√ºl√∂nb√∂z≈ë t√∂kfajt√°kra ugyanazzal a modellel. Azonban a `Variety` oszlop kiss√© elt√©r az olyan oszlopokt√≥l, mint a `Month`, mert nem numerikus √©rt√©keket tartalmaz. Az ilyen oszlopokat **kategorikus** oszlopoknak nevezz√ºk.

[![ML kezd≈ëknek - Kategorikus jellemz≈ëk el≈ërejelz√©se line√°ris regresszi√≥val](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML kezd≈ëknek - Kategorikus jellemz≈ëk el≈ërejelz√©se line√°ris regresszi√≥val")

> üé• Kattints a fenti k√©pre egy r√∂vid vide√≥s √°ttekint√©s√©rt a kategorikus jellemz≈ëk haszn√°lat√°r√≥l.

Itt l√°thatod, hogyan f√ºgg az √°tlag√°r a fajt√°t√≥l:

<img alt="√Åtlag√°r fajt√°nk√©nt" src="images/price-by-variety.png" width="50%" />

Ahhoz, hogy figyelembe vegy√ºk a fajt√°t, el≈ësz√∂r numerikus form√°ra kell √°talak√≠tanunk, vagyis **k√≥dolnunk** kell. T√∂bbf√©le m√≥don tehetj√ºk ezt meg:

* Az egyszer≈± **numerikus k√≥dol√°s** egy t√°bl√°zatot k√©sz√≠t a k√ºl√∂nb√∂z≈ë fajt√°kr√≥l, majd a fajta nev√©t egy indexszel helyettes√≠ti a t√°bl√°zatban. Ez nem a legjobb √∂tlet a line√°ris regresszi√≥hoz, mert a line√°ris regresszi√≥ az index t√©nyleges numerikus √©rt√©k√©t veszi figyelembe, √©s hozz√°adja az eredm√©nyhez, megszorozva egy egy√ºtthat√≥val. Eset√ºnkben az indexsz√°m √©s az √°r k√∂z√∂tti kapcsolat egy√©rtelm≈±en nem line√°ris, m√©g akkor sem, ha biztos√≠tjuk, hogy az indexek valamilyen specifikus sorrendben legyenek.
* A **one-hot k√≥dol√°s** a `Variety` oszlopot 4 k√ºl√∂nb√∂z≈ë oszlopra cser√©li, egyet minden fajt√°hoz. Minden oszlop `1`-et tartalmaz, ha az adott sor egy adott fajt√°hoz tartozik, √©s `0`-t, ha nem. Ez azt jelenti, hogy a line√°ris regresszi√≥ban n√©gy egy√ºtthat√≥ lesz, egy-egy minden t√∂kfajt√°hoz, amely felel≈ës az adott fajta "kezd≈ë √°r√°nak" (vagy ink√°bb "tov√°bbi √°r√°nak").

Az al√°bbi k√≥d megmutatja, hogyan k√≥dolhatjuk one-hot m√≥dszerrel a fajt√°t:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Ahhoz, hogy line√°ris regresszi√≥t tan√≠tsunk one-hot k√≥dolt fajta bemeneti adatokkal, csak helyesen kell inicializ√°lnunk az `X` √©s `y` adatokat:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

A t√∂bbi k√≥d ugyanaz, mint amit kor√°bban haszn√°ltunk a line√°ris regresszi√≥ tan√≠t√°s√°hoz. Ha kipr√≥b√°lod, l√°tni fogod, hogy az √°tlagos n√©gyzetes hiba k√∂r√ºlbel√ºl ugyanaz, de sokkal magasabb determiniz√°ci√≥s egy√ºtthat√≥t kapunk (~77%). Ahhoz, hogy m√©g pontosabb el≈ërejelz√©seket kapjunk, t√∂bb kategorikus jellemz≈ët is figyelembe vehet√ºnk, valamint numerikus jellemz≈ëket, mint p√©ld√°ul a `Month` vagy a `DayOfYear`. Egy nagy jellemz≈ët√∂mb l√©trehoz√°s√°hoz haszn√°lhatjuk a `join`-t:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Itt figyelembe vessz√ºk a `City` √©s a `Package` t√≠pus√°t is, ami 2.84-es MSE-t (10%) √©s 0.94-es determiniz√°ci√≥s egy√ºtthat√≥t eredm√©nyez!

## Mindent √∂sszefoglalva

A legjobb modell l√©trehoz√°s√°hoz haszn√°lhatjuk a fenti p√©ld√°b√≥l sz√°rmaz√≥ kombin√°lt (one-hot k√≥dolt kategorikus + numerikus) adatokat polinomi√°lis regresszi√≥val egy√ºtt. √çme a teljes k√≥d a k√©nyelmed √©rdek√©ben:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Ez k√∂zel 97%-os determiniz√°ci√≥s egy√ºtthat√≥t √©s MSE=2.23 (~8%-os el≈ërejelz√©si hiba) eredm√©nyez.

| Modell | MSE | Determiniz√°ci√≥ |
|-------|-----|---------------|
| `DayOfYear` Line√°ris | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polinomi√°lis | 2.73 (17.0%) | 0.08 |
| `Variety` Line√°ris | 5.24 (19.7%) | 0.77 |
| Minden jellemz≈ë Line√°ris | 2.84 (10.5%) | 0.94 |
| Minden jellemz≈ë Polinomi√°lis | 2.23 (8.25%) | 0.97 |

üèÜ Sz√©p munka! Egyetlen leck√©ben n√©gy regresszi√≥s modellt hozt√°l l√©tre, √©s a modell min≈ës√©g√©t 97%-ra jav√≠tottad. A regresszi√≥r√≥l sz√≥l√≥ utols√≥ r√©szben a logisztikus regresszi√≥val fogsz megismerkedni, amely kateg√≥ri√°k meghat√°roz√°s√°ra szolg√°l.

---
## üöÄKih√≠v√°s

Tesztelj t√∂bb k√ºl√∂nb√∂z≈ë v√°ltoz√≥t ebben a notebookban, hogy l√°sd, hogyan f√ºgg √∂ssze a korrel√°ci√≥ a modell pontoss√°g√°val.

## [Ut√≥lagos kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Ebben a leck√©ben a line√°ris regresszi√≥r√≥l tanultunk. Vannak m√°s fontos regresszi√≥s technik√°k is. Olvass a Stepwise, Ridge, Lasso √©s Elasticnet technik√°kr√≥l. Egy j√≥ kurzus, amelyet √©rdemes tanulm√°nyozni, a [Stanford Statistical Learning kurzus](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Feladat 

[√âp√≠ts egy modellt](assignment.md)

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.