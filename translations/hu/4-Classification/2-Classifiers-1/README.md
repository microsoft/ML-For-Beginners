<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T16:17:35+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "hu"
}
-->
# Konyhai oszt√°lyoz√≥k 1

Ebben a leck√©ben az el≈ëz≈ë leck√©ben elmentett, kiegyens√∫lyozott √©s tiszta adatokkal teli adat√°llom√°nyt fogod haszn√°lni, amely a k√ºl√∂nb√∂z≈ë konyh√°kr√≥l sz√≥l.

Ezt az adat√°llom√°nyt k√ºl√∂nf√©le oszt√°lyoz√≥kkal fogod haszn√°lni, hogy _egy adott nemzeti konyh√°t megj√≥solj egy √∂sszetev≈ëcsoport alapj√°n_. Ek√∂zben t√∂bbet megtudhatsz arr√≥l, hogyan lehet algoritmusokat alkalmazni oszt√°lyoz√°si feladatokhoz.

## [El≈ëad√°s el≈ëtti kv√≠z](https://ff-quizzes.netlify.app/en/ml/)
# Felk√©sz√ºl√©s

Felt√©telezve, hogy befejezted az [1. leck√©t](../1-Introduction/README.md), gy≈ëz≈ëdj meg r√≥la, hogy egy _cleaned_cuisines.csv_ f√°jl l√©tezik a gy√∂k√©r `/data` mapp√°ban ehhez a n√©gy leck√©hez.

## Gyakorlat - egy nemzeti konyha megj√≥sl√°sa

1. Dolgozz ebben a lecke _notebook.ipynb_ mapp√°j√°ban, √©s import√°ld a f√°jlt a Pandas k√∂nyvt√°rral egy√ºtt:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Az adatok √≠gy n√©znek ki:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Most import√°lj t√∂bb k√∂nyvt√°rat:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Oszd fel az X √©s y koordin√°t√°kat k√©t adatkeretre a tan√≠t√°shoz. A `cuisine` lehet a c√≠mk√©ket tartalmaz√≥ adatkeret:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Ez √≠gy fog kin√©zni:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. Dobd el az `Unnamed: 0` √©s a `cuisine` oszlopokat a `drop()` h√≠v√°s√°val. A t√∂bbi adatot mentsd el tan√≠that√≥ jellemz≈ëk√©nt:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    A jellemz≈ëk √≠gy n√©znek ki:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Most k√©szen √°llsz arra, hogy betan√≠tsd a modelledet!

## Oszt√°lyoz√≥ kiv√°laszt√°sa

Most, hogy az adataid tiszt√°k √©s k√©szen √°llnak a tan√≠t√°sra, el kell d√∂ntened, melyik algoritmust haszn√°lod a feladathoz.

A Scikit-learn a fel√ºgyelt tanul√°s kateg√≥ri√°j√°ba sorolja az oszt√°lyoz√°st, √©s ebben a kateg√≥ri√°ban sz√°mos m√≥dszert tal√°lsz az oszt√°lyoz√°shoz. [A v√°laszt√©k](https://scikit-learn.org/stable/supervised_learning.html) els≈ëre meglehet≈ësen zavarba ejt≈ë. Az al√°bbi m√≥dszerek mind tartalmaznak oszt√°lyoz√°si technik√°kat:

- Line√°ris modellek
- T√°mogat√≥ vektorg√©pek
- Stochasztikus gradiens cs√∂kken√©s
- Legk√∂zelebbi szomsz√©dok
- Gauss-folyamatok
- D√∂nt√©si f√°k
- Egy√ºttes m√≥dszerek (szavaz√≥ oszt√°lyoz√≥)
- T√∂bboszt√°lyos √©s t√∂bbkimenetes algoritmusok (t√∂bboszt√°lyos √©s t√∂bbc√≠mk√©s oszt√°lyoz√°s, t√∂bboszt√°lyos-t√∂bbkimenetes oszt√°lyoz√°s)

> Az adatok oszt√°lyoz√°s√°ra [neur√°lis h√°l√≥zatokat is haszn√°lhatsz](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), de ez a lecke keretein k√≠v√ºl esik.

### Melyik oszt√°lyoz√≥t v√°lasszuk?

Teh√°t, melyik oszt√°lyoz√≥t v√°lasszuk? Gyakran az a m√≥dszer, hogy t√∂bb algoritmust kipr√≥b√°lunk, √©s keres√ºnk egy j√≥ eredm√©nyt. A Scikit-learn k√≠n√°l egy [√∂sszehasonl√≠t√°st](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) egy l√©trehozott adat√°llom√°nyon, amely √∂sszehasonl√≠tja a KNeighbors, SVC k√©t m√≥dj√°t, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB √©s QuadraticDiscriminationAnalysis algoritmusokat, √©s vizualiz√°lja az eredm√©nyeket:

![oszt√°lyoz√≥k √∂sszehasonl√≠t√°sa](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Diagramok a Scikit-learn dokument√°ci√≥j√°b√≥l

> Az AutoML eleg√°nsan megoldja ezt a probl√©m√°t az√°ltal, hogy ezeket az √∂sszehasonl√≠t√°sokat a felh≈ëben futtatja, lehet≈ëv√© t√©ve, hogy kiv√°laszd a legjobb algoritmust az adataidhoz. Pr√≥b√°ld ki [itt](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### Egy jobb megk√∂zel√≠t√©s

Egy jobb m√≥dszer, mint a vad tal√°lgat√°s, az, hogy k√∂vetj√ºk az √∂tleteket ezen let√∂lthet≈ë [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott) alapj√°n. Itt felfedezz√ºk, hogy a t√∂bboszt√°lyos probl√©m√°nkhoz van n√©h√°ny v√°laszt√°si lehet≈ës√©g:

![cheatsheet t√∂bboszt√°lyos probl√©m√°khoz](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> A Microsoft Algoritmus Cheat Sheet egy szakasza, amely r√©szletezi a t√∂bboszt√°lyos oszt√°lyoz√°si lehet≈ës√©geket

‚úÖ T√∂ltsd le ezt a cheat sheetet, nyomtasd ki, √©s tedd ki a faladra!

### √ârvel√©s

N√©zz√ºk meg, hogy az adott korl√°tok alapj√°n milyen megk√∂zel√≠t√©seket v√°laszthatunk:

- **A neur√°lis h√°l√≥zatok t√∫l nehezek**. Tekintve, hogy az adat√°llom√°nyunk tiszta, de minim√°lis, √©s hogy a tan√≠t√°st helyben, notebookokon kereszt√ºl v√©gezz√ºk, a neur√°lis h√°l√≥zatok t√∫l neh√©zkesek ehhez a feladathoz.
- **Nincs k√©toszt√°lyos oszt√°lyoz√≥**. Nem haszn√°lunk k√©toszt√°lyos oszt√°lyoz√≥t, √≠gy az egy-vs-minden kiz√°rhat√≥.
- **D√∂nt√©si fa vagy logisztikus regresszi√≥ m≈±k√∂dhet**. Egy d√∂nt√©si fa m≈±k√∂dhet, vagy logisztikus regresszi√≥ t√∂bboszt√°lyos adatokhoz.
- **T√∂bboszt√°lyos Boosted Decision Trees m√°s probl√©m√°t old meg**. A t√∂bboszt√°lyos Boosted Decision Tree legink√°bb nemparam√©teres feladatokhoz alkalmas, p√©ld√°ul rangsorok l√©trehoz√°s√°ra, √≠gy sz√°munkra nem hasznos.

### Scikit-learn haszn√°lata 

A Scikit-learn-t fogjuk haszn√°lni az adataink elemz√©s√©re. Azonban sz√°mos m√≥dja van annak, hogy logisztikus regresszi√≥t haszn√°ljunk a Scikit-learn-ben. N√©zd meg a [param√©tereket](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression), amelyeket megadhatsz.

L√©nyeg√©ben k√©t fontos param√©ter van - `multi_class` √©s `solver` -, amelyeket meg kell hat√°roznunk, amikor arra k√©rj√ºk a Scikit-learn-t, hogy v√©gezzen logisztikus regresszi√≥t. A `multi_class` √©rt√©k egy bizonyos viselked√©st alkalmaz. A solver √©rt√©ke az algoritmus, amelyet haszn√°lni kell. Nem minden solver p√°ros√≠that√≥ minden `multi_class` √©rt√©kkel.

A dokument√°ci√≥ szerint a t√∂bboszt√°lyos esetben a tan√≠t√°si algoritmus:

- **Az egy-vs-minden (OvR) s√©m√°t haszn√°lja**, ha a `multi_class` opci√≥ `ovr`-re van √°ll√≠tva
- **A keresztentr√≥pia vesztes√©get haszn√°lja**, ha a `multi_class` opci√≥ `multinomial`-ra van √°ll√≠tva. (Jelenleg a `multinomial` opci√≥t csak az ‚Äòlbfgs‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô √©s ‚Äònewton-cg‚Äô solvers t√°mogatj√°k.)

> üéì A 's√©ma' itt lehet 'ovr' (egy-vs-minden) vagy 'multinomial'. Mivel a logisztikus regresszi√≥ val√≥j√°ban bin√°ris oszt√°lyoz√°s t√°mogat√°s√°ra lett tervezve, ezek a s√©m√°k lehet≈ëv√© teszik, hogy jobban kezelje a t√∂bboszt√°lyos oszt√°lyoz√°si feladatokat. [forr√°s](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> üéì A 'solver' √∫gy van defini√°lva, mint "az algoritmus, amelyet az optimaliz√°l√°si probl√©m√°ban haszn√°lni kell". [forr√°s](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

A Scikit-learn ezt a t√°bl√°zatot k√≠n√°lja, hogy megmagyar√°zza, hogyan kezelik a solvers a k√ºl√∂nb√∂z≈ë kih√≠v√°sokat, amelyeket a k√ºl√∂nb√∂z≈ë adatstrukt√∫r√°k jelentenek:

![solvers](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## Gyakorlat - az adatok feloszt√°sa

Koncentr√°ljunk a logisztikus regresszi√≥ra az els≈ë tan√≠t√°si pr√≥b√°nk sor√°n, mivel nemr√©g tanult√°l r√≥la egy kor√°bbi leck√©ben.
Oszd fel az adataidat tan√≠t√°si √©s tesztel√©si csoportokra a `train_test_split()` h√≠v√°s√°val:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## Gyakorlat - logisztikus regresszi√≥ alkalmaz√°sa

Mivel a t√∂bboszt√°lyos esetet haszn√°lod, ki kell v√°lasztanod, hogy milyen _s√©m√°t_ √©s milyen _solvert_ √°ll√≠ts be. Haszn√°lj LogisticRegression-t t√∂bboszt√°lyos be√°ll√≠t√°ssal √©s a **liblinear** solverrel a tan√≠t√°shoz.

1. Hozz l√©tre egy logisztikus regresszi√≥t, ahol a multi_class `ovr`-re van √°ll√≠tva, √©s a solver `liblinear`-re:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    ‚úÖ Pr√≥b√°lj ki egy m√°sik solvert, p√©ld√°ul `lbfgs`, amelyet gyakran alap√©rtelmezettk√©nt √°ll√≠tanak be
> Megjegyz√©s: Haszn√°lja a Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) f√ºggv√©nyt az adatok lap√≠t√°s√°hoz, amikor sz√ºks√©ges.
A pontoss√°g **80%** felett j√≥!

1. Ezt a modellt m≈±k√∂d√©s k√∂zben l√°thatod, ha tesztelsz egy adat sort (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Az eredm√©ny ki van nyomtatva:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   ‚úÖ Pr√≥b√°lj ki egy m√°sik sor sz√°mot, √©s ellen≈ërizd az eredm√©nyeket.

1. M√©lyebbre √°sva ellen≈ërizheted ennek az el≈ërejelz√©snek a pontoss√°g√°t:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Az eredm√©ny ki van nyomtatva - az indiai konyha a legjobb tippje, j√≥ val√≥sz√≠n≈±s√©ggel:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    ‚úÖ Meg tudod magyar√°zni, mi√©rt gondolja a modell, hogy ez biztosan indiai konyha?

1. Szerezz t√∂bb r√©szletet egy oszt√°lyoz√°si jelent√©s kinyomtat√°s√°val, ahogy a regresszi√≥s leck√©kben tetted:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | pontoss√°g | visszah√≠v√°s | f1-√©rt√©k | t√°mogat√°s |
    | ------------ | --------- | ----------- | -------- | --------- |
    | chinese      | 0.73      | 0.71        | 0.72     | 229       |
    | indian       | 0.91      | 0.93        | 0.92     | 254       |
    | japanese     | 0.70      | 0.75        | 0.72     | 220       |
    | korean       | 0.86      | 0.76        | 0.81     | 242       |
    | thai         | 0.79      | 0.85        | 0.82     | 254       |
    | pontoss√°g    | 0.80      | 1199        |          |           |
    | makro √°tlag  | 0.80      | 0.80        | 0.80     | 1199      |
    | s√∫lyozott √°tlag | 0.80   | 0.80        | 0.80     | 1199      |

## üöÄKih√≠v√°s

Ebben a leck√©ben a megtiszt√≠tott adataidat haszn√°ltad egy g√©pi tanul√°si modell fel√©p√≠t√©s√©hez, amely k√©pes nemzeti konyh√°t el≈ëre jelezni egy sor √∂sszetev≈ë alapj√°n. Sz√°nj id≈ët arra, hogy √°tn√©zd a Scikit-learn √°ltal k√≠n√°lt sz√°mos lehet≈ës√©get az adatok oszt√°lyoz√°s√°ra. √Åss m√©lyebbre a 'solver' fogalm√°ba, hogy meg√©rtsd, mi zajlik a h√°tt√©rben.

## [El≈ëad√°s ut√°ni kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

√Åss egy kicsit m√©lyebbre a logisztikus regresszi√≥ matematik√°j√°ban [ebben a leck√©ben](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## Feladat 

[Tanulm√°nyozd a solvereket](assignment.md)

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s, a [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.