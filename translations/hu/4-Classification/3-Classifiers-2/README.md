<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49047911108adc49d605cddfb455749c",
  "translation_date": "2025-09-05T16:23:30+00:00",
  "source_file": "4-Classification/3-Classifiers-2/README.md",
  "language_code": "hu"
}
-->
# Konyhai oszt√°lyoz√≥k 2

Ebben a m√°sodik oszt√°lyoz√°si leck√©ben tov√°bbi m√≥dszereket fedezhetsz fel a numerikus adatok oszt√°lyoz√°s√°ra. Megismerheted azt is, hogy milyen k√∂vetkezm√©nyekkel j√°r, ha egyik oszt√°lyoz√≥t v√°lasztod a m√°sik helyett.

## [El≈ëzetes kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

### El≈ëfelt√©tel

Felt√©telezz√ºk, hogy elv√©gezted az el≈ëz≈ë leck√©ket, √©s van egy megtiszt√≠tott adat√°llom√°nyod a `data` mapp√°ban, amely _cleaned_cuisines.csv_ n√©ven tal√°lhat√≥ a 4-leck√©s mappa gy√∂k√©rk√∂nyvt√°r√°ban.

### El≈ëk√©sz√ºlet

Bet√∂lt√∂tt√ºk a _notebook.ipynb_ f√°jlodat a megtiszt√≠tott adat√°llom√°nnyal, √©s X √©s y adatkeretekre osztottuk, k√©szen √°llva a modell√©p√≠t√©si folyamatra.

## Egy oszt√°lyoz√°si t√©rk√©p

Kor√°bban megismerkedt√©l a k√ºl√∂nb√∂z≈ë lehet≈ës√©gekkel, amelyekkel adatokat oszt√°lyozhatsz a Microsoft csal√≥lapja seg√≠ts√©g√©vel. A Scikit-learn egy hasonl√≥, de r√©szletesebb csal√≥lapot k√≠n√°l, amely tov√°bb seg√≠thet az oszt√°lyoz√≥k (m√°s n√©ven becsl≈ëk) sz≈±k√≠t√©s√©ben:

![ML t√©rk√©p a Scikit-learn-t≈ël](../../../../4-Classification/3-Classifiers-2/images/map.png)
> Tipp: [n√©zd meg ezt a t√©rk√©pet online](https://scikit-learn.org/stable/tutorial/machine_learning_map/), √©s kattints az √∫tvonalakon, hogy elolvashasd a dokument√°ci√≥t.

### A terv

Ez a t√©rk√©p nagyon hasznos, ha tiszt√°ban vagy az adataiddal, mivel ‚Äûv√©gigj√°rhatod‚Äù az √∫tvonalait, hogy d√∂nt√©st hozz:

- T√∂bb mint 50 mint√°nk van
- Kateg√≥ri√°t szeretn√©nk el≈ëre jelezni
- C√≠mk√©zett adataink vannak
- Kevesebb mint 100 ezer mint√°nk van
- ‚ú® V√°laszthatunk egy Linear SVC-t
- Ha ez nem m≈±k√∂dik, mivel numerikus adataink vannak
    - Kipr√≥b√°lhatunk egy ‚ú® KNeighbors Classifiert 
      - Ha ez sem m≈±k√∂dik, pr√≥b√°ljuk ki a ‚ú® SVC-t √©s ‚ú® Ensemble Classifiert

Ez egy nagyon hasznos √∫tvonal, amit k√∂vethet√ºnk.

## Gyakorlat - az adatok feloszt√°sa

Ezt az √∫tvonalat k√∂vetve kezdj√ºk azzal, hogy import√°lunk n√©h√°ny sz√ºks√©ges k√∂nyvt√°rat.

1. Import√°ld a sz√ºks√©ges k√∂nyvt√°rakat:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Oszd fel a tanul√≥ √©s tesztadatokat:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC oszt√°lyoz√≥

A Support-Vector clustering (SVC) a Support-Vector g√©pek ML technik√°inak csal√°dj√°ba tartozik (tov√°bbi inform√°ci√≥k al√°bb). Ebben a m√≥dszerben egy 'kernel'-t v√°laszthatsz, amely meghat√°rozza, hogyan csoportos√≠tja a c√≠mk√©ket. A 'C' param√©ter a 'regulariz√°ci√≥t' jelenti, amely szab√°lyozza a param√©terek hat√°s√°t. A kernel lehet [t√∂bbf√©le](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); itt 'linear'-re √°ll√≠tjuk, hogy line√°ris SVC-t haszn√°ljunk. Az alap√©rtelmezett val√≥sz√≠n≈±s√©g 'false'; itt 'true'-ra √°ll√≠tjuk, hogy val√≥sz√≠n≈±s√©gi becsl√©seket kapjunk. A random state '0'-ra van √°ll√≠tva, hogy az adatokat keverj√ºk a val√≥sz√≠n≈±s√©gek el√©r√©s√©hez.

### Gyakorlat - alkalmazz line√°ris SVC-t

Kezdj egy oszt√°lyoz√≥k t√∂mbj√©nek l√©trehoz√°s√°val. Ehhez fokozatosan hozz√°adunk elemeket, ahogy tesztel√ºnk.

1. Kezdj egy Linear SVC-vel:

    ```python
    C = 10
    # Create different classifiers.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Tan√≠tsd be a modelledet a Linear SVC-vel, √©s nyomtass ki egy jelent√©st:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Az eredm√©ny el√©g j√≥:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors oszt√°lyoz√≥

A K-Neighbors az ML m√≥dszerek "szomsz√©dok" csal√°dj√°ba tartozik, amelyeket fel√ºgyelt √©s nem fel√ºgyelt tanul√°sra is lehet haszn√°lni. Ebben a m√≥dszerben el≈ëre meghat√°rozott sz√°m√∫ pontot hozunk l√©tre, √©s az adatok ezek k√∂r√© gy≈±lnek, hogy √°ltal√°nos√≠tott c√≠mk√©ket lehessen el≈ëre jelezni az adatokhoz.

### Gyakorlat - alkalmazd a K-Neighbors oszt√°lyoz√≥t

Az el≈ëz≈ë oszt√°lyoz√≥ j√≥ volt, √©s j√≥l m≈±k√∂d√∂tt az adatokkal, de tal√°n jobb pontoss√°got √©rhet√ºnk el. Pr√≥b√°lj ki egy K-Neighbors oszt√°lyoz√≥t.

1. Adj hozz√° egy sort az oszt√°lyoz√≥k t√∂mbj√©hez (tegy√©l vessz≈ët a Linear SVC elem ut√°n):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Az eredm√©ny kicsit rosszabb:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ Tudj meg t√∂bbet a [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors) m√≥dszerr≈ël.

## Support Vector oszt√°lyoz√≥

A Support-Vector oszt√°lyoz√≥k az ML m√≥dszerek [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) csal√°dj√°ba tartoznak, amelyeket oszt√°lyoz√°si √©s regresszi√≥s feladatokra haszn√°lnak. Az SVM-ek "a tanul√≥ p√©ld√°kat pontokk√° t√©rk√©pezik az ≈±rben", hogy maximaliz√°lj√°k a t√°vols√°got k√©t kateg√≥ria k√∂z√∂tt. A k√©s≈ëbbi adatok ebbe az ≈±rbe ker√ºlnek, hogy el≈ëre jelezz√©k a kateg√≥ri√°jukat.

### Gyakorlat - alkalmazz Support Vector oszt√°lyoz√≥t

Pr√≥b√°ljunk meg egy kicsit jobb pontoss√°got el√©rni egy Support Vector oszt√°lyoz√≥val.

1. Tegy√©l vessz≈ët a K-Neighbors elem ut√°n, majd add hozz√° ezt a sort:

    ```python
    'SVC': SVC(),
    ```

    Az eredm√©ny el√©g j√≥!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ Tudj meg t√∂bbet a [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm) m√≥dszerr≈ël.

## Ensemble oszt√°lyoz√≥k

K√∂vess√ºk az √∫tvonalat eg√©szen a v√©g√©ig, m√©g akkor is, ha az el≈ëz≈ë teszt el√©g j√≥ volt. Pr√≥b√°ljunk ki n√©h√°ny 'Ensemble oszt√°lyoz√≥t', k√ºl√∂n√∂sen a Random Forest √©s AdaBoost m√≥dszereket:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Az eredm√©ny nagyon j√≥, k√ºl√∂n√∂sen a Random Forest eset√©ben:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ Tudj meg t√∂bbet az [Ensemble oszt√°lyoz√≥kr√≥l](https://scikit-learn.org/stable/modules/ensemble.html).

Ez a g√©pi tanul√°si m√≥dszer "t√∂bb alapbecsl≈ë el≈ërejelz√©seit kombin√°lja", hogy jav√≠tsa a modell min≈ës√©g√©t. P√©ld√°nkban Random Trees √©s AdaBoost m√≥dszereket haszn√°ltunk.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), egy √°tlagol√°si m√≥dszer, amely 'd√∂nt√©si f√°k' 'erd≈ëj√©t' √©p√≠ti fel v√©letlenszer≈±s√©ggel, hogy elker√ºlje a t√∫ltanul√°st. Az n_estimators param√©ter a f√°k sz√°m√°t hat√°rozza meg.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) egy oszt√°lyoz√≥t illeszt az adat√°llom√°nyhoz, majd ennek m√°solatait illeszti ugyanarra az adat√°llom√°nyra. Azokra az elemekre √∂sszpontos√≠t, amelyeket helytelen√ºl oszt√°lyoztak, √©s a k√∂vetkez≈ë oszt√°lyoz√≥ illeszt√©s√©t √∫gy √°ll√≠tja be, hogy jav√≠tsa azokat.

---

## üöÄKih√≠v√°s

Ezeknek a technik√°knak sz√°mos param√©tere van, amelyeket m√≥dos√≠thatsz. Kutass ut√°na mindegyik alap√©rtelmezett param√©tereinek, √©s gondold √°t, hogy ezek m√≥dos√≠t√°sa mit jelentene a modell min≈ës√©g√©re n√©zve.

## [Ut√≥lagos kv√≠z](https://ff-quizzes.netlify.app/en/ml/)

## √Åttekint√©s √©s √∂n√°ll√≥ tanul√°s

Ezekben a leck√©kben sok a szakzsargon, ez√©rt sz√°nj egy percet arra, hogy √°tn√©zd [ezt a list√°t](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) a hasznos terminol√≥gi√°r√≥l!

## Feladat 

[Param√©terek j√°t√©ka](assignment.md)

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Kritikus inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.