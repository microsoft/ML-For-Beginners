<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9660fbd80845c59c15715cb418cd6e23",
  "translation_date": "2025-09-03T18:38:52+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "lt"
}
-->
# CartPole ÄŒiuoÅ¾imas

Problema, kuriÄ… sprendÄ—me ankstesnÄ—je pamokoje, gali atrodyti kaip Å¾aislinÄ— problema, neturinti realaus pritaikymo gyvenime. TaÄiau taip nÄ—ra, nes daugelis realaus pasaulio problemÅ³ taip pat turi panaÅ¡iÄ… struktÅ«rÄ… â€“ Ä¯skaitant Å¡achmatÅ³ ar Go Å¾aidimÄ…. Jos yra panaÅ¡ios, nes taip pat turime lentÄ… su nustatytomis taisyklÄ—mis ir **diskreÄia bÅ«sena**.

## [PrieÅ¡ paskaitÄ… atlikite testÄ…](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)

## Ä®vadas

Å ioje pamokoje taikysime tuos paÄius Q-mokymosi principus problemai su **tÄ™stine bÅ«sena**, t. y. bÅ«sena, kuriÄ… apibrÄ—Å¾ia vienas ar daugiau realiÅ³jÅ³ skaiÄiÅ³. SprÄ™sime Å¡iÄ… problemÄ…:

> **Problema**: Jei Petras nori pabÄ—gti nuo vilko, jis turi iÅ¡mokti judÄ—ti greiÄiau. PaÅ¾iÅ«rÄ—sime, kaip Petras gali iÅ¡mokti ÄiuoÅ¾ti, ypaÄ iÅ¡laikyti pusiausvyrÄ…, naudojant Q-mokymÄ…si.

![Didysis pabÄ—gimas!](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.lt.png)

> Petras ir jo draugai tampa kÅ«rybingi, norÄ—dami pabÄ—gti nuo vilko! Iliustracija: [Jen Looper](https://twitter.com/jenlooper)

Naudosime supaprastintÄ… pusiausvyros iÅ¡laikymo versijÄ…, vadinamÄ… **CartPole** problema. CartPole pasaulyje turime horizontalÅ³ slankiklÄ¯, kuris gali judÄ—ti Ä¯ kairÄ™ arba Ä¯ deÅ¡inÄ™, o tikslas yra iÅ¡laikyti vertikaliÄ… lazdÄ… ant slankiklio.

## Reikalavimai

Å ioje pamokoje naudosime bibliotekÄ… **OpenAI Gym**, skirtÄ… Ä¯vairiÅ³ **aplinkÅ³** simuliacijai. Pamokos kodÄ… galite vykdyti lokaliai (pvz., Visual Studio Code), tokiu atveju simuliacija bus atidaryta naujame lange. Jei vykdote kodÄ… internete, gali reikÄ—ti atlikti tam tikrus pakeitimus, kaip apraÅ¡yta [Äia](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

AnkstesnÄ—je pamokoje Å¾aidimo taisyklÄ—s ir bÅ«sena buvo apibrÄ—Å¾tos mÅ«sÅ³ paÄiÅ³ sukurtoje `Board` klasÄ—je. Å ioje pamokoje naudosime specialiÄ… **simuliacijos aplinkÄ…**, kuri simuliuos fizikos dÄ—snius, susijusius su lazdos balansavimu. Viena populiariausiÅ³ simuliacijos aplinkÅ³, skirtÅ³ stiprinamojo mokymosi algoritmams, vadinama [Gym](https://gym.openai.com/), kuriÄ… priÅ¾iÅ«ri [OpenAI](https://openai.com/). Naudodami Å¡iÄ… aplinkÄ… galime kurti Ä¯vairias **aplinkas**, nuo CartPole simuliacijos iki Atari Å¾aidimÅ³.

> **Pastaba**: Kitas OpenAI Gym aplinkas galite pamatyti [Äia](https://gym.openai.com/envs/#classic_control).

Pirmiausia Ä¯diekime Gym ir importuokime reikalingas bibliotekas (kodo blokas 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## UÅ¾duotis â€“ inicializuoti CartPole aplinkÄ…

NorÄ—dami dirbti su CartPole balansavimo problema, turime inicializuoti atitinkamÄ… aplinkÄ…. Kiekviena aplinka turi:

- **StebÄ—jimÅ³ erdvÄ™**, kuri apibrÄ—Å¾ia informacijÄ…, kuriÄ… gauname iÅ¡ aplinkos. CartPole problemai gauname lazdos padÄ—tÄ¯, greitÄ¯ ir kitus parametrus.

- **VeiksmÅ³ erdvÄ™**, kuri apibrÄ—Å¾ia galimus veiksmus. MÅ«sÅ³ atveju veiksmÅ³ erdvÄ— yra diskreti, ir susideda iÅ¡ dviejÅ³ veiksmÅ³ â€“ **kairÄ—** ir **deÅ¡inÄ—**. (kodo blokas 2)

1. Inicializuokite aplinkÄ… naudodami Å¡Ä¯ kodÄ…:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

NorÄ—dami pamatyti, kaip veikia aplinka, paleiskime trumpÄ… simuliacijÄ…, trunkanÄiÄ… 100 Å¾ingsniÅ³. Kiekviename Å¾ingsnyje pateikiame veiksmÄ…, kurÄ¯ reikia atlikti â€“ Å¡ioje simuliacijoje atsitiktinai pasirenkame veiksmÄ… iÅ¡ `action_space`.

1. Paleiskite Å¾emiau pateiktÄ… kodÄ… ir paÅ¾iÅ«rÄ—kite, kas nutiks.

    âœ… Atminkite, kad Å¡Ä¯ kodÄ… geriausia vykdyti lokaliai, naudojant Python! (kodo blokas 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    TurÄ—tumÄ—te matyti kaÅ¾kÄ… panaÅ¡aus Ä¯ Å¡Ä¯ vaizdÄ…:

    ![nebalansuojantis CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Simuliacijos metu turime gauti stebÄ—jimus, kad nusprÄ™stume, kaip veikti. IÅ¡ tiesÅ³, `step` funkcija grÄ…Å¾ina dabartinius stebÄ—jimus, atlygio funkcijÄ… ir `done` vÄ—liavÄ—lÄ™, kuri nurodo, ar verta tÄ™sti simuliacijÄ…: (kodo blokas 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    JÅ«sÅ³ uÅ¾raÅ¡Å³ knygelÄ—s iÅ¡vestyje turÄ—tÅ³ bÅ«ti kaÅ¾kas panaÅ¡aus Ä¯ tai:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    StebÄ—jimÅ³ vektorius, kuris grÄ…Å¾inamas kiekviename simuliacijos Å¾ingsnyje, apima Å¡ias reikÅ¡mes:
    - VeÅ¾imÄ—lio padÄ—tis
    - VeÅ¾imÄ—lio greitis
    - Lazdos kampas
    - Lazdos sukimosi greitis

1. Gaukite Å¡iÅ³ skaiÄiÅ³ minimaliÄ… ir maksimaliÄ… reikÅ¡mes: (kodo blokas 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Taip pat galite pastebÄ—ti, kad atlygio reikÅ¡mÄ— kiekviename simuliacijos Å¾ingsnyje visada yra 1. Taip yra todÄ—l, kad mÅ«sÅ³ tikslas yra iÅ¡gyventi kuo ilgiau, t. y. iÅ¡laikyti lazdÄ… pakankamai vertikalioje padÄ—tyje kuo ilgesnÄ¯ laikÄ….

    âœ… IÅ¡ tiesÅ³, CartPole simuliacija laikoma iÅ¡sprÄ™sta, jei mums pavyksta gauti vidutinÄ¯ 195 atlygÄ¯ per 100 iÅ¡ eilÄ—s vykdomÅ³ bandymÅ³.

## BÅ«senos diskretizavimas

Q-mokymesi turime sukurti Q-lentelÄ™, kuri nurodo, kÄ… daryti kiekvienoje bÅ«senoje. NorÄ—dami tai padaryti, bÅ«sena turi bÅ«ti **diskreti**, tiksliau, ji turi turÄ—ti baigtinÄ¯ diskreÄiÅ³ reikÅ¡miÅ³ skaiÄiÅ³. TodÄ—l turime kaÅ¾kaip **diskretizuoti** savo stebÄ—jimus, susiedami juos su baigtiniu bÅ«senÅ³ rinkiniu.

Yra keletas bÅ«dÅ³, kaip tai padaryti:

- **Padalijimas Ä¯ intervalus**. Jei Å¾inome tam tikros reikÅ¡mÄ—s intervalÄ…, galime padalyti Å¡Ä¯ intervalÄ… Ä¯ tam tikrÄ… skaiÄiÅ³ **intervalÅ³** ir tada pakeisti reikÅ¡mÄ™ Ä¯ intervalÄ…, kuriam ji priklauso. Tai galima padaryti naudojant numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) metodÄ…. Tokiu atveju tiksliai Å¾inosime bÅ«senos dydÄ¯, nes jis priklausys nuo pasirinktÅ³ intervalÅ³ skaiÄiaus.

âœ… Galime naudoti linijinÄ™ interpolacijÄ…, kad reikÅ¡mes pritaikytume tam tikram baigtiniam intervalui (pvz., nuo -20 iki 20), o tada konvertuoti skaiÄius Ä¯ sveikuosius skaiÄius juos suapvalinant. Tai suteikia maÅ¾iau kontrolÄ—s bÅ«senos dydÅ¾iui, ypaÄ jei neÅ¾inome tiksliÅ³ Ä¯vesties reikÅ¡miÅ³ ribÅ³. PavyzdÅ¾iui, mÅ«sÅ³ atveju 2 iÅ¡ 4 reikÅ¡miÅ³ neturi virÅ¡utiniÅ³/apatiniÅ³ ribÅ³, o tai gali lemti begalinÄ¯ bÅ«senÅ³ skaiÄiÅ³.

MÅ«sÅ³ pavyzdyje naudosime antrÄ…jÄ¯ metodÄ…. Kaip pastebÄ—site vÄ—liau, nepaisant neapibrÄ—Å¾tÅ³ ribÅ³, Å¡ios reikÅ¡mÄ—s retai virÅ¡ija tam tikrus baigtinius intervalus, todÄ—l bÅ«senos su ekstremaliomis reikÅ¡mÄ—mis bus labai retos.

1. Å tai funkcija, kuri paims mÅ«sÅ³ modelio stebÄ—jimus ir sukurs 4 sveikÅ³jÅ³ skaiÄiÅ³ tuple: (kodo blokas 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Taip pat iÅ¡bandykime kitÄ… diskretizavimo metodÄ…, naudodami intervalus: (kodo blokas 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Dabar paleiskime trumpÄ… simuliacijÄ… ir stebÄ—kime Å¡ias diskretizuotas aplinkos reikÅ¡mes. Galite iÅ¡bandyti tiek `discretize`, tiek `discretize_bins` ir paÅ¾iÅ«rÄ—ti, ar yra skirtumas.

    âœ… `discretize_bins` grÄ…Å¾ina intervalo numerÄ¯, kuris prasideda nuo 0. TodÄ—l Ä¯vesties kintamojo reikÅ¡mÄ—ms, esanÄioms apie 0, jis grÄ…Å¾ina skaiÄiÅ³ iÅ¡ intervalo vidurio (10). `discretize` funkcijoje mums nerÅ«pÄ—jo iÅ¡vesties reikÅ¡miÅ³ diapazonas, leidÅ¾iant joms bÅ«ti neigiamoms, todÄ—l bÅ«senos reikÅ¡mÄ—s nÄ—ra perstumtos, o 0 atitinka 0. (kodo blokas 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Atkomentuokite eilutÄ™, prasidedanÄiÄ… `env.render`, jei norite pamatyti, kaip vykdoma aplinka. PrieÅ¡ingu atveju galite vykdyti jÄ… fone, kas yra greiÄiau. Å Ä¯ â€nematomÄ…â€œ vykdymÄ… naudosime Q-mokymosi procese.

## Q-lentelÄ—s struktÅ«ra

AnkstesnÄ—je pamokoje bÅ«sena buvo paprasta pora skaiÄiÅ³ nuo 0 iki 8, todÄ—l buvo patogu Q-lentelÄ™ atvaizduoti kaip numpy tensorÄ…, kurio forma yra 8x8x2. Jei naudojame intervalÅ³ diskretizavimÄ…, mÅ«sÅ³ bÅ«senos vektoriaus dydis taip pat yra Å¾inomas, todÄ—l galime naudoti tÄ… patÄ¯ metodÄ… ir bÅ«senÄ… atvaizduoti kaip masyvÄ…, kurio forma yra 20x20x10x10x2 (Äia 2 yra veiksmÅ³ erdvÄ—s dimensija, o pirmosios dimensijos atitinka pasirinktÅ³ intervalÅ³ skaiÄiÅ³ kiekvienam stebÄ—jimÅ³ erdvÄ—s parametrui).

TaÄiau kartais stebÄ—jimÅ³ erdvÄ—s tikslios dimensijos nÄ—ra Å¾inomos. Naudojant `discretize` funkcijÄ…, niekada negalime bÅ«ti tikri, kad mÅ«sÅ³ bÅ«sena iÅ¡liks tam tikrose ribose, nes kai kurios pradinÄ—s reikÅ¡mÄ—s nÄ—ra apribotos. TodÄ—l naudosime Å¡iek tiek kitokÄ¯ metodÄ… ir Q-lentelÄ™ atvaizduosime kaip Å¾odynÄ….

1. Naudokite porÄ… *(bÅ«sena, veiksmas)* kaip Å¾odyno raktÄ…, o reikÅ¡mÄ— atitiks Q-lentelÄ—s Ä¯raÅ¡o reikÅ¡mÄ™. (kodo blokas 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    ÄŒia taip pat apibrÄ—Å¾iame funkcijÄ… `qvalues()`, kuri grÄ…Å¾ina Q-lentelÄ—s reikÅ¡miÅ³ sÄ…raÅ¡Ä…, atitinkantÄ¯ visus galimus veiksmus tam tikroje bÅ«senoje. Jei Ä¯raÅ¡as nÄ—ra Q-lentelÄ—je, grÄ…Å¾insime 0 kaip numatytÄ…jÄ… reikÅ¡mÄ™.

## PradÄ—kime Q-mokymÄ…si

Dabar esame pasiruoÅ¡Ä™ iÅ¡mokyti PetrÄ… iÅ¡laikyti pusiausvyrÄ…!

1. Pirmiausia nustatykime keletÄ… hiperparametrÅ³: (kodo blokas 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    ÄŒia `alpha` yra **mokymosi greitis**, kuris nurodo, kiek turÄ—tume koreguoti dabartines Q-lentelÄ—s reikÅ¡mes kiekviename Å¾ingsnyje. AnkstesnÄ—je pamokoje pradÄ—jome nuo 1, o vÄ—liau maÅ¾inome `alpha` mokymo metu. Å iame pavyzdyje iÅ¡laikysime jÄ¯ pastovÅ³ dÄ—l paprastumo, taÄiau vÄ—liau galite eksperimentuoti su `alpha` reikÅ¡mÄ—mis.

    `gamma` yra **nuolaidos faktorius**, kuris parodo, kiek turÄ—tume teikti pirmenybÄ™ bÅ«simam atlygiui, palyginti su dabartiniu atlygiu.

    `epsilon` yra **tyrinÄ—jimo/naudojimo faktorius**, kuris nustato, ar turÄ—tume teikti pirmenybÄ™ tyrinÄ—jimui, ar naudojimui. MÅ«sÅ³ algoritme `epsilon` procentais atvejÅ³ pasirinksime kitÄ… veiksmÄ… pagal Q-lentelÄ—s reikÅ¡mes, o likusiais atvejais vykdysime atsitiktinÄ¯ veiksmÄ…. Tai leis mums iÅ¡tirti paieÅ¡kos erdvÄ—s sritis, kuriÅ³ dar nematÄ—me.

    âœ… Kalbant apie balansavimÄ… â€“ atsitiktinio veiksmo pasirinkimas (tyrinÄ—jimas) veiktÅ³ kaip atsitiktinis smÅ«gis netinkama kryptimi, ir lazda turÄ—tÅ³ iÅ¡mokti, kaip atgauti pusiausvyrÄ… po tokiÅ³ â€klaidÅ³â€œ.

### Tobulinkime algoritmÄ…

Taip pat galime atlikti du patobulinimus mÅ«sÅ³ algoritmui iÅ¡ ankstesnÄ—s pamokos:

- **ApskaiÄiuoti vidutinÄ¯ kaupiamÄ…jÄ¯ atlygÄ¯** per tam tikrÄ… simuliacijÅ³ skaiÄiÅ³. Spausdinsime paÅ¾angÄ… kas 5000 iteracijÅ³ ir vidurkinsime kaupiamÄ…jÄ¯ atlygÄ¯ per tÄ… laikotarpÄ¯. Tai reiÅ¡kia, kad jei gausime daugiau nei 195 taÅ¡kus â€“ galime laikyti problemÄ… iÅ¡sprÄ™sta, net su aukÅ¡tesne kokybe nei reikalaujama.

- **ApskaiÄiuoti maksimalÅ³ vidutinÄ¯ kaupiamÄ…jÄ¯ rezultatÄ…**, `Qmax`, ir iÅ¡saugosime Q-lentelÄ™, atitinkanÄiÄ… tÄ… rezultatÄ…. Kai paleisite mokymÄ…, pastebÄ—site, kad kartais vidutinis kaupiamasis rezultatas pradeda maÅ¾Ä—ti, ir norime iÅ¡saugoti Q-lentelÄ—s reikÅ¡mes, atitinkanÄias geriausiÄ… modelÄ¯, pastebÄ—tÄ… mokymo metu.

1. Surinkite visus kaupiamuosius atlygius kiekvienoje simuliacijoje Ä¯ `rewards` vektoriÅ³, kad galÄ—tumÄ—te juos vÄ—liau pavaizduoti grafike. (kodo blokas 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

KÄ… galite pastebÄ—ti iÅ¡ Å¡iÅ³ rezultatÅ³:

- **ArtÄ—jame prie tikslo**. Esame labai arti tikslo â€“ gauti 195 kaupiamuosius atlygius per 100+ iÅ¡ eilÄ—s vykdomÅ³ simuliacijÅ³, arba galbÅ«t jau jÄ¯ pasiekÄ—me! Net jei gauname maÅ¾esnius skaiÄius, vis tiek neÅ¾inome, nes vidurkiname per 5000 vykdymÅ³, o oficialiame kriterijuje reikalaujama tik 100 vykdymÅ³.

- **Atlygis pradeda maÅ¾Ä—ti**. Kartais atlygis pradeda maÅ¾Ä—ti, o tai reiÅ¡kia, kad galime â€sugadintiâ€œ jau iÅ¡moktas Q-lentelÄ—s reikÅ¡mes naujomis, kurios pablogina situacijÄ….

Å is pastebÄ—jimas tampa aiÅ¡kesnis, jei pavaizduojame mokymo paÅ¾angÄ… grafike.

## Mokymo paÅ¾angos vaizdavimas

Mokymo metu surinkome kaupiamojo atlygio reikÅ¡mes kiekvienoje iteracijoje Ä¯ `rewards` vektoriÅ³. Å tai kaip tai atrodo, kai pavaizduojame prieÅ¡ iteracijÅ³ skaiÄiÅ³:

```python
plt.plot(rewards)
```

![neapdorota paÅ¾anga](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.lt.png)

IÅ¡ Å¡io grafiko sunku kÄ… nors pasakyti, nes dÄ—l stochastinio mokymo proceso treniruoÄiÅ³ sesijÅ³ trukmÄ— labai skiriasi. Kad Å¡is grafikas bÅ«tÅ³ prasmingesnis, galime apskaiÄiuoti **slankÅ³jÄ¯ vidurkÄ¯** per serijÄ… eksperimentÅ³, tarkime, 100. Tai galima patogiai atlikti naudojant `np.convolve`: (kodo blokas 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![mokymo paÅ¾anga](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.lt.png)

## HiperparametrÅ³ keitimas

Norint, kad mokymasis bÅ«tÅ³ stabilesnis, verta reguliuoti kai kuriuos hiperparametrus mokymo metu. KonkreÄiai:

- **Mokymosi greiÄiui**, `alpha`, galime pradÄ—ti nuo reikÅ¡miÅ³, artimÅ³ 1, ir tada palaipsniui maÅ¾inti Å¡Ä¯ parametrÄ…. Laikui bÄ—gant gausime geras tikimybiÅ³ reikÅ¡mes Q-lentelÄ—je, todÄ—l turÄ—tume jas Å¡iek tiek koreguoti, o ne visiÅ¡kai perraÅ¡yti naujomis reikÅ¡mÄ—mis.

- **Didinti epsilon**.
> **UÅ¾duotis 1**: Pakeiskite hiperparametrÅ³ reikÅ¡mes ir paÅ¾iÅ«rÄ—kite, ar galite pasiekti didesnÄ¯ bendrÄ… atlygÄ¯. Ar pasiekiate daugiau nei 195?
> **UÅ¾duotis 2**: Norint oficialiai iÅ¡sprÄ™sti problemÄ…, reikia pasiekti 195 vidutinÄ¯ atlygÄ¯ per 100 iÅ¡ eilÄ—s vykdomÅ³ bandymÅ³. StebÄ—kite tai mokymosi metu ir Ä¯sitikinkite, kad problema oficialiai iÅ¡sprÄ™sta!

## Rezultato stebÄ—jimas veiksmuose

BÅ«tÅ³ Ä¯domu pamatyti, kaip iÅ¡mokytas modelis elgiasi. Paleiskime simuliacijÄ… ir laikykimÄ—s tos paÄios veiksmÅ³ pasirinkimo strategijos kaip mokymosi metu, imdami mÄ—ginius pagal tikimybiÅ³ pasiskirstymÄ… Q-lentelÄ—je: (kodo blokas 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

TurÄ—tumÄ—te pamatyti kaÅ¾kÄ… panaÅ¡aus Ä¯ tai:

![balansuojantis veÅ¾imÄ—lis](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€IÅ¡Å¡Å«kis

> **UÅ¾duotis 3**: ÄŒia naudojome galutinÄ™ Q-lentelÄ—s kopijÄ…, kuri gali nebÅ«ti geriausia. Prisiminkite, kad geriausiai veikianÄiÄ… Q-lentelÄ™ iÅ¡saugojome `Qbest` kintamajame! IÅ¡bandykite tÄ… patÄ¯ pavyzdÄ¯ su geriausiai veikianÄia Q-lentele, nukopijuodami `Qbest` Ä¯ `Q`, ir paÅ¾iÅ«rÄ—kite, ar pastebÄ—site skirtumÄ….

> **UÅ¾duotis 4**: ÄŒia kiekviename Å¾ingsnyje nepasirinkome geriausio veiksmo, o rinkomÄ—s pagal atitinkamÄ… tikimybiÅ³ pasiskirstymÄ…. Ar bÅ«tÅ³ prasmingiau visada pasirinkti geriausiÄ… veiksmÄ…, turintÄ¯ didÅ¾iausiÄ… Q-lentelÄ—s vertÄ™? Tai galima padaryti naudojant `np.argmax` funkcijÄ…, kad suÅ¾inotumÄ—te veiksmo numerÄ¯, atitinkantÄ¯ didÅ¾iausiÄ… Q-lentelÄ—s vertÄ™. Ä®gyvendinkite Å¡iÄ… strategijÄ… ir paÅ¾iÅ«rÄ—kite, ar tai pagerina balansavimÄ….

## [Po paskaitos testas](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## UÅ¾duotis
[Treniruokite kalnÅ³ automobilÄ¯](assignment.md)

## IÅ¡vada

Dabar iÅ¡mokome treniruoti agentus, kad jie pasiektÅ³ gerÅ³ rezultatÅ³, tiesiog pateikdami jiems atlygio funkcijÄ…, apibrÄ—Å¾ianÄiÄ… norimÄ… Å¾aidimo bÅ«senÄ…, ir suteikdami galimybÄ™ protingai tyrinÄ—ti paieÅ¡kos erdvÄ™. SÄ—kmingai pritaikÄ—me Q-mokymosi algoritmÄ… diskretiniÅ³ ir tÄ™stiniÅ³ aplinkÅ³ atvejais, taÄiau su diskretiniais veiksmais.

Taip pat svarbu nagrinÄ—ti situacijas, kai veiksmo bÅ«sena yra tÄ™stinÄ—, o stebÄ—jimo erdvÄ— yra daug sudÄ—tingesnÄ—, pavyzdÅ¾iui, vaizdas iÅ¡ Atari Å¾aidimo ekrano. Tokiose problemose daÅ¾nai reikia naudoti galingesnes maÅ¡ininio mokymosi technikas, tokias kaip neuroniniai tinklai, kad bÅ«tÅ³ pasiekti geri rezultatai. Å ios paÅ¾angesnÄ—s temos bus aptartos mÅ«sÅ³ bÅ«simame paÅ¾angesniame dirbtinio intelekto kurse.

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.