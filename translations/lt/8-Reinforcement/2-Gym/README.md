<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T08:03:21+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "lt"
}
-->
# CartPole ÄiuoÅ¾imas

Problema, kuriÄ… sprendÄ—me ankstesnÄ—je pamokoje, gali atrodyti kaip Å¾aislinÄ— uÅ¾duotis, neturinti realaus pritaikymo. TaÄiau tai nÄ—ra tiesa, nes daugelis realaus pasaulio problemÅ³ taip pat turi panaÅ¡Å³ scenarijÅ³ â€“ Ä¯skaitant Å¡achmatÅ³ ar Go Å¾aidimÄ…. Jos yra panaÅ¡ios, nes taip pat turime lentÄ… su tam tikromis taisyklÄ—mis ir **diskretinÄ™ bÅ«senÄ…**.

## [PrieÅ¡ paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## Ä®vadas

Å ioje pamokoje taikysime tuos paÄius Q-Learning principus problemoms su **nepertraukiama bÅ«sena**, t. y. bÅ«sena, kuriÄ… apibrÄ—Å¾ia vienas ar daugiau realiÅ³ skaiÄiÅ³. SprÄ™sime Å¡iÄ… problemÄ…:

> **Problema**: Jei Petras nori pabÄ—gti nuo vilko, jis turi iÅ¡mokti judÄ—ti greiÄiau. PaÅ¾iÅ«rÄ—sime, kaip Petras gali iÅ¡mokti ÄiuoÅ¾ti, ypaÄ iÅ¡laikyti pusiausvyrÄ…, naudojant Q-Learning.

![Didysis pabÄ—gimas!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> Petras ir jo draugai tampa kÅ«rybingi, kad pabÄ—gtÅ³ nuo vilko! Vaizdas sukurtas [Jen Looper](https://twitter.com/jenlooper)

Naudosime supaprastintÄ… pusiausvyros iÅ¡laikymo versijÄ…, Å¾inomÄ… kaip **CartPole** problema. CartPole pasaulyje turime horizontalÅ³ slankiklÄ¯, kuris gali judÄ—ti Ä¯ kairÄ™ arba Ä¯ deÅ¡inÄ™, o tikslas yra iÅ¡laikyti vertikalÅ³ stulpÄ… ant slankiklio.

## Reikalavimai

Å ioje pamokoje naudosime bibliotekÄ… **OpenAI Gym**, kad simuliuotume Ä¯vairias **aplinkas**. Pamokos kodÄ… galite vykdyti lokaliai (pvz., Visual Studio Code), tokiu atveju simuliacija atsidarys naujame lange. Jei kodÄ… vykdote internete, gali reikÄ—ti atlikti tam tikrus pakeitimus, kaip apraÅ¡yta [Äia](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

AnkstesnÄ—je pamokoje Å¾aidimo taisykles ir bÅ«senÄ… apibrÄ—Å¾Ä— mÅ«sÅ³ sukurta `Board` klasÄ—. ÄŒia naudosime specialiÄ… **simuliacijos aplinkÄ…**, kuri simuliuos fizikÄ… uÅ¾ balansuojanÄio stulpo. Viena populiariausiÅ³ simuliacijos aplinkÅ³, skirtÅ³ mokyti stiprinamojo mokymosi algoritmus, vadinama [Gym](https://gym.openai.com/), kuriÄ… priÅ¾iÅ«ri [OpenAI](https://openai.com/). Naudodami Å¡iÄ… aplinkÄ… galime kurti Ä¯vairias **aplinkas** â€“ nuo CartPole simuliacijos iki Atari Å¾aidimÅ³.

> **Pastaba**: Kitas OpenAI Gym aplinkas galite pamatyti [Äia](https://gym.openai.com/envs/#classic_control).

Pirmiausia Ä¯diekime Gym ir importuokime reikalingas bibliotekas (kodo blokas 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## UÅ¾duotis â€“ inicializuoti CartPole aplinkÄ…

NorÄ—dami dirbti su CartPole balanso problema, turime inicializuoti atitinkamÄ… aplinkÄ…. Kiekviena aplinka yra susijusi su:

- **StebÄ—jimo erdve**, kuri apibrÄ—Å¾ia informacijos struktÅ«rÄ…, kuriÄ… gauname iÅ¡ aplinkos. CartPole problemai gauname stulpo pozicijÄ…, greitÄ¯ ir kitus parametrus.

- **VeiksmÅ³ erdve**, kuri apibrÄ—Å¾ia galimus veiksmus. MÅ«sÅ³ atveju veiksmÅ³ erdvÄ— yra diskretiÅ¡ka ir susideda iÅ¡ dviejÅ³ veiksmÅ³ â€“ **kairÄ—** ir **deÅ¡inÄ—**. (kodo blokas 2)

1. NorÄ—dami inicializuoti, Ä¯veskite Å¡Ä¯ kodÄ…:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

NorÄ—dami pamatyti, kaip veikia aplinka, paleiskime trumpÄ… simuliacijÄ… 100 Å¾ingsniÅ³. Kiekviename Å¾ingsnyje pateikiame vienÄ… veiksmÄ… â€“ Å¡ioje simuliacijoje atsitiktinai pasirenkame veiksmÄ… iÅ¡ `action_space`.

1. Paleiskite Å¾emiau esantÄ¯ kodÄ… ir paÅ¾iÅ«rÄ—kite, kÄ… jis duoda.

    âœ… Rekomenduojama Å¡Ä¯ kodÄ… vykdyti lokaliai, naudojant Python! (kodo blokas 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    TurÄ—tumÄ—te matyti kaÅ¾kÄ… panaÅ¡aus Ä¯ Å¡Ä¯ vaizdÄ…:

    ![nebalansuojantis CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Simuliacijos metu turime gauti stebÄ—jimus, kad nusprÄ™stume, kaip veikti. IÅ¡ tiesÅ³, `step` funkcija grÄ…Å¾ina dabartinius stebÄ—jimus, atlygio funkcijÄ… ir `done` vÄ—liavÄ—lÄ™, kuri nurodo, ar verta tÄ™sti simuliacijÄ…: (kodo blokas 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Galutinis rezultatas turÄ—tÅ³ bÅ«ti panaÅ¡us Ä¯ Å¡Ä¯:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    StebÄ—jimo vektorius, grÄ…Å¾inamas kiekviename simuliacijos Å¾ingsnyje, apima Å¡ias reikÅ¡mes:
    - VeÅ¾imÄ—lio pozicija
    - VeÅ¾imÄ—lio greitis
    - Stulpo kampas
    - Stulpo sukimosi greitis

1. Gaukite Å¡iÅ³ skaiÄiÅ³ minimaliÄ… ir maksimaliÄ… reikÅ¡mÄ™: (kodo blokas 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Taip pat galite pastebÄ—ti, kad atlygio reikÅ¡mÄ— kiekviename simuliacijos Å¾ingsnyje visada yra 1. Taip yra todÄ—l, kad mÅ«sÅ³ tikslas yra iÅ¡gyventi kuo ilgiau, t. y. iÅ¡laikyti stulpÄ… pakankamai vertikalioje padÄ—tyje kuo ilgesnÄ¯ laikÄ….

    âœ… IÅ¡ tiesÅ³, CartPole simuliacija laikoma iÅ¡sprÄ™sta, jei sugebame pasiekti vidutinÄ¯ 195 atlygÄ¯ per 100 iÅ¡ eilÄ—s vykdomÅ³ bandymÅ³.

## BÅ«senos diskretizavimas

Q-Learning algoritme turime sukurti Q-lentelÄ™, kuri apibrÄ—Å¾ia, kÄ… daryti kiekvienoje bÅ«senoje. Kad tai bÅ«tÅ³ Ä¯manoma, bÅ«sena turi bÅ«ti **diskretiÅ¡ka**, tiksliau, ji turi turÄ—ti baigtinÄ¯ skaiÄiÅ³ diskreÄiÅ³ reikÅ¡miÅ³. TodÄ—l turime kaÅ¾kaip **diskretizuoti** savo stebÄ—jimus, susiedami juos su baigtiniu bÅ«senÅ³ rinkiniu.

Yra keletas bÅ«dÅ³, kaip tai padaryti:

- **Padalinti Ä¯ intervalus**. Jei Å¾inome tam tikros reikÅ¡mÄ—s intervalÄ…, galime padalinti Å¡Ä¯ intervalÄ… Ä¯ tam tikrÄ… skaiÄiÅ³ **intervalÅ³** ir tada pakeisti reikÅ¡mÄ™ Ä¯ intervalÄ…, kuriam ji priklauso. Tai galima padaryti naudojant numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) metodÄ…. Tokiu atveju tiksliai Å¾inosime bÅ«senos dydÄ¯, nes jis priklausys nuo pasirinkto intervalÅ³ skaiÄiaus.

âœ… Galime naudoti linijinÄ™ interpolacijÄ…, kad reikÅ¡mes paverstume tam tikru baigtiniu intervalu (pvz., nuo -20 iki 20), o tada skaiÄius paversti sveikaisiais skaiÄiais, juos suapvalinant. Tai suteikia maÅ¾iau kontrolÄ—s bÅ«senos dydÅ¾iui, ypaÄ jei neÅ¾inome tiksliÅ³ Ä¯vesties reikÅ¡miÅ³ diapazonÅ³. PavyzdÅ¾iui, mÅ«sÅ³ atveju 2 iÅ¡ 4 reikÅ¡miÅ³ neturi virÅ¡utinÄ—s/apatinÄ—s ribos, todÄ—l gali atsirasti begalinis bÅ«senÅ³ skaiÄius.

MÅ«sÅ³ pavyzdyje naudosime antrÄ…jÄ¯ metodÄ…. Kaip pastebÄ—site vÄ—liau, nepaisant neapibrÄ—Å¾tÅ³ ribÅ³, Å¡ios reikÅ¡mÄ—s retai virÅ¡ija tam tikrus baigtinius intervalus, todÄ—l bÅ«senos su ekstremaliomis reikÅ¡mÄ—mis bus labai retos.

1. Å tai funkcija, kuri paims stebÄ—jimÄ… iÅ¡ mÅ«sÅ³ modelio ir sukurs 4 sveikÅ³jÅ³ skaiÄiÅ³ tuple: (kodo blokas 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Taip pat iÅ¡bandykime kitÄ… diskretizavimo metodÄ…, naudojant intervalus: (kodo blokas 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Dabar paleiskime trumpÄ… simuliacijÄ… ir stebÄ—kime tas diskretizuotas aplinkos reikÅ¡mes. Galite iÅ¡bandyti tiek `discretize`, tiek `discretize_bins` ir paÅ¾iÅ«rÄ—ti, ar yra skirtumas.

    âœ… `discretize_bins` grÄ…Å¾ina intervalo numerÄ¯, kuris prasideda nuo 0. TodÄ—l Ä¯vesties reikÅ¡mÄ—ms, esanÄioms apie 0, jis grÄ…Å¾ina numerÄ¯ iÅ¡ intervalo vidurio (10). `discretize` funkcijoje nesirÅ«pinome iÅ¡vesties reikÅ¡miÅ³ diapazonu, leidome joms bÅ«ti neigiamoms, todÄ—l bÅ«senos reikÅ¡mÄ—s nÄ—ra perstumtos, o 0 atitinka 0. (kodo blokas 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Jei norite pamatyti, kaip vykdoma aplinka, iÅ¡komentuokite eilutÄ™, prasidedanÄiÄ… `env.render`. PrieÅ¡ingu atveju galite vykdyti jÄ… fone, kas yra greiÄiau. Å Ä¯ â€nematomÄ…â€œ vykdymÄ… naudosime Q-Learning procese.

## Q-lentelÄ—s struktÅ«ra

AnkstesnÄ—je pamokoje bÅ«sena buvo paprasta pora skaiÄiÅ³ nuo 0 iki 8, todÄ—l buvo patogu Q-lentelÄ™ atvaizduoti kaip numpy tensorÄ…, kurio forma yra 8x8x2. Jei naudojame intervalÅ³ diskretizavimÄ…, mÅ«sÅ³ bÅ«senos vektoriaus dydis taip pat yra Å¾inomas, todÄ—l galime naudoti tÄ… patÄ¯ metodÄ… ir atvaizduoti bÅ«senÄ… kaip masyvÄ…, kurio forma yra 20x20x10x10x2 (Äia 2 yra veiksmÅ³ erdvÄ—s dimensija, o pirmosios dimensijos atitinka intervalÅ³ skaiÄiÅ³, kurÄ¯ pasirinkome kiekvienam stebÄ—jimo erdvÄ—s parametrui).

TaÄiau kartais stebÄ—jimo erdvÄ—s tikslios dimensijos nÄ—ra Å¾inomos. Naudojant `discretize` funkcijÄ…, niekada negalime bÅ«ti tikri, kad mÅ«sÅ³ bÅ«sena iÅ¡liks tam tikrose ribose, nes kai kurios pradinÄ—s reikÅ¡mÄ—s nÄ—ra ribotos. TodÄ—l naudosime Å¡iek tiek kitokÄ¯ metodÄ… ir Q-lentelÄ™ atvaizduosime kaip Å¾odynÄ….

1. Naudokite porÄ… *(bÅ«sena, veiksmas)* kaip Å¾odyno raktÄ…, o reikÅ¡mÄ— atitiks Q-lentelÄ—s Ä¯raÅ¡o reikÅ¡mÄ™. (kodo blokas 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    ÄŒia taip pat apibrÄ—Å¾iame funkcijÄ… `qvalues()`, kuri grÄ…Å¾ina Q-lentelÄ—s reikÅ¡miÅ³ sÄ…raÅ¡Ä…, atitinkantÄ¯ visus galimus veiksmus tam tikroje bÅ«senoje. Jei Ä¯raÅ¡o nÄ—ra Q-lentelÄ—je, grÄ…Å¾insime 0 kaip numatytÄ…jÄ… reikÅ¡mÄ™.

## PradÄ—kime Q-Learning

Dabar esame pasiruoÅ¡Ä™ mokyti PetrÄ… iÅ¡laikyti pusiausvyrÄ…!

1. Pirmiausia nustatykime keletÄ… hiperparametrÅ³: (kodo blokas 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    ÄŒia `alpha` yra **mokymosi greitis**, kuris apibrÄ—Å¾ia, kiek turÄ—tume koreguoti dabartines Q-lentelÄ—s reikÅ¡mes kiekviename Å¾ingsnyje. AnkstesnÄ—je pamokoje pradÄ—jome nuo 1, o vÄ—liau sumaÅ¾inome `alpha` iki maÅ¾esniÅ³ reikÅ¡miÅ³ mokymo metu. Å iame pavyzdyje iÅ¡laikysime jÄ¯ pastovÅ³, kad bÅ«tÅ³ paprasÄiau, o vÄ—liau galite eksperimentuoti su `alpha` reikÅ¡miÅ³ koregavimu.

    `gamma` yra **nuolaidos faktorius**, kuris parodo, kiek turÄ—tume teikti pirmenybÄ™ bÅ«simam atlygiui, palyginti su dabartiniu.

    `epsilon` yra **tyrimo/naudojimo faktorius**, kuris nustato, ar turÄ—tume teikti pirmenybÄ™ tyrimui, ar naudojimui. MÅ«sÅ³ algoritme `epsilon` procentais atvejÅ³ pasirinksime kitÄ… veiksmÄ… pagal Q-lentelÄ—s reikÅ¡mes, o likusiais atvejais vykdysime atsitiktinÄ¯ veiksmÄ…. Tai leis mums iÅ¡tirti paieÅ¡kos erdvÄ—s sritis, kuriÅ³ dar niekada nematÄ—me.

    âœ… Kalbant apie balansavimÄ… â€“ atsitiktinio veiksmo pasirinkimas (tyrimas) veiktÅ³ kaip atsitiktinis smÅ«gis neteisinga kryptimi, ir stulpas turÄ—tÅ³ iÅ¡mokti, kaip atgauti pusiausvyrÄ… po tokiÅ³ â€klaidÅ³â€œ.

### Tobulinkime algoritmÄ…

Taip pat galime atlikti du patobulinimus mÅ«sÅ³ algoritme iÅ¡ ankstesnÄ—s pamokos:

- **SkaiÄiuoti vidutinÄ¯ kumuliacinÄ¯ atlygÄ¯** per tam tikrÄ… simuliacijÅ³ skaiÄiÅ³. Spausdinsime progresÄ… kas 5000 iteracijÅ³ ir vidutiniÅ¡kai skaiÄiuosime kumuliacinÄ¯ atlygÄ¯ per tÄ… laikotarpÄ¯. Tai reiÅ¡kia, kad jei pasieksime daugiau nei 195 taÅ¡kus, galime laikyti problemÄ… iÅ¡sprÄ™sta, net geresne kokybe nei reikalaujama.

- **SkaiÄiuoti maksimalÅ³ vidutinÄ¯ kumuliacinÄ¯ rezultatÄ…**, `Qmax`, ir iÅ¡saugosime Q-lentelÄ™, atitinkanÄiÄ… tÄ… rezultatÄ…. Kai vykdysite mokymÄ…, pastebÄ—site, kad kartais vidutinis kumuliacinis rezultatas pradeda maÅ¾Ä—ti, ir norime iÅ¡saugoti Q-lentelÄ—s reikÅ¡mes, kurios atitinka geriausiÄ… modelÄ¯, pastebÄ—tÄ… mokymo metu.

1. Surinkite visus kumuliacinius atlygius kiekvienoje simuliacijoje Ä¯ `rewards` vektoriÅ³, kad galÄ—tumÄ—te juos vÄ—liau pavaizduoti. (kodo blokas 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

KÄ… galite pastebÄ—ti iÅ¡ Å¡iÅ³ rezultatÅ³:

- **ArtÄ—jame prie tikslo**. Esame labai arti tikslo pasiekti 195 kumuliacinius atlygius per 100+ iÅ¡ eilÄ—s vykdomÅ³ simuliacijÅ³, arba galbÅ«t jau pasiekÄ—me! Net jei gauname maÅ¾esnius skaiÄius, vis tiek neÅ¾inome, nes vidutiniÅ¡kai skaiÄiuojame per 5000 vykdymÅ³, o formalÅ«s kriterijai reikalauja tik 100 vykdymÅ³.

- **Atlygis pradeda maÅ¾Ä—ti**. Kartais atlygis pradeda maÅ¾Ä—ti, o tai reiÅ¡kia, kad galime â€sugadintiâ€œ jau iÅ¡moktas Q-lentelÄ—s reikÅ¡mes naujomis, kurios pablogina situacijÄ….

Å is pastebÄ—jimas tampa aiÅ¡kesnis, jei pavaizduojame mokymo progresÄ….

## Mokymo progreso vaizdavimas

Mokymo metu surinkome kumuliacinio atlygio reikÅ¡mes kiekvienoje iteracijoje Ä¯ `rewards` vektoriÅ³. Å tai kaip tai atrodo, kai pavaizduojame prieÅ¡ iteracijÅ³ skaiÄiÅ³:

```python
plt.plot(rewards)
```

![Å¾alias progresas](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

IÅ¡ Å¡io grafiko neÄ¯manoma nieko pasakyti, nes dÄ—l stochastinio mokymo proceso mokymo sesijÅ³ ilgis labai skiriasi. Kad Å¡is grafikas bÅ«tÅ³ prasmingesnis, galime apskaiÄiuoti **slankÅ³jÄ¯ vidurkÄ¯** per eksperimentÅ³ serijÄ…, tarkime, 100. Tai galima patogiai atlikti naudojant `np.convolve`: (kodo blokas 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![mokymo progresas](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## HiperparametrÅ³ keitimas

Kad mokymasis bÅ«tÅ³ stabilesnis, verta koreguoti kai kuriuos hiperparametrus mokymo metu. VisÅ³ pirma:

- **Mokymosi greiÄiui**, `alpha`, galime pradÄ—ti nuo reikÅ¡miÅ³, artimÅ³ 1, ir palaipsniui maÅ¾inti parametrÄ…. Laikui bÄ—gant gausime geras tikimybiÅ³ reikÅ¡mes Q-lentelÄ—je, todÄ—l turÄ—tume jas koreguoti Å¡velniai, o ne visiÅ¡kai perraÅ¡yti naujomis reikÅ¡mÄ—mis.

- **Didinti epsilon**. Galime norÄ—ti palaipsniui didinti `epsilon`, kad maÅ¾iau tyrinÄ—tume ir daugiau naudotume. Tikriausiai prasminga pradÄ—ti nuo maÅ¾esnÄ—s `epsilon` reikÅ¡mÄ—s ir palaipsniui didinti iki beveik
> **UÅ¾duotis 1**: Pakeiskite hiperparametrÅ³ reikÅ¡mes ir paÅ¾iÅ«rÄ—kite, ar galite pasiekti didesnÄ¯ bendrÄ… atlygÄ¯. Ar pasiekiate daugiau nei 195?
> **UÅ¾duotis 2**: NorÄ—dami oficialiai iÅ¡sprÄ™sti problemÄ…, turite pasiekti 195 vidutinÄ¯ atlygÄ¯ per 100 iÅ¡ eilÄ—s vykdomÅ³ bandymÅ³. StebÄ—kite tai mokymosi metu ir Ä¯sitikinkite, kad problema oficialiai iÅ¡sprÄ™sta!

## Rezultato stebÄ—jimas veiksmuose

BÅ«tÅ³ Ä¯domu pamatyti, kaip iÅ¡mokytas modelis elgiasi. Paleiskime simuliacijÄ… ir laikykimÄ—s tos paÄios veiksmÅ³ pasirinkimo strategijos kaip mokymosi metu, imdami mÄ—ginius pagal tikimybiÅ³ pasiskirstymÄ… Q-lentelÄ—je: (kodo blokas 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

TurÄ—tumÄ—te pamatyti kaÅ¾kÄ… panaÅ¡aus Ä¯ tai:

![balansuojantis veÅ¾imÄ—lis](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€IÅ¡Å¡Å«kis

> **UÅ¾duotis 3**: ÄŒia naudojome galutinÄ™ Q-lentelÄ—s kopijÄ…, kuri gali nebÅ«ti geriausia. Atminkite, kad geriausiai veikianÄiÄ… Q-lentelÄ™ iÅ¡saugojome `Qbest` kintamajame! IÅ¡bandykite tÄ… patÄ¯ pavyzdÄ¯ su geriausiai veikianÄia Q-lentele, nukopijuodami `Qbest` Ä¯ `Q`, ir paÅ¾iÅ«rÄ—kite, ar pastebÄ—site skirtumÄ….

> **UÅ¾duotis 4**: ÄŒia kiekviename Å¾ingsnyje nepasirinkome geriausio veiksmo, o rinkomÄ—s pagal atitinkamÄ… tikimybiÅ³ pasiskirstymÄ…. Ar bÅ«tÅ³ prasmingiau visada pasirinkti geriausiÄ… veiksmÄ…, turintÄ¯ didÅ¾iausiÄ… Q-lentelÄ—s vertÄ™? Tai galima padaryti naudojant `np.argmax` funkcijÄ…, kad suÅ¾inotumÄ—te veiksmo numerÄ¯, atitinkantÄ¯ didÅ¾iausiÄ… Q-lentelÄ—s vertÄ™. Ä®gyvendinkite Å¡iÄ… strategijÄ… ir paÅ¾iÅ«rÄ—kite, ar tai pagerina balansavimÄ….

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## UÅ¾duotis
[Treniruokite kalnÅ³ automobilÄ¯](assignment.md)

## IÅ¡vada

Dabar iÅ¡mokome, kaip treniruoti agentus, kad jie pasiektÅ³ gerÅ³ rezultatÅ³, tiesiog suteikdami jiems atlygio funkcijÄ…, apibrÄ—Å¾ianÄiÄ… norimÄ… Å¾aidimo bÅ«senÄ…, ir suteikdami galimybÄ™ protingai tyrinÄ—ti paieÅ¡kos erdvÄ™. SÄ—kmingai pritaikÄ—me Q-mokymosi algoritmÄ… tiek diskretinÄ—se, tiek tÄ™stinÄ—se aplinkose, taÄiau su diskretiniais veiksmais.

Svarbu taip pat nagrinÄ—ti situacijas, kai veiksmo bÅ«sena yra tÄ™stinÄ—, o stebÄ—jimo erdvÄ— yra daug sudÄ—tingesnÄ—, pavyzdÅ¾iui, vaizdas iÅ¡ Atari Å¾aidimo ekrano. Tokiose problemose daÅ¾nai reikia naudoti galingesnes maÅ¡ininio mokymosi technikas, tokias kaip neuroniniai tinklai, kad pasiektume gerÅ³ rezultatÅ³. Å ios paÅ¾angesnÄ—s temos bus aptartos mÅ«sÅ³ bÅ«simame paÅ¾angesniame dirbtinio intelekto kurse.

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant dirbtinio intelekto vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, atkreipiame dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudotis profesionaliÅ³ vertÄ—jÅ³ paslaugomis. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus aiÅ¡kinimus, kylanÄius dÄ—l Å¡io vertimo naudojimo.