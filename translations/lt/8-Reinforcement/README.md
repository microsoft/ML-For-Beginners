<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-03T18:25:56+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "lt"
}
-->
# Ä®vadas Ä¯ stiprinamÄ…jÄ¯ mokymÄ…si

Stiprinamasis mokymasis (RL) laikomas vienu iÅ¡ pagrindiniÅ³ maÅ¡ininio mokymosi paradigmÅ³, greta priÅ¾iÅ«rimo mokymosi ir nepriÅ¾iÅ«rimo mokymosi. RL yra susijÄ™s su sprendimais: priimti tinkamus sprendimus arba bent jau mokytis iÅ¡ jÅ³.

Ä®sivaizduokite, kad turite simuliuotÄ… aplinkÄ…, pavyzdÅ¾iui, akcijÅ³ rinkÄ…. Kas nutiks, jei Ä¯vesite tam tikrÄ… reguliavimÄ…? Ar tai turÄ—s teigiamÄ… ar neigiamÄ… poveikÄ¯? Jei nutiks kaÅ¾kas neigiamo, turite priimti Å¡Ä¯ _neigiamÄ… stiprinimÄ…_, pasimokyti iÅ¡ jo ir pakeisti kryptÄ¯. Jei rezultatas yra teigiamas, turite remtis tuo _teigiamu stiprinimu_.

![peter ir vilkas](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.lt.png)

> Petras ir jo draugai turi pabÄ—gti nuo alkano vilko! Vaizdas sukurtas [Jen Looper](https://twitter.com/jenlooper)

## RegioninÄ— tema: Petras ir Vilkas (Rusija)

[Petras ir Vilkas](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) yra muzikinÄ— pasaka, kuriÄ… paraÅ¡Ä— rusÅ³ kompozitorius [Sergejus Prokofjevas](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Tai pasakojimas apie jaunÄ… pionieriÅ³ PetrÄ…, kuris drÄ…siai iÅ¡eina iÅ¡ namÅ³ Ä¯ miÅ¡ko laukymÄ™, kad sugautÅ³ vilkÄ…. Å ioje dalyje mes treniruosime maÅ¡ininio mokymosi algoritmus, kurie padÄ—s Petrui:

- **TyrinÄ—ti** aplinkÄ… ir sukurti optimizuotÄ… navigacijos Å¾emÄ—lapÄ¯
- **IÅ¡mokti** naudotis riedlente ir iÅ¡laikyti pusiausvyrÄ…, kad galÄ—tÅ³ greiÄiau judÄ—ti.

[![Petras ir Vilkas](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esantÄ¯ vaizdÄ…, kad iÅ¡klausytumÄ—te Prokofjevo kÅ«rinÄ¯ â€Petras ir Vilkasâ€œ

## Stiprinamasis mokymasis

AnkstesnÄ—se dalyse matÄ—te du maÅ¡ininio mokymosi problemÅ³ pavyzdÅ¾ius:

- **PriÅ¾iÅ«rimas mokymasis**, kai turime duomenÅ³ rinkinius, kurie siÅ«lo pavyzdinius sprendimus problemai, kuriÄ… norime iÅ¡sprÄ™sti. [Klasifikacija](../4-Classification/README.md) ir [regresija](../2-Regression/README.md) yra priÅ¾iÅ«rimo mokymosi uÅ¾duotys.
- **NepriÅ¾iÅ«rimas mokymasis**, kai neturime paÅ¾ymÄ—tÅ³ mokymo duomenÅ³. Pagrindinis nepriÅ¾iÅ«rimo mokymosi pavyzdys yra [Grupavimas](../5-Clustering/README.md).

Å ioje dalyje mes supaÅ¾indinsime jus su naujo tipo mokymosi problema, kuriai nereikia paÅ¾ymÄ—tÅ³ mokymo duomenÅ³. Yra keletas tokiÅ³ problemÅ³ tipÅ³:

- **[Pusiau priÅ¾iÅ«rimas mokymasis](https://wikipedia.org/wiki/Semi-supervised_learning)**, kai turime daug nepaÅ¾ymÄ—tÅ³ duomenÅ³, kuriuos galima naudoti modelio iÅ¡ankstiniam mokymui.
- **[Stiprinamasis mokymasis](https://wikipedia.org/wiki/Reinforcement_learning)**, kai agentas mokosi elgtis atlikdamas eksperimentus tam tikroje simuliuotoje aplinkoje.

### Pavyzdys - kompiuterinis Å¾aidimas

Tarkime, norite iÅ¡mokyti kompiuterÄ¯ Å¾aisti Å¾aidimÄ…, pavyzdÅ¾iui, Å¡achmatus ar [Super Mario](https://wikipedia.org/wiki/Super_Mario). Kad kompiuteris galÄ—tÅ³ Å¾aisti Å¾aidimÄ…, reikia, kad jis numatytÅ³, kokÄ¯ Ä—jimÄ… atlikti kiekvienoje Å¾aidimo bÅ«senoje. Nors tai gali atrodyti kaip klasifikacijos problema, taip nÄ—ra - nes neturime duomenÅ³ rinkinio su bÅ«senomis ir atitinkamais veiksmais. Nors galime turÄ—ti duomenÅ³, tokiÅ³ kaip esamos Å¡achmatÅ³ partijos ar Å¾aidÄ—jÅ³ â€Super Marioâ€œ Å¾aidimo Ä¯raÅ¡ai, tikÄ—tina, kad tie duomenys nepakankamai apims didelÄ¯ galimÅ³ bÅ«senÅ³ skaiÄiÅ³.

UÅ¾uot ieÅ¡kojÄ™ esamÅ³ Å¾aidimo duomenÅ³, **Stiprinamasis mokymasis** (RL) remiasi idÄ—ja, kad *kompiuteris Å¾aistÅ³* daug kartÅ³ ir stebÄ—tÅ³ rezultatÄ…. Taigi, norint taikyti stiprinamÄ…jÄ¯ mokymÄ…si, mums reikia dviejÅ³ dalykÅ³:

- **Aplinkos** ir **simuliatoriaus**, kurie leistÅ³ mums Å¾aisti Å¾aidimÄ… daug kartÅ³. Å is simuliatorius apibrÄ—Å¾tÅ³ visas Å¾aidimo taisykles, galimas bÅ«senas ir veiksmus.

- **Atlygio funkcijos**, kuri nurodytÅ³, kaip gerai pasirodÄ—me kiekvieno Ä—jimo ar Å¾aidimo metu.

Pagrindinis skirtumas tarp kitÅ³ maÅ¡ininio mokymosi tipÅ³ ir RL yra tas, kad RL daÅ¾niausiai neÅ¾inome, ar laimime, ar pralaimime, kol nebaigiame Å¾aidimo. Taigi, negalime pasakyti, ar tam tikras Ä—jimas vienas pats yra geras, ar ne - atlygio gauname tik Å¾aidimo pabaigoje. MÅ«sÅ³ tikslas yra sukurti algoritmus, kurie leistÅ³ mums treniruoti modelÄ¯ esant neapibrÄ—Å¾toms sÄ…lygoms. Mes iÅ¡moksime apie vienÄ… RL algoritmÄ…, vadinamÄ… **Q-mokymusi**.

## Pamokos

1. [Ä®vadas Ä¯ stiprinamÄ…jÄ¯ mokymÄ…si ir Q-mokymÄ…si](1-QLearning/README.md)
2. [SimuliacinÄ—s aplinkos naudojimas su â€Gymâ€œ](2-Gym/README.md)

## Kreditas

â€Ä®vadas Ä¯ stiprinamÄ…jÄ¯ mokymÄ…siâ€œ buvo paraÅ¡ytas su â™¥ï¸ [Dmitrijaus SoÅ¡nikovo](http://soshnikov.com)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.