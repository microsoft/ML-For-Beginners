<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T08:02:11+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "lt"
}
-->
# Ä®vadas Ä¯ stiprinamÄ…jÄ¯ mokymÄ… ir Q-mokymÄ…

![Stiprinamojo mokymosi santrauka sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote sukÅ«rÄ— [Tomomi Imura](https://www.twitter.com/girlie_mac)

Stiprinamasis mokymasis apima tris svarbias sÄ…vokas: agentÄ…, tam tikras bÅ«senas ir veiksmÅ³ rinkinÄ¯ kiekvienai bÅ«senai. Atlikdamas veiksmÄ… tam tikroje bÅ«senoje, agentas gauna atlygÄ¯. Ä®sivaizduokite kompiuterinÄ¯ Å¾aidimÄ… â€Super Marioâ€œ. JÅ«s esate Mario, esate Å¾aidimo lygyje, stovite Å¡alia uolos kraÅ¡to. VirÅ¡ jÅ«sÅ³ yra moneta. JÅ«s, bÅ«damas Mario, tam tikroje Å¾aidimo lygio pozicijoje... tai yra jÅ«sÅ³ bÅ«sena. Å½engus Å¾ingsnÄ¯ Ä¯ deÅ¡inÄ™ (veiksmas), nukristumÄ—te nuo uolos, ir tai suteiktÅ³ jums maÅ¾Ä… skaitinÄ¯ rezultatÄ…. TaÄiau paspaudus Å¡uolio mygtukÄ…, pelnytumÄ—te taÅ¡kÄ… ir iÅ¡liktumÄ—te gyvas. Tai yra teigiamas rezultatas, kuris turÄ—tÅ³ suteikti jums teigiamÄ… skaitinÄ¯ rezultatÄ….

Naudodami stiprinamÄ…jÄ¯ mokymÄ… ir simuliatoriÅ³ (Å¾aidimÄ…), galite iÅ¡mokti Å¾aisti Å¾aidimÄ… taip, kad maksimaliai padidintumÄ—te atlygÄ¯, t. y. iÅ¡liktumÄ—te gyvas ir surinktumÄ—te kuo daugiau taÅ¡kÅ³.

[![Ä®vadas Ä¯ stiprinamÄ…jÄ¯ mokymÄ…](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad iÅ¡girstumÄ—te DmitrijÅ³ kalbant apie stiprinamÄ…jÄ¯ mokymÄ…

## [PrieÅ¡ paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## Reikalavimai ir paruoÅ¡imas

Å ioje pamokoje eksperimentuosime su Python kodu. TurÄ—tumÄ—te sugebÄ—ti paleisti Jupyter Notebook kodÄ… iÅ¡ Å¡ios pamokos, arba savo kompiuteryje, arba debesyje.

Galite atidaryti [pamokos uÅ¾raÅ¡Å³ knygelÄ™](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) ir perÅ¾iÅ«rÄ—ti Å¡iÄ… pamokÄ…, kad jÄ… sukurtumÄ—te.

> **Pastaba:** Jei atidarote Å¡Ä¯ kodÄ… iÅ¡ debesies, taip pat turite gauti [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) failÄ…, kuris naudojamas uÅ¾raÅ¡Å³ knygelÄ—s kode. Ä®dÄ—kite jÄ¯ Ä¯ tÄ… patÄ¯ katalogÄ… kaip ir uÅ¾raÅ¡Å³ knygelÄ™.

## Ä®vadas

Å ioje pamokoje tyrinÄ—sime **[Petro ir vilko](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)** pasaulÄ¯, Ä¯kvÄ—ptÄ… muzikinÄ—s pasakos, sukurtos rusÅ³ kompozitoriaus [Sergejaus Prokofjevo](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Naudosime **stiprinamÄ…jÄ¯ mokymÄ…**, kad Petras galÄ—tÅ³ tyrinÄ—ti savo aplinkÄ…, rinkti skanius obuolius ir vengti susitikimo su vilku.

**Stiprinamasis mokymasis** (RL) yra mokymosi technika, leidÅ¾ianti iÅ¡mokti optimalaus **agento** elgesio tam tikroje **aplinkoje**, atliekant daugybÄ™ eksperimentÅ³. Agentas Å¡ioje aplinkoje turÄ—tÅ³ turÄ—ti tam tikrÄ… **tikslÄ…**, apibrÄ—Å¾tÄ… **atlygio funkcija**.

## Aplinka

PaprasÄiausiai, Ä¯sivaizduokime Petro pasaulÄ¯ kaip kvadratinÄ™ lentÄ…, kurios dydis yra `plotis` x `aukÅ¡tis`, panaÅ¡iai kaip Å¡i:

![Petro aplinka](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Kiekviena lÄ…stelÄ— Å¡ioje lentoje gali bÅ«ti:

* **Å¾emÄ—**, ant kurios Petras ir kiti padarai gali vaikÅ¡Äioti.
* **vanduo**, ant kurio, akivaizdu, negalima vaikÅ¡Äioti.
* **medis** arba **Å¾olÄ—**, vieta, kur galima pailsÄ—ti.
* **obuolys**, kurÄ¯ Petras norÄ—tÅ³ rasti, kad pasimaitintÅ³.
* **vilkas**, kuris yra pavojingas ir kurio reikÄ—tÅ³ vengti.

Yra atskiras Python modulis, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kuriame yra kodas, skirtas dirbti su Å¡ia aplinka. Kadangi Å¡is kodas nÄ—ra svarbus mÅ«sÅ³ koncepcijoms suprasti, mes importuosime modulÄ¯ ir naudosime jÄ¯, kad sukurtume pavyzdinÄ™ lentÄ… (kodo blokas 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Å is kodas turÄ—tÅ³ atspausdinti aplinkos vaizdÄ…, panaÅ¡Å³ Ä¯ aukÅ¡Äiau pateiktÄ….

## Veiksmai ir politika

MÅ«sÅ³ pavyzdyje Petro tikslas bÅ«tÅ³ rasti obuolÄ¯, vengiant vilko ir kitÅ³ kliÅ«ÄiÅ³. Tam jis iÅ¡ esmÄ—s gali vaikÅ¡Äioti, kol suras obuolÄ¯.

TodÄ—l bet kurioje pozicijoje jis gali pasirinkti vienÄ… iÅ¡ Å¡iÅ³ veiksmÅ³: aukÅ¡tyn, Å¾emyn, kairÄ—n ir deÅ¡inÄ—n.

Å iuos veiksmus apibrÄ—Å¡ime kaip Å¾odynÄ… ir susiesime juos su atitinkamais koordinaÄiÅ³ pokyÄiÅ³ poromis. PavyzdÅ¾iui, judÄ—jimas deÅ¡inÄ—n (`R`) atitiktÅ³ porÄ… `(1,0)`. (kodo blokas 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Apibendrinant, Å¡io scenarijaus strategija ir tikslas yra tokie:

- **Strategija**, mÅ«sÅ³ agento (Petro) yra apibrÄ—Å¾ta vadinamÄ…ja **politika**. Politika yra funkcija, kuri grÄ…Å¾ina veiksmÄ… bet kurioje bÅ«senoje. MÅ«sÅ³ atveju problemos bÅ«sena yra lentos vaizdas, Ä¯skaitant Å¾aidÄ—jo dabartinÄ™ pozicijÄ….

- **Tikslas**, stiprinamojo mokymosi yra galiausiai iÅ¡mokti gerÄ… politikÄ…, kuri leis efektyviai iÅ¡sprÄ™sti problemÄ…. TaÄiau kaip pagrindÄ…, apsvarstykime paprasÄiausiÄ… politikÄ…, vadinamÄ… **atsitiktiniu vaikÅ¡Äiojimu**.

## Atsitiktinis vaikÅ¡Äiojimas

Pirmiausia iÅ¡sprÄ™skime mÅ«sÅ³ problemÄ… Ä¯gyvendindami atsitiktinio vaikÅ¡Äiojimo strategijÄ…. Naudodami atsitiktinÄ¯ vaikÅ¡ÄiojimÄ…, atsitiktinai pasirinksime kitÄ… veiksmÄ… iÅ¡ leidÅ¾iamÅ³ veiksmÅ³, kol pasieksime obuolÄ¯ (kodo blokas 3).

1. Ä®gyvendinkite atsitiktinÄ¯ vaikÅ¡ÄiojimÄ… naudodami Å¾emiau pateiktÄ… kodÄ…:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Funkcijos `walk` iÅ¡kvietimas turÄ—tÅ³ grÄ…Å¾inti atitinkamo kelio ilgÄ¯, kuris gali skirtis kiekvieno paleidimo metu.

1. Paleiskite vaikÅ¡Äiojimo eksperimentÄ… kelis kartus (pvz., 100) ir atspausdinkite gautÄ… statistikÄ… (kodo blokas 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Atkreipkite dÄ—mesÄ¯, kad vidutinis kelio ilgis yra apie 30â€“40 Å¾ingsniÅ³, o tai yra gana daug, atsiÅ¾velgiant Ä¯ tai, kad vidutinis atstumas iki artimiausio obuolio yra apie 5â€“6 Å¾ingsnius.

    Taip pat galite pamatyti, kaip atrodo Petro judÄ—jimas atsitiktinio vaikÅ¡Äiojimo metu:

    ![Petro atsitiktinis vaikÅ¡Äiojimas](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Atlygio funkcija

Kad mÅ«sÅ³ politika bÅ«tÅ³ protingesnÄ—, turime suprasti, kurie judesiai yra â€geresniâ€œ uÅ¾ kitus. Tam reikia apibrÄ—Å¾ti mÅ«sÅ³ tikslÄ….

Tikslas gali bÅ«ti apibrÄ—Å¾tas **atlygio funkcijos** forma, kuri grÄ…Å¾ins tam tikrÄ… balo reikÅ¡mÄ™ kiekvienai bÅ«senai. Kuo didesnis skaiÄius, tuo geresnÄ— atlygio funkcija. (kodo blokas 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Ä®domus dalykas apie atlygio funkcijas yra tas, kad daugeliu atvejÅ³ *mes gauname reikÅ¡mingÄ… atlygÄ¯ tik Å¾aidimo pabaigoje*. Tai reiÅ¡kia, kad mÅ«sÅ³ algoritmas turÄ—tÅ³ kaÅ¾kaip prisiminti â€gerusâ€œ Å¾ingsnius, kurie veda Ä¯ teigiamÄ… atlygÄ¯ pabaigoje, ir padidinti jÅ³ svarbÄ…. PanaÅ¡iai visi judesiai, kurie veda Ä¯ blogus rezultatus, turÄ—tÅ³ bÅ«ti atgrasomi.

## Q-mokymasis

Algoritmas, kurÄ¯ aptarsime Äia, vadinamas **Q-mokymu**. Å iame algoritme politika apibrÄ—Å¾iama funkcija (arba duomenÅ³ struktÅ«ra), vadinama **Q-lentele**. Ji registruoja kiekvieno veiksmo â€gerumÄ…â€œ tam tikroje bÅ«senoje.

Ji vadinama Q-lentele, nes daÅ¾nai patogu jÄ… vaizduoti kaip lentelÄ™ arba daugiamaÄio masyvo formÄ…. Kadangi mÅ«sÅ³ lenta turi matmenis `plotis` x `aukÅ¡tis`, Q-lentelÄ™ galime vaizduoti naudodami numpy masyvÄ…, kurio forma yra `plotis` x `aukÅ¡tis` x `len(veiksmai)`: (kodo blokas 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Atkreipkite dÄ—mesÄ¯, kad mes inicializuojame visas Q-lentelÄ—s reikÅ¡mes lygiomis reikÅ¡mÄ—mis, mÅ«sÅ³ atveju - 0.25. Tai atitinka â€atsitiktinio vaikÅ¡Äiojimoâ€œ politikÄ…, nes visi judesiai kiekvienoje bÅ«senoje yra vienodai geri. Q-lentelÄ™ galime perduoti funkcijai `plot`, kad vizualizuotume lentelÄ™ lentoje: `m.plot(Q)`.

![Petro aplinka](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

Kiekvienos lÄ…stelÄ—s centre yra â€rodyklÄ—â€œ, rodanti pageidaujamÄ… judÄ—jimo kryptÄ¯. Kadangi visos kryptys yra vienodos, rodomas taÅ¡kas.

Dabar turime paleisti simuliacijÄ…, tyrinÄ—ti savo aplinkÄ… ir iÅ¡mokti geresnÄ¯ Q-lentelÄ—s reikÅ¡miÅ³ pasiskirstymÄ…, kuris leis mums daug greiÄiau rasti keliÄ… iki obuolio.

## Q-mokymosi esmÄ—: Bellmano lygtis

Kai pradedame judÄ—ti, kiekvienas veiksmas turÄ—s atitinkamÄ… atlygÄ¯, t. y. teoriÅ¡kai galime pasirinkti kitÄ… veiksmÄ… pagal didÅ¾iausiÄ… tiesioginÄ¯ atlygÄ¯. TaÄiau daugumoje bÅ«senÅ³ judesys nepasieks mÅ«sÅ³ tikslo pasiekti obuolÄ¯, todÄ—l negalime iÅ¡ karto nusprÄ™sti, kuri kryptis yra geresnÄ—.

> Atminkite, kad svarbus ne tiesioginis rezultatas, o galutinis rezultatas, kurÄ¯ gausime simuliacijos pabaigoje.

Kad atsiÅ¾velgtume Ä¯ Å¡Ä¯ uÅ¾delstÄ… atlygÄ¯, turime naudoti **[dinaminio programavimo](https://en.wikipedia.org/wiki/Dynamic_programming)** principus, kurie leidÅ¾ia apie problemÄ… galvoti rekursyviai.

Tarkime, kad dabar esame bÅ«senoje *s*, ir norime pereiti Ä¯ kitÄ… bÅ«senÄ… *s'*. Tai darydami gausime tiesioginÄ¯ atlygÄ¯ *r(s,a)*, apibrÄ—Å¾tÄ… atlygio funkcija, plius tam tikrÄ… bÅ«simÄ… atlygÄ¯. Jei manome, kad mÅ«sÅ³ Q-lentelÄ— teisingai atspindi kiekvieno veiksmo â€patrauklumÄ…â€œ, tada bÅ«senoje *s'* pasirinksime veiksmÄ… *a*, kuris atitinka didÅ¾iausiÄ… *Q(s',a')* reikÅ¡mÄ™. Taigi, geriausias galimas bÅ«simas atlygis, kurÄ¯ galÄ—tume gauti bÅ«senoje *s*, bus apibrÄ—Å¾tas kaip `max`

## Tikriname politikÄ…

Kadangi Q-lentelÄ—je pateikiamas kiekvieno veiksmo "patrauklumas" kiekvienoje bÅ«senoje, jÄ… gana lengva naudoti efektyviam navigavimui mÅ«sÅ³ pasaulyje apibrÄ—Å¾ti. PaprasÄiausiu atveju galime pasirinkti veiksmÄ…, atitinkantÄ¯ didÅ¾iausiÄ… Q-lentelÄ—s reikÅ¡mÄ™: (kodo blokas 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Jei kelis kartus paleisite aukÅ¡Äiau pateiktÄ… kodÄ…, galite pastebÄ—ti, kad kartais jis "uÅ¾stringa", ir jums reikia paspausti STOP mygtukÄ… uÅ¾raÅ¡inÄ—je, kad jÄ¯ nutrauktumÄ—te. Taip nutinka, nes gali bÅ«ti situacijÅ³, kai dvi bÅ«senos "rodo" viena Ä¯ kitÄ… pagal optimaliÄ… Q-reikÅ¡mÄ™, tokiu atveju agentas nuolat juda tarp tÅ³ bÅ«senÅ³.

## ğŸš€IÅ¡Å¡Å«kis

> **UÅ¾duotis 1:** Pakeiskite `walk` funkcijÄ… taip, kad kelio ilgis bÅ«tÅ³ apribotas tam tikru Å¾ingsniÅ³ skaiÄiumi (pvz., 100), ir stebÄ—kite, kaip aukÅ¡Äiau pateiktas kodas kartais grÄ…Å¾ina Å¡iÄ… reikÅ¡mÄ™.

> **UÅ¾duotis 2:** Pakeiskite `walk` funkcijÄ… taip, kad ji negrÄ¯Å¾tÅ³ Ä¯ vietas, kuriose jau buvo. Tai neleis `walk` funkcijai uÅ¾strigti cikle, taÄiau agentas vis tiek gali "Ä¯strigti" vietoje, iÅ¡ kurios negali pabÄ—gti.

## Navigacija

GeresnÄ— navigacijos politika bÅ«tÅ³ ta, kuriÄ… naudojome mokymo metu, derinant iÅ¡naudojimÄ… ir tyrinÄ—jimÄ…. Pagal Å¡iÄ… politikÄ… kiekvienas veiksmas pasirenkamas su tam tikra tikimybe, proporcinga Q-lentelÄ—s reikÅ¡mÄ—ms. Å i strategija vis dar gali lemti, kad agentas grÄ¯Å¡ Ä¯ jau iÅ¡tirtÄ… vietÄ…, taÄiau, kaip matote iÅ¡ Å¾emiau pateikto kodo, ji lemia labai trumpÄ… vidutinÄ¯ keliÄ… iki norimos vietos (prisiminkite, kad `print_statistics` paleidÅ¾ia simuliacijÄ… 100 kartÅ³): (kodo blokas 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Paleidus Å¡Ä¯ kodÄ…, turÄ—tumÄ—te gauti daug maÅ¾esnÄ¯ vidutinÄ¯ kelio ilgÄ¯ nei anksÄiau, maÅ¾daug 3-6 diapazone.

## Mokymosi proceso tyrinÄ—jimas

Kaip jau minÄ—jome, mokymosi procesas yra pusiausvyra tarp tyrinÄ—jimo ir sukauptÅ³ Å¾iniÅ³ apie problemos erdvÄ—s struktÅ«rÄ… iÅ¡naudojimo. MatÄ—me, kad mokymosi rezultatai (gebÄ—jimas padÄ—ti agentui rasti trumpÄ… keliÄ… iki tikslo) pagerÄ—jo, taÄiau taip pat Ä¯domu stebÄ—ti, kaip vidutinis kelio ilgis keiÄiasi mokymosi proceso metu:

## Mokymosi apibendrinimas:

- **Vidutinis kelio ilgis didÄ—ja.** IÅ¡ pradÅ¾iÅ³ matome, kad vidutinis kelio ilgis didÄ—ja. Taip tikriausiai yra todÄ—l, kad, kai nieko neÅ¾inome apie aplinkÄ…, esame linkÄ™ Ä¯strigti blogose bÅ«senose, tokiose kaip vanduo ar vilkas. Kai daugiau suÅ¾inome ir pradedame naudoti Å¡ias Å¾inias, galime ilgiau tyrinÄ—ti aplinkÄ…, taÄiau vis dar nelabai Å¾inome, kur yra obuoliai.

- **Kelio ilgis maÅ¾Ä—ja, kai daugiau iÅ¡mokstame.** Kai pakankamai iÅ¡mokstame, agentui tampa lengviau pasiekti tikslÄ…, ir kelio ilgis pradeda maÅ¾Ä—ti. TaÄiau mes vis dar atviri tyrinÄ—jimui, todÄ—l daÅ¾nai nukrypstame nuo geriausio kelio ir tyrinÄ—jame naujas galimybes, dÄ—l ko kelias tampa ilgesnis nei optimalus.

- **Ilgis staiga padidÄ—ja.** Grafike taip pat pastebime, kad tam tikru momentu ilgis staiga padidÄ—ja. Tai rodo proceso stochastiÅ¡kumÄ… ir tai, kad tam tikru momentu galime "sugadinti" Q-lentelÄ—s koeficientus, perraÅ¡ydami juos naujomis reikÅ¡mÄ—mis. Tai idealiai turÄ—tÅ³ bÅ«ti sumaÅ¾inta maÅ¾inant mokymosi greitÄ¯ (pavyzdÅ¾iui, mokymo pabaigoje Q-lentelÄ—s reikÅ¡mes koreguojame tik nedidele verte).

Apskritai svarbu prisiminti, kad mokymosi proceso sÄ—kmÄ— ir kokybÄ— labai priklauso nuo parametrÅ³, tokiÅ³ kaip mokymosi greitis, mokymosi greiÄio maÅ¾Ä—jimas ir diskonto faktorius. Å ie parametrai daÅ¾nai vadinami **hiperparametrais**, kad bÅ«tÅ³ atskirti nuo **parametrÅ³**, kuriuos optimizuojame mokymo metu (pavyzdÅ¾iui, Q-lentelÄ—s koeficientai). GeriausiÅ³ hiperparametrÅ³ reikÅ¡miÅ³ paieÅ¡kos procesas vadinamas **hiperparametrÅ³ optimizavimu**, ir tai yra atskira tema.

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## UÅ¾duotis 
[Daugiau realistiÅ¡kas pasaulis](assignment.md)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.