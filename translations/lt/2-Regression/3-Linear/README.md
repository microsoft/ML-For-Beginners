<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f88fbc741d792890ff2f1430fe0dae0",
  "translation_date": "2025-09-03T16:18:11+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "lt"
}
-->
# Sukurkite regresijos modelÄ¯ naudodami Scikit-learn: keturi regresijos bÅ«dai

![Linear vs polynomial regression infographic](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.lt.png)
> Infografikas sukurtas [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Klausimynas prieÅ¡ paskaitÄ…](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [Å i pamoka pasiekiama ir R kalba!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Ä®vadas

Iki Å¡iol jÅ«s tyrinÄ—jote, kas yra regresija, naudodami pavyzdinius duomenis iÅ¡ moliÅ«gÅ³ kainÅ³ duomenÅ³ rinkinio, kurÄ¯ naudosime visoje Å¡ioje pamokoje. Taip pat vizualizavote duomenis naudodami Matplotlib.

Dabar esate pasiruoÅ¡Ä™ gilintis Ä¯ regresijÄ… maÅ¡ininio mokymosi kontekste. Nors vizualizacija leidÅ¾ia suprasti duomenis, tikroji maÅ¡ininio mokymosi galia slypi _modeliÅ³ mokyme_. Modeliai mokomi naudojant istorinius duomenis, kad automatiÅ¡kai uÅ¾fiksuotÅ³ duomenÅ³ priklausomybes, ir jie leidÅ¾ia prognozuoti rezultatus naujiems duomenims, kuriÅ³ modelis dar nematÄ—.

Å ioje pamokoje suÅ¾inosite daugiau apie du regresijos tipus: _paprastÄ… linijinÄ™ regresijÄ…_ ir _polinominÄ™ regresijÄ…_, kartu su kai kuriais matematiniais pagrindais, kurie yra Å¡iÅ³ metodÅ³ pagrindas. Å ie modeliai leis mums prognozuoti moliÅ«gÅ³ kainas, atsiÅ¾velgiant Ä¯ skirtingus Ä¯vesties duomenis.

[![ML pradedantiesiems - suprasti linijinÄ™ regresijÄ…](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pradedantiesiems - suprasti linijinÄ™ regresijÄ…")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie linijinÄ™ regresijÄ….

> Visame Å¡ioje mokymo programoje mes darome prielaidÄ…, kad turite minimalias matematikos Å¾inias, ir siekiame, kad ji bÅ«tÅ³ prieinama studentams iÅ¡ kitÅ³ sriÄiÅ³. TodÄ—l atkreipkite dÄ—mesÄ¯ Ä¯ pastabas, ğŸ§® iÅ¡naÅ¡as, diagramas ir kitus mokymosi Ä¯rankius, kurie padÄ—s suprasti.

### BÅ«tinos Å¾inios

Iki Å¡iol turÄ—tumÄ—te bÅ«ti susipaÅ¾inÄ™ su moliÅ«gÅ³ duomenÅ³ struktÅ«ra, kuriÄ… mes nagrinÄ—jame. Å iuos duomenis galite rasti iÅ¡ anksto Ä¯keltus ir iÅ¡ anksto iÅ¡valytus Å¡ios pamokos _notebook.ipynb_ faile. Faile moliÅ«gÅ³ kaina pateikiama uÅ¾ buÅ¡elÄ¯ naujame duomenÅ³ rÄ—melyje. Ä®sitikinkite, kad galite paleisti Å¡iuos uÅ¾raÅ¡Å³ knygeles Visual Studio Code aplinkoje.

### PasiruoÅ¡imas

Primename, kad Ä¯keliame Å¡iuos duomenis tam, kad galÄ—tume uÅ¾duoti jiems klausimus.

- Kada geriausias laikas pirkti moliÅ«gus?
- Kokia kaina galima tikÄ—tis uÅ¾ miniatiÅ«riniÅ³ moliÅ«gÅ³ dÄ—Å¾Ä™?
- Ar verta pirkti juos pusÄ—s buÅ¡elio krepÅ¡iuose ar 1 1/9 buÅ¡elio dÄ—Å¾Ä—se?
TÄ™skime duomenÅ³ tyrimÄ….

AnkstesnÄ—je pamokoje sukÅ«rÄ—te Pandas duomenÅ³ rÄ—melÄ¯ ir uÅ¾pildÄ—te jÄ¯ dalimi originalaus duomenÅ³ rinkinio, standartizuodami kainas pagal buÅ¡elÄ¯. TaÄiau tai leido surinkti tik apie 400 duomenÅ³ taÅ¡kÅ³ ir tik rudens mÄ—nesiams.

PaÅ¾velkite Ä¯ duomenis, kurie iÅ¡ anksto Ä¯kelti Å¡ios pamokos pridedamame uÅ¾raÅ¡Å³ knygelÄ—je. Duomenys yra iÅ¡ anksto Ä¯kelti, o pradinÄ— sklaidos diagrama sudaryta, kad parodytÅ³ mÄ—nesio duomenis. GalbÅ«t galime gauti Å¡iek tiek daugiau informacijos apie duomenÅ³ pobÅ«dÄ¯, juos dar labiau iÅ¡valydami.

## LinijinÄ—s regresijos linija

Kaip suÅ¾inojote 1 pamokoje, linijinÄ—s regresijos tikslas yra sugebÄ—ti nubrÄ—Å¾ti linijÄ…, kad:

- **ParodytÅ³ kintamÅ³jÅ³ ryÅ¡ius**. ParodytÅ³ ryÅ¡Ä¯ tarp kintamÅ³jÅ³
- **DarytÅ³ prognozes**. Tiksliai prognozuotÅ³, kur naujas duomenÅ³ taÅ¡kas atsidurtÅ³ santykyje su ta linija.

TipiÅ¡ka **maÅ¾iausiÅ³ kvadratÅ³ regresija** nubrÄ—Å¾ia tokio tipo linijÄ…. Terminas â€maÅ¾iausi kvadrataiâ€œ reiÅ¡kia, kad visi duomenÅ³ taÅ¡kai aplink regresijos linijÄ… yra kvadratuojami ir tada sudedami. Idealiu atveju galutinÄ— suma yra kuo maÅ¾esnÄ—, nes norime maÅ¾o klaidÅ³ skaiÄiaus arba `maÅ¾iausiÅ³ kvadratÅ³`.

Tai darome, nes norime modeliuoti linijÄ…, kuri turi maÅ¾iausiÄ… bendrÄ… atstumÄ… nuo visÅ³ mÅ«sÅ³ duomenÅ³ taÅ¡kÅ³. Taip pat kvadratuojame terminus prieÅ¡ juos sudÄ—dami, nes mums rÅ«pi jÅ³ dydis, o ne kryptis.

> **ğŸ§® Parodykite matematikÄ…**
>
> Å i linija, vadinama _geriausiai tinkama linija_, gali bÅ«ti iÅ¡reikÅ¡ta [lygtimi](https://en.wikipedia.org/wiki/Simple_linear_regression):
>
> ```
> Y = a + bX
> ```
>
> `X` yra â€paaiÅ¡kinamasis kintamasisâ€œ. `Y` yra â€priklausomas kintamasisâ€œ. Linijos nuolydis yra `b`, o `a` yra y-aÅ¡ies perÄ—jimo taÅ¡kas, kuris nurodo `Y` reikÅ¡mÄ™, kai `X = 0`.
>
>![apskaiÄiuoti nuolydÄ¯](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.lt.png)
>
> Pirmiausia apskaiÄiuokite nuolydÄ¯ `b`. Infografikas sukurtas [Jen Looper](https://twitter.com/jenlooper)
>
> Kitaip tariant, remiantis mÅ«sÅ³ moliÅ«gÅ³ duomenÅ³ pradiniu klausimu: â€prognozuoti moliÅ«go kainÄ… uÅ¾ buÅ¡elÄ¯ pagal mÄ—nesÄ¯â€œ, `X` reikÅ¡tÅ³ kainÄ…, o `Y` reikÅ¡tÅ³ pardavimo mÄ—nesÄ¯.
>
>![uÅ¾baigti lygtÄ¯](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.lt.png)
>
> ApskaiÄiuokite `Y` reikÅ¡mÄ™. Jei mokate apie 4 dolerius, tai turi bÅ«ti balandis! Infografikas sukurtas [Jen Looper](https://twitter.com/jenlooper)
>
> Matematiniai skaiÄiavimai, kurie apskaiÄiuoja linijÄ…, turi parodyti linijos nuolydÄ¯, kuris taip pat priklauso nuo perÄ—jimo taÅ¡ko, arba kur `Y` yra, kai `X = 0`.
>
> Å iÅ³ reikÅ¡miÅ³ skaiÄiavimo metodÄ… galite stebÄ—ti [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) svetainÄ—je. Taip pat apsilankykite [maÅ¾iausiÅ³ kvadratÅ³ skaiÄiuoklÄ—je](https://www.mathsisfun.com/data/least-squares-calculator.html), kad pamatytumÄ—te, kaip skaiÄiÅ³ reikÅ¡mÄ—s veikia linijÄ….

## Koreliacija

Dar vienas terminas, kurÄ¯ reikia suprasti, yra **koreliacijos koeficientas** tarp duotÅ³ X ir Y kintamÅ³jÅ³. Naudodami sklaidos diagramÄ…, galite greitai vizualizuoti Å¡Ä¯ koeficientÄ…. Diagrama, kurioje duomenÅ³ taÅ¡kai iÅ¡sidÄ—stÄ™ tvarkinga linija, turi aukÅ¡tÄ… koreliacijÄ…, taÄiau diagrama, kurioje duomenÅ³ taÅ¡kai iÅ¡sidÄ—stÄ™ chaotiÅ¡kai tarp X ir Y, turi Å¾emÄ… koreliacijÄ….

Geras linijinÄ—s regresijos modelis bus tas, kuris turi aukÅ¡tÄ… (artimesnÄ¯ 1 nei 0) koreliacijos koeficientÄ…, naudojant maÅ¾iausiÅ³ kvadratÅ³ regresijos metodÄ… su regresijos linija.

âœ… Paleiskite Å¡ios pamokos pridedamÄ… uÅ¾raÅ¡Å³ knygelÄ™ ir paÅ¾iÅ«rÄ—kite Ä¯ sklaidos diagramÄ…, kurioje mÄ—nuo susietas su kaina. Ar duomenys, susijÄ™ su mÄ—nesiu ir moliÅ«gÅ³ pardavimo kaina, atrodo turintys aukÅ¡tÄ… ar Å¾emÄ… koreliacijÄ…, remiantis jÅ«sÅ³ vizualine sklaidos diagramos interpretacija? Ar tai pasikeiÄia, jei naudojate smulkesnÄ¯ matÄ…, pvz., *metÅ³ dienÄ…* (t. y. dienÅ³ skaiÄiÅ³ nuo metÅ³ pradÅ¾ios)?

Å½emiau pateiktame kode darome prielaidÄ…, kad iÅ¡valÄ—me duomenis ir gavome duomenÅ³ rÄ—melÄ¯, vadinamÄ… `new_pumpkins`, panaÅ¡Å³ Ä¯ Å¡Ä¯:

ID | MÄ—nuo | MetÅ³ diena | VeislÄ— | Miestas | PakuotÄ— | MaÅ¾a kaina | AukÅ¡ta kaina | Kaina
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 15.0 | 15.0 | 13.636364

> DuomenÅ³ valymo kodas pateiktas [`notebook.ipynb`](notebook.ipynb). Atlikome tuos paÄius valymo veiksmus kaip ir ankstesnÄ—je pamokoje, ir apskaiÄiavome `MetÅ³ diena` stulpelÄ¯ naudodami Å¡iÄ… iÅ¡raiÅ¡kÄ…:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Dabar, kai suprantate linijinÄ—s regresijos matematikÄ…, sukurkime regresijos modelÄ¯, kad pamatytume, ar galime prognozuoti, kuri moliÅ«gÅ³ pakuotÄ— turÄ—s geriausias moliÅ«gÅ³ kainas. KaÅ¾kas, perkantis moliÅ«gus Å¡ventiniam moliÅ«gÅ³ sodui, gali norÄ—ti Å¡ios informacijos, kad galÄ—tÅ³ optimizuoti savo moliÅ«gÅ³ pakuoÄiÅ³ pirkimus sodui.

## IeÅ¡kome koreliacijos

[![ML pradedantiesiems - ieÅ¡kome koreliacijos: linijinÄ—s regresijos raktas](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pradedantiesiems - ieÅ¡kome koreliacijos: linijinÄ—s regresijos raktas")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie koreliacijÄ….

IÅ¡ ankstesnÄ—s pamokos turbÅ«t pastebÄ—jote, kad vidutinÄ— kaina skirtingais mÄ—nesiais atrodo taip:

<img alt="VidutinÄ— kaina pagal mÄ—nesÄ¯" src="../2-Data/images/barchart.png" width="50%"/>

Tai rodo, kad turÄ—tÅ³ bÅ«ti tam tikra koreliacija, ir galime pabandyti treniruoti linijinÄ¯ regresijos modelÄ¯, kad prognozuotume ryÅ¡Ä¯ tarp `MÄ—nuo` ir `Kaina`, arba tarp `MetÅ³ diena` ir `Kaina`. Å tai sklaidos diagrama, rodanti pastarÄ…jÄ¯ ryÅ¡Ä¯:

<img alt="Sklaidos diagrama: Kaina vs MetÅ³ diena" src="images/scatter-dayofyear.png" width="50%" /> 

PaÅ¾iÅ«rÄ—kime, ar yra koreliacija, naudodami `corr` funkcijÄ…:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Atrodo, kad koreliacija yra gana maÅ¾a, -0.15 pagal `MÄ—nuo` ir -0.17 pagal `MetÅ³ diena`, taÄiau gali bÅ«ti kita svarbi priklausomybÄ—. Atrodo, kad yra skirtingos kainÅ³ grupÄ—s, atitinkanÄios skirtingas moliÅ«gÅ³ veisles. NorÄ—dami patvirtinti Å¡iÄ… hipotezÄ™, nubrÄ—Å¾kime kiekvienÄ… moliÅ«gÅ³ kategorijÄ… naudodami skirtingÄ… spalvÄ…. Perduodami `ax` parametrÄ… `scatter` funkcijai, galime nubrÄ—Å¾ti visus taÅ¡kus toje paÄioje diagramoje:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Sklaidos diagrama: Kaina vs MetÅ³ diena" src="images/scatter-dayofyear-color.png" width="50%" /> 

MÅ«sÅ³ tyrimas rodo, kad veislÄ— turi didesnÄ¯ poveikÄ¯ bendrai kainai nei faktinÄ— pardavimo data. Tai galime pamatyti naudodami stulpelinÄ™ diagramÄ…:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="StulpelinÄ— diagrama: kaina vs veislÄ—" src="images/price-by-variety.png" width="50%" /> 

Sutelksime dÄ—mesÄ¯ tik Ä¯ vienÄ… moliÅ«gÅ³ veislÄ™, â€pie typeâ€œ, ir paÅ¾iÅ«rÄ—sime, kokÄ¯ poveikÄ¯ data turi kainai:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Sklaidos diagrama: Kaina vs MetÅ³ diena" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Jei dabar apskaiÄiuosime koreliacijÄ… tarp `Kaina` ir `MetÅ³ diena` naudodami `corr` funkcijÄ…, gausime maÅ¾daug `-0.27` - tai reiÅ¡kia, kad treniruoti prognozavimo modelÄ¯ yra prasminga.

> PrieÅ¡ treniruojant linijinÄ¯ regresijos modelÄ¯, svarbu Ä¯sitikinti, kad mÅ«sÅ³ duomenys yra Å¡varÅ«s. LinijinÄ— regresija neveikia gerai su trÅ«kstamomis reikÅ¡mÄ—mis, todÄ—l verta atsikratyti visÅ³ tuÅ¡ÄiÅ³ langeliÅ³:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Kitas poÅ¾iÅ«ris bÅ«tÅ³ uÅ¾pildyti tuÅ¡Äias reikÅ¡mes vidutinÄ—mis reikÅ¡mÄ—mis iÅ¡ atitinkamo stulpelio.

## Paprasta linijinÄ— regresija

[![ML pradedantiesiems - linijinÄ— ir polinominÄ— regresija naudojant Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pradedantiesiems - linijinÄ— ir polinominÄ— regresija naudojant Scikit-learn")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie linijinÄ™ ir polinominÄ™ regresijÄ….

NorÄ—dami treniruoti mÅ«sÅ³ linijinÄ¯ regresijos modelÄ¯, naudosime **Scikit-learn** bibliotekÄ….

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

PradÄ—sime atskirdami Ä¯vesties reikÅ¡mes (funkcijas) ir laukiamÄ… rezultatÄ… (etiketÄ™) Ä¯ atskirus numpy masyvus:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Atkreipkite dÄ—mesÄ¯, kad turÄ—jome atlikti `reshape` Ä¯vesties duomenims, kad linijinÄ—s regresijos paketas juos suprastÅ³ teisingai. LinijinÄ— regresija tikisi 2D masyvo kaip Ä¯vesties, kur kiekviena masyvo eilutÄ— atitinka Ä¯vesties funkcijÅ³ vektoriÅ³. MÅ«sÅ³ atveju, kadangi turime tik vienÄ… Ä¯vestÄ¯, mums reikia masyvo su forma NÃ—1, kur N yra duomenÅ³ rinkinio dydis.

Tada turime padalyti duomenis Ä¯ treniravimo ir testavimo duomenÅ³ rinkinius, kad galÄ—tume patikrinti savo modelÄ¯ po treniravimo:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Galiausiai, faktinis linijinÄ—s regresijos modelio treniravimas uÅ¾trunka tik dvi kodo eilutes. ApibrÄ—Å¾iame `LinearRegression` objektÄ… ir pritaikome jÄ¯ mÅ«sÅ³ duomenims naudodami `fit` metodÄ…:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression` objektas po `fit`-inimo turi visus regresijos koeficientus, kuriuos galima pasiekti naudojant `.coef_` savybÄ™. MÅ«sÅ³ atveju yra tik vienas koeficientas, kuris turÄ—tÅ³ bÅ«ti apie `-0.017`. Tai reiÅ¡kia, kad kainos atrodo Å¡iek tiek maÅ¾Ä—janÄios laikui bÄ—gant, bet ne per daug, maÅ¾daug 2 centais per dienÄ…. Taip pat galime
MÅ«sÅ³ klaida atrodo susijusi su 2 taÅ¡kais, tai yra ~17%. Nelabai gerai. Kitas modelio kokybÄ—s rodiklis yra **determinacijos koeficientas**, kurÄ¯ galima gauti taip:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Jei reikÅ¡mÄ— yra 0, tai reiÅ¡kia, kad modelis neatsiÅ¾velgia Ä¯ Ä¯vesties duomenis ir veikia kaip *blogiausias linijinis prognozuotojas*, kuris tiesiog yra rezultatÅ³ vidutinÄ— reikÅ¡mÄ—. ReikÅ¡mÄ— 1 reiÅ¡kia, kad galime tobulai prognozuoti visus numatomus rezultatus. MÅ«sÅ³ atveju determinacijos koeficientas yra apie 0.06, kas yra gana Å¾ema reikÅ¡mÄ—.

Taip pat galime nubrÄ—Å¾ti testavimo duomenis kartu su regresijos linija, kad geriau suprastume, kaip regresija veikia mÅ«sÅ³ atveju:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="LinijinÄ— regresija" src="images/linear-results.png" width="50%" />

## PolinominÄ— regresija

Kitas linijinÄ—s regresijos tipas yra polinominÄ— regresija. Nors kartais tarp kintamÅ³jÅ³ yra linijinis ryÅ¡ys â€“ kuo didesnis moliÅ«go tÅ«ris, tuo didesnÄ— kaina â€“ kartais Å¡iÅ³ ryÅ¡iÅ³ negalima pavaizduoti kaip plokÅ¡tumos ar tiesÄ—s.

âœ… Å tai [keletas pavyzdÅ¾iÅ³](https://online.stat.psu.edu/stat501/lesson/9/9.8) duomenÅ³, kuriems galÄ—tÅ³ bÅ«ti taikoma polinominÄ— regresija.

PaÅ¾velkite dar kartÄ… Ä¯ ryÅ¡Ä¯ tarp datos ir kainos. Ar Å¡is sklaidos grafikas atrodo kaip toks, kurÄ¯ bÅ«tinai reikÄ—tÅ³ analizuoti tiesÄ—s pagalba? Ar kainos negali svyruoti? Tokiu atveju galite iÅ¡bandyti polinominÄ™ regresijÄ….

âœ… Polinomai yra matematinÄ—s iÅ¡raiÅ¡kos, kurios gali susidÄ—ti iÅ¡ vieno ar daugiau kintamÅ³jÅ³ ir koeficientÅ³.

PolinominÄ— regresija sukuria kreivÄ™, kuri geriau atitinka nelinijinius duomenis. MÅ«sÅ³ atveju, jei Ä¯vesties duomenyse Ä¯trauksime kvadratinÄ¯ `DayOfYear` kintamÄ…jÄ¯, turÄ—tume galÄ—ti pritaikyti savo duomenis parabolinei kreivei, kuri turÄ—s minimumÄ… tam tikru metÅ³ momentu.

Scikit-learn turi naudingÄ… [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), leidÅ¾iantÄ¯ sujungti skirtingus duomenÅ³ apdorojimo Å¾ingsnius. **Pipeline** yra **vertintojÅ³** grandinÄ—. MÅ«sÅ³ atveju sukursime pipeline, kuris pirmiausia prideda polinomines savybes prie mÅ«sÅ³ modelio, o tada treniruoja regresijÄ…:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

Naudojant `PolynomialFeatures(2)` reiÅ¡kia, kad Ä¯trauksime visus antro laipsnio polinomus iÅ¡ Ä¯vesties duomenÅ³. MÅ«sÅ³ atveju tai tiesiog reikÅ¡ `DayOfYear`<sup>2</sup>, bet turint du Ä¯vesties kintamuosius X ir Y, tai pridÄ—s X<sup>2</sup>, XY ir Y<sup>2</sup>. Jei norime, galime naudoti aukÅ¡tesnio laipsnio polinomus.

Pipeline galima naudoti taip pat, kaip ir originalÅ³ `LinearRegression` objektÄ…, t.y. galime `fit` pipeline, o tada naudoti `predict`, kad gautume prognozÄ—s rezultatus. Å tai grafikas, rodantis testavimo duomenis ir aproksimacijos kreivÄ™:

<img alt="PolinominÄ— regresija" src="images/poly-results.png" width="50%" />

Naudojant polinominÄ™ regresijÄ…, galime gauti Å¡iek tiek maÅ¾esnÄ¯ MSE ir aukÅ¡tesnÄ¯ determinacijos koeficientÄ…, bet ne Å¾ymiai. Reikia atsiÅ¾velgti Ä¯ kitas savybes!

> Galite pastebÄ—ti, kad maÅ¾iausios moliÅ«gÅ³ kainos stebimos kaÅ¾kur apie HelovinÄ…. Kaip tai paaiÅ¡kintumÄ—te?

ğŸƒ Sveikiname, kÄ… tik sukÅ«rÄ—te modelÄ¯, kuris gali padÄ—ti prognozuoti pyraginiÅ³ moliÅ«gÅ³ kainÄ…. Tikriausiai galite pakartoti tÄ… paÄiÄ… procedÅ«rÄ… visiems moliÅ«gÅ³ tipams, bet tai bÅ«tÅ³ varginantis darbas. Dabar iÅ¡mokime, kaip Ä¯traukti moliÅ«gÅ³ veislÄ™ Ä¯ mÅ«sÅ³ modelÄ¯!

## KategorinÄ—s savybÄ—s

Idealiame pasaulyje norÄ—tume sugebÄ—ti prognozuoti kainas skirtingoms moliÅ«gÅ³ veislÄ—ms naudodami tÄ… patÄ¯ modelÄ¯. TaÄiau `Variety` stulpelis Å¡iek tiek skiriasi nuo tokiÅ³ stulpeliÅ³ kaip `Month`, nes jame yra ne skaitinÄ—s reikÅ¡mÄ—s. Tokie stulpeliai vadinami **kategoriniais**.

[![ML pradedantiesiems - KategoriniÅ³ savybiÅ³ prognozÄ—s naudojant linijinÄ™ regresijÄ…](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pradedantiesiems - KategoriniÅ³ savybiÅ³ prognozÄ—s naudojant linijinÄ™ regresijÄ…")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie kategoriniÅ³ savybiÅ³ naudojimÄ….

ÄŒia galite pamatyti, kaip vidutinÄ— kaina priklauso nuo veislÄ—s:

<img alt="VidutinÄ— kaina pagal veislÄ™" src="images/price-by-variety.png" width="50%" />

NorÄ—dami atsiÅ¾velgti Ä¯ veislÄ™, pirmiausia turime jÄ… konvertuoti Ä¯ skaitinÄ™ formÄ…, arba **uÅ¾koduoti**. Yra keli bÅ«dai, kaip tai padaryti:

* Paprastas **skaitinis kodavimas** sukurs skirtingÅ³ veisliÅ³ lentelÄ™, o tada pakeis veislÄ—s pavadinimÄ… indeksu toje lentelÄ—je. Tai nÄ—ra geriausia idÄ—ja linijinei regresijai, nes linijinÄ— regresija naudoja faktinÄ™ indekso skaitinÄ™ reikÅ¡mÄ™ ir prideda jÄ… prie rezultato, padaugindama iÅ¡ tam tikro koeficiento. MÅ«sÅ³ atveju ryÅ¡ys tarp indekso numerio ir kainos yra aiÅ¡kiai nelinijinis, net jei uÅ¾tikrinsime, kad indeksai bÅ«tÅ³ iÅ¡dÄ—styti tam tikra tvarka.
* **Vieno karÅ¡to kodavimo** metodas pakeis `Variety` stulpelÄ¯ Ä¯ 4 skirtingus stulpelius, po vienÄ… kiekvienai veislei. Kiekviename stulpelyje bus `1`, jei atitinkama eilutÄ— priklauso tam tikrai veislei, ir `0` kitu atveju. Tai reiÅ¡kia, kad linijinÄ—je regresijoje bus keturi koeficientai, po vienÄ… kiekvienai moliÅ«gÅ³ veislei, atsakingi uÅ¾ "pradinÄ™ kainÄ…" (arba "papildomÄ… kainÄ…") tai konkreÄiai veislei.

Å½emiau pateiktas kodas rodo, kaip galime vieno karÅ¡to kodavimo bÅ«du uÅ¾koduoti veislÄ™:

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

NorÄ—dami treniruoti linijinÄ™ regresijÄ…, naudodami vieno karÅ¡to kodavimo bÅ«du uÅ¾koduotÄ… veislÄ™ kaip Ä¯vestÄ¯, tiesiog turime tinkamai inicializuoti `X` ir `y` duomenis:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

LikÄ™s kodas yra toks pat, kaip ir tas, kurÄ¯ naudojome aukÅ¡Äiau linijinei regresijai treniruoti. Jei tai iÅ¡bandysite, pamatysite, kad vidutinÄ— kvadratinÄ— klaida yra maÅ¾daug tokia pati, taÄiau gauname daug aukÅ¡tesnÄ¯ determinacijos koeficientÄ… (~77%). NorÄ—dami gauti dar tikslesnes prognozes, galime atsiÅ¾velgti Ä¯ daugiau kategoriniÅ³ savybiÅ³, taip pat Ä¯ skaitines savybes, tokias kaip `Month` ar `DayOfYear`. NorÄ—dami gauti vienÄ… didelÄ¯ savybiÅ³ masyvÄ…, galime naudoti `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

ÄŒia taip pat atsiÅ¾velgiame Ä¯ `City` ir `Package` tipÄ…, kas suteikia MSE 2.84 (10%) ir determinacijÄ… 0.94!

## Viskas kartu

NorÄ—dami sukurti geriausiÄ… modelÄ¯, galime naudoti kombinuotus (vieno karÅ¡to kodavimo bÅ«du uÅ¾koduotus kategorinius + skaitinius) duomenis iÅ¡ aukÅ¡Äiau pateikto pavyzdÅ¾io kartu su polinomine regresija. Å tai visas kodas jÅ«sÅ³ patogumui:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Tai turÄ—tÅ³ suteikti geriausiÄ… determinacijos koeficientÄ…, beveik 97%, ir MSE=2.23 (~8% prognozÄ—s klaida).

| Modelis | MSE | Determinacija |  
|---------|-----|---------------|  
| `DayOfYear` Linijinis | 2.77 (17.2%) | 0.07 |  
| `DayOfYear` Polinominis | 2.73 (17.0%) | 0.08 |  
| `Variety` Linijinis | 5.24 (19.7%) | 0.77 |  
| Visos savybÄ—s Linijinis | 2.84 (10.5%) | 0.94 |  
| Visos savybÄ—s Polinominis | 2.23 (8.25%) | 0.97 |  

ğŸ† Puikiai padirbÄ—ta! JÅ«s sukÅ«rÄ—te keturis regresijos modelius per vienÄ… pamokÄ… ir pagerinote modelio kokybÄ™ iki 97%. PaskutinÄ—je regresijos dalyje iÅ¡moksite apie logistinÄ™ regresijÄ…, skirtÄ… kategorijoms nustatyti.

---

## ğŸš€IÅ¡Å¡Å«kis

IÅ¡bandykite kelis skirtingus kintamuosius Å¡iame uÅ¾raÅ¡Å³ knygelÄ—je, kad pamatytumÄ—te, kaip koreliacija atitinka modelio tikslumÄ….

## [Po paskaitos testas](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Å ioje pamokoje iÅ¡mokome apie linijinÄ™ regresijÄ…. Yra ir kitÅ³ svarbiÅ³ regresijos tipÅ³. Perskaitykite apie Stepwise, Ridge, Lasso ir Elasticnet metodus. Geras kursas, norint suÅ¾inoti daugiau, yra [Stanfordo statistinio mokymosi kursas](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## UÅ¾duotis

[Sukurkite modelÄ¯](assignment.md)  

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama profesionali Å¾mogaus vertimo paslauga. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar neteisingus interpretavimus, atsiradusius naudojant Å¡Ä¯ vertimÄ….