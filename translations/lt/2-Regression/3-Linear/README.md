<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T07:43:58+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "lt"
}
-->
# Sukurkite regresijos modelÄ¯ naudodami Scikit-learn: keturi regresijos bÅ«dai

![LinijinÄ—s ir polinominÄ—s regresijos infografika](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika: [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [PrieÅ¡ paskaitÄ… atlikite testÄ…](https://ff-quizzes.netlify.app/en/ml/)

> ### [Å i pamoka taip pat prieinama R kalba!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Ä®vadas

Iki Å¡iol jÅ«s tyrinÄ—jote, kas yra regresija, naudodami pavyzdinius duomenis iÅ¡ moliÅ«gÅ³ kainÅ³ duomenÅ³ rinkinio, kurÄ¯ naudosime visoje Å¡ioje pamokoje. Taip pat vizualizavote duomenis naudodami Matplotlib.

Dabar esate pasiruoÅ¡Ä™ giliau pasinerti Ä¯ regresijÄ… maÅ¡ininio mokymosi (ML) kontekste. Nors vizualizacija leidÅ¾ia suprasti duomenis, tikroji maÅ¡ininio mokymosi galia slypi _modeliÅ³ treniravime_. Modeliai yra treniruojami naudojant istorinius duomenis, kad automatiÅ¡kai uÅ¾fiksuotÅ³ duomenÅ³ priklausomybes, ir jie leidÅ¾ia prognozuoti rezultatus naujiems duomenims, kuriÅ³ modelis dar nematÄ—.

Å ioje pamokoje suÅ¾inosite daugiau apie du regresijos tipus: _paprastÄ… linijinÄ™ regresijÄ…_ ir _polinominÄ™ regresijÄ…_, kartu su kai kuriais Å¡iÅ³ metodÅ³ matematiniais pagrindais. Å ie modeliai leis mums prognozuoti moliÅ«gÅ³ kainas, atsiÅ¾velgiant Ä¯ skirtingus Ä¯vesties duomenis.

[![ML pradedantiesiems - LinijinÄ—s regresijos supratimas](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pradedantiesiems - LinijinÄ—s regresijos supratimas")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie linijinÄ™ regresijÄ….

> Visame Å¡ioje mokymo programoje darome prielaidÄ…, kad turite minimalias matematikos Å¾inias, ir siekiame, kad ji bÅ«tÅ³ prieinama studentams iÅ¡ kitÅ³ sriÄiÅ³. Atkreipkite dÄ—mesÄ¯ Ä¯ pastabas, ğŸ§® iÅ¡naÅ¡as, diagramas ir kitus mokymosi Ä¯rankius, kurie padÄ—s suprasti.

### PrieÅ¡ pradedant

Iki Å¡iol turÄ—tumÄ—te bÅ«ti susipaÅ¾inÄ™ su moliÅ«gÅ³ duomenÅ³ struktÅ«ra, kuriÄ… nagrinÄ—jame. Å ioje pamokoje galite rasti iÅ¡ anksto Ä¯keltus ir iÅ¡valytus duomenis _notebook.ipynb_ faile. Faile moliÅ«gÅ³ kaina pateikiama uÅ¾ buÅ¡elÄ¯ naujame duomenÅ³ rÄ—melyje. Ä®sitikinkite, kad galite paleisti Å¡iuos uÅ¾raÅ¡us naudodami Visual Studio Code branduolius.

### PasiruoÅ¡imas

Primename, kad Ä¯keliame Å¡iuos duomenis tam, kad galÄ—tume uÅ¾duoti klausimus:

- Kada geriausia pirkti moliÅ«gus?
- KokiÄ… kainÄ… galiu tikÄ—tis uÅ¾ miniatiÅ«riniÅ³ moliÅ«gÅ³ dÄ—Å¾Ä™?
- Ar turÄ—Äiau juos pirkti pusÄ—s buÅ¡elio krepÅ¡iuose ar 1 1/9 buÅ¡elio dÄ—Å¾Ä—se?
PaÅ¾iÅ«rÄ—kime giliau Ä¯ Å¡iuos duomenis.

AnkstesnÄ—je pamokoje sukÅ«rÄ—te Pandas duomenÅ³ rÄ—melÄ¯ ir uÅ¾pildÄ—te jÄ¯ dalimi pradinio duomenÅ³ rinkinio, standartizuodami kainas pagal buÅ¡elÄ¯. TaÄiau taip galÄ—jote surinkti tik apie 400 duomenÅ³ taÅ¡kÅ³ ir tik rudens mÄ—nesiams.

PaÅ¾velkite Ä¯ duomenis, kurie yra iÅ¡ anksto Ä¯kelti Å¡ios pamokos pridedamame uÅ¾raÅ¡Å³ faile. Duomenys yra iÅ¡ anksto Ä¯kelti, o pradinis sklaidos grafikas yra sudarytas, kad parodytÅ³ mÄ—nesio duomenis. GalbÅ«t galime gauti Å¡iek tiek daugiau informacijos apie duomenÅ³ pobÅ«dÄ¯, juos dar labiau iÅ¡valydami.

## LinijinÄ—s regresijos linija

Kaip suÅ¾inojote 1-oje pamokoje, linijinÄ—s regresijos tikslas yra nubrÄ—Å¾ti linijÄ…, kuri:

- **Parodo kintamÅ³jÅ³ ryÅ¡ius**. Parodo ryÅ¡Ä¯ tarp kintamÅ³jÅ³
- **LeidÅ¾ia prognozuoti**. LeidÅ¾ia tiksliai prognozuoti, kur naujas duomenÅ³ taÅ¡kas atsidurs santykyje su ta linija.

Ä®prasta **maÅ¾iausiÅ³ kvadratÅ³ regresija** naudoti Å¡io tipo linijai braiÅ¾yti. Terminas â€maÅ¾iausi kvadrataiâ€œ reiÅ¡kia, kad visi duomenÅ³ taÅ¡kai aplink regresijos linijÄ… yra pakelti kvadratu ir tada sudedami. Idealiu atveju, galutinÄ— suma yra kuo maÅ¾esnÄ—, nes norime kuo maÅ¾iau klaidÅ³ arba `maÅ¾iausiÅ³ kvadratÅ³`.

Mes tai darome, nes norime modeliuoti linijÄ…, kuri turi maÅ¾iausiÄ… bendrÄ… atstumÄ… nuo visÅ³ mÅ«sÅ³ duomenÅ³ taÅ¡kÅ³. Taip pat kvadratuojame terminus prieÅ¡ juos sudÄ—dami, nes mums rÅ«pi jÅ³ dydis, o ne kryptis.

> **ğŸ§® Parodykite matematikÄ…**
> 
> Å i linija, vadinama _geriausiai tinkanÄia linija_, gali bÅ«ti iÅ¡reikÅ¡ta [lygtimi](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` yra â€paaiÅ¡kinamasis kintamasisâ€œ. `Y` yra â€priklausomas kintamasisâ€œ. Linijos nuolydis yra `b`, o `a` yra y-aÅ¡ies perÄ—mimas, kuris nurodo `Y` reikÅ¡mÄ™, kai `X = 0`. 
>
>![apskaiÄiuoti nuolydÄ¯](../../../../2-Regression/3-Linear/images/slope.png)
>
> Pirmiausia apskaiÄiuokite nuolydÄ¯ `b`. Infografika: [Jen Looper](https://twitter.com/jenlooper)
>
> Kitaip tariant, remiantis mÅ«sÅ³ moliÅ«gÅ³ duomenÅ³ pradiniu klausimu: â€prognozuokite moliÅ«go kainÄ… uÅ¾ buÅ¡elÄ¯ pagal mÄ—nesÄ¯â€œ, `X` reikÅ¡tÅ³ kainÄ…, o `Y` reikÅ¡tÅ³ pardavimo mÄ—nesÄ¯. 
>
>![uÅ¾baigti lygtÄ¯](../../../../2-Regression/3-Linear/images/calculation.png)
>
> ApskaiÄiuokite `Y` reikÅ¡mÄ™. Jei mokate apie 4 dolerius, tai turi bÅ«ti balandis! Infografika: [Jen Looper](https://twitter.com/jenlooper)
>
> Matematikos, kuri apskaiÄiuoja linijÄ…, tikslas yra parodyti linijos nuolydÄ¯, kuris taip pat priklauso nuo perÄ—mimo, arba kur `Y` yra, kai `X = 0`.
>
> Galite stebÄ—ti Å¡iÅ³ reikÅ¡miÅ³ skaiÄiavimo metodÄ… svetainÄ—je [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Taip pat apsilankykite [maÅ¾iausiÅ³ kvadratÅ³ skaiÄiuoklÄ—je](https://www.mathsisfun.com/data/least-squares-calculator.html), kad pamatytumÄ—te, kaip skaiÄiÅ³ reikÅ¡mÄ—s veikia linijÄ….

## Koreliacija

Dar vienas terminas, kurÄ¯ reikia suprasti, yra **koreliacijos koeficientas** tarp duotÅ³ X ir Y kintamÅ³jÅ³. Naudodami sklaidos grafikÄ…, galite greitai vizualizuoti Å¡Ä¯ koeficientÄ…. Grafikas, kuriame duomenÅ³ taÅ¡kai iÅ¡sidÄ—stÄ™ tvarkinga linija, turi aukÅ¡tÄ… koreliacijÄ…, taÄiau grafikas, kuriame duomenÅ³ taÅ¡kai iÅ¡mÄ—tyti tarp X ir Y, turi Å¾emÄ… koreliacijÄ….

Geras linijinÄ—s regresijos modelis bus tas, kuris turi aukÅ¡tÄ… (artimesnÄ¯ 1 nei 0) koreliacijos koeficientÄ…, naudojant maÅ¾iausiÅ³ kvadratÅ³ regresijos metodÄ… su regresijos linija.

âœ… Paleiskite Å¡ios pamokos pridedamÄ… uÅ¾raÅ¡Å³ failÄ… ir paÅ¾iÅ«rÄ—kite Ä¯ mÄ—nesio ir kainos sklaidos grafikÄ…. Ar duomenys, siejantys mÄ—nesÄ¯ su moliÅ«gÅ³ pardavimo kaina, atrodo turintys aukÅ¡tÄ… ar Å¾emÄ… koreliacijÄ…, remiantis jÅ«sÅ³ vizualine sklaidos grafiko interpretacija? Ar tai pasikeiÄia, jei naudojate tikslesnÄ¯ matÄ…, pvz., *metÅ³ dienÄ…* (t. y. dienÅ³ skaiÄiÅ³ nuo metÅ³ pradÅ¾ios)?

Toliau pateiktame kode darome prielaidÄ…, kad iÅ¡valÄ—me duomenis ir gavome duomenÅ³ rÄ—melÄ¯, vadinamÄ… `new_pumpkins`, panaÅ¡Å³ Ä¯ Å¡Ä¯:

ID | MÄ—nuo | MetÅ³Diena | RÅ«Å¡is | Miestas | PakuotÄ— | MaÅ¾iausia Kaina | DidÅ¾iausia Kaina | Kaina
---|-------|-----------|-------|---------|---------|-----------------|------------------|------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡elio dÄ—Å¾Ä—s | 15.0 | 15.0 | 13.636364

> DuomenÅ³ valymo kodas yra pateiktas [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Atlikome tuos paÄius valymo veiksmus kaip ir ankstesnÄ—je pamokoje, ir apskaiÄiavome `MetÅ³Diena` stulpelÄ¯ naudodami Å¡iÄ… iÅ¡raiÅ¡kÄ…:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Dabar, kai suprantate linijinÄ—s regresijos matematikÄ…, sukurkime regresijos modelÄ¯, kad pamatytume, ar galime prognozuoti, kuri moliÅ«gÅ³ pakuotÄ— turÄ—s geriausias kainas. KaÅ¾kas, perkantis moliÅ«gus Å¡ventinei moliÅ«gÅ³ aikÅ¡telei, galÄ—tÅ³ norÄ—ti Å¡ios informacijos, kad optimizuotÅ³ savo pirkinius.

## IeÅ¡kome koreliacijos

[![ML pradedantiesiems - Koreliacijos paieÅ¡ka: LinijinÄ—s regresijos raktas](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pradedantiesiems - Koreliacijos paieÅ¡ka: LinijinÄ—s regresijos raktas")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie koreliacijÄ….

IÅ¡ ankstesnÄ—s pamokos tikriausiai matÄ—te, kad vidutinÄ— kaina skirtingais mÄ—nesiais atrodo taip:

<img alt="VidutinÄ— kaina pagal mÄ—nesÄ¯" src="../2-Data/images/barchart.png" width="50%"/>

Tai rodo, kad turÄ—tÅ³ bÅ«ti tam tikra koreliacija, ir galime pabandyti treniruoti linijinÄ¯ regresijos modelÄ¯, kad prognozuotume ryÅ¡Ä¯ tarp `MÄ—nuo` ir `Kaina`, arba tarp `MetÅ³Diena` ir `Kaina`. Å tai sklaidos grafikas, rodantis pastarÄ…jÄ¯ ryÅ¡Ä¯:

<img alt="Sklaidos grafikas: Kaina vs. MetÅ³ Diena" src="images/scatter-dayofyear.png" width="50%" /> 

PaÅ¾iÅ«rÄ—kime, ar yra koreliacija, naudodami `corr` funkcijÄ…:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Atrodo, kad koreliacija yra gana maÅ¾a: -0.15 pagal `MÄ—nuo` ir -0.17 pagal `MetÅ³Diena`, taÄiau gali bÅ«ti kita svarbi priklausomybÄ—. Atrodo, kad yra skirtingos kainÅ³ grupÄ—s, atitinkanÄios skirtingas moliÅ«gÅ³ rÅ«Å¡is. NorÄ—dami patvirtinti Å¡iÄ… hipotezÄ™, nubrÄ—Å¾kime kiekvienÄ… moliÅ«gÅ³ kategorijÄ… skirtinga spalva. Perduodami `ax` parametrÄ… `scatter` braiÅ¾ymo funkcijai, galime nubrÄ—Å¾ti visus taÅ¡kus tame paÄiame grafike:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Sklaidos grafikas: Kaina vs. MetÅ³ Diena" src="images/scatter-dayofyear-color.png" width="50%" /> 

MÅ«sÅ³ tyrimas rodo, kad rÅ«Å¡is turi didesnÄ™ Ä¯takÄ… bendrai kainai nei faktinÄ— pardavimo data. Tai galime pamatyti stulpeline diagrama:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="StulpelinÄ— diagrama: Kaina pagal rÅ«Å¡Ä¯" src="images/price-by-variety.png" width="50%" /> 

Dabar sutelkime dÄ—mesÄ¯ tik Ä¯ vienÄ… moliÅ«gÅ³ rÅ«Å¡Ä¯, â€pie typeâ€œ, ir paÅ¾iÅ«rÄ—kime, kokiÄ… Ä¯takÄ… data turi kainai:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Sklaidos grafikas: Kaina vs. MetÅ³ Diena" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Jei dabar apskaiÄiuosime koreliacijÄ… tarp `Kaina` ir `MetÅ³Diena` naudodami `corr` funkcijÄ…, gausime maÅ¾daug `-0.27` - tai reiÅ¡kia, kad treniruoti prognozavimo modelÄ¯ yra prasminga.

> PrieÅ¡ treniruojant linijinÄ¯ regresijos modelÄ¯, svarbu Ä¯sitikinti, kad mÅ«sÅ³ duomenys yra Å¡varÅ«s. LinijinÄ— regresija neveikia gerai su trÅ«kstamomis reikÅ¡mÄ—mis, todÄ—l verta paÅ¡alinti visas tuÅ¡Äias lÄ…steles:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Kitas poÅ¾iÅ«ris bÅ«tÅ³ uÅ¾pildyti tas tuÅ¡Äias reikÅ¡mes vidutinÄ—mis reikÅ¡mÄ—mis iÅ¡ atitinkamo stulpelio.

## Paprasta linijinÄ— regresija

[![ML pradedantiesiems - LinijinÄ— ir polinominÄ— regresija naudojant Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pradedantiesiems - LinijinÄ— ir polinominÄ— regresija naudojant Scikit-learn")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie linijinÄ™ ir polinominÄ™ regresijÄ….

NorÄ—dami treniruoti mÅ«sÅ³ linijinÄ¯ regresijos modelÄ¯, naudosime **Scikit-learn** bibliotekÄ….

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

PradÄ—sime atskirdami Ä¯vesties reikÅ¡mes (savybes) ir laukiamÄ… rezultatÄ… (etiketÄ™) Ä¯ atskirus numpy masyvus:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Atkreipkite dÄ—mesÄ¯, kad turÄ—jome atlikti `reshape` Ä¯vesties duomenims, kad linijinÄ—s regresijos paketas juos suprastÅ³ teisingai. LinijinÄ— regresija tikisi 2D masyvo kaip Ä¯vesties, kur kiekviena masyvo eilutÄ— atitinka Ä¯vesties savybiÅ³ vektoriÅ³. MÅ«sÅ³ atveju, kadangi turime tik vienÄ… Ä¯vestÄ¯, mums reikia masyvo su forma NÃ—1, kur N yra duomenÅ³ rinkinio dydis.

Tada turime padalyti duomenis Ä¯ treniravimo ir testavimo duomenÅ³ rinkinius, kad galÄ—tume patikrinti savo modelÄ¯ po treniravimo:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Galiausiai, treniruoti faktinÄ¯ linijinÄ¯ regresijos modelÄ¯ uÅ¾trunka tik dvi kodo eilutes. ApibrÄ—Å¾iame `LinearRegression` objektÄ… ir pritaikome jÄ¯ mÅ«sÅ³ duomenims naudodami `fit` metodÄ…:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression` objektas po `fit`-inimo turi visus regresijos koeficientus, kuriuos galima pasiekti naudojant `.coef_` savybÄ™. MÅ«sÅ³ atveju yra tik vienas koeficientas, kuris turÄ—tÅ³ bÅ«ti apie `-0.017`. Tai reiÅ¡kia, kad kainos atrodo Å¡iek tiek maÅ¾Ä—janÄios laikui bÄ—gant, bet ne per daug - apie 2 centus per dienÄ…. Taip pat galime pasiekti regresijos susikirtimo taÅ¡kÄ… su Y aÅ¡imi naudodami `lin_reg.intercept_` - mÅ«sÅ³ atveju
MÅ«sÅ³ klaida atrodo susijusi su 2 taÅ¡kais, tai yra ~17%. Nelabai gerai. Kitas modelio kokybÄ—s rodiklis yra **determinacijos koeficientas**, kurÄ¯ galima gauti taip:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Jei reikÅ¡mÄ— yra 0, tai reiÅ¡kia, kad modelis neatsiÅ¾velgia Ä¯ Ä¯vesties duomenis ir veikia kaip *blogiausias linijinis prognozuotojas*, kuris tiesiog yra rezultatÅ³ vidutinÄ— reikÅ¡mÄ—. ReikÅ¡mÄ— 1 reiÅ¡kia, kad galime tobulai prognozuoti visus numatomus rezultatus. MÅ«sÅ³ atveju determinacijos koeficientas yra apie 0.06, kas yra gana Å¾ema reikÅ¡mÄ—.

Taip pat galime nubraiÅ¾yti testinius duomenis kartu su regresijos linija, kad geriau suprastume, kaip regresija veikia mÅ«sÅ³ atveju:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="LinijinÄ— regresija" src="images/linear-results.png" width="50%" />

## PolinominÄ— regresija  

Kitas linijinÄ—s regresijos tipas yra polinominÄ— regresija. Nors kartais tarp kintamÅ³jÅ³ yra linijinis ryÅ¡ys â€“ kuo didesnis moliÅ«gas pagal tÅ«rÄ¯, tuo didesnÄ— kaina â€“ kartais Å¡iÅ³ ryÅ¡iÅ³ negalima pavaizduoti kaip plokÅ¡tumos ar tiesÄ—s.

âœ… Å tai [keletas pavyzdÅ¾iÅ³](https://online.stat.psu.edu/stat501/lesson/9/9.8) duomenÅ³, kuriems galÄ—tÅ³ bÅ«ti taikoma polinominÄ— regresija.

PaÅ¾velkite dar kartÄ… Ä¯ ryÅ¡Ä¯ tarp datos ir kainos. Ar Å¡is sklaidos grafikas atrodo taip, kad jÄ¯ bÅ«tinai reikÄ—tÅ³ analizuoti tiesÄ—s pagalba? Ar kainos negali svyruoti? Tokiu atveju galite iÅ¡bandyti polinominÄ™ regresijÄ….

âœ… Polinomai yra matematinÄ—s iÅ¡raiÅ¡kos, kurios gali susidÄ—ti iÅ¡ vieno ar daugiau kintamÅ³jÅ³ ir koeficientÅ³.

PolinominÄ— regresija sukuria kreivÄ™, kuri geriau atitinka nelinijinius duomenis. MÅ«sÅ³ atveju, jei Ä¯vesties duomenyse Ä¯trauksime kvadratinÄ¯ `DayOfYear` kintamÄ…jÄ¯, turÄ—tume galÄ—ti pritaikyti savo duomenis parabolinei kreivei, kuri turÄ—s minimumÄ… tam tikru metÅ³ momentu.

Scikit-learn turi naudingÄ… [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), leidÅ¾iantÄ¯ sujungti skirtingus duomenÅ³ apdorojimo Å¾ingsnius. **Pipeline** yra **vertintojÅ³** grandinÄ—. MÅ«sÅ³ atveju sukursime pipeline, kuris pirmiausia prideda polinomines savybes prie mÅ«sÅ³ modelio, o tada treniruoja regresijÄ…:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

Naudojant `PolynomialFeatures(2)` reiÅ¡kia, kad Ä¯trauksime visus antro laipsnio polinomus iÅ¡ Ä¯vesties duomenÅ³. MÅ«sÅ³ atveju tai tiesiog reikÅ¡ `DayOfYear`<sup>2</sup>, bet turint du Ä¯vesties kintamuosius X ir Y, tai pridÄ—s X<sup>2</sup>, XY ir Y<sup>2</sup>. Jei norime, galime naudoti aukÅ¡tesnio laipsnio polinomus.

Pipeline galima naudoti taip pat, kaip ir originalÅ³ `LinearRegression` objektÄ…, t.y. galime `fit` pipeline, o tada naudoti `predict`, kad gautume prognozÄ—s rezultatus. Å tai grafikas, rodantis testinius duomenis ir aproksimacijos kreivÄ™:

<img alt="PolinominÄ— regresija" src="images/poly-results.png" width="50%" />

Naudojant polinominÄ™ regresijÄ…, galime gauti Å¡iek tiek maÅ¾esnÄ¯ MSE ir aukÅ¡tesnÄ¯ determinacijos koeficientÄ…, bet ne Å¾ymiai. Turime atsiÅ¾velgti Ä¯ kitas savybes!

> Galite pastebÄ—ti, kad maÅ¾iausios moliÅ«gÅ³ kainos stebimos kaÅ¾kur apie HelovinÄ…. Kaip tai paaiÅ¡kintumÄ—te?

ğŸƒ Sveikiname, kÄ… tik sukÅ«rÄ—te modelÄ¯, kuris gali padÄ—ti prognozuoti pyraginiÅ³ moliÅ«gÅ³ kainÄ…. Tikriausiai galite pakartoti tÄ… paÄiÄ… procedÅ«rÄ… visiems moliÅ«gÅ³ tipams, bet tai bÅ«tÅ³ varginantis darbas. Dabar iÅ¡mokime, kaip Ä¯traukti moliÅ«gÅ³ veislÄ™ Ä¯ mÅ«sÅ³ modelÄ¯!

## KategorinÄ—s savybÄ—s  

Idealiame pasaulyje norÄ—tume sugebÄ—ti prognozuoti kainas skirtingoms moliÅ«gÅ³ veislÄ—ms naudodami tÄ… patÄ¯ modelÄ¯. TaÄiau `Variety` stulpelis Å¡iek tiek skiriasi nuo tokiÅ³ stulpeliÅ³ kaip `Month`, nes jame yra ne skaitinÄ—s reikÅ¡mÄ—s. Tokie stulpeliai vadinami **kategoriniais**.

[![ML pradedantiesiems - KategoriniÅ³ savybiÅ³ prognozÄ—s naudojant linijinÄ™ regresijÄ…](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pradedantiesiems - KategoriniÅ³ savybiÅ³ prognozÄ—s naudojant linijinÄ™ regresijÄ…")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie kategoriniÅ³ savybiÅ³ naudojimÄ….

ÄŒia galite pamatyti, kaip vidutinÄ— kaina priklauso nuo veislÄ—s:

<img alt="VidutinÄ— kaina pagal veislÄ™" src="images/price-by-variety.png" width="50%" />

NorÄ—dami atsiÅ¾velgti Ä¯ veislÄ™, pirmiausia turime jÄ… konvertuoti Ä¯ skaitinÄ™ formÄ…, arba **uÅ¾koduoti**. Yra keli bÅ«dai, kaip tai padaryti:

* Paprastas **skaitinis kodavimas** sukurs skirtingÅ³ veisliÅ³ lentelÄ™, o tada pakeis veislÄ—s pavadinimÄ… indeksu toje lentelÄ—je. Tai nÄ—ra geriausia idÄ—ja linijinei regresijai, nes linijinÄ— regresija naudoja faktinÄ™ indekso skaitinÄ™ reikÅ¡mÄ™ ir prideda jÄ… prie rezultato, padaugindama iÅ¡ tam tikro koeficiento. MÅ«sÅ³ atveju ryÅ¡ys tarp indekso numerio ir kainos yra aiÅ¡kiai nelinijinis, net jei uÅ¾tikrinsime, kad indeksai bÅ«tÅ³ iÅ¡dÄ—styti tam tikra tvarka.
* **Vieno karÅ¡to kodavimo (One-hot encoding)** pakeis `Variety` stulpelÄ¯ 4 skirtingais stulpeliais, po vienÄ… kiekvienai veislei. Kiekviename stulpelyje bus `1`, jei atitinkama eilutÄ— priklauso tam tikrai veislei, ir `0` kitu atveju. Tai reiÅ¡kia, kad linijinÄ—je regresijoje bus keturi koeficientai, po vienÄ… kiekvienai moliÅ«gÅ³ veislei, atsakingi uÅ¾ "pradinÄ™ kainÄ…" (arba "papildomÄ… kainÄ…") tai konkreÄiai veislei.

Å½emiau pateiktas kodas rodo, kaip galime vieno karÅ¡to kodavimo bÅ«du uÅ¾koduoti veislÄ™:

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

NorÄ—dami treniruoti linijinÄ™ regresijÄ…, naudodami vieno karÅ¡to kodavimo bÅ«du uÅ¾koduotÄ… veislÄ™ kaip Ä¯vestÄ¯, tiesiog turime tinkamai inicializuoti `X` ir `y` duomenis:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

LikÄ™s kodas yra toks pat, kaip tas, kurÄ¯ naudojome aukÅ¡Äiau linijinei regresijai treniruoti. Jei tai iÅ¡bandysite, pamatysite, kad vidutinÄ— kvadratinÄ— klaida yra maÅ¾daug tokia pati, taÄiau gauname daug aukÅ¡tesnÄ¯ determinacijos koeficientÄ… (~77%). NorÄ—dami gauti dar tikslesnes prognozes, galime atsiÅ¾velgti Ä¯ daugiau kategoriniÅ³ savybiÅ³, taip pat skaitines savybes, tokias kaip `Month` ar `DayOfYear`. NorÄ—dami gauti vienÄ… didelÄ¯ savybiÅ³ masyvÄ…, galime naudoti `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

ÄŒia taip pat atsiÅ¾velgiame Ä¯ `City` ir `Package` tipÄ…, kas suteikia MSE 2.84 (10%) ir determinacijÄ… 0.94!

## Viskas kartu  

NorÄ—dami sukurti geriausiÄ… modelÄ¯, galime naudoti kombinuotus (vieno karÅ¡to kodavimo kategorinius + skaitinius) duomenis iÅ¡ aukÅ¡Äiau pateikto pavyzdÅ¾io kartu su polinomine regresija. Å tai visas kodas jÅ«sÅ³ patogumui:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Tai turÄ—tÅ³ suteikti geriausiÄ… determinacijos koeficientÄ… beveik 97% ir MSE=2.23 (~8% prognozÄ—s klaida).

| Modelis | MSE | Determinacija |  
|---------|-----|---------------|  
| `DayOfYear` Linijinis | 2.77 (17.2%) | 0.07 |  
| `DayOfYear` Polinominis | 2.73 (17.0%) | 0.08 |  
| `Variety` Linijinis | 5.24 (19.7%) | 0.77 |  
| Visos savybÄ—s Linijinis | 2.84 (10.5%) | 0.94 |  
| Visos savybÄ—s Polinominis | 2.23 (8.25%) | 0.97 |  

ğŸ† Puikiai padirbÄ—ta! JÅ«s sukÅ«rÄ—te keturis regresijos modelius per vienÄ… pamokÄ… ir pagerinote modelio kokybÄ™ iki 97%. PaskutinÄ—je regresijos dalyje iÅ¡moksite apie logistinÄ™ regresijÄ…, skirtÄ… kategorijoms nustatyti.

---  
## ğŸš€IÅ¡Å¡Å«kis  

IÅ¡bandykite kelis skirtingus kintamuosius Å¡iame uÅ¾raÅ¡Å³ knygelÄ—je, kad pamatytumÄ—te, kaip koreliacija atitinka modelio tikslumÄ….

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)  

## ApÅ¾valga ir savarankiÅ¡kas mokymasis  

Å ioje pamokoje iÅ¡mokome apie linijinÄ™ regresijÄ…. Yra ir kitÅ³ svarbiÅ³ regresijos tipÅ³. Perskaitykite apie Stepwise, Ridge, Lasso ir Elasticnet metodus. Geras kursas, norint suÅ¾inoti daugiau, yra [Stanfordo statistinio mokymosi kursas](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## UÅ¾duotis  

[Sukurkite modelÄ¯](assignment.md)  

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant dirbtinio intelekto vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, atkreipiame dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus aiÅ¡kinimus, kylanÄius dÄ—l Å¡io vertimo naudojimo.