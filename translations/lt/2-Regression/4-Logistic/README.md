<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T07:45:13+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "lt"
}
-->
# LogistinÄ— regresija kategorijoms prognozuoti

![LogistinÄ—s ir linijinÄ—s regresijos infografika](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [PrieÅ¡ paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

> ### [Å i pamoka pasiekiama ir R kalba!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Ä®vadas

Å ioje paskutinÄ—je pamokoje apie regresijÄ…, vienÄ… iÅ¡ pagrindiniÅ³ _klasikiniÅ³_ ML technikÅ³, apÅ¾velgsime logistinÄ—s regresijos metodÄ…. Å iÄ… technikÄ… galite naudoti norÄ—dami atrasti dÄ—sningumus ir prognozuoti dvejetaines kategorijas. Ar Å¡is saldainis yra Å¡okoladas, ar ne? Ar Å¡i liga yra uÅ¾kreÄiama, ar ne? Ar Å¡is klientas pasirinks Å¡Ä¯ produktÄ…, ar ne?

Å ioje pamokoje suÅ¾inosite:

- NaujÄ… bibliotekÄ… duomenÅ³ vizualizacijai
- LogistinÄ—s regresijos technikas

âœ… Gilinkite savo supratimÄ… apie darbÄ… su Å¡io tipo regresija Å¡iame [mokymosi modulyje](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## BÅ«tinos Å¾inios

Dirbdami su moliÅ«gÅ³ duomenimis jau pakankamai susipaÅ¾inome su jais, kad suprastume, jog yra viena dvejetainÄ— kategorija, su kuria galime dirbti: `Spalva`.

Sukurkime logistinÄ—s regresijos modelÄ¯, kuris prognozuotÅ³, _kokia spalva greiÄiausiai bus tam tikras moliÅ«gas_ (oranÅ¾inÄ— ğŸƒ ar balta ğŸ‘»).

> KodÄ—l kalbame apie dvejetainÄ™ klasifikacijÄ… pamokoje apie regresijÄ…? Tik dÄ—l lingvistinio patogumo, nes logistinÄ— regresija iÅ¡ tiesÅ³ yra [klasifikacijos metodas](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), nors ir pagrÄ¯stas linijine regresija. SuÅ¾inokite apie kitus duomenÅ³ klasifikavimo bÅ«dus kitame pamokÅ³ cikle.

## ApibrÄ—Å¾kite klausimÄ…

MÅ«sÅ³ tikslams iÅ¡reikÅ¡ime tai kaip dvejetainÄ™ kategorijÄ…: â€Baltaâ€œ arba â€Ne baltaâ€œ. MÅ«sÅ³ duomenÅ³ rinkinyje taip pat yra â€dryÅ¾uotaâ€œ kategorija, taÄiau jos pavyzdÅ¾iÅ³ yra nedaug, todÄ—l jos nenaudosime. Ji vis tiek iÅ¡nyksta, kai paÅ¡aliname tuÅ¡Äias reikÅ¡mes iÅ¡ duomenÅ³ rinkinio.

> ğŸƒ Smagus faktas: baltus moliÅ«gus kartais vadiname â€vaiduokliaisâ€œ. Juos nÄ—ra lengva iÅ¡skaptuoti, todÄ—l jie nÄ—ra tokie populiarÅ«s kaip oranÅ¾iniai, bet atrodo Ä¯spÅ«dingai! Taigi galÄ—tume reformuluoti savo klausimÄ… kaip: â€Vaiduoklisâ€œ arba â€Ne vaiduoklisâ€œ. ğŸ‘»

## Apie logistinÄ™ regresijÄ…

LogistinÄ— regresija skiriasi nuo linijinÄ—s regresijos, kuriÄ… jau iÅ¡mokote, keliais svarbiais aspektais.

[![ML pradedantiesiems - LogistinÄ—s regresijos supratimas maÅ¡ininio mokymosi klasifikacijai](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML pradedantiesiems - LogistinÄ—s regresijos supratimas maÅ¡ininio mokymosi klasifikacijai")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie logistinÄ—s regresijos apÅ¾valgÄ….

### DvejetainÄ— klasifikacija

LogistinÄ— regresija nepasiÅ«lo tÅ³ paÄiÅ³ funkcijÅ³ kaip linijinÄ— regresija. Pirmoji pateikia prognozÄ™ apie dvejetainÄ™ kategorijÄ… (â€balta arba ne baltaâ€œ), o antroji gali prognozuoti tÄ™stines reikÅ¡mes, pavyzdÅ¾iui, atsiÅ¾velgiant Ä¯ moliÅ«go kilmÄ™ ir derliaus nuÄ—mimo laikÄ…, _kaip padidÄ—s jo kaina_.

![MoliÅ«gÅ³ klasifikavimo modelis](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> InfografikÄ… sukÅ«rÄ— [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Kitos klasifikacijos

Yra ir kitÅ³ logistinÄ—s regresijos tipÅ³, Ä¯skaitant daugianarÄ™ ir tvarkÄ… turinÄiÄ…:

- **DaugianarÄ—**, kai yra daugiau nei viena kategorija - â€OranÅ¾inÄ—, Balta ir DryÅ¾uotaâ€œ.
- **TvarkÄ… turinti**, kai kategorijos yra iÅ¡dÄ—stytos logiÅ¡ka tvarka, naudinga, jei norÄ—tume logiÅ¡kai iÅ¡dÄ—styti rezultatus, pavyzdÅ¾iui, moliÅ«gus, kurie yra iÅ¡dÄ—styti pagal ribotÄ… dydÅ¾iÅ³ skaiÄiÅ³ (mini, maÅ¾as, vidutinis, didelis, labai didelis, milÅ¾iniÅ¡kas).

![DaugianarÄ— vs tvarkÄ… turinti regresija](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Kintamieji NETURI bÅ«ti koreliuoti

Prisiminkite, kaip linijinÄ— regresija geriau veikÄ— su labiau koreliuotais kintamaisiais? LogistinÄ— regresija yra prieÅ¡inga â€“ kintamieji neturi bÅ«ti susijÄ™. Tai tinka Å¡iems duomenims, kuriÅ³ koreliacijos yra gana silpnos.

### Reikia daug Å¡variÅ³ duomenÅ³

LogistinÄ— regresija pateiks tikslesnius rezultatus, jei naudosite daugiau duomenÅ³; mÅ«sÅ³ maÅ¾as duomenÅ³ rinkinys nÄ—ra optimalus Å¡iai uÅ¾duoÄiai, todÄ—l turÄ—kite tai omenyje.

[![ML pradedantiesiems - DuomenÅ³ analizÄ— ir paruoÅ¡imas logistinei regresijai](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML pradedantiesiems - DuomenÅ³ analizÄ— ir paruoÅ¡imas logistinei regresijai")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie duomenÅ³ paruoÅ¡imÄ… linijinei regresijai

âœ… Pagalvokite apie duomenÅ³ tipus, kurie geriausiai tiktÅ³ logistinei regresijai

## UÅ¾duotis - sutvarkykite duomenis

Pirmiausia Å¡iek tiek iÅ¡valykite duomenis, paÅ¡alindami tuÅ¡Äias reikÅ¡mes ir pasirinkdami tik kelis stulpelius:

1. PridÄ—kite Å¡Ä¯ kodÄ…:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Visada galite paÅ¾velgti Ä¯ savo naujÄ… duomenÅ³ rÄ—melÄ¯:

    ```python
    pumpkins.info
    ```

### Vizualizacija - kategorinis grafikas

Iki Å¡iol Ä¯kÄ—lÄ—te [pradinÄ¯ uÅ¾raÅ¡Å³ knygelÄ™](../../../../2-Regression/4-Logistic/notebook.ipynb) su moliÅ«gÅ³ duomenimis ir iÅ¡valÄ—te jÄ…, kad iÅ¡saugotumÄ—te duomenÅ³ rinkinÄ¯, kuriame yra keli kintamieji, Ä¯skaitant `SpalvÄ…`. Vizualizuokime duomenÅ³ rÄ—melÄ¯ uÅ¾raÅ¡Å³ knygelÄ—je naudodami kitÄ… bibliotekÄ…: [Seaborn](https://seaborn.pydata.org/index.html), kuri yra sukurta ant Matplotlib, kurÄ¯ naudojome anksÄiau.

Seaborn siÅ«lo keletÄ… Ä¯domiÅ³ bÅ«dÅ³ vizualizuoti duomenis. PavyzdÅ¾iui, galite palyginti duomenÅ³ pasiskirstymÄ… pagal `Variety` ir `Color` kategoriniame grafike.

1. Sukurkite tokÄ¯ grafikÄ… naudodami `catplot` funkcijÄ…, naudodami mÅ«sÅ³ moliÅ«gÅ³ duomenis `pumpkins` ir nurodydami spalvÅ³ Å¾emÄ—lapÄ¯ kiekvienai moliÅ«gÅ³ kategorijai (oranÅ¾inÄ— arba balta):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![DuomenÅ³ vizualizacijos tinklelis](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    StebÄ—dami duomenis galite pamatyti, kaip `Spalva` duomenys susijÄ™ su `Variety`.

    âœ… AtsiÅ¾velgdami Ä¯ Å¡Ä¯ kategorinÄ¯ grafikÄ…, kokius Ä¯domius tyrimus galite Ä¯sivaizduoti?

### DuomenÅ³ paruoÅ¡imas: poÅ¾ymiÅ³ ir etikeÄiÅ³ kodavimas

MÅ«sÅ³ moliÅ«gÅ³ duomenÅ³ rinkinyje visos stulpeliÅ³ reikÅ¡mÄ—s yra tekstinÄ—s. Dirbti su kategoriniais duomenimis Å¾monÄ—ms yra intuityvu, taÄiau maÅ¡inoms â€“ ne. MaÅ¡ininio mokymosi algoritmai geriau veikia su skaitiniais duomenimis. TodÄ—l kodavimas yra labai svarbus duomenÅ³ paruoÅ¡imo etapas, nes jis leidÅ¾ia paversti kategorinius duomenis skaitiniais, neprarandant informacijos. Geras kodavimas padeda sukurti gerÄ… modelÄ¯.

PoÅ¾ymiÅ³ kodavimui yra du pagrindiniai kodavimo tipai:

1. Ordinalinis kodavimas: jis gerai tinka tvarkÄ… turintiems kintamiesiems, kurie yra kategoriniai kintamieji, kuriÅ³ duomenys turi logiÅ¡kÄ… tvarkÄ…, kaip `Item Size` stulpelis mÅ«sÅ³ duomenÅ³ rinkinyje. Jis sukuria Å¾emÄ—lapÄ¯, kuriame kiekviena kategorija yra atvaizduojama skaiÄiumi, kuris atitinka kategorijos tvarkÄ… stulpelyje.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorinis kodavimas: jis gerai tinka nominaliems kintamiesiems, kurie yra kategoriniai kintamieji, kuriÅ³ duomenys neturi logiÅ¡kos tvarkos, kaip visi poÅ¾ymiai, iÅ¡skyrus `Item Size` mÅ«sÅ³ duomenÅ³ rinkinyje. Tai yra vieno karÅ¡to kodavimo metodas, kuris reiÅ¡kia, kad kiekviena kategorija yra atvaizduojama dvejetainiu stulpeliu: uÅ¾koduota reikÅ¡mÄ— yra lygi 1, jei moliÅ«gas priklauso tai `Variety`, ir 0, jei ne.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```

Tada `ColumnTransformer` naudojamas keliems kodavimo metodams sujungti Ä¯ vienÄ… Å¾ingsnÄ¯ ir pritaikyti juos tinkamiems stulpeliams.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```

Kita vertus, etiketÄ—s kodavimui naudojame scikit-learn `LabelEncoder` klasÄ™, kuri yra pagalbinÄ— klasÄ—, padedanti normalizuoti etiketes, kad jos turÄ—tÅ³ tik reikÅ¡mes tarp 0 ir n_classes-1 (Äia, 0 ir 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

Kai uÅ¾koduojame poÅ¾ymius ir etiketes, galime juos sujungti Ä¯ naujÄ… duomenÅ³ rÄ—melÄ¯ `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

âœ… Kokie yra ordinalinio kodavimo privalumai `Item Size` stulpeliui?

### Analizuokite kintamÅ³jÅ³ tarpusavio ryÅ¡ius

Dabar, kai paruoÅ¡Ä—me duomenis, galime analizuoti poÅ¾ymiÅ³ ir etikeÄiÅ³ tarpusavio ryÅ¡ius, kad suprastume, kaip gerai modelis galÄ—s prognozuoti etiketÄ™ pagal poÅ¾ymius. Geriausias bÅ«das atlikti tokio tipo analizÄ™ yra duomenÅ³ vizualizavimas. VÄ—l naudosime Seaborn `catplot` funkcijÄ…, kad vizualizuotume `Item Size`, `Variety` ir `Color` tarpusavio ryÅ¡ius kategoriniame grafike. NorÄ—dami geriau vizualizuoti duomenis, naudosime uÅ¾koduotÄ… `Item Size` stulpelÄ¯ ir neuÅ¾koduotÄ… `Variety` stulpelÄ¯.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

![DuomenÅ³ vizualizacijos grafikas](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Naudokite â€swarmâ€œ grafikÄ…

Kadangi `Color` yra dvejetainÄ— kategorija (Balta arba Ne), jai reikia â€[specialaus poÅ¾iÅ«rio](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) vizualizacijaiâ€œ. Yra ir kitÅ³ bÅ«dÅ³ vizualizuoti Å¡ios kategorijos ryÅ¡Ä¯ su kitais kintamaisiais.

Galite vizualizuoti kintamuosius Å¡alia vienas kito naudodami Seaborn grafikus.

1. IÅ¡bandykite â€swarmâ€œ grafikÄ…, kad parodytumÄ—te reikÅ¡miÅ³ pasiskirstymÄ…:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![DuomenÅ³ vizualizacijos â€swarmâ€œ grafikas](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Atkreipkite dÄ—mesÄ¯**: aukÅ¡Äiau pateiktas kodas gali generuoti Ä¯spÄ—jimÄ…, nes Seaborn nepavyksta atvaizduoti tokio kiekio duomenÅ³ taÅ¡kÅ³ â€swarmâ€œ grafike. Galimas sprendimas yra sumaÅ¾inti Å¾ymeklio dydÄ¯, naudojant â€sizeâ€œ parametrÄ…. TaÄiau atminkite, kad tai gali paveikti grafiko skaitomumÄ….

> **ğŸ§® Parodykite matematikÄ…**
>
> LogistinÄ— regresija remiasi â€maksimalaus tikÄ—tinumoâ€œ koncepcija, naudojant [sigmoidines funkcijas](https://wikipedia.org/wiki/Sigmoid_function). SigmoidinÄ— funkcija grafike atrodo kaip â€Sâ€œ formos kreivÄ—. Ji paima reikÅ¡mÄ™ ir priskiria jÄ… intervalui tarp 0 ir 1. Jos kreivÄ— taip pat vadinama â€logistine kreiveâ€œ. Jos formulÄ— atrodo taip:
>
> ![logistinÄ— funkcija](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> kur sigmoidÄ—s vidurys yra x aÅ¡ies 0 taÅ¡ke, L yra kreivÄ—s maksimali reikÅ¡mÄ—, o k yra kreivÄ—s statumas. Jei funkcijos rezultatas yra didesnis nei 0.5, atitinkama etiketÄ— bus priskirta â€1â€œ klasei iÅ¡ dvejetainio pasirinkimo. Jei ne, ji bus priskirta â€0â€œ klasei.

## Sukurkite savo modelÄ¯

Sukurti modelÄ¯, kuris rastÅ³ Å¡ias dvejetaines klasifikacijas, yra stebÄ—tinai paprasta naudojant Scikit-learn.

[![ML pradedantiesiems - LogistinÄ— regresija duomenÅ³ klasifikacijai](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML pradedantiesiems - LogistinÄ— regresija duomenÅ³ klasifikacijai")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie linijinÄ—s regresijos modelio kÅ«rimÄ…

1. Pasirinkite kintamuosius, kuriuos norite naudoti savo klasifikavimo modelyje, ir padalykite mokymo bei testavimo rinkinius, iÅ¡kviesdami `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Dabar galite apmokyti savo modelÄ¯, iÅ¡kviesdami `fit()` su mokymo duomenimis, ir atspausdinti jo rezultatÄ…:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    PaÅ¾velkite Ä¯ savo modelio rezultatÅ³ lentelÄ™. Ji nÄ—ra bloga, atsiÅ¾velgiant Ä¯ tai, kad turite tik apie 1000 duomenÅ³ eiluÄiÅ³:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Geresnis supratimas per klaidÅ³ matricÄ…

Nors galite gauti rezultatÅ³ lentelÄ™ [terminais](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report), atspausdindami aukÅ¡Äiau pateiktus elementus, galbÅ«t galÄ—site geriau suprasti savo modelÄ¯ naudodami [klaidÅ³ matricÄ…](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), kuri padeda suprasti, kaip modelis veikia.

> ğŸ“ â€[KlaidÅ³ matrica](https://wikipedia.org/wiki/Confusion_matrix)â€œ (arba â€klaidÅ³ matricaâ€œ) yra lentelÄ—, kuri iÅ¡reiÅ¡kia jÅ«sÅ³ modelio tikrus ir netikrus teigiamus bei neigiamus rezultatus, taip Ä¯vertinant prognoziÅ³ tikslumÄ….

1. NorÄ—dami naudoti klaidÅ³ matricÄ…, iÅ¡kvieskite `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    PaÅ¾velkite Ä¯ savo modelio klaidÅ³ matricÄ…:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

Scikit-learn klaidÅ³ matricose eilutÄ—s (0 aÅ¡is) yra tikros etiketÄ—s, o stulpeliai (1 aÅ¡is) yra prognozuotos etiketÄ—s.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Kas Äia vyksta? Tarkime, mÅ«sÅ³ modelis yra papraÅ¡ytas klasifikuoti moliÅ«gus tarp dviejÅ³ dvejetainiÅ³ kategorijÅ³, kategorijos â€baltaâ€œ ir kategorijos â€ne baltaâ€œ.

- Jei jÅ«sÅ³ modelis prognozuoja moliÅ«gÄ… kaip ne baltÄ…, o jis iÅ¡ tikrÅ³jÅ³ priklauso kategorijai â€ne baltaâ€œ, tai vadiname tikru neigiamu rezultatu, kurÄ¯ rodo virÅ¡utinis
Kaip painiavos matrica susijusi su tikslumu ir atÅ¡aukimu? Atminkite, kad aukÅ¡Äiau pateiktoje klasifikacijos ataskaitoje buvo nurodytas tikslumas (0.85) ir atÅ¡aukimas (0.67).

Tikslumas = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461

AtÅ¡aukimas = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666

âœ… Klausimas: Remiantis painiavos matrica, kaip sekÄ—si modeliui? Atsakymas: Ne blogai; yra nemaÅ¾ai teigiamÅ³ neigiamÅ³ atvejÅ³, taÄiau taip pat keletas klaidingÅ³ neigiamÅ³ atvejÅ³.

GrÄ¯Å¾kime prie terminÅ³, kuriuos matÄ—me anksÄiau, naudodamiesi painiavos matricos TP/TN ir FP/FN Å¾emÄ—lapiu:

ğŸ“ Tikslumas: TP/(TP + FP) ReikÅ¡mingÅ³ atvejÅ³ dalis tarp gautÅ³ atvejÅ³ (pvz., kurie Å¾ymÄ—jimai buvo gerai paÅ¾ymÄ—ti)

ğŸ“ AtÅ¡aukimas: TP/(TP + FN) ReikÅ¡mingÅ³ atvejÅ³ dalis, kurie buvo gauti, nesvarbu, ar jie buvo gerai paÅ¾ymÄ—ti, ar ne

ğŸ“ f1-rezultatas: (2 * tikslumas * atÅ¡aukimas)/(tikslumas + atÅ¡aukimas) Tikslumo ir atÅ¡aukimo svertinis vidurkis, geriausias yra 1, blogiausias - 0

ğŸ“ Palaikymas: Kiekvieno gauto Å¾ymÄ—jimo pasikartojimÅ³ skaiÄius

ğŸ“ Tikslumas: (TP + TN)/(TP + TN + FP + FN) Procentas Å¾ymÄ—jimÅ³, kurie buvo tiksliai nuspÄ—ti mÄ—ginyje.

ğŸ“ Makro vidurkis: NeÄ¯vertintÅ³ vidutiniÅ³ metrikÅ³ skaiÄiavimas kiekvienam Å¾ymÄ—jimui, neatsiÅ¾velgiant Ä¯ Å¾ymÄ—jimÅ³ disbalansÄ….

ğŸ“ Svertinis vidurkis: VidutiniÅ³ metrikÅ³ skaiÄiavimas kiekvienam Å¾ymÄ—jimui, atsiÅ¾velgiant Ä¯ Å¾ymÄ—jimÅ³ disbalansÄ…, sveriant juos pagal jÅ³ palaikymÄ… (tikrÅ³ atvejÅ³ skaiÄiÅ³ kiekvienam Å¾ymÄ—jimui).

âœ… Ar galite pagalvoti, kuriÄ… metrikÄ… reikÄ—tÅ³ stebÄ—ti, jei norite, kad jÅ«sÅ³ modelis sumaÅ¾intÅ³ klaidingÅ³ neigiamÅ³ atvejÅ³ skaiÄiÅ³?

## Vizualizuokite Å¡io modelio ROC kreivÄ™

[![ML pradedantiesiems - LogistinÄ—s regresijos naÅ¡umo analizÄ— su ROC kreivÄ—mis](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML pradedantiesiems - LogistinÄ—s regresijos naÅ¡umo analizÄ— su ROC kreivÄ—mis")

> ğŸ¥ SpustelÄ—kite aukÅ¡Äiau esanÄiÄ… nuotraukÄ…, kad perÅ¾iÅ«rÄ—tumÄ—te trumpÄ… vaizdo Ä¯raÅ¡Ä… apie ROC kreives

Atlikime dar vienÄ… vizualizacijÄ…, kad pamatytume vadinamÄ…jÄ… â€ROCâ€œ kreivÄ™:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Naudodami Matplotlib, nubrÄ—Å¾kite modelio [Gavimo veikimo charakteristikos](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) arba ROC kreivÄ™. ROC kreivÄ—s daÅ¾nai naudojamos norint pamatyti klasifikatoriaus iÅ¡vestÄ¯ pagal tikrus ir klaidingus teigiamus atvejus. â€ROC kreivÄ—se paprastai Y aÅ¡yje pateikiamas tikrÅ³ teigiamÅ³ atvejÅ³ rodiklis, o X aÅ¡yje - klaidingÅ³ teigiamÅ³ atvejÅ³ rodiklis.â€œ Taigi kreivÄ—s statumas ir erdvÄ— tarp vidurio linijos ir kreivÄ—s yra svarbÅ«s: norite kreivÄ—s, kuri greitai kyla aukÅ¡tyn ir virÅ¡ linijos. MÅ«sÅ³ atveju pradÅ¾ioje yra klaidingÅ³ teigiamÅ³ atvejÅ³, o tada linija tinkamai kyla aukÅ¡tyn ir virÅ¡.

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Galiausiai naudokite Scikit-learn [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score), kad apskaiÄiuotumÄ—te faktinÄ™ â€PlotÄ… po kreiveâ€œ (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Rezultatas yra `0.9749908725812341`. Kadangi AUC svyruoja nuo 0 iki 1, norite didelio rezultato, nes modelis, kuris 100% tiksliai prognozuoja, turÄ—s AUC lygiÄ… 1; Å¡iuo atveju modelis yra _gana geras_.

Ateities pamokose apie klasifikacijas suÅ¾inosite, kaip iteruoti, kad pagerintumÄ—te savo modelio rezultatus. Bet kol kas sveikiname! JÅ«s baigÄ—te Å¡ias regresijos pamokas!

---
## ğŸš€IÅ¡Å¡Å«kis

LogistinÄ—je regresijoje yra daug kÄ… iÅ¡nagrinÄ—ti! TaÄiau geriausias bÅ«das mokytis yra eksperimentuoti. Suraskite duomenÅ³ rinkinÄ¯, kuris tinka tokio tipo analizei, ir sukurkite modelÄ¯ su juo. KÄ… iÅ¡mokote? patarimas: pabandykite [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) ieÅ¡koti Ä¯domiÅ³ duomenÅ³ rinkiniÅ³.

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Perskaitykite pirmuosius kelis [Å¡io Stanfordo dokumento](https://web.stanford.edu/~jurafsky/slp3/5.pdf) puslapius apie praktinius logistinÄ—s regresijos panaudojimus. Pagalvokite apie uÅ¾duotis, kurios geriau tinka vienam ar kitam regresijos tipui, kuriuos studijavome iki Å¡iol. Kas veiktÅ³ geriausiai?

## UÅ¾duotis

[Pakartokite Å¡iÄ… regresijÄ…](assignment.md)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudoti profesionalÅ³ Å¾mogaus vertimÄ…. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius dÄ—l Å¡io vertimo naudojimo.