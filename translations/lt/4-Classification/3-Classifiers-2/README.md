<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49047911108adc49d605cddfb455749c",
  "translation_date": "2025-09-05T08:00:00+00:00",
  "source_file": "4-Classification/3-Classifiers-2/README.md",
  "language_code": "lt"
}
-->
# VirtuvÄ—s klasifikatoriai 2

Å ioje antroje klasifikavimo pamokoje jÅ«s tyrinÄ—site daugiau bÅ«dÅ³, kaip klasifikuoti skaitmeninius duomenis. Taip pat suÅ¾inosite apie pasekmes, renkantis vienÄ… klasifikatoriÅ³ vietoj kito.

## [PrieÅ¡ paskaitÄ…: testas](https://ff-quizzes.netlify.app/en/ml/)

### BÅ«tinos Å¾inios

Daroma prielaida, kad jau baigÄ—te ankstesnes pamokas ir turite iÅ¡valytÄ… duomenÅ³ rinkinÄ¯ savo `data` aplanke, pavadintÄ… _cleaned_cuisines.csv_, esanÄiame Å¡io 4 pamokÅ³ aplanko Å¡aknyje.

### PasiruoÅ¡imas

Mes Ä¯kÄ—lÄ—me jÅ«sÅ³ _notebook.ipynb_ failÄ… su iÅ¡valytu duomenÅ³ rinkiniu ir padalijome jÄ¯ Ä¯ X ir y duomenÅ³ rÄ—melius, paruoÅ¡tus modelio kÅ«rimo procesui.

## Klasifikavimo Å¾emÄ—lapis

AnkstesnÄ—je pamokoje suÅ¾inojote apie Ä¯vairias galimybes klasifikuoti duomenis, naudodamiesi â€Microsoftâ€œ apgaulÄ—s lapu. Scikit-learn siÅ«lo panaÅ¡Å³, bet detalesnÄ¯ apgaulÄ—s lapÄ…, kuris gali dar labiau padÄ—ti susiaurinti jÅ«sÅ³ pasirinkimÄ… (kitaip vadinamÄ… klasifikatoriais):

![ML Å¾emÄ—lapis iÅ¡ Scikit-learn](../../../../4-Classification/3-Classifiers-2/images/map.png)
> Patarimas: [aplankykite Å¡Ä¯ Å¾emÄ—lapÄ¯ internete](https://scikit-learn.org/stable/tutorial/machine_learning_map/) ir spustelÄ—kite keliÄ…, kad perskaitytumÄ—te dokumentacijÄ….

### Planas

Å is Å¾emÄ—lapis yra labai naudingas, kai aiÅ¡kiai suprantate savo duomenis, nes galite â€eitiâ€œ jo keliais iki sprendimo:

- Turime >50 pavyzdÅ¾iÅ³
- Norime prognozuoti kategorijÄ…
- Turime paÅ¾ymÄ—tus duomenis
- Turime maÅ¾iau nei 100 tÅ«kst. pavyzdÅ¾iÅ³
- âœ¨ Galime pasirinkti Linear SVC
- Jei tai neveikia, kadangi turime skaitmeninius duomenis
    - Galime iÅ¡bandyti âœ¨ KNeighbors Classifier 
      - Jei tai neveikia, iÅ¡bandykite âœ¨ SVC ir âœ¨ Ensemble Classifiers

Tai labai naudingas kelias, kurio verta laikytis.

## UÅ¾duotis â€“ padalykite duomenis

Sekdami Å¡Ä¯ keliÄ…, turÄ—tume pradÄ—ti importuodami kai kurias reikalingas bibliotekas.

1. Importuokite reikalingas bibliotekas:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Padalykite savo mokymo ir testavimo duomenis:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC klasifikatorius

Support-Vector Clustering (SVC) yra Support-Vector Machines Å¡eimos ML technikÅ³ dalis (daugiau apie jas suÅ¾inosite Å¾emiau). Å iame metode galite pasirinkti â€branduolÄ¯â€œ (kernel), kuris nusprendÅ¾ia, kaip suskirstyti etiketes. Parametras â€Câ€œ reiÅ¡kia â€reguliavimÄ…â€œ, kuris reguliuoja parametrÅ³ Ä¯takÄ…. Branduolys gali bÅ«ti vienas iÅ¡ [keliÅ³](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); Äia mes nustatome jÄ¯ kaip â€linearâ€œ, kad uÅ¾tikrintume Linear SVC naudojimÄ…. TikimybÄ— pagal nutylÄ—jimÄ… yra â€falseâ€œ; Äia mes nustatome jÄ… kaip â€trueâ€œ, kad gautume tikimybiÅ³ Ä¯vertinimus. AtsitiktinÄ™ bÅ«senÄ… nustatome kaip â€0â€œ, kad sumaiÅ¡ytume duomenis ir gautume tikimybes.

### UÅ¾duotis â€“ pritaikykite Linear SVC

PradÄ—kite kurdami klasifikatoriÅ³ masyvÄ…. JÅ«s palaipsniui pridÄ—site prie Å¡io masyvo, kai testuosite.

1. PradÄ—kite nuo Linear SVC:

    ```python
    C = 10
    # Create different classifiers.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Treniruokite savo modelÄ¯ naudodami Linear SVC ir iÅ¡spausdinkite ataskaitÄ…:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Rezultatas yra gana geras:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors klasifikatorius

K-Neighbors yra â€kaimynÅ³â€œ Å¡eimos ML metodÅ³ dalis, kuriuos galima naudoti tiek priÅ¾iÅ«rimam, tiek nepriÅ¾iÅ«rimam mokymuisi. Å iame metode sukuriamas iÅ¡ anksto nustatytas taÅ¡kÅ³ skaiÄius, o duomenys renkami aplink Å¡iuos taÅ¡kus, kad bÅ«tÅ³ galima prognozuoti apibendrintas etiketes.

### UÅ¾duotis â€“ pritaikykite K-Neighbors klasifikatoriÅ³

Ankstesnis klasifikatorius buvo geras ir gerai veikÄ— su duomenimis, bet galbÅ«t galime pasiekti geresnÄ¯ tikslumÄ…. IÅ¡bandykite K-Neighbors klasifikatoriÅ³.

1. PridÄ—kite eilutÄ™ prie savo klasifikatoriÅ³ masyvo (po Linear SVC elemento pridÄ—kite kablelÄ¯):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Rezultatas yra Å¡iek tiek blogesnis:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    âœ… SuÅ¾inokite daugiau apie [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector Classifier

Support-Vector klasifikatoriai yra [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) Å¡eimos ML metodÅ³ dalis, naudojama klasifikavimo ir regresijos uÅ¾duotims. SVM â€sudeda mokymo pavyzdÅ¾ius Ä¯ taÅ¡kus erdvÄ—jeâ€œ, kad maksimaliai padidintÅ³ atstumÄ… tarp dviejÅ³ kategorijÅ³. VÄ—lesni duomenys yra sudedami Ä¯ Å¡iÄ… erdvÄ™, kad bÅ«tÅ³ galima prognozuoti jÅ³ kategorijÄ….

### UÅ¾duotis â€“ pritaikykite Support Vector Classifier

Pabandykime pasiekti Å¡iek tiek geresnÄ¯ tikslumÄ… naudodami Support Vector Classifier.

1. Po K-Neighbors elemento pridÄ—kite kablelÄ¯, tada pridÄ—kite Å¡iÄ… eilutÄ™:

    ```python
    'SVC': SVC(),
    ```

    Rezultatas yra gana geras!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    âœ… SuÅ¾inokite daugiau apie [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble Classifiers

Sekime keliÄ… iki galo, nors ankstesnis testas buvo gana geras. IÅ¡bandykime â€Ensemble Classifiersâ€œ, konkreÄiai Random Forest ir AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Rezultatas yra labai geras, ypaÄ Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

âœ… SuÅ¾inokite daugiau apie [Ensemble Classifiers](https://scikit-learn.org/stable/modules/ensemble.html)

Å is maÅ¡ininio mokymosi metodas â€sujungia keliÅ³ baziniÅ³ Ä¯vertintojÅ³ prognozesâ€œ, kad pagerintÅ³ modelio kokybÄ™. MÅ«sÅ³ pavyzdyje naudojome Random Trees ir AdaBoost.

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), vidurkinimo metodas, sukuria â€miÅ¡kÄ…â€œ iÅ¡ â€sprendimÅ³ medÅ¾iÅ³â€œ, Ä¯terptÅ³ su atsitiktinumu, kad bÅ«tÅ³ iÅ¡vengta per didelio pritaikymo. Parametras n_estimators nustatomas kaip medÅ¾iÅ³ skaiÄius.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) pritaiko klasifikatoriÅ³ duomenÅ³ rinkiniui, o tada pritaiko Å¡io klasifikatoriaus kopijas tam paÄiam duomenÅ³ rinkiniui. Jis sutelkia dÄ—mesÄ¯ Ä¯ neteisingai klasifikuotÅ³ elementÅ³ svorius ir koreguoja kitÄ… klasifikatoriÅ³, kad iÅ¡taisytÅ³ klaidas.

---

## ğŸš€IÅ¡Å¡Å«kis

Kiekviena iÅ¡ Å¡iÅ³ technikÅ³ turi daug parametrÅ³, kuriuos galite koreguoti. IÅ¡tyrinÄ—kite kiekvieno numatytuosius parametrus ir pagalvokite, kÄ… Å¡iÅ³ parametrÅ³ koregavimas reikÅ¡tÅ³ modelio kokybei.

## [Po paskaitos: testas](https://ff-quizzes.netlify.app/en/ml/)

## PerÅ¾iÅ«ra ir savarankiÅ¡kas mokymasis

Å iose pamokose yra daug terminologijos, todÄ—l skirkite minutÄ™ perÅ¾iÅ«rÄ—ti [Å¡Ä¯ sÄ…raÅ¡Ä…](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) naudingÅ³ terminÅ³!

## UÅ¾duotis 

[ParametrÅ³ Å¾aidimas](assignment.md)

---

**AtsakomybÄ—s apribojimas**:  
Å is dokumentas buvo iÅ¡verstas naudojant dirbtinio intelekto vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, atkreipiame dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama naudotis profesionaliÅ³ vertÄ—jÅ³ paslaugomis. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus aiÅ¡kinimus, kylanÄius dÄ—l Å¡io vertimo naudojimo.