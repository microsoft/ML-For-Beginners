<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T07:58:37+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "lt"
}
-->
# VirtuvÄ—s klasifikatoriai 1

Å ioje pamokoje naudosite duomenÅ³ rinkinÄ¯, kurÄ¯ iÅ¡saugojote iÅ¡ ankstesnÄ—s pamokos, pilnÄ… subalansuotÅ³ ir Å¡variÅ³ duomenÅ³ apie virtuvÄ—s tipus.

Naudodami Å¡Ä¯ duomenÅ³ rinkinÄ¯ su Ä¯vairiais klasifikatoriais, _prognozuosite tam tikrÄ… nacionalinÄ™ virtuvÄ™ pagal ingredientÅ³ grupÄ™_. Tuo paÄiu suÅ¾inosite daugiau apie tai, kaip algoritmai gali bÅ«ti naudojami klasifikavimo uÅ¾duotims.

## [PrieÅ¡ paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)
# PasiruoÅ¡imas

Jei baigÄ—te [1 pamokÄ…](../1-Introduction/README.md), Ä¯sitikinkite, kad _cleaned_cuisines.csv_ failas yra `/data` aplanke, skirtame Å¡ioms keturioms pamokoms.

## UÅ¾duotis - prognozuoti nacionalinÄ™ virtuvÄ™

1. Dirbdami Å¡ios pamokos _notebook.ipynb_ aplanke, importuokite failÄ… kartu su Pandas biblioteka:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Duomenys atrodo taip:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Dabar importuokite dar kelias bibliotekas:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Padalinkite X ir y koordinates Ä¯ du duomenÅ³ rÄ—melius mokymui. `cuisine` gali bÅ«ti etikeÄiÅ³ duomenÅ³ rÄ—melis:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Tai atrodys taip:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. PaÅ¡alinkite `Unnamed: 0` stulpelÄ¯ ir `cuisine` stulpelÄ¯, naudodami `drop()`. Likusius duomenis iÅ¡saugokite kaip mokymui tinkamus poÅ¾ymius:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    JÅ«sÅ³ poÅ¾ymiai atrodo taip:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Dabar esate pasiruoÅ¡Ä™ treniruoti savo modelÄ¯!

## Klasifikatoriaus pasirinkimas

Kai jÅ«sÅ³ duomenys yra Å¡varÅ«s ir paruoÅ¡ti mokymui, turite nusprÄ™sti, kokÄ¯ algoritmÄ… naudoti.

Scikit-learn klasifikavimÄ… priskiria prie PriÅ¾iÅ«rimo Mokymosi (Supervised Learning), ir Å¡ioje kategorijoje rasite daugybÄ™ bÅ«dÅ³ klasifikuoti. [Ä®vairovÄ—](https://scikit-learn.org/stable/supervised_learning.html) iÅ¡ pradÅ¾iÅ³ gali atrodyti gana paini. Å ie metodai apima klasifikavimo technikas:

- Linijiniai modeliai
- Atramos vektoriÅ³ maÅ¡inos (Support Vector Machines)
- Stochastinis gradientinis nusileidimas
- Artimiausi kaimynai
- Gauso procesai
- SprendimÅ³ medÅ¾iai
- Ansamblio metodai (balsavimo klasifikatorius)
- Daugiaklasiai ir daugiatiksliai algoritmai (daugiaklasis ir daugiatikslis klasifikavimas)

> Taip pat galite naudoti [neuroninius tinklus duomenims klasifikuoti](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), taÄiau tai nÄ—ra Å¡ios pamokos tema.

### KurÄ¯ klasifikatoriÅ³ pasirinkti?

Taigi, kurÄ¯ klasifikatoriÅ³ pasirinkti? DaÅ¾nai verta iÅ¡bandyti kelis ir ieÅ¡koti geriausio rezultato. Scikit-learn siÅ«lo [palyginimÄ…](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) sukurto duomenÅ³ rinkinio pagrindu, lyginant KNeighbors, SVC dviem bÅ«dais, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB ir QuadraticDiscriminationAnalysis, vizualizuojant rezultatus:

![klasifikatoriÅ³ palyginimas](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Grafikai iÅ¡ Scikit-learn dokumentacijos

> AutoML iÅ¡sprendÅ¾ia Å¡iÄ… problemÄ… efektyviai, atlikdamas Å¡iuos palyginimus debesyje, leidÅ¾iant jums pasirinkti geriausiÄ… algoritmÄ… jÅ«sÅ³ duomenims. IÅ¡bandykite [Äia](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### Geresnis poÅ¾iÅ«ris

Geresnis bÅ«das nei spÄ—liojimas yra vadovautis idÄ—jomis iÅ¡ Å¡io atsisiunÄiamo [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). ÄŒia suÅ¾inome, kad mÅ«sÅ³ daugiaklasio problemos atveju turime keletÄ… pasirinkimÅ³:

![daugiaklasio problemos cheat sheet](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> Microsoft algoritmÅ³ cheat sheet dalis, apibÅ«dinanti daugiaklasio klasifikavimo galimybes

âœ… AtsisiÅ³skite Å¡Ä¯ cheat sheet, atsispausdinkite ir pakabinkite ant sienos!

### Argumentavimas

PaÅ¾iÅ«rÄ—kime, ar galime logiÅ¡kai pasirinkti skirtingus metodus, atsiÅ¾velgdami Ä¯ turimus apribojimus:

- **Neuroniniai tinklai per sudÄ—tingi**. AtsiÅ¾velgiant Ä¯ mÅ«sÅ³ Å¡varius, bet minimalius duomenis ir tai, kad mokymÄ… vykdome lokaliai per uÅ¾raÅ¡Å³ knygeles, neuroniniai tinklai yra per sudÄ—tingi Å¡iai uÅ¾duoÄiai.
- **DviejÅ³ klasiÅ³ klasifikatorius netinka**. Mes nenaudojame dviejÅ³ klasiÅ³ klasifikatoriaus, todÄ—l tai atmeta one-vs-all metodÄ….
- **SprendimÅ³ medis arba logistinÄ— regresija galÄ—tÅ³ veikti**. SprendimÅ³ medis galÄ—tÅ³ veikti, arba logistinÄ— regresija daugiaklasiams duomenims.
- **Daugiaklasiai sustiprinti sprendimÅ³ medÅ¾iai sprendÅ¾ia kitÄ… problemÄ…**. Daugiaklasiai sustiprinti sprendimÅ³ medÅ¾iai labiausiai tinka neparametrinÄ—ms uÅ¾duotims, pvz., uÅ¾duotims, skirtoms sudaryti reitingus, todÄ—l jie mums nÄ—ra naudingi.

### Naudojant Scikit-learn 

Naudosime Scikit-learn analizuoti mÅ«sÅ³ duomenis. TaÄiau yra daug bÅ«dÅ³ naudoti logistinÄ™ regresijÄ… Scikit-learn. PaÅ¾velkite Ä¯ [parametrus, kuriuos galima perduoti](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

IÅ¡ esmÄ—s yra du svarbÅ«s parametrai - `multi_class` ir `solver` - kuriuos reikia nurodyti, kai praÅ¡ome Scikit-learn atlikti logistinÄ™ regresijÄ…. `multi_class` reikÅ¡mÄ— taiko tam tikrÄ… elgesÄ¯. Solver reikÅ¡mÄ— nurodo, kokÄ¯ algoritmÄ… naudoti. Ne visi solver gali bÅ«ti derinami su visomis `multi_class` reikÅ¡mÄ—mis.

Pagal dokumentacijÄ…, daugiaklasio atveju mokymo algoritmas:

- **Naudoja one-vs-rest (OvR) schemÄ…**, jei `multi_class` parinktis nustatyta kaip `ovr`
- **Naudoja kryÅ¾minio entropijos nuostolÄ¯**, jei `multi_class` parinktis nustatyta kaip `multinomial`. (Å iuo metu `multinomial` parinktis palaikoma tik su â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™ ir â€˜newton-cgâ€™ solver.)

> ğŸ“ ÄŒia "schema" gali bÅ«ti 'ovr' (one-vs-rest) arba 'multinomial'. Kadangi logistinÄ— regresija iÅ¡ esmÄ—s skirta dvejetainiam klasifikavimui, Å¡ios schemos leidÅ¾ia jai geriau tvarkyti daugiaklasio klasifikavimo uÅ¾duotis. [Å¡altinis](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> ğŸ“ "Solver" apibrÄ—Å¾iamas kaip "algoritmas, naudojamas optimizavimo problemoms sprÄ™sti". [Å¡altinis](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn siÅ«lo Å¡iÄ… lentelÄ™, kad paaiÅ¡kintÅ³, kaip solver sprendÅ¾ia skirtingus iÅ¡Å¡Å«kius, kuriuos kelia skirtingos duomenÅ³ struktÅ«ros:

![solver](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## UÅ¾duotis - padalyti duomenis

Galime sutelkti dÄ—mesÄ¯ Ä¯ logistinÄ™ regresijÄ… pirmajam mokymo bandymui, nes neseniai apie jÄ… mokÄ—tÄ—s ankstesnÄ—je pamokoje.
Padalykite savo duomenis Ä¯ mokymo ir testavimo grupes, naudodami `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## UÅ¾duotis - taikyti logistinÄ™ regresijÄ…

Kadangi naudojate daugiaklasio atvejÄ¯, turite pasirinkti, kokiÄ… _schemÄ…_ naudoti ir kokÄ¯ _solver_ nustatyti. Naudokite LogisticRegression su multi_class nustatytu kaip `ovr` ir solver nustatytu kaip `liblinear` mokymui.

1. Sukurkite logistinÄ™ regresijÄ… su multi_class nustatytu kaip `ovr` ir solver nustatytu kaip `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    âœ… IÅ¡bandykite kitÄ… solver, pvz., `lbfgs`, kuris daÅ¾nai nustatomas kaip numatytasis.
> Pastaba, naudokite Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) funkcijÄ…, kad prireikus suplokÅ¡tintumÄ—te savo duomenis.
Tikslumas yra geras - daugiau nei **80%**!

1. Galite pamatyti Å¡io modelio veikimÄ…, iÅ¡bandydami vienÄ… duomenÅ³ eilutÄ™ (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Rezultatas atspausdinamas:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   âœ… IÅ¡bandykite kitÄ… eilutÄ—s numerÄ¯ ir patikrinkite rezultatus.

1. Gilinantis, galite patikrinti Å¡ios prognozÄ—s tikslumÄ…:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Rezultatas atspausdinamas - Indijos virtuvÄ— yra geriausia spÄ—jimo galimybÄ—, su gera tikimybe:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    âœ… Ar galite paaiÅ¡kinti, kodÄ—l modelis yra gana tikras, kad tai Indijos virtuvÄ—?

1. Gaukite daugiau informacijos, atspausdindami klasifikacijos ataskaitÄ…, kaip tai darÄ—te regresijos pamokose:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | tikslumas | atÅ¡aukimas | f1-rezultatas | palaikymas |
    | ------------ | --------- | ---------- | ------------ | ---------- |
    | chinese      | 0.73      | 0.71       | 0.72         | 229        |
    | indian       | 0.91      | 0.93       | 0.92         | 254        |
    | japanese     | 0.70      | 0.75       | 0.72         | 220        |
    | korean       | 0.86      | 0.76       | 0.81         | 242        |
    | thai         | 0.79      | 0.85       | 0.82         | 254        |
    | tikslumas    | 0.80      | 1199       |              |            |
    | vidurkis     | 0.80      | 0.80       | 0.80         | 1199       |
    | svertinis vidurkis | 0.80 | 0.80       | 0.80         | 1199       |

## ğŸš€IÅ¡Å¡Å«kis

Å ioje pamokoje naudojote iÅ¡valytus duomenis, kad sukurtumÄ—te maÅ¡ininio mokymosi modelÄ¯, galintÄ¯ prognozuoti nacionalinÄ™ virtuvÄ™ pagal ingredientÅ³ serijÄ…. Skirkite laiko perskaityti daugybÄ™ Scikit-learn siÅ«lomÅ³ galimybiÅ³ duomenÅ³ klasifikavimui. GilinkitÄ—s Ä¯ â€sprendiklioâ€œ (solver) koncepcijÄ…, kad suprastumÄ—te, kas vyksta uÅ¾kulisiuose.

## [Po paskaitos testas](https://ff-quizzes.netlify.app/en/ml/)

## ApÅ¾valga ir savarankiÅ¡kas mokymasis

Pasigilinkite Ä¯ matematikÄ…, slypinÄiÄ… uÅ¾ logistinÄ—s regresijos, [Å¡ioje pamokoje](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## UÅ¾duotis 

[Studijuokite sprendiklius](assignment.md)

---

**AtsakomybÄ—s atsisakymas**:  
Å is dokumentas buvo iÅ¡verstas naudojant AI vertimo paslaugÄ… [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, praÅ¡ome atkreipti dÄ—mesÄ¯, kad automatiniai vertimai gali turÄ—ti klaidÅ³ ar netikslumÅ³. Originalus dokumentas jo gimtÄ…ja kalba turÄ—tÅ³ bÅ«ti laikomas autoritetingu Å¡altiniu. Kritinei informacijai rekomenduojama profesionali Å¾mogaus vertimo paslauga. Mes neprisiimame atsakomybÄ—s uÅ¾ nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant Å¡Ä¯ vertimÄ….