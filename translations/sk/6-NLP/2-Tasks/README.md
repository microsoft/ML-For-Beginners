<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5f3cb462e3122e1afe7ab0050ccf2bd3",
  "translation_date": "2025-09-05T16:51:10+00:00",
  "source_file": "6-NLP/2-Tasks/README.md",
  "language_code": "sk"
}
-->
# BeÅ¾nÃ© Ãºlohy a techniky spracovania prirodzenÃ©ho jazyka

Pri vÃ¤ÄÅ¡ine Ãºloh *spracovania prirodzenÃ©ho jazyka* je potrebnÃ© text rozloÅ¾iÅ¥, analyzovaÅ¥ a vÃ½sledky uloÅ¾iÅ¥ alebo porovnaÅ¥ s pravidlami a dÃ¡tovÃ½mi sÃºbormi. Tieto Ãºlohy umoÅ¾ÅˆujÃº programÃ¡torovi odvodiÅ¥ _vÃ½znam_, _zÃ¡mer_ alebo len _frekvenciu_ termÃ­nov a slov v texte.

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ml/)

PoÄme objaviÅ¥ beÅ¾nÃ© techniky pouÅ¾Ã­vanÃ© pri spracovanÃ­ textu. V kombinÃ¡cii so strojovÃ½m uÄenÃ­m vÃ¡m tieto techniky pomÃ´Å¾u efektÃ­vne analyzovaÅ¥ veÄ¾kÃ© mnoÅ¾stvo textu. Pred aplikÃ¡ciou ML na tieto Ãºlohy je vÅ¡ak dÃ´leÅ¾itÃ© pochopiÅ¥ problÃ©my, s ktorÃ½mi sa Å¡pecialista na NLP stretÃ¡va.

## BeÅ¾nÃ© Ãºlohy v NLP

Existuje mnoho spÃ´sobov, ako analyzovaÅ¥ text, na ktorom pracujete. SÃº tu Ãºlohy, ktorÃ© mÃ´Å¾ete vykonÃ¡vaÅ¥, a prostrednÃ­ctvom nich dokÃ¡Å¾ete pochopiÅ¥ text a vyvodiÅ¥ zÃ¡very. Tieto Ãºlohy sa zvyÄajne vykonÃ¡vajÃº v sekvencii.

### TokenizÃ¡cia

Pravdepodobne prvou vecou, ktorÃº vÃ¤ÄÅ¡ina algoritmov NLP musÃ­ urobiÅ¥, je rozdelenie textu na tokeny alebo slovÃ¡. Aj keÄ to znie jednoducho, zohÄ¾adnenie interpunkcie a rÃ´znych jazykovÃ½ch oddelovaÄov slov a viet mÃ´Å¾e byÅ¥ zloÅ¾itÃ©. MÃ´Å¾ete potrebovaÅ¥ rÃ´zne metÃ³dy na urÄenie hranÃ­c.

![tokenizÃ¡cia](../../../../6-NLP/2-Tasks/images/tokenization.png)
> TokenizÃ¡cia vety z **PÃ½chy a predsudku**. Infografika od [Jen Looper](https://twitter.com/jenlooper)

### Vstupy (Embeddings)

[Vstupy slov](https://wikipedia.org/wiki/Word_embedding) sÃº spÃ´sob, ako numericky konvertovaÅ¥ vaÅ¡e textovÃ© dÃ¡ta. Vstupy sÃº vytvorenÃ© tak, aby slovÃ¡ s podobnÃ½m vÃ½znamom alebo slovÃ¡ pouÅ¾Ã­vanÃ© spolu vytvÃ¡rali zhluky.

![vstupy slov](../../../../6-NLP/2-Tasks/images/embedding.png)
> "MÃ¡m najvÃ¤ÄÅ¡Ã­ reÅ¡pekt k vaÅ¡im nervom, sÃº to moji starÃ­ priatelia." - Vstupy slov pre vetu z **PÃ½chy a predsudku**. Infografika od [Jen Looper](https://twitter.com/jenlooper)

âœ… VyskÃºÅ¡ajte [tento zaujÃ­mavÃ½ nÃ¡stroj](https://projector.tensorflow.org/) na experimentovanie s vstupmi slov. KliknutÃ­m na jedno slovo sa zobrazia zhluky podobnÃ½ch slov: 'hraÄka' sa zhlukuje s 'disney', 'lego', 'playstation' a 'konzola'.

### Parsovanie a oznaÄovanie ÄastÃ­ reÄi

KaÅ¾dÃ© slovo, ktorÃ© bolo tokenizovanÃ©, mÃ´Å¾e byÅ¥ oznaÄenÃ© ako ÄasÅ¥ reÄi - podstatnÃ© meno, sloveso alebo prÃ­davnÃ© meno. Veta `rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil cez lenivÃ©ho hnedÃ©ho psa` mÃ´Å¾e byÅ¥ oznaÄenÃ¡ ako lÃ­Å¡ka = podstatnÃ© meno, preskoÄil = sloveso.

![parsovanie](../../../../6-NLP/2-Tasks/images/parse.png)

> Parsovanie vety z **PÃ½chy a predsudku**. Infografika od [Jen Looper](https://twitter.com/jenlooper)

Parsovanie znamenÃ¡ rozpoznanie, ktorÃ© slovÃ¡ sÃº vo vete navzÃ¡jom prepojenÃ© - naprÃ­klad `rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil` je sekvencia prÃ­davnÃ© meno-podstatnÃ© meno-sloveso, ktorÃ¡ je oddelenÃ¡ od sekvencie `lenivÃ½ hnedÃ½ pes`.

### Frekvencie slov a frÃ¡z

UÅ¾itoÄnÃ½m postupom pri analÃ½ze veÄ¾kÃ©ho mnoÅ¾stva textu je vytvorenie slovnÃ­ka kaÅ¾dÃ©ho slova alebo frÃ¡zy, ktorÃ© vÃ¡s zaujÃ­majÃº, a ich frekvencie vÃ½skytu. FrÃ¡za `rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil cez lenivÃ©ho hnedÃ©ho psa` mÃ¡ frekvenciu slova 2 pre slovo "cez".

Pozrime sa na prÃ­klad textu, kde poÄÃ­tame frekvenciu slov. BÃ¡seÅˆ Rudyard Kiplinga VÃ­Å¥azi obsahuje nasledujÃºci verÅ¡:

```output
What the moral? Who rides may read.
When the night is thick and the tracks are blind
A friend at a pinch is a friend, indeed,
But a fool to wait for the laggard behind.
Down to Gehenna or up to the Throne,
He travels the fastest who travels alone.
```

KeÄÅ¾e frekvencie frÃ¡z mÃ´Å¾u byÅ¥ citlivÃ© na veÄ¾kosÅ¥ pÃ­smen alebo nie, frÃ¡za `priateÄ¾` mÃ¡ frekvenciu 2, `cez` mÃ¡ frekvenciu 6 a `cestuje` mÃ¡ frekvenciu 2.

### N-gramy

Text mÃ´Å¾e byÅ¥ rozdelenÃ½ na sekvencie slov urÄitej dÄºÅ¾ky, jedno slovo (unigram), dve slovÃ¡ (bigramy), tri slovÃ¡ (trigramy) alebo akÃ½koÄ¾vek poÄet slov (n-gramy).

NaprÃ­klad `rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil cez lenivÃ©ho hnedÃ©ho psa` s hodnotou n-gramu 2 vytvÃ¡ra nasledujÃºce n-gramy:

1. rÃ½chly ÄervenÃ½  
2. ÄervenÃ½ lÃ­Å¡ka  
3. lÃ­Å¡ka preskoÄil  
4. preskoÄil cez  
5. cez lenivÃ©ho  
6. lenivÃ©ho hnedÃ©ho  
7. hnedÃ©ho psa  

Je jednoduchÅ¡ie si to predstaviÅ¥ ako posuvnÃ© okno nad vetou. Tu je to pre n-gramy s 3 slovami, n-gram je zvÃ½raznenÃ½ v kaÅ¾dej vete:

1.   <u>**rÃ½chly ÄervenÃ½ lÃ­Å¡ka**</u> preskoÄil cez lenivÃ©ho hnedÃ©ho psa  
2.   rÃ½chly **<u>ÄervenÃ½ lÃ­Å¡ka preskoÄil</u>** cez lenivÃ©ho hnedÃ©ho psa  
3.   rÃ½chly ÄervenÃ½ **<u>lÃ­Å¡ka preskoÄil cez</u>** lenivÃ©ho hnedÃ©ho psa  
4.   rÃ½chly ÄervenÃ½ lÃ­Å¡ka **<u>preskoÄil cez lenivÃ©ho</u>** hnedÃ©ho psa  
5.   rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil **<u>cez lenivÃ©ho hnedÃ©ho</u>** psa  
6.   rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil cez <u>**lenivÃ©ho hnedÃ©ho psa**</u>  

![posuvnÃ© okno n-gramov](../../../../6-NLP/2-Tasks/images/n-grams.gif)

> Hodnota n-gramu 3: Infografika od [Jen Looper](https://twitter.com/jenlooper)

### Extrakcia podstatnÃ½ch frÃ¡z

VÃ¤ÄÅ¡ina viet obsahuje podstatnÃ© meno, ktorÃ© je predmetom alebo objektom vety. V angliÄtine je Äasto identifikovateÄ¾nÃ© tÃ½m, Å¾e pred nÃ­m stojÃ­ 'a', 'an' alebo 'the'. IdentifikÃ¡cia predmetu alebo objektu vety prostrednÃ­ctvom 'extrakcie podstatnej frÃ¡zy' je beÅ¾nou Ãºlohou v NLP pri pokuse o pochopenie vÃ½znamu vety.

âœ… Vo vete "NemÃ´Å¾em si spomenÃºÅ¥ na hodinu, miesto, pohÄ¾ad alebo slovÃ¡, ktorÃ© poloÅ¾ili zÃ¡klady. Je to prÃ­liÅ¡ dÃ¡vno. Bol som uprostred, neÅ¾ som si uvedomil, Å¾e som zaÄal." dokÃ¡Å¾ete identifikovaÅ¥ podstatnÃ© frÃ¡zy?

Vo vete `rÃ½chly ÄervenÃ½ lÃ­Å¡ka preskoÄil cez lenivÃ©ho hnedÃ©ho psa` sÃº 2 podstatnÃ© frÃ¡zy: **rÃ½chly ÄervenÃ½ lÃ­Å¡ka** a **lenivÃ½ hnedÃ½ pes**.

### AnalÃ½za sentimentu

Veta alebo text mÃ´Å¾e byÅ¥ analyzovanÃ½ na sentiment, teda ako *pozitÃ­vny* alebo *negatÃ­vny* je. Sentiment sa meria v *polarite* a *objektivite/subjektivite*. Polarita sa meria od -1.0 do 1.0 (negatÃ­vna aÅ¾ pozitÃ­vna) a od 0.0 do 1.0 (najviac objektÃ­vna aÅ¾ najviac subjektÃ­vna).

âœ… NeskÃ´r sa nauÄÃ­te, Å¾e existujÃº rÃ´zne spÃ´soby urÄovania sentimentu pomocou strojovÃ©ho uÄenia, ale jednÃ½m zo spÃ´sobov je maÅ¥ zoznam slov a frÃ¡z, ktorÃ© sÃº kategorizovanÃ© ako pozitÃ­vne alebo negatÃ­vne Ä¾udskÃ½m expertom, a aplikovaÅ¥ tento model na text na vÃ½poÄet skÃ³re polarity. VidÃ­te, ako by to mohlo fungovaÅ¥ v niektorÃ½ch prÃ­padoch a menej dobre v inÃ½ch?

### Inflekcia

Inflekcia vÃ¡m umoÅ¾Åˆuje vziaÅ¥ slovo a zÃ­skaÅ¥ jeho jednotnÃ© alebo mnoÅ¾nÃ© ÄÃ­slo.

### LemmatizÃ¡cia

*Lema* je koreÅˆovÃ© alebo zÃ¡kladnÃ© slovo pre mnoÅ¾inu slov, naprÃ­klad *letel*, *lietajÃº*, *lietanie* majÃº lemu slovesa *lietanie*.

ExistujÃº aj uÅ¾itoÄnÃ© databÃ¡zy dostupnÃ© pre vÃ½skumnÃ­ka NLP, najmÃ¤:

### WordNet

[WordNet](https://wordnet.princeton.edu/) je databÃ¡za slov, synonym, antonym a mnohÃ½ch ÄalÅ¡Ã­ch detailov pre kaÅ¾dÃ© slovo v mnohÃ½ch rÃ´znych jazykoch. Je neuveriteÄ¾ne uÅ¾itoÄnÃ¡ pri pokuse o vytvÃ¡ranie prekladov, kontrolu pravopisu alebo jazykovÃ½ch nÃ¡strojov akÃ©hokoÄ¾vek typu.

## KniÅ¾nice NLP

NaÅ¡Å¥astie, nemusÃ­te vÅ¡etky tieto techniky vytvÃ¡raÅ¥ sami, pretoÅ¾e existujÃº vynikajÃºce kniÅ¾nice v Pythone, ktorÃ© ich sprÃ­stupÅˆujÃº vÃ½vojÃ¡rom, ktorÃ­ nie sÃº Å¡pecializovanÃ­ na spracovanie prirodzenÃ©ho jazyka alebo strojovÃ© uÄenie. V nasledujÃºcich lekciÃ¡ch nÃ¡jdete viac prÃ­kladov, ale tu sa nauÄÃ­te niekoÄ¾ko uÅ¾itoÄnÃ½ch prÃ­kladov, ktorÃ© vÃ¡m pomÃ´Å¾u s ÄalÅ¡ou Ãºlohou.

### CviÄenie - pouÅ¾itie kniÅ¾nice `TextBlob`

PouÅ¾ime kniÅ¾nicu TextBlob, ktorÃ¡ obsahuje uÅ¾itoÄnÃ© API na rieÅ¡enie tÃ½chto typov Ãºloh. TextBlob "stojÃ­ na obrovskÃ½ch pleciach [NLTK](https://nltk.org) a [pattern](https://github.com/clips/pattern) a dobre spolupracuje s oboma." MÃ¡ znaÄnÃ© mnoÅ¾stvo ML zabudovanÃ© vo svojom API.

> PoznÃ¡mka: UÅ¾itoÄnÃ½ [RÃ½chly Å¡tart](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) sprievodca je dostupnÃ½ pre TextBlob a odporÃºÄa sa skÃºsenÃ½m vÃ½vojÃ¡rom v Pythone.

Pri pokuse o identifikÃ¡ciu *podstatnÃ½ch frÃ¡z* ponÃºka TextBlob niekoÄ¾ko moÅ¾nostÃ­ extraktorov na nÃ¡jdenie podstatnÃ½ch frÃ¡z.

1. Pozrite sa na `ConllExtractor`.

    ```python
    from textblob import TextBlob
    from textblob.np_extractors import ConllExtractor
    # import and create a Conll extractor to use later 
    extractor = ConllExtractor()
    
    # later when you need a noun phrase extractor:
    user_input = input("> ")
    user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified
    np = user_input_blob.noun_phrases                                    
    ```

    > ÄŒo sa tu deje? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) je "Extraktor podstatnÃ½ch frÃ¡z, ktorÃ½ pouÅ¾Ã­va chunk parsing trÃ©novanÃ½ na korpuse ConLL-2000." ConLL-2000 odkazuje na konferenciu o vÃ½poÄtovom uÄenÃ­ prirodzenÃ©ho jazyka z roku 2000. KaÅ¾dÃ½ rok konferencia hostila workshop na rieÅ¡enie zloÅ¾itÃ©ho problÃ©mu NLP, a v roku 2000 to bolo chunkovanie podstatnÃ½ch frÃ¡z. Model bol trÃ©novanÃ½ na Wall Street Journal, s "sekciami 15-18 ako trÃ©ningovÃ© dÃ¡ta (211727 tokenov) a sekciou 20 ako testovacie dÃ¡ta (47377 tokenov)". MÃ´Å¾ete si pozrieÅ¥ pouÅ¾itÃ© postupy [tu](https://www.clips.uantwerpen.be/conll2000/chunking/) a [vÃ½sledky](https://ifarm.nl/erikt/research/np-chunking.html).

### VÃ½zva - zlepÅ¡enie vÃ¡Å¡ho bota pomocou NLP

V predchÃ¡dzajÃºcej lekcii ste vytvorili veÄ¾mi jednoduchÃ©ho Q&A bota. Teraz urobÃ­te Marvina trochu sympatickejÅ¡Ã­m tÃ½m, Å¾e analyzujete vÃ¡Å¡ vstup na sentiment a vytlaÄÃ­te odpoveÄ, ktorÃ¡ zodpovedÃ¡ sentimentu. Budete tieÅ¾ musieÅ¥ identifikovaÅ¥ `noun_phrase` a opÃ½taÅ¥ sa na Åˆu.

VaÅ¡e kroky pri vytvÃ¡ranÃ­ lepÅ¡ieho konverzaÄnÃ©ho bota:

1. VytlaÄte pokyny, ktorÃ© pouÅ¾Ã­vateÄ¾ovi poradia, ako komunikovaÅ¥ s botom  
2. Spustite sluÄku  
   1. Prijmite vstup od pouÅ¾Ã­vateÄ¾a  
   2. Ak pouÅ¾Ã­vateÄ¾ poÅ¾iadal o ukonÄenie, ukonÄite  
   3. Spracujte vstup pouÅ¾Ã­vateÄ¾a a urÄte vhodnÃº odpoveÄ na sentiment  
   4. Ak je v sentimentu detekovanÃ¡ podstatnÃ¡ frÃ¡za, zmeÅˆte ju na mnoÅ¾nÃ© ÄÃ­slo a opÃ½tajte sa na tÃºto tÃ©mu  
   5. VytlaÄte odpoveÄ  
3. VrÃ¡Å¥te sa spÃ¤Å¥ na krok 2  

Tu je Ãºryvok kÃ³du na urÄenie sentimentu pomocou TextBlob. VÅ¡imnite si, Å¾e existujÃº iba Å¡tyri *stupne* odpovede na sentiment (mÃ´Å¾ete ich maÅ¥ viac, ak chcete):

```python
if user_input_blob.polarity <= -0.5:
  response = "Oh dear, that sounds bad. "
elif user_input_blob.polarity <= 0:
  response = "Hmm, that's not great. "
elif user_input_blob.polarity <= 0.5:
  response = "Well, that sounds positive. "
elif user_input_blob.polarity <= 1:
  response = "Wow, that sounds great. "
```

Tu je ukÃ¡Å¾ka vÃ½stupu na usmernenie (vstup pouÅ¾Ã­vateÄ¾a je na riadkoch zaÄÃ­najÃºcich >):

```output
Hello, I am Marvin, the friendly robot.
You can end this conversation at any time by typing 'bye'
After typing each answer, press 'enter'
How are you today?
> I am ok
Well, that sounds positive. Can you tell me more?
> I went for a walk and saw a lovely cat
Well, that sounds positive. Can you tell me more about lovely cats?
> cats are the best. But I also have a cool dog
Wow, that sounds great. Can you tell me more about cool dogs?
> I have an old hounddog but he is sick
Hmm, that's not great. Can you tell me more about old hounddogs?
> bye
It was nice talking to you, goodbye!
```

Jedno moÅ¾nÃ© rieÅ¡enie Ãºlohy je [tu](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/2-Tasks/solution/bot.py)

âœ… Kontrola vedomostÃ­

1. MyslÃ­te si, Å¾e sympatickÃ© odpovede by 'oklamali' niekoho, aby si myslel, Å¾e bot ich skutoÄne rozumie?  
2. RobÃ­ identifikÃ¡cia podstatnej frÃ¡zy bota viac 'uveriteÄ¾nÃ½m'?  
3. PreÄo by extrakcia 'podstatnej frÃ¡zy' z vety bola uÅ¾itoÄnÃ¡ vec na vykonanie?  

---

Implementujte bota v predchÃ¡dzajÃºcej kontrole vedomostÃ­ a otestujte ho na priateÄ¾ovi. DokÃ¡Å¾e ich oklamaÅ¥? DokÃ¡Å¾ete urobiÅ¥ vÃ¡Å¡ho bota viac 'uveriteÄ¾nÃ½m'?

## ğŸš€VÃ½zva

Vezmite Ãºlohu z predchÃ¡dzajÃºcej kontroly vedomostÃ­ a skÃºste ju implementovaÅ¥. Otestujte bota na priateÄ¾ovi. DokÃ¡Å¾e ich oklamaÅ¥? DokÃ¡Å¾ete urobiÅ¥ vÃ¡Å¡ho bota viac 'uveriteÄ¾nÃ½m'?

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ml/)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

V nasledujÃºcich lekciÃ¡ch sa dozviete viac o analÃ½ze sentimentu. PreskÃºmajte tÃºto zaujÃ­mavÃº techniku v ÄlÃ¡nkoch, ako sÃº tieto na [KDNuggets](https://www.kdnuggets.com/tag/nlp)

## Zadanie 

[Urobte bota, ktorÃ½ odpovedÃ¡](assignment.md)

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.