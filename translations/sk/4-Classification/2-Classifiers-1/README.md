<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T16:18:25+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "sk"
}
-->
# Klasifik√°tory kuch√Ω≈à 1

V tejto lekcii pou≈æijete dataset, ktor√Ω ste si ulo≈æili z predch√°dzaj√∫cej lekcie, pln√Ω vyv√°≈æen√Ωch a ƒçist√Ωch √∫dajov o kuchyniach.

Tento dataset pou≈æijete s r√¥znymi klasifik√°tormi na _predpovedanie n√°rodnej kuchyne na z√°klade skupiny ingredienci√≠_. Pri tom sa dozviete viac o tom, ako m√¥≈æu by≈• algoritmy vyu≈æ√≠van√© na klasifikaƒçn√© √∫lohy.

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ml/)
# Pr√≠prava

Za predpokladu, ≈æe ste dokonƒçili [Lekciu 1](../1-Introduction/README.md), uistite sa, ≈æe s√∫bor _cleaned_cuisines.csv_ existuje v kore≈àovom prieƒçinku `/data` pre tieto ≈°tyri lekcie.

## Cviƒçenie - predpovedanie n√°rodnej kuchyne

1. Pracujte v prieƒçinku _notebook.ipynb_ tejto lekcie, importujte tento s√∫bor spolu s kni≈ænicou Pandas:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    √ödaje vyzeraj√∫ takto:

|     | Unnamed: 0 | kuchy≈àa | mandƒæa | angelika | an√≠z | an√≠zov√© semeno | jablko | jablkov√Ω brandy | marhuƒæa | armagnac | ... | whiskey | biely chlieb | biele v√≠no | celozrnn√° p≈°eniƒçn√° m√∫ka | v√≠no | drevo | yam | dro≈ædie | jogurt | cuketa |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indick√° | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indick√° | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indick√° | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indick√° | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indick√° | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Teraz importujte niekoƒæko ƒèal≈°√≠ch kni≈æn√≠c:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Rozdeƒæte X a y s√∫radnice do dvoch dataframeov na tr√©novanie. `cuisine` m√¥≈æe by≈• dataframe s oznaƒçeniami:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Bude to vyzera≈• takto:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. Odstr√°≈àte stƒ∫pec `Unnamed: 0` a stƒ∫pec `cuisine` pomocou `drop()`. Zvy≈°ok √∫dajov ulo≈æte ako tr√©novateƒæn√© vlastnosti:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    Va≈°e vlastnosti vyzeraj√∫ takto:

|      | mandƒæa | angelika | an√≠z | an√≠zov√© semeno | jablko | jablkov√Ω brandy | marhuƒæa | armagnac | artemisia | artiƒçok |  ... | whiskey | biely chlieb | biele v√≠no | celozrnn√° p≈°eniƒçn√° m√∫ka | v√≠no | drevo | yam | dro≈ædie | jogurt | cuketa |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Teraz ste pripraven√≠ tr√©nova≈• v√°≈° model!

## V√Ωber klasifik√°tora

Keƒè s√∫ va≈°e √∫daje ƒçist√© a pripraven√© na tr√©novanie, mus√≠te sa rozhodn√∫≈•, ktor√Ω algoritmus pou≈æi≈• na t√∫to √∫lohu.

Scikit-learn zaraƒèuje klasifik√°ciu pod Supervised Learning, a v tejto kateg√≥rii n√°jdete mnoho sp√¥sobov klasifik√°cie. [Rozmanitos≈•](https://scikit-learn.org/stable/supervised_learning.html) je na prv√Ω pohƒæad dos≈• zar√°≈æaj√∫ca. Nasleduj√∫ce met√≥dy zah≈ï≈àaj√∫ techniky klasifik√°cie:

- Line√°rne modely
- Support Vector Machines
- Stochastick√Ω gradientn√Ω zostup
- Najbli≈æ≈°√≠ susedia
- Gaussovsk√© procesy
- Rozhodovacie stromy
- Ensemble met√≥dy (hlasovac√≠ klasifik√°tor)
- Multiclass a multioutput algoritmy (multiclass a multilabel klasifik√°cia, multiclass-multioutput klasifik√°cia)

> Na klasifik√°ciu √∫dajov m√¥≈æete pou≈æi≈• aj [neur√≥nov√© siete](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), ale to je mimo rozsah tejto lekcie.

### Ak√Ω klasifik√°tor zvoli≈•?

Tak≈æe, ktor√Ω klasifik√°tor by ste si mali vybra≈•? ƒåasto je dobr√© vysk√∫≈°a≈• niekoƒæko a hƒæada≈• dobr√Ω v√Ωsledok. Scikit-learn pon√∫ka [porovnanie vedƒæa seba](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) na vytvorenom datasete, kde porovn√°va KNeighbors, SVC dvoma sp√¥sobmi, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB a QuadraticDiscriminationAnalysis, priƒçom v√Ωsledky vizualizuje:

![porovnanie klasifik√°torov](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Grafy generovan√© na dokument√°cii Scikit-learn

> AutoML tento probl√©m elegantne rie≈°i t√Ωm, ≈æe vykon√°va tieto porovnania v cloude, ƒço v√°m umo≈æ≈àuje vybra≈• najlep≈°√≠ algoritmus pre va≈°e √∫daje. Vysk√∫≈°ajte to [tu](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### Lep≈°√≠ pr√≠stup

Lep≈°√≠ sp√¥sob ako n√°hodne h√°da≈• je v≈°ak nasledova≈• n√°pady z tejto stiahnuteƒænej [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). Tu zist√≠me, ≈æe pre n√°≈° multiclass probl√©m m√°me niekoƒæko mo≈ænost√≠:

![cheatsheet pre multiclass probl√©my](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> ƒåas≈• Microsoftovej Algorithm Cheat Sheet, ktor√° podrobne opisuje mo≈ænosti multiclass klasifik√°cie

‚úÖ Stiahnite si tento cheat sheet, vytlaƒçte ho a zaveste na stenu!

### √övahy

Pozrime sa, ƒçi dok√°≈æeme rozumne zhodnoti≈• r√¥zne pr√≠stupy vzhƒæadom na obmedzenia, ktor√© m√°me:

- **Neur√≥nov√© siete s√∫ pr√≠li≈° n√°roƒçn√©**. Vzhƒæadom na n√°≈° ƒçist√Ω, ale minim√°lny dataset a fakt, ≈æe tr√©novanie prebieha lok√°lne cez notebooky, s√∫ neur√≥nov√© siete pr√≠li≈° n√°roƒçn√© na t√∫to √∫lohu.
- **≈Ωiadny dvojtriedny klasifik√°tor**. Nepou≈æ√≠vame dvojtriedny klasifik√°tor, tak≈æe to vyluƒçuje one-vs-all.
- **Rozhodovac√≠ strom alebo logistick√° regresia by mohli fungova≈•**. Rozhodovac√≠ strom by mohol fungova≈•, alebo logistick√° regresia pre multiclass √∫daje.
- **Multiclass Boosted Decision Trees rie≈°ia in√Ω probl√©m**. Multiclass Boosted Decision Tree je najvhodnej≈°√≠ pre neparametrick√© √∫lohy, napr. √∫lohy urƒçen√© na vytv√°ranie rebr√≠ƒçkov, tak≈æe pre n√°s nie je u≈æitoƒçn√Ω.

### Pou≈æitie Scikit-learn 

Budeme pou≈æ√≠va≈• Scikit-learn na anal√Ωzu na≈°ich √∫dajov. Existuje v≈°ak mnoho sp√¥sobov, ako pou≈æi≈• logistick√∫ regresiu v Scikit-learn. Pozrite sa na [parametre na nastavenie](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

V podstate existuj√∫ dva d√¥le≈æit√© parametre - `multi_class` a `solver` - ktor√© mus√≠me ≈°pecifikova≈•, keƒè po≈æiadame Scikit-learn o vykonanie logistickej regresie. Hodnota `multi_class` aplikuje urƒçit√∫ logiku. Hodnota solveru urƒçuje, ak√Ω algoritmus sa pou≈æije. Nie v≈°etky solvery m√¥≈æu by≈• sp√°rovan√© so v≈°etk√Ωmi hodnotami `multi_class`.

Podƒæa dokument√°cie, v pr√≠pade multiclass, tr√©ningov√Ω algoritmus:

- **Pou≈æ√≠va sch√©mu one-vs-rest (OvR)**, ak je mo≈ænos≈• `multi_class` nastaven√° na `ovr`
- **Pou≈æ√≠va cross-entropy loss**, ak je mo≈ænos≈• `multi_class` nastaven√° na `multinomial`. (Moment√°lne je mo≈ænos≈• `multinomial` podporovan√° iba solvermi ‚Äòlbfgs‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô a ‚Äònewton-cg‚Äô.)

> üéì Sch√©ma tu m√¥≈æe by≈• buƒè 'ovr' (one-vs-rest) alebo 'multinomial'. Keƒè≈æe logistick√° regresia je skutoƒçne navrhnut√° na podporu bin√°rnej klasifik√°cie, tieto sch√©my jej umo≈æ≈àuj√∫ lep≈°ie zvl√°dnu≈• √∫lohy multiclass klasifik√°cie. [zdroj](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> üéì Solver je definovan√Ω ako "algoritmus, ktor√Ω sa pou≈æije na optimalizaƒçn√Ω probl√©m". [zdroj](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn pon√∫ka t√∫to tabuƒæku na vysvetlenie, ako solvery zvl√°daj√∫ r√¥zne v√Ωzvy, ktor√© predstavuj√∫ r√¥zne typy d√°tov√Ωch ≈°trukt√∫r:

![solvery](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## Cviƒçenie - rozdelenie √∫dajov

M√¥≈æeme sa zamera≈• na logistick√∫ regresiu pre n√°≈° prv√Ω tr√©ningov√Ω pokus, keƒè≈æe ste sa o nej ned√°vno uƒçili v predch√°dzaj√∫cej lekcii.
Rozdeƒæte svoje √∫daje na tr√©novacie a testovacie skupiny pomocou `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## Cviƒçenie - aplik√°cia logistickej regresie

Keƒè≈æe pou≈æ√≠vate pr√≠pad multiclass, mus√≠te si vybra≈•, ak√∫ _sch√©mu_ pou≈æi≈• a ak√Ω _solver_ nastavi≈•. Pou≈æite LogisticRegression s nastaven√≠m multiclass a solverom **liblinear** na tr√©novanie.

1. Vytvorte logistick√∫ regresiu s multi_class nastavenou na `ovr` a solverom nastaven√Ωm na `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    ‚úÖ Vysk√∫≈°ajte in√Ω solver, napr√≠klad `lbfgs`, ktor√Ω je ƒçasto nastaven√Ω ako predvolen√Ω.
> Pozn√°mka: Pou≈æite funkciu Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) na zjednodu≈°enie va≈°ich √∫dajov, keƒè je to potrebn√©.
Presnos≈• je dobr√° na viac ako **80%**!

1. Tento model si m√¥≈æete vysk√∫≈°a≈• na jednom riadku d√°t (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    V√Ωsledok sa zobraz√≠:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   ‚úÖ Sk√∫ste in√© ƒç√≠slo riadku a skontrolujte v√Ωsledky.

1. Ak chcete √≠s≈• hlb≈°ie, m√¥≈æete skontrolova≈• presnos≈• tejto predikcie:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    V√Ωsledok sa zobraz√≠ - indick√° kuchy≈àa je najpravdepodobnej≈°√≠ odhad s dobrou pravdepodobnos≈•ou:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    ‚úÖ Dok√°≈æete vysvetli≈•, preƒço si model mysl√≠, ≈æe ide o indick√∫ kuchy≈àu?

1. Z√≠skajte viac detailov vytlaƒçen√≠m klasifikaƒçnej spr√°vy, ako ste to robili v lekci√°ch o regresii:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | presnos≈• | recall | f1-sk√≥re | podpora |
    | ------------ | -------- | ------ | -------- | ------- |
    | chinese      | 0.73     | 0.71   | 0.72     | 229     |
    | indian       | 0.91     | 0.93   | 0.92     | 254     |
    | japanese     | 0.70     | 0.75   | 0.72     | 220     |
    | korean       | 0.86     | 0.76   | 0.81     | 242     |
    | thai         | 0.79     | 0.85   | 0.82     | 254     |
    | presnos≈•     | 0.80     | 1199   |          |         |
    | priemer makro| 0.80     | 0.80   | 0.80     | 1199    |
    | v√°≈æen√Ω priemer| 0.80     | 0.80   | 0.80     | 1199    |

## üöÄV√Ωzva

V tejto lekcii ste pou≈æili vyƒçisten√© d√°ta na vytvorenie modelu strojov√©ho uƒçenia, ktor√Ω dok√°≈æe predpoveda≈• n√°rodn√∫ kuchy≈àu na z√°klade s√©rie ingredienci√≠. Venujte ƒças presk√∫maniu mnoh√Ωch mo≈ænost√≠, ktor√© Scikit-learn pon√∫ka na klasifik√°ciu d√°t. Ponorte sa hlb≈°ie do konceptu 'solver', aby ste pochopili, ƒço sa deje v z√°kulis√≠.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ml/)

## Prehƒæad & Samo≈°t√∫dium

Presk√∫majte matematiku za logistickou regresiou v [tejto lekcii](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## Zadanie 

[Presk√∫majte solvery](assignment.md)

---

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nie sme zodpovedn√≠ za ≈æiadne nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.