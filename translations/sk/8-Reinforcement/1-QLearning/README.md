<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T16:40:14+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "sk"
}
-->
# Ãšvod do posilÅˆovacieho uÄenia a Q-Learningu

![Zhrnutie posilÅˆovacieho uÄenia v strojovom uÄenÃ­ v sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote od [Tomomi Imura](https://www.twitter.com/girlie_mac)

PosilÅˆovacie uÄenie zahÅ•Åˆa tri dÃ´leÅ¾itÃ© koncepty: agenta, urÄitÃ© stavy a sÃºbor akciÃ­ pre kaÅ¾dÃ½ stav. VykonanÃ­m akcie v Å¡pecifikovanom stave dostane agent odmenu. Predstavte si poÄÃ­taÄovÃº hru Super Mario. Vy ste Mario, nachÃ¡dzate sa v Ãºrovni hry, stojÃ­te vedÄ¾a okraja Ãºtesu. Nad vami je minca. Vy ako Mario, v Ãºrovni hry, na konkrÃ©tnej pozÃ­cii... to je vÃ¡Å¡ stav. Pohyb o jeden krok doprava (akcia) vÃ¡s zavedie cez okraj, Äo by vÃ¡m prinieslo nÃ­zke ÄÃ­selnÃ© skÃ³re. AvÅ¡ak stlaÄenÃ­m tlaÄidla skoku by ste zÃ­skali bod a zostali naÅ¾ive. To je pozitÃ­vny vÃ½sledok, ktorÃ½ by vÃ¡m mal priniesÅ¥ pozitÃ­vne ÄÃ­selnÃ© skÃ³re.

Pomocou posilÅˆovacieho uÄenia a simulÃ¡tora (hry) sa mÃ´Å¾ete nauÄiÅ¥, ako hraÅ¥ hru tak, aby ste maximalizovali odmenu, Äo znamenÃ¡ zostaÅ¥ naÅ¾ive a zÃ­skaÅ¥ Äo najviac bodov.

[![Ãšvod do posilÅˆovacieho uÄenia](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie a vypoÄujte si Dmitryho diskusiu o posilÅˆovacom uÄenÃ­

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ml/)

## Predpoklady a nastavenie

V tejto lekcii budeme experimentovaÅ¥ s kÃ³dom v Pythone. Mali by ste byÅ¥ schopnÃ­ spustiÅ¥ kÃ³d Jupyter Notebook z tejto lekcie, buÄ na svojom poÄÃ­taÄi alebo niekde v cloude.

MÃ´Å¾ete otvoriÅ¥ [notebook lekcie](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) a prejsÅ¥ touto lekciou, aby ste si ju osvojili.

> **PoznÃ¡mka:** Ak otvÃ¡rate tento kÃ³d z cloudu, musÃ­te tieÅ¾ stiahnuÅ¥ sÃºbor [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), ktorÃ½ sa pouÅ¾Ã­va v kÃ³de notebooku. Pridajte ho do rovnakÃ©ho adresÃ¡ra ako notebook.

## Ãšvod

V tejto lekcii preskÃºmame svet **[Peter a vlk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inÅ¡pirovanÃ½ hudobnou rozprÃ¡vkou od ruskÃ©ho skladateÄ¾a [Sergeja Prokofieva](https://en.wikipedia.org/wiki/Sergei_Prokofiev). PouÅ¾ijeme **posilÅˆovacie uÄenie**, aby sme umoÅ¾nili Petrovi preskÃºmaÅ¥ jeho prostredie, zbieraÅ¥ chutnÃ© jablkÃ¡ a vyhnÃºÅ¥ sa stretnutiu s vlkom.

**PosilÅˆovacie uÄenie** (RL) je technika uÄenia, ktorÃ¡ nÃ¡m umoÅ¾Åˆuje nauÄiÅ¥ sa optimÃ¡lne sprÃ¡vanie **agenta** v urÄitom **prostredÃ­** vykonÃ¡vanÃ­m mnohÃ½ch experimentov. Agent v tomto prostredÃ­ by mal maÅ¥ nejakÃ½ **cieÄ¾**, definovanÃ½ pomocou **funkcie odmeny**.

## Prostredie

Pre jednoduchosÅ¥ si predstavme Petrov svet ako Å¡tvorcovÃº dosku veÄ¾kosti `Å¡Ã­rka` x `vÃ½Å¡ka`, ako je tÃ¡to:

![Petrovo prostredie](../../../../8-Reinforcement/1-QLearning/images/environment.png)

KaÅ¾dÃ¡ bunka na tejto doske mÃ´Å¾e byÅ¥:

* **zem**, po ktorej Peter a inÃ© bytosti mÃ´Å¾u chodiÅ¥.
* **voda**, po ktorej samozrejme nemÃ´Å¾ete chodiÅ¥.
* **strom** alebo **trÃ¡va**, miesto, kde si mÃ´Å¾ete oddÃ½chnuÅ¥.
* **jablko**, ktorÃ© predstavuje nieÄo, Äo by Peter rÃ¡d naÅ¡iel, aby sa nakÅ•mil.
* **vlk**, ktorÃ½ je nebezpeÄnÃ½ a treba sa mu vyhnÃºÅ¥.

Existuje samostatnÃ½ Python modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), ktorÃ½ obsahuje kÃ³d na prÃ¡cu s tÃ½mto prostredÃ­m. KeÄÅ¾e tento kÃ³d nie je dÃ´leÅ¾itÃ½ pre pochopenie naÅ¡ich konceptov, importujeme modul a pouÅ¾ijeme ho na vytvorenie vzorovej dosky (blok kÃ³du 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Tento kÃ³d by mal vytlaÄiÅ¥ obrÃ¡zok prostredia podobnÃ½ tomu vyÅ¡Å¡ie.

## Akcie a politika

V naÅ¡om prÃ­klade by PetrovÃ½m cieÄ¾om bolo nÃ¡jsÅ¥ jablko, priÄom sa vyhne vlkovi a inÃ½m prekÃ¡Å¾kam. Na to mÃ´Å¾e v podstate chodiÅ¥, kÃ½m nenÃ¡jde jablko.

Preto si na akejkoÄ¾vek pozÃ­cii mÃ´Å¾e vybraÅ¥ jednu z nasledujÃºcich akciÃ­: hore, dole, doÄ¾ava a doprava.

Tieto akcie definujeme ako slovnÃ­k a mapujeme ich na dvojice zodpovedajÃºcich zmien sÃºradnÃ­c. NaprÃ­klad pohyb doprava (`R`) by zodpovedal dvojici `(1,0)`. (blok kÃ³du 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Aby sme to zhrnuli, stratÃ©gia a cieÄ¾ tohto scenÃ¡ra sÃº nasledovnÃ©:

- **StratÃ©gia** nÃ¡Å¡ho agenta (Petra) je definovanÃ¡ tzv. **politikou**. Politika je funkcia, ktorÃ¡ vracia akciu v akomkoÄ¾vek danom stave. V naÅ¡om prÃ­pade je stav problÃ©mu reprezentovanÃ½ doskou vrÃ¡tane aktuÃ¡lnej pozÃ­cie hrÃ¡Äa.

- **CieÄ¾** posilÅˆovacieho uÄenia je nakoniec nauÄiÅ¥ sa dobrÃº politiku, ktorÃ¡ nÃ¡m umoÅ¾nÃ­ efektÃ­vne vyrieÅ¡iÅ¥ problÃ©m. Ako zÃ¡klad vÅ¡ak zvÃ¡Å¾me najjednoduchÅ¡iu politiku nazÃ½vanÃº **nÃ¡hodnÃ¡ chÃ´dza**.

## NÃ¡hodnÃ¡ chÃ´dza

Najprv vyrieÅ¡me nÃ¡Å¡ problÃ©m implementÃ¡ciou stratÃ©gie nÃ¡hodnej chÃ´dze. Pri nÃ¡hodnej chÃ´dzi budeme nÃ¡hodne vyberaÅ¥ ÄalÅ¡iu akciu z povolenÃ½ch akciÃ­, kÃ½m nedosiahneme jablko (blok kÃ³du 3).

1. Implementujte nÃ¡hodnÃº chÃ´dzu pomocou niÅ¾Å¡ie uvedenÃ©ho kÃ³du:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Volanie `walk` by malo vrÃ¡tiÅ¥ dÄºÅ¾ku zodpovedajÃºcej cesty, ktorÃ¡ sa mÃ´Å¾e lÃ­Å¡iÅ¥ od jednÃ©ho spustenia k druhÃ©mu.

1. Spustite experiment chÃ´dze niekoÄ¾kokrÃ¡t (povedzme 100-krÃ¡t) a vytlaÄte vÃ½slednÃ© Å¡tatistiky (blok kÃ³du 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    VÅ¡imnite si, Å¾e priemernÃ¡ dÄºÅ¾ka cesty je okolo 30-40 krokov, Äo je dosÅ¥ veÄ¾a, vzhÄ¾adom na to, Å¾e priemernÃ¡ vzdialenosÅ¥ k najbliÅ¾Å¡iemu jablku je okolo 5-6 krokov.

    MÃ´Å¾ete tieÅ¾ vidieÅ¥, ako vyzerÃ¡ Petrov pohyb poÄas nÃ¡hodnej chÃ´dze:

    ![Petrova nÃ¡hodnÃ¡ chÃ´dza](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funkcia odmeny

Aby bola naÅ¡a politika inteligentnejÅ¡ia, musÃ­me pochopiÅ¥, ktorÃ© pohyby sÃº "lepÅ¡ie" ako ostatnÃ©. Na to musÃ­me definovaÅ¥ nÃ¡Å¡ cieÄ¾.

CieÄ¾ mÃ´Å¾e byÅ¥ definovanÃ½ pomocou **funkcie odmeny**, ktorÃ¡ vrÃ¡ti nejakÃº hodnotu skÃ³re pre kaÅ¾dÃ½ stav. ÄŒÃ­m vyÅ¡Å¡ie ÄÃ­slo, tÃ½m lepÅ¡ia funkcia odmeny. (blok kÃ³du 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

ZaujÃ­mavÃ© na funkciÃ¡ch odmeny je, Å¾e vo vÃ¤ÄÅ¡ine prÃ­padov *dostaneme podstatnÃº odmenu aÅ¾ na konci hry*. To znamenÃ¡, Å¾e nÃ¡Å¡ algoritmus by mal nejako zapamÃ¤taÅ¥ "dobrÃ©" kroky, ktorÃ© vedÃº k pozitÃ­vnej odmene na konci, a zvÃ½Å¡iÅ¥ ich dÃ´leÅ¾itosÅ¥. Podobne by mali byÅ¥ odradenÃ© vÅ¡etky pohyby, ktorÃ© vedÃº k zlÃ½m vÃ½sledkom.

## Q-Learning

Algoritmus, ktorÃ½ tu budeme diskutovaÅ¥, sa nazÃ½va **Q-Learning**. V tomto algoritme je politika definovanÃ¡ funkciou (alebo dÃ¡tovou Å¡truktÃºrou) nazÃ½vanou **Q-TabuÄ¾ka**. TÃ¡ zaznamenÃ¡va "kvalitu" kaÅ¾dej akcie v danom stave.

NazÃ½va sa Q-TabuÄ¾ka, pretoÅ¾e je Äasto vÃ½hodnÃ© ju reprezentovaÅ¥ ako tabuÄ¾ku alebo viacrozmernÃ© pole. KeÄÅ¾e naÅ¡a doska mÃ¡ rozmery `Å¡Ã­rka` x `vÃ½Å¡ka`, mÃ´Å¾eme Q-TabuÄ¾ku reprezentovaÅ¥ pomocou numpy poÄ¾a s tvarom `Å¡Ã­rka` x `vÃ½Å¡ka` x `len(akcie)`: (blok kÃ³du 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

VÅ¡imnite si, Å¾e inicializujeme vÅ¡etky hodnoty Q-TabuÄ¾ky rovnakou hodnotou, v naÅ¡om prÃ­pade - 0.25. To zodpovedÃ¡ politike "nÃ¡hodnej chÃ´dze", pretoÅ¾e vÅ¡etky pohyby v kaÅ¾dom stave sÃº rovnako dobrÃ©. Q-TabuÄ¾ku mÃ´Å¾eme odovzdaÅ¥ funkcii `plot`, aby sme tabuÄ¾ku vizualizovali na doske: `m.plot(Q)`.

![Petrovo prostredie](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

V strede kaÅ¾dej bunky je "Å¡Ã­pka", ktorÃ¡ oznaÄuje preferovanÃ½ smer pohybu. KeÄÅ¾e vÅ¡etky smery sÃº rovnakÃ©, zobrazÃ­ sa bodka.

Teraz musÃ­me spustiÅ¥ simulÃ¡ciu, preskÃºmaÅ¥ naÅ¡e prostredie a nauÄiÅ¥ sa lepÅ¡ie rozdelenie hodnÃ´t Q-TabuÄ¾ky, ktorÃ© nÃ¡m umoÅ¾nÃ­ oveÄ¾a rÃ½chlejÅ¡ie nÃ¡jsÅ¥ cestu k jablku.

## Podstata Q-Learningu: Bellmanova rovnica

KeÄ sa zaÄneme pohybovaÅ¥, kaÅ¾dÃ¡ akcia bude maÅ¥ zodpovedajÃºcu odmenu, t.j. teoreticky mÃ´Å¾eme vybraÅ¥ ÄalÅ¡iu akciu na zÃ¡klade najvyÅ¡Å¡ej okamÅ¾itej odmeny. AvÅ¡ak vo vÃ¤ÄÅ¡ine stavov pohyb nedosiahne nÃ¡Å¡ cieÄ¾ dosiahnuÅ¥ jablko, a preto nemÃ´Å¾eme okamÅ¾ite rozhodnÃºÅ¥, ktorÃ½ smer je lepÅ¡Ã­.

> PamÃ¤tajte, Å¾e nezÃ¡leÅ¾Ã­ na okamÅ¾itom vÃ½sledku, ale skÃ´r na koneÄnom vÃ½sledku, ktorÃ½ dosiahneme na konci simulÃ¡cie.

Aby sme zohÄ¾adnili tÃºto oneskorenÃº odmenu, musÃ­me pouÅ¾iÅ¥ princÃ­py **[dynamickÃ©ho programovania](https://en.wikipedia.org/wiki/Dynamic_programming)**, ktorÃ© nÃ¡m umoÅ¾ÅˆujÃº premÃ½Å¡Ä¾aÅ¥ o naÅ¡om problÃ©me rekurzÃ­vne.

Predpokladajme, Å¾e sa teraz nachÃ¡dzame v stave *s*, a chceme sa presunÃºÅ¥ do ÄalÅ¡ieho stavu *s'*. TÃ½mto krokom zÃ­skame okamÅ¾itÃº odmenu *r(s,a)*, definovanÃº funkciou odmeny, plus nejakÃº budÃºcu odmenu. Ak predpokladÃ¡me, Å¾e naÅ¡a Q-TabuÄ¾ka sprÃ¡vne odrÃ¡Å¾a "atraktivitu" kaÅ¾dej akcie, potom v stave *s'* si vyberieme akciu *a*, ktorÃ¡ zodpovedÃ¡ maximÃ¡lnej hodnote *Q(s',a')*. TakÅ¾e najlepÅ¡ia moÅ¾nÃ¡ budÃºca odmena, ktorÃº by sme mohli zÃ­skaÅ¥ v stave *s*, bude definovanÃ¡ ako `max`

## Kontrola politiky

KeÄÅ¾e Q-TabuÄ¾ka uvÃ¡dza â€atraktivituâ€œ kaÅ¾dej akcie v kaÅ¾dom stave, je pomerne jednoduchÃ© pouÅ¾iÅ¥ ju na definovanie efektÃ­vnej navigÃ¡cie v naÅ¡om svete. V najjednoduchÅ¡om prÃ­pade mÃ´Å¾eme vybraÅ¥ akciu zodpovedajÃºcu najvyÅ¡Å¡ej hodnote v Q-TabuÄ¾ke: (kÃ³dovÃ½ blok 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Ak vyskÃºÅ¡ate vyÅ¡Å¡ie uvedenÃ½ kÃ³d niekoÄ¾kokrÃ¡t, mÃ´Å¾ete si vÅ¡imnÃºÅ¥, Å¾e sa niekedy â€zasekneâ€œ a musÃ­te stlaÄiÅ¥ tlaÄidlo STOP v notebooku, aby ste ho preruÅ¡ili. K tomu dochÃ¡dza, pretoÅ¾e mÃ´Å¾u existovaÅ¥ situÃ¡cie, keÄ si dva stavy â€ukazujÃºâ€œ navzÃ¡jom z hÄ¾adiska optimÃ¡lnej hodnoty Q, v takom prÃ­pade agent skonÄÃ­ pohybom medzi tÃ½mito stavmi donekoneÄna.

## ğŸš€VÃ½zva

> **Ãšloha 1:** UpraviÅ¥ funkciu `walk` tak, aby obmedzila maximÃ¡lnu dÄºÅ¾ku cesty na urÄitÃ½ poÄet krokov (naprÃ­klad 100) a pozorovaÅ¥, ako vyÅ¡Å¡ie uvedenÃ½ kÃ³d obÄas vrÃ¡ti tÃºto hodnotu.

> **Ãšloha 2:** UpraviÅ¥ funkciu `walk` tak, aby sa nevracala na miesta, kde uÅ¾ predtÃ½m bola. TÃ½m sa zabrÃ¡ni tomu, aby sa `walk` opakoval, avÅ¡ak agent sa stÃ¡le mÃ´Å¾e ocitnÃºÅ¥ â€uvÃ¤znenÃ½â€œ na mieste, z ktorÃ©ho sa nedokÃ¡Å¾e dostaÅ¥.

## NavigÃ¡cia

LepÅ¡ia navigaÄnÃ¡ politika by bola tÃ¡, ktorÃº sme pouÅ¾ili poÄas trÃ©ningu, ktorÃ¡ kombinuje vyuÅ¾Ã­vanie a skÃºmanie. V tejto politike vyberieme kaÅ¾dÃº akciu s urÄitou pravdepodobnosÅ¥ou, Ãºmernou hodnotÃ¡m v Q-TabuÄ¾ke. TÃ¡to stratÃ©gia mÃ´Å¾e stÃ¡le viesÅ¥ k tomu, Å¾e sa agent vrÃ¡ti na pozÃ­ciu, ktorÃº uÅ¾ preskÃºmal, ale, ako mÃ´Å¾ete vidieÅ¥ z niÅ¾Å¡ie uvedenÃ©ho kÃ³du, vÃ½sledkom je veÄ¾mi krÃ¡tka priemernÃ¡ cesta k poÅ¾adovanÃ©mu miestu (nezabudnite, Å¾e `print_statistics` spÃºÅ¡Å¥a simulÃ¡ciu 100-krÃ¡t): (kÃ³dovÃ½ blok 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Po spustenÃ­ tohto kÃ³du by ste mali zÃ­skaÅ¥ oveÄ¾a kratÅ¡iu priemernÃº dÄºÅ¾ku cesty ako predtÃ½m, v rozmedzÃ­ 3-6.

## SkÃºmanie procesu uÄenia

Ako sme spomenuli, proces uÄenia je rovnovÃ¡hou medzi skÃºmanÃ­m a vyuÅ¾Ã­vanÃ­m zÃ­skanÃ½ch znalostÃ­ o Å¡truktÃºre problÃ©movÃ©ho priestoru. Videli sme, Å¾e vÃ½sledky uÄenia (schopnosÅ¥ pomÃ´cÅ¥ agentovi nÃ¡jsÅ¥ krÃ¡tku cestu k cieÄ¾u) sa zlepÅ¡ili, ale je tieÅ¾ zaujÃ­mavÃ© pozorovaÅ¥, ako sa priemernÃ¡ dÄºÅ¾ka cesty sprÃ¡va poÄas procesu uÄenia:

## Zhrnutie poznatkov:

- **PriemernÃ¡ dÄºÅ¾ka cesty sa zvyÅ¡uje**. Na zaÄiatku vidÃ­me, Å¾e priemernÃ¡ dÄºÅ¾ka cesty sa zvyÅ¡uje. Pravdepodobne je to spÃ´sobenÃ© tÃ½m, Å¾e keÄ o prostredÃ­ niÄ nevieme, mÃ¡me tendenciu uviaznuÅ¥ v zlÃ½ch stavoch, ako je voda alebo vlk. KeÄ sa dozvieme viac a zaÄneme tieto znalosti vyuÅ¾Ã­vaÅ¥, mÃ´Å¾eme prostredie skÃºmaÅ¥ dlhÅ¡ie, ale stÃ¡le nevieme presne, kde sÃº jablkÃ¡.

- **DÄºÅ¾ka cesty sa zniÅ¾uje, ako sa uÄÃ­me viac**. KeÄ sa nauÄÃ­me dosÅ¥, agentovi sa Ä¾ahÅ¡ie dosahuje cieÄ¾ a dÄºÅ¾ka cesty sa zaÄne zniÅ¾ovaÅ¥. StÃ¡le vÅ¡ak skÃºmame novÃ© moÅ¾nosti, takÅ¾e sa Äasto odklonÃ­me od najlepÅ¡ej cesty a skÃºmame novÃ© moÅ¾nosti, Äo predlÅ¾uje cestu nad optimÃ¡lnu hodnotu.

- **DÄºÅ¾ka sa nÃ¡hle zvÃ½Å¡i**. Na grafe tieÅ¾ vidÃ­me, Å¾e v urÄitom bode sa dÄºÅ¾ka nÃ¡hle zvÃ½Å¡ila. To poukazuje na stochastickÃº povahu procesu a na to, Å¾e mÃ´Å¾eme v urÄitom bode â€pokaziÅ¥â€œ koeficienty Q-TabuÄ¾ky tÃ½m, Å¾e ich prepÃ­Å¡eme novÃ½mi hodnotami. IdeÃ¡lne by sa tomu malo predÃ­sÅ¥ znÃ­Å¾enÃ­m rÃ½chlosti uÄenia (naprÃ­klad ku koncu trÃ©ningu upravujeme hodnoty Q-TabuÄ¾ky len o malÃº hodnotu).

Celkovo je dÃ´leÅ¾itÃ© pamÃ¤taÅ¥ na to, Å¾e Ãºspech a kvalita procesu uÄenia vÃ½znamne zÃ¡visÃ­ od parametrov, ako je rÃ½chlosÅ¥ uÄenia, pokles rÃ½chlosti uÄenia a diskontnÃ½ faktor. Tieto sa Äasto nazÃ½vajÃº **hyperparametre**, aby sa odlÃ­Å¡ili od **parametrov**, ktorÃ© optimalizujeme poÄas trÃ©ningu (naprÃ­klad koeficienty Q-TabuÄ¾ky). Proces hÄ¾adania najlepÅ¡Ã­ch hodnÃ´t hyperparametrov sa nazÃ½va **optimalizÃ¡cia hyperparametrov** a zaslÃºÅ¾i si samostatnÃº tÃ©mu.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ml/)

## Zadanie 
[RealistickejÅ¡Ã­ svet](assignment.md)

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.