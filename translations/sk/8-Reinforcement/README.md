<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T16:35:50+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "sk"
}
-->
# Ãšvod do posilÅˆovacieho uÄenia

PosilÅˆovacie uÄenie, RL, je povaÅ¾ovanÃ© za jeden zo zÃ¡kladnÃ½ch paradigmatov strojovÃ©ho uÄenia, vedÄ¾a uÄenia s uÄiteÄ¾om a uÄenia bez uÄiteÄ¾a. RL je o rozhodnutiach: robiÅ¥ sprÃ¡vne rozhodnutia alebo sa aspoÅˆ z nich uÄiÅ¥.

Predstavte si, Å¾e mÃ¡te simulovanÃ© prostredie, naprÃ­klad akciovÃ½ trh. ÄŒo sa stane, ak zavediete urÄitÃº regulÃ¡ciu? MÃ¡ to pozitÃ­vny alebo negatÃ­vny efekt? Ak sa stane nieÄo negatÃ­vne, musÃ­te prijaÅ¥ toto _negatÃ­vne posilnenie_, pouÄiÅ¥ sa z neho a zmeniÅ¥ smer. Ak je vÃ½sledok pozitÃ­vny, musÃ­te na tom _pozitÃ­vnom posilnenÃ­_ stavaÅ¥.

![Peter a vlk](../../../8-Reinforcement/images/peter.png)

> Peter a jeho priatelia musia uniknÃºÅ¥ hladnÃ©mu vlkovi! ObrÃ¡zok od [Jen Looper](https://twitter.com/jenlooper)

## RegionÃ¡lna tÃ©ma: Peter a vlk (Rusko)

[Peter a vlk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) je hudobnÃ¡ rozprÃ¡vka napÃ­sanÃ¡ ruskÃ½m skladateÄ¾om [Sergejom Prokofievom](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Je to prÃ­beh o mladom pionierovi Petrovi, ktorÃ½ odvÃ¡Å¾ne vyjde z domu na lesnÃº Äistinu, aby prenasledoval vlka. V tejto sekcii budeme trÃ©novaÅ¥ algoritmy strojovÃ©ho uÄenia, ktorÃ© pomÃ´Å¾u Petrovi:

- **PreskÃºmaÅ¥** okolitÃº oblasÅ¥ a vytvoriÅ¥ optimÃ¡lnu navigaÄnÃº mapu
- **NauÄiÅ¥ sa** pouÅ¾Ã­vaÅ¥ skateboard a udrÅ¾iavaÅ¥ rovnovÃ¡hu, aby sa mohol pohybovaÅ¥ rÃ½chlejÅ¡ie.

[![Peter a vlk](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie a vypoÄujte si Peter a vlk od Prokofieva

## PosilÅˆovacie uÄenie

V predchÃ¡dzajÃºcich sekciÃ¡ch ste videli dva prÃ­klady problÃ©mov strojovÃ©ho uÄenia:

- **S uÄiteÄ¾om**, kde mÃ¡me datasety, ktorÃ© naznaÄujÃº vzorovÃ© rieÅ¡enia problÃ©mu, ktorÃ½ chceme vyrieÅ¡iÅ¥. [KlasifikÃ¡cia](../4-Classification/README.md) a [regresia](../2-Regression/README.md) sÃº Ãºlohy uÄenia s uÄiteÄ¾om.
- **Bez uÄiteÄ¾a**, kde nemÃ¡me oznaÄenÃ© trÃ©ningovÃ© dÃ¡ta. HlavnÃ½m prÃ­kladom uÄenia bez uÄiteÄ¾a je [Zhlukovanie](../5-Clustering/README.md).

V tejto sekcii vÃ¡s zoznÃ¡mime s novÃ½m typom problÃ©mu uÄenia, ktorÃ½ nevyÅ¾aduje oznaÄenÃ© trÃ©ningovÃ© dÃ¡ta. Existuje niekoÄ¾ko typov takÃ½chto problÃ©mov:

- **[PolouÄenie s uÄiteÄ¾om](https://wikipedia.org/wiki/Semi-supervised_learning)**, kde mÃ¡me veÄ¾a neoznaÄenÃ½ch dÃ¡t, ktorÃ© mÃ´Å¾eme pouÅ¾iÅ¥ na predtrÃ©ning modelu.
- **[PosilÅˆovacie uÄenie](https://wikipedia.org/wiki/Reinforcement_learning)**, v ktorom sa agent uÄÃ­, ako sa sprÃ¡vaÅ¥, vykonÃ¡vanÃ­m experimentov v nejakom simulovanom prostredÃ­.

### PrÃ­klad - poÄÃ­taÄovÃ¡ hra

Predstavte si, Å¾e chcete nauÄiÅ¥ poÄÃ­taÄ hraÅ¥ hru, naprÃ­klad Å¡ach alebo [Super Mario](https://wikipedia.org/wiki/Super_Mario). Aby poÄÃ­taÄ hral hru, potrebujeme, aby predpovedal, akÃ½ Å¥ah urobiÅ¥ v kaÅ¾dom stave hry. Aj keÄ sa to mÃ´Å¾e zdaÅ¥ ako problÃ©m klasifikÃ¡cie, nie je to tak - pretoÅ¾e nemÃ¡me dataset so stavmi a zodpovedajÃºcimi akciami. Aj keÄ mÃ´Å¾eme maÅ¥ nejakÃ© dÃ¡ta, ako existujÃºce Å¡achovÃ© partie alebo zÃ¡znamy hrÃ¡Äov hrajÃºcich Super Mario, je pravdepodobnÃ©, Å¾e tieto dÃ¡ta nebudÃº dostatoÄne pokrÃ½vaÅ¥ veÄ¾kÃ© mnoÅ¾stvo moÅ¾nÃ½ch stavov.

Namiesto hÄ¾adania existujÃºcich hernÃ½ch dÃ¡t je **PosilÅˆovacie uÄenie** (RL) zaloÅ¾enÃ© na myÅ¡lienke *nechaÅ¥ poÄÃ­taÄ hraÅ¥* mnohokrÃ¡t a pozorovaÅ¥ vÃ½sledok. Na aplikÃ¡ciu posilÅˆovacieho uÄenia potrebujeme dve veci:

- **Prostredie** a **simulÃ¡tor**, ktorÃ© nÃ¡m umoÅ¾nia hraÅ¥ hru mnohokrÃ¡t. Tento simulÃ¡tor by definoval vÅ¡etky pravidlÃ¡ hry, ako aj moÅ¾nÃ© stavy a akcie.

- **Funkciu odmeny**, ktorÃ¡ nÃ¡m povie, ako dobre sme si poÄÃ­nali poÄas kaÅ¾dÃ©ho Å¥ahu alebo hry.

HlavnÃ½ rozdiel medzi inÃ½mi typmi strojovÃ©ho uÄenia a RL je ten, Å¾e v RL zvyÄajne nevieme, Äi vyhrÃ¡me alebo prehrÃ¡me, kÃ½m nedokonÄÃ­me hru. Preto nemÃ´Å¾eme povedaÅ¥, Äi je urÄitÃ½ Å¥ah sÃ¡m o sebe dobrÃ½ alebo nie - odmenu dostaneme aÅ¾ na konci hry. NaÅ¡Ã­m cieÄ¾om je navrhnÃºÅ¥ algoritmy, ktorÃ© nÃ¡m umoÅ¾nia trÃ©novaÅ¥ model za neistÃ½ch podmienok. NauÄÃ­me sa o jednom RL algoritme nazÃ½vanom **Q-learning**.

## Lekcie

1. [Ãšvod do posilÅˆovacieho uÄenia a Q-Learningu](1-QLearning/README.md)
2. [PouÅ¾Ã­vanie simulaÄnÃ©ho prostredia Gym](2-Gym/README.md)

## Kredity

"Ãšvod do posilÅˆovacieho uÄenia" napÃ­sal s â™¥ï¸ [Dmitry Soshnikov](http://soshnikov.com)

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.