<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T16:46:17+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "sk"
}
-->
## Predpoklady

V tejto lekcii budeme pouÅ¾Ã­vaÅ¥ kniÅ¾nicu **OpenAI Gym** na simulÃ¡ciu rÃ´znych **prostredÃ­**. KÃ³d z tejto lekcie mÃ´Å¾ete spustiÅ¥ lokÃ¡lne (napr. vo Visual Studio Code), v takom prÃ­pade sa simulÃ¡cia otvorÃ­ v novom okne. Pri spÃºÅ¡Å¥anÃ­ kÃ³du online mÃ´Å¾e byÅ¥ potrebnÃ© upraviÅ¥ kÃ³d, ako je popÃ­sanÃ© [tu](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

V predchÃ¡dzajÃºcej lekcii boli pravidlÃ¡ hry a stav definovanÃ© triedou `Board`, ktorÃº sme si sami vytvorili. Tu pouÅ¾ijeme Å¡peciÃ¡lne **simulaÄnÃ© prostredie**, ktorÃ© bude simulovaÅ¥ fyziku za balansujÃºcou tyÄou. JednÃ½m z najpopulÃ¡rnejÅ¡Ã­ch simulaÄnÃ½ch prostredÃ­ na trÃ©novanie algoritmov posilnenÃ©ho uÄenia je [Gym](https://gym.openai.com/), ktorÃ½ spravuje [OpenAI](https://openai.com/). Pomocou Gym mÃ´Å¾eme vytvÃ¡raÅ¥ rÃ´zne **prostredia**, od simulÃ¡cie CartPole aÅ¾ po hry Atari.

> **PoznÃ¡mka**: ÄalÅ¡ie prostredia dostupnÃ© v OpenAI Gym si mÃ´Å¾ete pozrieÅ¥ [tu](https://gym.openai.com/envs/#classic_control).

NajskÃ´r nainÅ¡talujme Gym a importujme potrebnÃ© kniÅ¾nice (kÃ³dovÃ½ blok 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## CviÄenie - inicializÃ¡cia prostredia CartPole

Aby sme mohli pracovaÅ¥ s problÃ©mom balansovania CartPole, musÃ­me inicializovaÅ¥ prÃ­sluÅ¡nÃ© prostredie. KaÅ¾dÃ© prostredie je spojenÃ© s:

- **Priestorom pozorovanÃ­**, ktorÃ½ definuje Å¡truktÃºru informÃ¡ciÃ­, ktorÃ© dostÃ¡vame z prostredia. Pri problÃ©me CartPole dostÃ¡vame polohu tyÄe, rÃ½chlosÅ¥ a ÄalÅ¡ie hodnoty.

- **Priestorom akciÃ­**, ktorÃ½ definuje moÅ¾nÃ© akcie. V naÅ¡om prÃ­pade je priestor akciÃ­ diskrÃ©tny a pozostÃ¡va z dvoch akciÃ­ - **vÄ¾avo** a **vpravo**. (kÃ³dovÃ½ blok 2)

1. Na inicializÃ¡ciu zadajte nasledujÃºci kÃ³d:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Aby sme videli, ako prostredie funguje, spustime krÃ¡tku simulÃ¡ciu na 100 krokov. Pri kaÅ¾dom kroku poskytneme jednu z akciÃ­, ktorÃ© sa majÃº vykonaÅ¥ - v tejto simulÃ¡cii nÃ¡hodne vyberÃ¡me akciu z `action_space`.

1. Spustite nasledujÃºci kÃ³d a pozrite sa, Äo sa stane.

    âœ… PamÃ¤tajte, Å¾e je preferovanÃ© spustiÅ¥ tento kÃ³d na lokÃ¡lnej inÅ¡talÃ¡cii Pythonu! (kÃ³dovÃ½ blok 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Mali by ste vidieÅ¥ nieÄo podobnÃ© ako na tomto obrÃ¡zku:

    ![nebalansujÃºci CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. PoÄas simulÃ¡cie musÃ­me zÃ­skavaÅ¥ pozorovania, aby sme sa rozhodli, ako konaÅ¥. V skutoÄnosti funkcia `step` vracia aktuÃ¡lne pozorovania, funkciu odmeny a prÃ­znak `done`, ktorÃ½ indikuje, Äi mÃ¡ zmysel pokraÄovaÅ¥ v simulÃ¡cii alebo nie: (kÃ³dovÃ½ blok 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    V notebooku by ste mali vidieÅ¥ nieÄo podobnÃ© ako toto:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Vektor pozorovanÃ­, ktorÃ½ sa vracia pri kaÅ¾dom kroku simulÃ¡cie, obsahuje nasledujÃºce hodnoty:
    - Poloha vozÃ­ka
    - RÃ½chlosÅ¥ vozÃ­ka
    - Uhol tyÄe
    - RÃ½chlosÅ¥ rotÃ¡cie tyÄe

1. ZÃ­skajte minimÃ¡lnu a maximÃ¡lnu hodnotu tÃ½chto ÄÃ­sel: (kÃ³dovÃ½ blok 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    MÃ´Å¾ete si tieÅ¾ vÅ¡imnÃºÅ¥, Å¾e hodnota odmeny pri kaÅ¾dom kroku simulÃ¡cie je vÅ¾dy 1. Je to preto, Å¾e naÅ¡Ã­m cieÄ¾om je preÅ¾iÅ¥ Äo najdlhÅ¡ie, t. j. udrÅ¾aÅ¥ tyÄ v primerane vertikÃ¡lnej polohe Äo najdlhÅ¡ie.

    âœ… V skutoÄnosti sa simulÃ¡cia CartPole povaÅ¾uje za vyrieÅ¡enÃº, ak sa nÃ¡m podarÃ­ dosiahnuÅ¥ priemernÃº odmenu 195 poÄas 100 po sebe nasledujÃºcich pokusov.

## DiskretizÃ¡cia stavu

Pri Q-Learningu musÃ­me vytvoriÅ¥ Q-TabuÄ¾ku, ktorÃ¡ definuje, Äo robiÅ¥ v kaÅ¾dom stave. Aby sme to mohli urobiÅ¥, potrebujeme, aby bol stav **diskrÃ©tny**, presnejÅ¡ie, aby obsahoval koneÄnÃ½ poÄet diskrÃ©tnych hodnÃ´t. Preto musÃ­me nejako **diskretizovaÅ¥** naÅ¡e pozorovania, mapovaÅ¥ ich na koneÄnÃº mnoÅ¾inu stavov.

Existuje niekoÄ¾ko spÃ´sobov, ako to urobiÅ¥:

- **Rozdelenie na intervaly**. Ak poznÃ¡me interval urÄitej hodnoty, mÃ´Å¾eme tento interval rozdeliÅ¥ na niekoÄ¾ko **intervalov** a potom nahradiÅ¥ hodnotu ÄÃ­slom intervalu, do ktorÃ©ho patrÃ­. To sa dÃ¡ urobiÅ¥ pomocou metÃ³dy numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). V tomto prÃ­pade budeme presne vedieÅ¥ veÄ¾kosÅ¥ stavu, pretoÅ¾e bude zÃ¡visieÅ¥ od poÄtu intervalov, ktorÃ© vyberieme na digitalizÃ¡ciu.

âœ… MÃ´Å¾eme pouÅ¾iÅ¥ lineÃ¡rnu interpolÃ¡ciu na privedenie hodnÃ´t do urÄitÃ©ho koneÄnÃ©ho intervalu (naprÃ­klad od -20 do 20) a potom previesÅ¥ ÄÃ­sla na celÃ© ÄÃ­sla zaokrÃºhlenÃ­m. To nÃ¡m dÃ¡va o nieÄo menÅ¡iu kontrolu nad veÄ¾kosÅ¥ou stavu, najmÃ¤ ak nepoznÃ¡me presnÃ© rozsahy vstupnÃ½ch hodnÃ´t. NaprÃ­klad v naÅ¡om prÃ­pade 2 zo 4 hodnÃ´t nemajÃº hornÃ©/dolnÃ© hranice svojich hodnÃ´t, Äo mÃ´Å¾e viesÅ¥ k nekoneÄnÃ©mu poÄtu stavov.

V naÅ¡om prÃ­klade pouÅ¾ijeme druhÃ½ prÃ­stup. Ako si neskÃ´r vÅ¡imnete, napriek nedefinovanÃ½m hornÃ½m/dolnÃ½m hraniciam tieto hodnoty zriedka nadobÃºdajÃº hodnoty mimo urÄitÃ½ch koneÄnÃ½ch intervalov, takÅ¾e stavy s extrÃ©mnymi hodnotami budÃº veÄ¾mi zriedkavÃ©.

1. Tu je funkcia, ktorÃ¡ vezme pozorovanie z nÃ¡Å¡ho modelu a vytvorÃ­ z neho Å¡tvoricu 4 celÃ½ch ÄÃ­sel: (kÃ³dovÃ½ blok 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. PreskÃºmajme aj ÄalÅ¡iu metÃ³du diskretizÃ¡cie pomocou intervalov: (kÃ³dovÃ½ blok 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Teraz spustime krÃ¡tku simulÃ¡ciu a pozorujme tieto diskrÃ©tne hodnoty prostredia. SkÃºste pouÅ¾iÅ¥ `discretize` aj `discretize_bins` a zistite, Äi je medzi nimi rozdiel.

    âœ… `discretize_bins` vracia ÄÃ­slo intervalu, ktorÃ© je 0-based. TakÅ¾e pre hodnoty vstupnej premennej okolo 0 vracia ÄÃ­slo zo stredu intervalu (10). Pri `discretize` sme sa nestarali o rozsah vÃ½stupnÃ½ch hodnÃ´t, umoÅ¾nili sme im byÅ¥ zÃ¡pornÃ©, takÅ¾e hodnoty stavu nie sÃº posunutÃ© a 0 zodpovedÃ¡ 0. (kÃ³dovÃ½ blok 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Odkomentujte riadok zaÄÃ­najÃºci `env.render`, ak chcete vidieÅ¥, ako sa prostredie vykonÃ¡va. Inak ho mÃ´Å¾ete spustiÅ¥ na pozadÃ­, Äo je rÃ½chlejÅ¡ie. Tento "neviditeÄ¾nÃ½" spÃ´sob vykonÃ¡vania pouÅ¾ijeme poÄas procesu Q-Learningu.

## Å truktÃºra Q-TabuÄ¾ky

V naÅ¡ej predchÃ¡dzajÃºcej lekcii bol stav jednoduchou dvojicou ÄÃ­sel od 0 do 8, a preto bolo pohodlnÃ© reprezentovaÅ¥ Q-TabuÄ¾ku pomocou numpy tenzora s tvarom 8x8x2. Ak pouÅ¾ijeme diskretizÃ¡ciu pomocou intervalov, veÄ¾kosÅ¥ nÃ¡Å¡ho stavovÃ©ho vektora je tieÅ¾ znÃ¡ma, takÅ¾e mÃ´Å¾eme pouÅ¾iÅ¥ rovnakÃ½ prÃ­stup a reprezentovaÅ¥ stav pomocou poÄ¾a s tvarom 20x20x10x10x2 (tu 2 je dimenzia priestoru akciÃ­ a prvÃ© dimenzie zodpovedajÃº poÄtu intervalov, ktorÃ© sme vybrali na pouÅ¾itie pre kaÅ¾dÃº z parametrov v priestore pozorovanÃ­).

AvÅ¡ak niekedy presnÃ© dimenzie priestoru pozorovanÃ­ nie sÃº znÃ¡me. V prÃ­pade funkcie `discretize` si nikdy nemÃ´Å¾eme byÅ¥ istÃ­, Å¾e nÃ¡Å¡ stav zostane v urÄitÃ½ch hraniciach, pretoÅ¾e niektorÃ© z pÃ´vodnÃ½ch hodnÃ´t nie sÃº ohraniÄenÃ©. Preto pouÅ¾ijeme trochu inÃ½ prÃ­stup a reprezentujeme Q-TabuÄ¾ku pomocou slovnÃ­ka.

1. PouÅ¾ite dvojicu *(state,action)* ako kÄ¾ÃºÄ slovnÃ­ka a hodnota by zodpovedala hodnote zÃ¡znamu Q-TabuÄ¾ky. (kÃ³dovÃ½ blok 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Tu tieÅ¾ definujeme funkciu `qvalues()`, ktorÃ¡ vracia zoznam hodnÃ´t Q-TabuÄ¾ky pre danÃ½ stav, ktorÃ½ zodpovedÃ¡ vÅ¡etkÃ½m moÅ¾nÃ½m akciÃ¡m. Ak zÃ¡znam nie je prÃ­tomnÃ½ v Q-TabuÄ¾ke, vrÃ¡time 0 ako predvolenÃº hodnotu.

## ZaÄnime Q-Learning

Teraz sme pripravenÃ­ nauÄiÅ¥ Petra balansovaÅ¥!

1. NajskÃ´r nastavme niektorÃ© hyperparametre: (kÃ³dovÃ½ blok 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Tu `alpha` je **rÃ½chlosÅ¥ uÄenia**, ktorÃ¡ urÄuje, do akej miery by sme mali upraviÅ¥ aktuÃ¡lne hodnoty Q-TabuÄ¾ky pri kaÅ¾dom kroku. V predchÃ¡dzajÃºcej lekcii sme zaÄali s hodnotou 1 a potom sme zniÅ¾ovali `alpha` na niÅ¾Å¡ie hodnoty poÄas trÃ©ningu. V tomto prÃ­klade ju ponechÃ¡me konÅ¡tantnÃº len pre jednoduchosÅ¥ a mÃ´Å¾ete experimentovaÅ¥ s Ãºpravou hodnÃ´t `alpha` neskÃ´r.

    `gamma` je **faktor diskontovania**, ktorÃ½ ukazuje, do akej miery by sme mali uprednostniÅ¥ budÃºcu odmenu pred aktuÃ¡lnou odmenou.

    `epsilon` je **faktor prieskumu/vyuÅ¾itia**, ktorÃ½ urÄuje, Äi by sme mali uprednostniÅ¥ prieskum pred vyuÅ¾itÃ­m alebo naopak. V naÅ¡om algoritme v `epsilon` percentÃ¡ch prÃ­padov vyberieme ÄalÅ¡iu akciu podÄ¾a hodnÃ´t Q-TabuÄ¾ky a v zostÃ¡vajÃºcom poÄte prÃ­padov vykonÃ¡me nÃ¡hodnÃº akciu. To nÃ¡m umoÅ¾nÃ­ preskÃºmaÅ¥ oblasti vyhÄ¾adÃ¡vacieho priestoru, ktorÃ© sme nikdy predtÃ½m nevideli.

    âœ… Z hÄ¾adiska balansovania - vÃ½ber nÃ¡hodnej akcie (prieskum) by pÃ´sobil ako nÃ¡hodnÃ½ Ãºder nesprÃ¡vnym smerom a tyÄ by sa musela nauÄiÅ¥, ako obnoviÅ¥ rovnovÃ¡hu z tÃ½chto "chÃ½b".

### ZlepÅ¡enie algoritmu

MÃ´Å¾eme tieÅ¾ urobiÅ¥ dve vylepÅ¡enia nÃ¡Å¡ho algoritmu z predchÃ¡dzajÃºcej lekcie:

- **VypoÄÃ­taÅ¥ priemernÃº kumulatÃ­vnu odmenu** poÄas niekoÄ¾kÃ½ch simulÃ¡ciÃ­. Pokrok budeme tlaÄiÅ¥ kaÅ¾dÃ½ch 5000 iterÃ¡ciÃ­ a priemernÃº kumulatÃ­vnu odmenu vypoÄÃ­tame za toto obdobie. To znamenÃ¡, Å¾e ak zÃ­skame viac ako 195 bodov, mÃ´Å¾eme problÃ©m povaÅ¾ovaÅ¥ za vyrieÅ¡enÃ½, dokonca s vyÅ¡Å¡ou kvalitou, neÅ¾ je poÅ¾adovanÃ¡.

- **VypoÄÃ­taÅ¥ maximÃ¡lny priemernÃ½ kumulatÃ­vny vÃ½sledok**, `Qmax`, a uloÅ¾Ã­me Q-TabuÄ¾ku zodpovedajÃºcu tomuto vÃ½sledku. KeÄ spustÃ­te trÃ©ning, vÅ¡imnete si, Å¾e niekedy priemernÃ½ kumulatÃ­vny vÃ½sledok zaÄne klesaÅ¥, a chceme si ponechaÅ¥ hodnoty Q-TabuÄ¾ky, ktorÃ© zodpovedajÃº najlepÅ¡iemu modelu pozorovanÃ©mu poÄas trÃ©ningu.

1. Zbierajte vÅ¡etky kumulatÃ­vne odmeny pri kaÅ¾dej simulÃ¡cii do vektora `rewards` na ÄalÅ¡ie vykreslenie. (kÃ³dovÃ½ blok 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

ÄŒo si mÃ´Å¾ete vÅ¡imnÃºÅ¥ z tÃ½chto vÃ½sledkov:

- **BlÃ­zko nÃ¡Å¡ho cieÄ¾a**. Sme veÄ¾mi blÃ­zko k dosiahnutiu cieÄ¾a zÃ­skania 195 kumulatÃ­vnych odmien poÄas 100+ po sebe nasledujÃºcich simulÃ¡ciÃ­, alebo sme ho moÅ¾no uÅ¾ dosiahli! Aj keÄ zÃ­skame menÅ¡ie ÄÃ­sla, stÃ¡le to nevieme, pretoÅ¾e priemerujeme cez 5000 pokusov a formÃ¡lne kritÃ©rium vyÅ¾aduje iba 100 pokusov.

- **Odmena zaÄÃ­na klesaÅ¥**. Niekedy odmena zaÄne klesaÅ¥, Äo znamenÃ¡, Å¾e mÃ´Å¾eme "zniÄiÅ¥" uÅ¾ nauÄenÃ© hodnoty v Q-TabuÄ¾ke tÃ½mi, ktorÃ© situÃ¡ciu zhorÅ¡ujÃº.

Toto pozorovanie je jasnejÅ¡ie viditeÄ¾nÃ©, ak vykreslÃ­me pokrok trÃ©ningu.

## Vykreslenie pokroku trÃ©ningu

PoÄas trÃ©ningu sme zbierali hodnotu kumulatÃ­vnej odmeny pri kaÅ¾dej iterÃ¡cii do vektora `rewards`. Takto to vyzerÃ¡, keÄ to vykreslÃ­me proti ÄÃ­slu iterÃ¡cie:

```python
plt.plot(rewards)
```

![surovÃ½ pokrok](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Z tohto grafu nie je moÅ¾nÃ© niÄ povedaÅ¥, pretoÅ¾e kvÃ´li povahe stochastickÃ©ho trÃ©ningovÃ©ho procesu sa dÄºÅ¾ka trÃ©ningovÃ½ch relÃ¡ciÃ­ veÄ¾mi lÃ­Å¡i. Aby sme tento graf urobili zrozumiteÄ¾nejÅ¡Ã­m, mÃ´Å¾eme vypoÄÃ­taÅ¥ **beÅ¾iaci priemer** cez sÃ©riu experimentov, povedzme 100. To sa dÃ¡ pohodlne urobiÅ¥ pomocou `np.convolve`: (kÃ³dovÃ½ blok 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![pokrok trÃ©ningu](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Zmena hyperparametrov

Aby bolo uÄenie stabilnejÅ¡ie, mÃ¡ zmysel upraviÅ¥ niektorÃ© z naÅ¡ich hyperparametrov poÄas trÃ©ningu. KonkrÃ©tne:

- **Pre rÃ½chlosÅ¥ uÄenia**, `alpha`, mÃ´Å¾eme zaÄaÅ¥ s hodnotami blÃ­zkymi 1 a potom postupne zniÅ¾ovaÅ¥ parameter. S Äasom budeme zÃ­skavaÅ¥ dobrÃ© pravdepodobnostnÃ© hodnoty v Q-TabuÄ¾ke, a preto by sme ich mali upravovaÅ¥ mierne, a nie Ãºplne prepisovaÅ¥ novÃ½mi hodnotami.

- **ZvÃ½Å¡iÅ¥ epsilon**. MÃ´Å¾eme chcieÅ¥ pomaly zvyÅ¡ovaÅ¥ `epsilon`, aby sme menej skÃºmali a viac vyuÅ¾Ã­vali. Pravdepodobne mÃ¡ zmysel zaÄaÅ¥ s niÅ¾Å¡ou hodnotou `epsilon` a postupne ju zvÃ½Å¡iÅ¥ takmer na 1.
> **Ãšloha 1**: SkÃºste experimentovaÅ¥ s hodnotami hyperparametrov a zistite, Äi dokÃ¡Å¾ete dosiahnuÅ¥ vyÅ¡Å¡iu kumulatÃ­vnu odmenu. Dosahujete viac ako 195?
> **Ãšloha 2**: Na formÃ¡lne vyrieÅ¡enie problÃ©mu je potrebnÃ© dosiahnuÅ¥ priemernÃº odmenu 195 poÄas 100 po sebe idÃºcich spustenÃ­. Merajte to poÄas trÃ©ningu a uistite sa, Å¾e ste problÃ©m formÃ¡lne vyrieÅ¡ili!

## VidieÅ¥ vÃ½sledok v akcii

Bolo by zaujÃ­mavÃ© vidieÅ¥, ako sa vyÅ¡kolenÃ½ model sprÃ¡va. Spustime simulÃ¡ciu a pouÅ¾ime rovnakÃº stratÃ©giu vÃ½beru akciÃ­ ako poÄas trÃ©ningu, priÄom vzorkujeme podÄ¾a pravdepodobnostnÃ©ho rozdelenia v Q-TabuÄ¾ke: (blok kÃ³du 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Mali by ste vidieÅ¥ nieÄo takÃ©to:

![balansujÃºci cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€VÃ½zva

> **Ãšloha 3**: Tu sme pouÅ¾Ã­vali finÃ¡lnu verziu Q-TabuÄ¾ky, ktorÃ¡ nemusÃ­ byÅ¥ najlepÅ¡ia. PamÃ¤tajte, Å¾e sme uloÅ¾ili najlepÅ¡ie fungujÃºcu Q-TabuÄ¾ku do premennej `Qbest`! SkÃºste ten istÃ½ prÃ­klad s najlepÅ¡ie fungujÃºcou Q-TabuÄ¾kou tak, Å¾e skopÃ­rujete `Qbest` do `Q` a sledujte, Äi si vÅ¡imnete rozdiel.

> **Ãšloha 4**: Tu sme nevyberali najlepÅ¡iu akciu v kaÅ¾dom kroku, ale namiesto toho vzorkovali podÄ¾a zodpovedajÃºceho pravdepodobnostnÃ©ho rozdelenia. Malo by vÃ¤ÄÅ¡Ã­ zmysel vÅ¾dy vybraÅ¥ najlepÅ¡iu akciu s najvyÅ¡Å¡ou hodnotou v Q-TabuÄ¾ke? To sa dÃ¡ urobiÅ¥ pomocou funkcie `np.argmax`, ktorÃ¡ nÃ¡jde ÄÃ­slo akcie zodpovedajÃºce najvyÅ¡Å¡ej hodnote v Q-TabuÄ¾ke. Implementujte tÃºto stratÃ©giu a sledujte, Äi zlepÅ¡Ã­ balansovanie.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ml/)

## Zadanie
[VytrÃ©nujte Mountain Car](assignment.md)

## ZÃ¡ver

Teraz sme sa nauÄili, ako trÃ©novaÅ¥ agentov na dosiahnutie dobrÃ½ch vÃ½sledkov len tÃ½m, Å¾e im poskytneme funkciu odmeny, ktorÃ¡ definuje poÅ¾adovanÃ½ stav hry, a dÃ¡me im prÃ­leÅ¾itosÅ¥ inteligentne preskÃºmaÅ¥ priestor moÅ¾nostÃ­. ÃšspeÅ¡ne sme aplikovali algoritmus Q-Learning v prÃ­padoch diskrÃ©tnych a spojitÃ½ch prostredÃ­, ale s diskrÃ©tnymi akciami.

Je dÃ´leÅ¾itÃ© Å¡tudovaÅ¥ aj situÃ¡cie, kde je stav akciÃ­ spojitÃ½ a kde je priestor pozorovanÃ­ oveÄ¾a zloÅ¾itejÅ¡Ã­, naprÃ­klad obrazovka z hry Atari. Pri takÃ½chto problÃ©moch Äasto potrebujeme pouÅ¾iÅ¥ vÃ½konnejÅ¡ie techniky strojovÃ©ho uÄenia, ako sÃº neurÃ³novÃ© siete, aby sme dosiahli dobrÃ© vÃ½sledky. Tieto pokroÄilejÅ¡ie tÃ©my sÃº predmetom nÃ¡Å¡ho nadchÃ¡dzajÃºceho pokroÄilÃ©ho kurzu AI.

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho pÃ´vodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.