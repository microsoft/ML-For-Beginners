<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T15:46:29+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "sk"
}
-->
# K-Means zhlukovanie

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ml/)

V tejto lekcii sa nauƒç√≠te, ako vytv√°ra≈• zhluky pomocou Scikit-learn a datasetu nig√©rijskej hudby, ktor√Ω ste importovali sk√¥r. Pokryjeme z√°klady K-Means pre zhlukovanie. Pam√§tajte, ≈æe ako ste sa nauƒçili v predch√°dzaj√∫cej lekcii, existuje mnoho sp√¥sobov, ako pracova≈• so zhlukmi, a met√≥da, ktor√∫ pou≈æijete, z√°vis√≠ od va≈°ich d√°t. Sk√∫sime K-Means, preto≈æe je to najbe≈ænej≈°ia technika zhlukovania. Poƒème na to!

Pojmy, o ktor√Ωch sa dozviete:

- Silhouette sk√≥re
- Met√≥da lak≈•a
- Inercia
- Variancia

## √övod

[K-Means zhlukovanie](https://wikipedia.org/wiki/K-means_clustering) je met√≥da odvoden√° z oblasti spracovania sign√°lov. Pou≈æ√≠va sa na rozdelenie a rozƒçlenenie skup√≠n d√°t do 'k' zhlukov pomocou s√©rie pozorovan√≠. Ka≈æd√© pozorovanie pracuje na zoskupen√≠ dan√©ho d√°tov√©ho bodu najbli≈æ≈°ie k jeho najbli≈æ≈°iemu 'priemeru', alebo stredov√©mu bodu zhluku.

Zhluky je mo≈æn√© vizualizova≈• ako [Voronoi diagramy](https://wikipedia.org/wiki/Voronoi_diagram), ktor√© zah≈ï≈àaj√∫ bod (alebo 'semienko') a jeho zodpovedaj√∫cu oblas≈•.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografika od [Jen Looper](https://twitter.com/jenlooper)

Proces K-Means zhlukovania [prebieha v trojstup≈àovom procese](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritmus vyberie k-poƒçet stredov√Ωch bodov vzorkovan√≠m z datasetu. Potom cykluje:
    1. Prirad√≠ ka≈æd√∫ vzorku k najbli≈æ≈°iemu centroidu.
    2. Vytvor√≠ nov√© centroidy vypoƒç√≠tan√≠m priemernej hodnoty v≈°etk√Ωch vzoriek priraden√Ωch k predch√°dzaj√∫cim centroidom.
    3. Potom vypoƒç√≠ta rozdiel medzi nov√Ωmi a star√Ωmi centroidmi a opakuje, k√Ωm sa centroidy nestabilizuj√∫.

Jednou nev√Ωhodou pou≈æ√≠vania K-Means je fakt, ≈æe budete musie≈• urƒçi≈• 'k', teda poƒçet centroidov. Na≈°≈•astie met√≥da 'lak≈•a' pom√°ha odhadn√∫≈• dobr√∫ poƒçiatoƒçn√∫ hodnotu pre 'k'. Hneƒè si to vysk√∫≈°ate.

## Predpoklad

Budete pracova≈• v s√∫bore [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb), ktor√Ω obsahuje import d√°t a predbe≈æn√© ƒçistenie, ktor√© ste vykonali v poslednej lekcii.

## Cviƒçenie - pr√≠prava

Zaƒçnite t√Ωm, ≈æe sa znova pozriete na d√°ta piesn√≠.

1. Vytvorte boxplot, zavolan√≠m `boxplot()` pre ka≈æd√Ω stƒ∫pec:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Tieto d√°ta s√∫ trochu hluƒçn√©: pozorovan√≠m ka≈æd√©ho stƒ∫pca ako boxplotu m√¥≈æete vidie≈• odƒæahl√© hodnoty.

    ![odƒæahl√© hodnoty](../../../../5-Clustering/2-K-Means/images/boxplots.png)

M√¥≈æete prejs≈• dataset a odstr√°ni≈• tieto odƒæahl√© hodnoty, ale to by spravilo d√°ta dos≈• minim√°lne.

1. Zatiaƒæ si vyberte, ktor√© stƒ∫pce pou≈æijete pre va≈°e cviƒçenie zhlukovania. Vyberte tie s podobn√Ωmi rozsahmi a zak√≥dujte stƒ∫pec `artist_top_genre` ako numerick√© d√°ta:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Teraz mus√≠te vybra≈•, koƒæko zhlukov chcete cieli≈•. Viete, ≈æe existuj√∫ 3 hudobn√© ≈æ√°nre, ktor√© sme vyƒçlenili z datasetu, tak≈æe sk√∫sme 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Vid√≠te vytlaƒçen√© pole s predpovedan√Ωmi zhlukmi (0, 1 alebo 2) pre ka≈æd√Ω riadok dataframe.

1. Pou≈æite toto pole na v√Ωpoƒçet 'silhouette sk√≥re':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhouette sk√≥re

Hƒæadajte silhouette sk√≥re bli≈æ≈°ie k 1. Toto sk√≥re sa pohybuje od -1 do 1, a ak je sk√≥re 1, zhluk je hust√Ω a dobre oddelen√Ω od ostatn√Ωch zhlukov. Hodnota bl√≠zka 0 predstavuje prekr√Ωvaj√∫ce sa zhluky so vzorkami veƒæmi bl√≠zko rozhodovacej hranice susedn√Ωch zhlukov. [(Zdroj)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Na≈°e sk√≥re je **.53**, tak≈æe je presne v strede. To naznaƒçuje, ≈æe na≈°e d√°ta nie s√∫ obzvl√°≈°≈• vhodn√© pre tento typ zhlukovania, ale pokraƒçujme.

### Cviƒçenie - vytvorte model

1. Importujte `KMeans` a zaƒçnite proces zhlukovania.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Tu je niekoƒæko ƒçast√≠, ktor√© si zasl√∫≈æia vysvetlenie.

    > üéì range: Toto s√∫ iter√°cie procesu zhlukovania.

    > üéì random_state: "Urƒçuje generovanie n√°hodn√Ωch ƒç√≠sel pre inicializ√°ciu centroidov." [Zdroj](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "s√∫ƒçet ≈°tvorcov v r√°mci zhluku" meria ≈°tvorcov√∫ priemern√∫ vzdialenos≈• v≈°etk√Ωch bodov v r√°mci zhluku od centroidu zhluku. [Zdroj](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì Inercia: Algoritmy K-Means sa sna≈æia vybra≈• centroidy tak, aby minimalizovali 'inerciu', "mieru toho, ako s√∫ zhluky vn√∫torne koherentn√©." [Zdroj](https://scikit-learn.org/stable/modules/clustering.html). Hodnota sa prid√°va do premennej wcss pri ka≈ædej iter√°cii.

    > üéì k-means++: V [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) m√¥≈æete pou≈æi≈• optimaliz√°ciu 'k-means++', ktor√° "inicializuje centroidy tak, aby boli (zvyƒçajne) vzdialen√© od seba, ƒço vedie k pravdepodobne lep≈°√≠m v√Ωsledkom ako n√°hodn√° inicializ√°cia."

### Met√≥da lak≈•a

Predt√Ωm ste predpokladali, ≈æe keƒè≈æe ste cielili 3 hudobn√© ≈æ√°nre, mali by ste zvoli≈• 3 zhluky. Ale je to tak?

1. Pou≈æite met√≥du 'lak≈•a', aby ste si boli ist√≠.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Pou≈æite premenn√∫ `wcss`, ktor√∫ ste vytvorili v predch√°dzaj√∫com kroku, na vytvorenie grafu, ktor√Ω ukazuje, kde je 'ohyb' v lakti, ƒço naznaƒçuje optim√°lny poƒçet zhlukov. Mo≈æno je to **naozaj** 3!

    ![met√≥da lak≈•a](../../../../5-Clustering/2-K-Means/images/elbow.png)

## Cviƒçenie - zobrazte zhluky

1. Sk√∫ste proces znova, tentoraz nastavte tri zhluky a zobrazte zhluky ako scatterplot:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Skontrolujte presnos≈• modelu:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    Presnos≈• tohto modelu nie je veƒæmi dobr√° a tvar zhlukov v√°m d√°va n√°znak preƒço.

    ![zhluky](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Tieto d√°ta s√∫ pr√≠li≈° nevyv√°≈æen√©, m√°lo korelovan√© a medzi hodnotami stƒ∫pcov je pr√≠li≈° veƒæk√° variancia na to, aby sa dobre zhlukovali. V skutoƒçnosti s√∫ zhluky, ktor√© sa tvoria, pravdepodobne silne ovplyvnen√© alebo skreslen√© tromi kateg√≥riami ≈æ√°nrov, ktor√© sme definovali vy≈°≈°ie. To bol proces uƒçenia!

    V dokument√°cii Scikit-learn m√¥≈æete vidie≈•, ≈æe model ako tento, s nie veƒæmi dobre vyznaƒçen√Ωmi zhlukmi, m√° probl√©m s 'varianciou':

    ![probl√©mov√© modely](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografika zo Scikit-learn

## Variancia

Variancia je definovan√° ako "priemer ≈°tvorcov√Ωch rozdielov od priemeru" [(Zdroj)](https://www.mathsisfun.com/data/standard-deviation.html). V kontexte tohto probl√©mu zhlukovania sa vz≈•ahuje na d√°ta, kde ƒç√≠sla n√°≈°ho datasetu maj√∫ tendenciu odch√Ωli≈• sa trochu pr√≠li≈° od priemeru.

‚úÖ Toto je skvel√Ω moment na zamyslenie sa nad v≈°etk√Ωmi sp√¥sobmi, ako by ste mohli tento probl√©m opravi≈•. Upravi≈• d√°ta trochu viac? Pou≈æi≈• in√© stƒ∫pce? Pou≈æi≈• in√Ω algoritmus? Tip: Sk√∫ste [≈°k√°lova≈• va≈°e d√°ta](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) na ich normaliz√°ciu a otestova≈• in√© stƒ∫pce.

> Sk√∫ste tento '[kalkul√°tor variancie](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)', aby ste lep≈°ie pochopili tento koncept.

---

## üöÄV√Ωzva

Str√°vte nejak√Ω ƒças s t√Ωmto notebookom, upravujte parametre. Dok√°≈æete zlep≈°i≈• presnos≈• modelu ƒçisten√≠m d√°t (napr√≠klad odstr√°nen√≠m odƒæahl√Ωch hodn√¥t)? M√¥≈æete pou≈æi≈• v√°hy na pridanie v√§ƒç≈°ej v√°hy urƒçit√Ωm vzork√°m d√°t. ƒåo e≈°te m√¥≈æete urobi≈• na vytvorenie lep≈°√≠ch zhlukov?

Tip: Sk√∫ste ≈°k√°lova≈• va≈°e d√°ta. V notebooku je komentovan√Ω k√≥d, ktor√Ω prid√°va ≈°tandardn√© ≈°k√°lovanie, aby sa stƒ∫pce d√°t viac podobali z hƒæadiska rozsahu. Zist√≠te, ≈æe zatiaƒæ ƒço silhouette sk√≥re kles√°, 'ohyb' v grafe lak≈•a sa vyhladzuje. Je to preto, ≈æe ponechanie d√°t ne≈°k√°lovan√Ωch umo≈æ≈àuje d√°tam s men≈°ou varianciou nies≈• v√§ƒç≈°iu v√°hu. Preƒç√≠tajte si o tomto probl√©me [tu](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ml/)

## Prehƒæad a samostatn√© ≈°t√∫dium

Pozrite sa na simul√°tor K-Means [ako je tento](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). M√¥≈æete pou≈æi≈• tento n√°stroj na vizualiz√°ciu vzoriek d√°tov√Ωch bodov a urƒçenie ich centroidov. M√¥≈æete upravi≈• n√°hodnos≈• d√°t, poƒçet zhlukov a poƒçet centroidov. Pom√°ha v√°m to z√≠ska≈• predstavu o tom, ako m√¥≈æu by≈• d√°ta zoskupen√©?

Tie≈æ sa pozrite na [tento materi√°l o K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) zo Stanfordu.

## Zadanie

[Vysk√∫≈°ajte r√¥zne met√≥dy zhlukovania](assignment.md)

---

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho p√¥vodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nie sme zodpovedn√≠ za ak√©koƒævek nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.