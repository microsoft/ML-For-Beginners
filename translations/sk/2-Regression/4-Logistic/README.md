<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T15:16:31+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "sk"
}
-->
# LogistickÃ¡ regresia na predpovedanie kategÃ³riÃ­

![Infografika: LogistickÃ¡ vs. lineÃ¡rna regresia](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ml/)

> ### [TÃ¡to lekcia je dostupnÃ¡ aj v R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Ãšvod

V tejto poslednej lekcii o regresii, jednej zo zÃ¡kladnÃ½ch _klasickÃ½ch_ technÃ­k strojovÃ©ho uÄenia, sa pozrieme na logistickÃº regresiu. TÃºto techniku by ste pouÅ¾ili na objavenie vzorcov na predpovedanie binÃ¡rnych kategÃ³riÃ­. Je tÃ¡to cukrovinka ÄokolÃ¡dovÃ¡ alebo nie? Je tÃ¡to choroba nÃ¡kazlivÃ¡ alebo nie? Vyberie si tento zÃ¡kaznÃ­k tento produkt alebo nie?

V tejto lekcii sa nauÄÃ­te:

- NovÃº kniÅ¾nicu na vizualizÃ¡ciu dÃ¡t
- Techniky logistickej regresie

âœ… PrehÄºbte si svoje znalosti o prÃ¡ci s tÃ½mto typom regresie v tomto [uÄebnom module](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Predpoklady

Po prÃ¡ci s dÃ¡tami o tekviciach sme uÅ¾ dostatoÄne oboznÃ¡menÃ­ s tÃ½m, Å¾e existuje jedna binÃ¡rna kategÃ³ria, s ktorou mÃ´Å¾eme pracovaÅ¥: `Farba`.

Postavme model logistickej regresie na predpovedanie toho, na zÃ¡klade niektorÃ½ch premennÃ½ch, _akÃº farbu bude maÅ¥ danÃ¡ tekvica_ (oranÅ¾ovÃ¡ ğŸƒ alebo biela ğŸ‘»).

> PreÄo hovorÃ­me o binÃ¡rnej klasifikÃ¡cii v lekcii o regresii? Len z jazykovÃ©ho pohodlia, pretoÅ¾e logistickÃ¡ regresia je [v skutoÄnosti metÃ³da klasifikÃ¡cie](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), hoci zaloÅ¾enÃ¡ na lineÃ¡rnom prÃ­stupe. O ÄalÅ¡Ã­ch spÃ´soboch klasifikÃ¡cie dÃ¡t sa dozviete v nasledujÃºcej skupine lekciÃ­.

## Definovanie otÃ¡zky

Pre naÅ¡e ÃºÄely to vyjadrÃ­me ako binÃ¡rnu otÃ¡zku: 'Biela' alebo 'Nie biela'. V naÅ¡ej dÃ¡tovej sade je tieÅ¾ kategÃ³ria 'pruhovanÃ¡', ale mÃ¡ mÃ¡lo zÃ¡znamov, takÅ¾e ju nebudeme pouÅ¾Ã­vaÅ¥. Aj tak zmizne, keÄ odstrÃ¡nime nulovÃ© hodnoty z dÃ¡tovej sady.

> ğŸƒ ZaujÃ­mavÃ½ fakt: biele tekvice niekedy nazÃ½vame 'duchovÃ©' tekvice. Nie sÃº veÄ¾mi Ä¾ahkÃ© na vyrezÃ¡vanie, takÅ¾e nie sÃº tak populÃ¡rne ako oranÅ¾ovÃ©, ale vyzerajÃº zaujÃ­mavo! TakÅ¾e by sme mohli naÅ¡u otÃ¡zku preformulovaÅ¥ ako: 'Duch' alebo 'Nie duch'. ğŸ‘»

## O logistickej regresii

LogistickÃ¡ regresia sa lÃ­Å¡i od lineÃ¡rnej regresie, ktorÃº ste sa nauÄili predtÃ½m, v niekoÄ¾kÃ½ch dÃ´leÅ¾itÃ½ch ohÄ¾adoch.

[![StrojovÃ© uÄenie pre zaÄiatoÄnÃ­kov - Pochopenie logistickej regresie pre klasifikÃ¡ciu](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "StrojovÃ© uÄenie pre zaÄiatoÄnÃ­kov - Pochopenie logistickej regresie pre klasifikÃ¡ciu")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o logistickej regresii.

### BinÃ¡rna klasifikÃ¡cia

LogistickÃ¡ regresia neponÃºka rovnakÃ© funkcie ako lineÃ¡rna regresia. PrvÃ¡ ponÃºka predpoveÄ o binÃ¡rnej kategÃ³rii ("biela alebo nie biela"), zatiaÄ¾ Äo druhÃ¡ je schopnÃ¡ predpovedaÅ¥ kontinuÃ¡lne hodnoty, naprÃ­klad na zÃ¡klade pÃ´vodu tekvice a Äasu zberu, _ako veÄ¾mi sa zvÃ½Å¡i jej cena_.

![Model klasifikÃ¡cie tekvÃ­c](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)

### InÃ© typy klasifikÃ¡ciÃ­

ExistujÃº aj inÃ© typy logistickej regresie, vrÃ¡tane multinomiÃ¡lnej a ordinÃ¡lnej:

- **MultinomiÃ¡lna**, ktorÃ¡ zahÅ•Åˆa viac ako jednu kategÃ³riu - "OranÅ¾ovÃ¡, Biela a PruhovanÃ¡".
- **OrdinÃ¡lna**, ktorÃ¡ zahÅ•Åˆa usporiadanÃ© kategÃ³rie, uÅ¾itoÄnÃ©, ak by sme chceli usporiadaÅ¥ naÅ¡e vÃ½sledky logicky, ako naÅ¡e tekvice, ktorÃ© sÃº usporiadanÃ© podÄ¾a koneÄnÃ©ho poÄtu veÄ¾kostÃ­ (mini, sm, med, lg, xl, xxl).

![MultinomiÃ¡lna vs ordinÃ¡lna regresia](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### PremennÃ© NEMUSIA korelovaÅ¥

PamÃ¤tÃ¡te si, ako lineÃ¡rna regresia fungovala lepÅ¡ie s viac korelovanÃ½mi premennÃ½mi? LogistickÃ¡ regresia je opakom - premennÃ© nemusia byÅ¥ v sÃºlade. To funguje pre tieto dÃ¡ta, ktorÃ© majÃº pomerne slabÃ© korelÃ¡cie.

### Potrebujete veÄ¾a ÄistÃ½ch dÃ¡t

LogistickÃ¡ regresia poskytne presnejÅ¡ie vÃ½sledky, ak pouÅ¾ijete viac dÃ¡t; naÅ¡a malÃ¡ dÃ¡tovÃ¡ sada nie je optimÃ¡lna pre tÃºto Ãºlohu, takÅ¾e to majte na pamÃ¤ti.

[![StrojovÃ© uÄenie pre zaÄiatoÄnÃ­kov - AnalÃ½za a prÃ­prava dÃ¡t pre logistickÃº regresiu](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "StrojovÃ© uÄenie pre zaÄiatoÄnÃ­kov - AnalÃ½za a prÃ­prava dÃ¡t pre logistickÃº regresiu")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o prÃ­prave dÃ¡t pre lineÃ¡rnu regresiu.

âœ… PremÃ½Å¡Ä¾ajte o typoch dÃ¡t, ktorÃ© by sa hodili pre logistickÃº regresiu.

## CviÄenie - upravte dÃ¡ta

Najprv trochu upravte dÃ¡ta, odstrÃ¡Åˆte nulovÃ© hodnoty a vyberte len niektorÃ© stÄºpce:

1. Pridajte nasledujÃºci kÃ³d:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    VÅ¾dy sa mÃ´Å¾ete pozrieÅ¥ na svoj novÃ½ dataframe:

    ```python
    pumpkins.info
    ```

### VizualizÃ¡cia - kategÃ³riÃ¡lny graf

Teraz ste naÄÃ­tali [Å¡tartovacÃ­ notebook](../../../../2-Regression/4-Logistic/notebook.ipynb) s dÃ¡tami o tekviciach a upravili ho tak, aby obsahoval dÃ¡tovÃº sadu s niekoÄ¾kÃ½mi premennÃ½mi vrÃ¡tane `Farba`. Vizualizujme dataframe v notebooku pomocou inej kniÅ¾nice: [Seaborn](https://seaborn.pydata.org/index.html), ktorÃ¡ je postavenÃ¡ na Matplotlib, ktorÃ½ sme pouÅ¾ili skÃ´r.

Seaborn ponÃºka niekoÄ¾ko zaujÃ­mavÃ½ch spÃ´sobov vizualizÃ¡cie vaÅ¡ich dÃ¡t. NaprÃ­klad mÃ´Å¾ete porovnaÅ¥ distribÃºcie dÃ¡t pre kaÅ¾dÃº `Variety` a `Farba` v kategÃ³riÃ¡lnom grafe.

1. Vytvorte takÃ½to graf pomocou funkcie `catplot`, pouÅ¾ite naÅ¡e dÃ¡ta o tekviciach `pumpkins` a Å¡pecifikujte farebnÃ© mapovanie pre kaÅ¾dÃº kategÃ³riu tekvÃ­c (oranÅ¾ovÃ¡ alebo biela):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![MrieÅ¾ka vizualizovanÃ½ch dÃ¡t](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    PozorovanÃ­m dÃ¡t mÃ´Å¾ete vidieÅ¥, ako sa dÃ¡ta o farbe vzÅ¥ahujÃº na odrodu.

    âœ… Na zÃ¡klade tohto kategÃ³riÃ¡lneho grafu, akÃ© zaujÃ­mavÃ© skÃºmania si dokÃ¡Å¾ete predstaviÅ¥?

### Predspracovanie dÃ¡t: kÃ³dovanie vlastnostÃ­ a Å¡tÃ­tkov
NaÅ¡a dÃ¡tovÃ¡ sada o tekviciach obsahuje textovÃ© hodnoty vo vÅ¡etkÃ½ch svojich stÄºpcoch. PrÃ¡ca s kategÃ³riÃ¡lnymi dÃ¡tami je intuitÃ­vna pre Ä¾udÃ­, ale nie pre stroje. Algoritmy strojovÃ©ho uÄenia fungujÃº dobre s ÄÃ­slami. Preto je kÃ³dovanie veÄ¾mi dÃ´leÅ¾itÃ½m krokom vo fÃ¡ze predspracovania dÃ¡t, pretoÅ¾e nÃ¡m umoÅ¾Åˆuje premeniÅ¥ kategÃ³riÃ¡lne dÃ¡ta na ÄÃ­selnÃ© dÃ¡ta bez straty informÃ¡ciÃ­. DobrÃ© kÃ³dovanie vedie k vytvoreniu dobrÃ©ho modelu.

Pre kÃ³dovanie vlastnostÃ­ existujÃº dva hlavnÃ© typy kÃ³derov:

1. OrdinÃ¡lny kÃ³der: hodÃ­ sa pre ordinÃ¡lne premennÃ©, ktorÃ© sÃº kategÃ³riÃ¡lne premennÃ©, kde ich dÃ¡ta nasledujÃº logickÃ© usporiadanie, ako stÄºpec `Item Size` v naÅ¡ej dÃ¡tovej sade. VytvÃ¡ra mapovanie, kde kaÅ¾dÃ¡ kategÃ³ria je reprezentovanÃ¡ ÄÃ­slom, ktorÃ© je poradie kategÃ³rie v stÄºpci.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. KategÃ³riÃ¡lny kÃ³der: hodÃ­ sa pre nominÃ¡lne premennÃ©, ktorÃ© sÃº kategÃ³riÃ¡lne premennÃ©, kde ich dÃ¡ta nenasledujÃº logickÃ© usporiadanie, ako vÅ¡etky vlastnosti odliÅ¡nÃ© od `Item Size` v naÅ¡ej dÃ¡tovej sade. Ide o kÃ³dovanie typu one-hot, Äo znamenÃ¡, Å¾e kaÅ¾dÃ¡ kategÃ³ria je reprezentovanÃ¡ binÃ¡rnym stÄºpcom: kÃ³dovanÃ¡ premennÃ¡ je rovnÃ¡ 1, ak tekvica patrÃ­ do tejto odrody, a 0 inak.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Potom sa na kombinÃ¡ciu viacerÃ½ch kÃ³derov do jednÃ©ho kroku a ich aplikÃ¡ciu na prÃ­sluÅ¡nÃ© stÄºpce pouÅ¾Ã­va `ColumnTransformer`.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
Na druhej strane, na kÃ³dovanie Å¡tÃ­tku pouÅ¾Ã­vame triedu `LabelEncoder` zo scikit-learn, ktorÃ¡ je pomocnou triedou na normalizÃ¡ciu Å¡tÃ­tkov tak, aby obsahovali iba hodnoty medzi 0 a n_classes-1 (tu 0 a 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
KeÄ sme zakÃ³dovali vlastnosti a Å¡tÃ­tok, mÃ´Å¾eme ich zlÃºÄiÅ¥ do novÃ©ho dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
âœ… AkÃ© sÃº vÃ½hody pouÅ¾itia ordinÃ¡lneho kÃ³dera pre stÄºpec `Item Size`?

### AnalÃ½za vzÅ¥ahov medzi premennÃ½mi

Teraz, keÄ sme predspracovali naÅ¡e dÃ¡ta, mÃ´Å¾eme analyzovaÅ¥ vzÅ¥ahy medzi vlastnosÅ¥ami a Å¡tÃ­tkom, aby sme zÃ­skali predstavu o tom, ako dobre bude model schopnÃ½ predpovedaÅ¥ Å¡tÃ­tok na zÃ¡klade vlastnostÃ­.
NajlepÅ¡Ã­ spÃ´sob, ako vykonaÅ¥ tento druh analÃ½zy, je vizualizÃ¡cia dÃ¡t. OpÃ¤Å¥ pouÅ¾ijeme funkciu `catplot` zo Seaborn na vizualizÃ¡ciu vzÅ¥ahov medzi `Item Size`, `Variety` a `Farba` v kategÃ³riÃ¡lnom grafe. Na lepÅ¡iu vizualizÃ¡ciu dÃ¡t pouÅ¾ijeme zakÃ³dovanÃ½ stÄºpec `Item Size` a nezakÃ³dovanÃ½ stÄºpec `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![KategÃ³riÃ¡lny graf vizualizovanÃ½ch dÃ¡t](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### PouÅ¾itie swarm grafu

KeÄÅ¾e Farba je binÃ¡rna kategÃ³ria (Biela alebo Nie), potrebuje 'Å¡pecializovanÃ½ prÃ­stup [k vizualizÃ¡cii](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar)'. ExistujÃº aj inÃ© spÃ´soby vizualizÃ¡cie vzÅ¥ahu tejto kategÃ³rie s inÃ½mi premennÃ½mi.

PremennÃ© mÃ´Å¾ete vizualizovaÅ¥ vedÄ¾a seba pomocou grafov Seaborn.

1. SkÃºste 'swarm' graf na zobrazenie distribÃºcie hodnÃ´t:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Swarm graf vizualizovanÃ½ch dÃ¡t](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Pozor**: vyÅ¡Å¡ie uvedenÃ½ kÃ³d mÃ´Å¾e generovaÅ¥ varovanie, pretoÅ¾e Seaborn nedokÃ¡Å¾e reprezentovaÅ¥ takÃ© mnoÅ¾stvo dÃ¡tovÃ½ch bodov v swarm grafe. MoÅ¾nÃ½m rieÅ¡enÃ­m je zmenÅ¡enie veÄ¾kosti znaÄky pomocou parametra 'size'. BuÄte vÅ¡ak opatrnÃ­, pretoÅ¾e to ovplyvÅˆuje ÄitateÄ¾nosÅ¥ grafu.

> **ğŸ§® UkÃ¡Å¾te mi matematiku**
>
> LogistickÃ¡ regresia sa opiera o koncept 'maximÃ¡lnej pravdepodobnosti' pomocou [sigmoidnÃ½ch funkciÃ­](https://wikipedia.org/wiki/Sigmoid_function). 'SigmoidnÃ¡ funkcia' na grafe vyzerÃ¡ ako tvar 'S'. Berie hodnotu a mapuje ju na nieÄo medzi 0 a 1. Jej krivka sa tieÅ¾ nazÃ½va 'logistickÃ¡ krivka'. Jej vzorec vyzerÃ¡ takto:
>
> ![logistickÃ¡ funkcia](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> kde stred sigmoidnej funkcie sa nachÃ¡dza na 0 bode x, L je maximÃ¡lna hodnota krivky a k je strmosÅ¥ krivky. Ak je vÃ½sledok funkcie viac ako 0.5, danÃ½ Å¡tÃ­tok bude priradenÃ½ do triedy '1' binÃ¡rnej voÄ¾by. Ak nie, bude klasifikovanÃ½ ako '0'.

## Vytvorte svoj model

Vytvorenie modelu na nÃ¡jdenie tÃ½chto binÃ¡rnych klasifikÃ¡ciÃ­ je prekvapivo jednoduchÃ© v Scikit-learn.

[![StrojovÃ© uÄenie pre zaÄiatoÄnÃ­kov - LogistickÃ¡ regresia pre klasifikÃ¡ciu dÃ¡t](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "StrojovÃ© uÄenie pre zaÄiatoÄnÃ­kov - LogistickÃ¡ regresia pre klasifikÃ¡ciu dÃ¡t")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o vytvÃ¡ranÃ­ modelu lineÃ¡rnej regresie.

1. Vyberte premennÃ©, ktorÃ© chcete pouÅ¾iÅ¥ vo svojom klasifikaÄnom modeli, a rozdeÄ¾te trÃ©ningovÃ© a testovacie sady pomocou `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Teraz mÃ´Å¾ete trÃ©novaÅ¥ svoj model, zavolanÃ­m `fit()` s vaÅ¡imi trÃ©ningovÃ½mi dÃ¡tami, a vytlaÄiÅ¥ jeho vÃ½sledok:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Pozrite sa na skÃ³re vÃ¡Å¡ho modelu. Nie je to zlÃ©, vzhÄ¾adom na to, Å¾e mÃ¡te len asi 1000 riadkov dÃ¡t:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## LepÅ¡ie pochopenie pomocou matice zmÃ¤tku

ZatiaÄ¾ Äo mÃ´Å¾ete zÃ­skaÅ¥ sprÃ¡vu o skÃ³re [termÃ­ny](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) vytlaÄenÃ­m vyÅ¡Å¡ie uvedenÃ½ch poloÅ¾iek, mÃ´Å¾ete svoj model lepÅ¡ie pochopiÅ¥ pomocou [matice zmÃ¤tku](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), ktorÃ¡ nÃ¡m pomÃ¡ha pochopiÅ¥, ako model funguje.

> ğŸ“ '[Matica zmÃ¤tku](https://wikipedia.org/wiki/Confusion_matrix)' (alebo 'matica chÃ½b') je tabuÄ¾ka, ktorÃ¡ vyjadruje skutoÄnÃ© vs. faloÅ¡nÃ© pozitÃ­va a negatÃ­va vÃ¡Å¡ho modelu, ÄÃ­m hodnotÃ­ presnosÅ¥ predpovedÃ­.

1. Na pouÅ¾itie matice zmÃ¤tku zavolajte `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Pozrite sa na maticu zmÃ¤tku vÃ¡Å¡ho modelu:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

V Scikit-learn, riadky (os 0) sÃº skutoÄnÃ© Å¡tÃ­tky a stÄºpce (os 1) sÃº predpovedanÃ© Å¡tÃ­tky.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

ÄŒo sa tu deje? Povedzme, Å¾e nÃ¡Å¡ model je poÅ¾iadanÃ½ klasifikovaÅ¥ tekvice medzi dvoma binÃ¡rnymi kategÃ³riami, kategÃ³riou 'biela' a kategÃ³riou 'nie biela'.

- Ak vÃ¡Å¡ model predpovedÃ¡ tekvicu ako nie bielu a v skutoÄnosti patrÃ­ do kategÃ³rie 'nie biela', nazÃ½vame to pravÃ½ negatÃ­vny, zobrazenÃ½ hornÃ½m Ä¾avÃ½m ÄÃ­slom.
- Ak vÃ¡Å¡ model predpovedÃ¡ tekvicu ako bielu a v skutoÄnosti patrÃ­ do kategÃ³rie 'nie biela', nazÃ½vame to faloÅ¡nÃ½ negatÃ­vny, zobrazenÃ½ dolnÃ½m Ä¾avÃ½m ÄÃ­slom.
- Ak vÃ¡Å¡ model predpovedÃ¡ tekvicu ako nie bielu a v skutoÄnosti patrÃ­ do kategÃ³rie 'biela', nazÃ½vame to faloÅ¡nÃ½ pozitÃ­vny, zobrazenÃ½ hornÃ½m pravÃ½m ÄÃ­slom.
- Ak vÃ¡Å¡ model predpovedÃ¡ tekvicu ako bielu a v skutoÄnosti patrÃ­ do kategÃ³rie 'biela', nazÃ½vame to pravÃ½ pozitÃ­vny, zobrazenÃ½ dolnÃ½m pravÃ½m ÄÃ­slom.

Ako ste mohli uhÃ¡dnuÅ¥, je preferovanÃ© maÅ¥ vÃ¤ÄÅ¡Ã­ poÄet pravÃ½ch pozitÃ­vnych a pravÃ½ch negatÃ­vnych a niÅ¾Å¡Ã­ poÄet faloÅ¡nÃ½ch pozitÃ­vnych a faloÅ¡nÃ½ch negatÃ­vnych, Äo naznaÄuje, Å¾e model funguje lepÅ¡ie.
Ako sÃºvisÃ­ matica zÃ¡mien s presnosÅ¥ou a odvolanÃ­m? PamÃ¤tajte, Å¾e klasifikaÄnÃ¡ sprÃ¡va uvedenÃ¡ vyÅ¡Å¡ie ukÃ¡zala presnosÅ¥ (0,85) a odvolanie (0,67).

PresnosÅ¥ = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

Odvolanie = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

âœ… OtÃ¡zka: PodÄ¾a matice zÃ¡mien, ako si model viedol? OdpoveÄ: Nie zle; je tu dobrÃ½ poÄet sprÃ¡vne negatÃ­vnych, ale aj niekoÄ¾ko nesprÃ¡vne negatÃ­vnych.

PoÄme si znova prejsÅ¥ pojmy, ktorÃ© sme videli skÃ´r, s pomocou mapovania TP/TN a FP/FN v matici zÃ¡mien:

ğŸ“ PresnosÅ¥: TP/(TP + FP) Podiel relevantnÃ½ch prÃ­padov medzi zÃ­skanÃ½mi prÃ­padmi (napr. ktorÃ© Å¡tÃ­tky boli sprÃ¡vne oznaÄenÃ©)

ğŸ“ Odvolanie: TP/(TP + FN) Podiel relevantnÃ½ch prÃ­padov, ktorÃ© boli zÃ­skanÃ©, Äi uÅ¾ sprÃ¡vne oznaÄenÃ© alebo nie

ğŸ“ f1-skÃ³re: (2 * presnosÅ¥ * odvolanie)/(presnosÅ¥ + odvolanie) VÃ¡Å¾enÃ½ priemer presnosti a odvolania, priÄom najlepÅ¡ie je 1 a najhorÅ¡ie 0

ğŸ“ Podpora: PoÄet vÃ½skytov kaÅ¾dÃ©ho zÃ­skanÃ©ho Å¡tÃ­tku

ğŸ“ PresnosÅ¥: (TP + TN)/(TP + TN + FP + FN) Percento Å¡tÃ­tkov predpovedanÃ½ch sprÃ¡vne pre vzorku.

ğŸ“ Makro priemer: VÃ½poÄet nevyvÃ¡Å¾enÃ©ho priemeru metrÃ­k pre kaÅ¾dÃ½ Å¡tÃ­tok, bez ohÄ¾adu na nerovnovÃ¡hu Å¡tÃ­tkov.

ğŸ“ VÃ¡Å¾enÃ½ priemer: VÃ½poÄet priemeru metrÃ­k pre kaÅ¾dÃ½ Å¡tÃ­tok, priÄom sa berie do Ãºvahy nerovnovÃ¡ha Å¡tÃ­tkov vÃ¡Å¾enÃ­m podÄ¾a ich podpory (poÄet skutoÄnÃ½ch prÃ­padov pre kaÅ¾dÃ½ Å¡tÃ­tok).

âœ… DokÃ¡Å¾ete si predstaviÅ¥, ktorÃº metriku by ste mali sledovaÅ¥, ak chcete, aby vÃ¡Å¡ model znÃ­Å¾il poÄet nesprÃ¡vne negatÃ­vnych?

## VizualizÃ¡cia ROC krivky tohto modelu

[![ML pre zaÄiatoÄnÃ­kov - AnalÃ½za vÃ½konu logistickej regresie pomocou ROC kriviek](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML pre zaÄiatoÄnÃ­kov - AnalÃ½za vÃ½konu logistickej regresie pomocou ROC kriviek")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad ROC kriviek

PoÄme urobiÅ¥ eÅ¡te jednu vizualizÃ¡ciu, aby sme videli tzv. 'ROC' krivku:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Pomocou Matplotlibu vykreslite [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) alebo ROC modelu. ROC krivky sa Äasto pouÅ¾Ã­vajÃº na zobrazenie vÃ½stupu klasifikÃ¡tora z hÄ¾adiska jeho sprÃ¡vne vs. nesprÃ¡vne pozitÃ­vnych. "ROC krivky zvyÄajne zobrazujÃº mieru sprÃ¡vne pozitÃ­vnych na osi Y a mieru nesprÃ¡vne pozitÃ­vnych na osi X." Preto zÃ¡leÅ¾Ã­ na strmosti krivky a priestore medzi stredovou Äiarou a krivkou: chcete krivku, ktorÃ¡ rÃ½chlo stÃºpa a prechÃ¡dza nad Äiaru. V naÅ¡om prÃ­pade sÃº na zaÄiatku nesprÃ¡vne pozitÃ­vne, a potom krivka sprÃ¡vne stÃºpa a prechÃ¡dza nad Äiaru:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Nakoniec pouÅ¾ite Scikit-learn [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) na vÃ½poÄet skutoÄnej 'Plochy pod krivkou' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
VÃ½sledok je `0.9749908725812341`. KeÄÅ¾e AUC sa pohybuje od 0 do 1, chcete vysokÃ© skÃ³re, pretoÅ¾e model, ktorÃ½ je 100% sprÃ¡vny vo svojich predpovediach, bude maÅ¥ AUC 1; v tomto prÃ­pade je model _celkom dobrÃ½_. 

V budÃºcich lekciÃ¡ch o klasifikÃ¡ciÃ¡ch sa nauÄÃ­te, ako iterovaÅ¥ na zlepÅ¡enie skÃ³re vÃ¡Å¡ho modelu. Ale zatiaÄ¾ gratulujeme! DokonÄili ste tieto lekcie o regresii!

---
## ğŸš€VÃ½zva

LogistickÃ¡ regresia mÃ¡ oveÄ¾a viac, Äo sa dÃ¡ preskÃºmaÅ¥! Ale najlepÅ¡Ã­ spÃ´sob, ako sa uÄiÅ¥, je experimentovaÅ¥. NÃ¡jdite dataset, ktorÃ½ sa hodÃ­ na tento typ analÃ½zy, a vytvorte s nÃ­m model. ÄŒo ste sa nauÄili? tip: skÃºste [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) pre zaujÃ­mavÃ© datasety.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ml/)

## PrehÄ¾ad a samostatnÃ© Å¡tÃºdium

PreÄÃ­tajte si prvÃ© strÃ¡nky [tohto dokumentu zo Stanfordu](https://web.stanford.edu/~jurafsky/slp3/5.pdf) o niektorÃ½ch praktickÃ½ch vyuÅ¾itiach logistickej regresie. PremÃ½Å¡Ä¾ajte o ÃºlohÃ¡ch, ktorÃ© sÃº lepÅ¡ie vhodnÃ© pre jeden alebo druhÃ½ typ regresnÃ½ch Ãºloh, ktorÃ© sme doteraz Å¡tudovali. ÄŒo by fungovalo najlepÅ¡ie?

## Zadanie 

[Opakovanie tejto regresie](assignment.md)

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za akÃ©koÄ¾vek nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.