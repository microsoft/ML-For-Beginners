<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T15:16:31+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "sk"
}
-->
# Logistick√° regresia na predpovedanie kateg√≥ri√≠

![Infografika: Logistick√° vs. line√°rna regresia](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ml/)

> ### [T√°to lekcia je dostupn√° aj v R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## √övod

V tejto poslednej lekcii o regresii, jednej zo z√°kladn√Ωch _klasick√Ωch_ techn√≠k strojov√©ho uƒçenia, sa pozrieme na logistick√∫ regresiu. T√∫to techniku by ste pou≈æili na objavenie vzorcov na predpovedanie bin√°rnych kateg√≥ri√≠. Je t√°to cukrovinka ƒçokol√°dov√° alebo nie? Je t√°to choroba n√°kazliv√° alebo nie? Vyberie si tento z√°kazn√≠k tento produkt alebo nie?

V tejto lekcii sa nauƒç√≠te:

- Nov√∫ kni≈ænicu na vizualiz√°ciu d√°t
- Techniky logistickej regresie

‚úÖ Prehƒ∫bte si svoje znalosti o pr√°ci s t√Ωmto typom regresie v tomto [uƒçebnom module](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Predpoklady

Po pr√°ci s d√°tami o tekviciach sme u≈æ dostatoƒçne obozn√°men√≠ s t√Ωm, ≈æe existuje jedna bin√°rna kateg√≥ria, s ktorou m√¥≈æeme pracova≈•: `Farba`.

Postavme model logistickej regresie na predpovedanie toho, na z√°klade niektor√Ωch premenn√Ωch, _ak√∫ farbu bude ma≈• dan√° tekvica_ (oran≈æov√° üéÉ alebo biela üëª).

> Preƒço hovor√≠me o bin√°rnej klasifik√°cii v lekcii o regresii? Len z jazykov√©ho pohodlia, preto≈æe logistick√° regresia je [v skutoƒçnosti met√≥da klasifik√°cie](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), hoci zalo≈æen√° na line√°rnom pr√≠stupe. O ƒèal≈°√≠ch sp√¥soboch klasifik√°cie d√°t sa dozviete v nasleduj√∫cej skupine lekci√≠.

## Definovanie ot√°zky

Pre na≈°e √∫ƒçely to vyjadr√≠me ako bin√°rnu ot√°zku: 'Biela' alebo 'Nie biela'. V na≈°ej d√°tovej sade je tie≈æ kateg√≥ria 'pruhovan√°', ale m√° m√°lo z√°znamov, tak≈æe ju nebudeme pou≈æ√≠va≈•. Aj tak zmizne, keƒè odstr√°nime nulov√© hodnoty z d√°tovej sady.

> üéÉ Zauj√≠mav√Ω fakt: biele tekvice niekedy naz√Ωvame 'duchov√©' tekvice. Nie s√∫ veƒæmi ƒæahk√© na vyrez√°vanie, tak≈æe nie s√∫ tak popul√°rne ako oran≈æov√©, ale vyzeraj√∫ zauj√≠mavo! Tak≈æe by sme mohli na≈°u ot√°zku preformulova≈• ako: 'Duch' alebo 'Nie duch'. üëª

## O logistickej regresii

Logistick√° regresia sa l√≠≈°i od line√°rnej regresie, ktor√∫ ste sa nauƒçili predt√Ωm, v niekoƒæk√Ωch d√¥le≈æit√Ωch ohƒæadoch.

[![Strojov√© uƒçenie pre zaƒçiatoƒçn√≠kov - Pochopenie logistickej regresie pre klasifik√°ciu](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "Strojov√© uƒçenie pre zaƒçiatoƒçn√≠kov - Pochopenie logistickej regresie pre klasifik√°ciu")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o logistickej regresii.

### Bin√°rna klasifik√°cia

Logistick√° regresia nepon√∫ka rovnak√© funkcie ako line√°rna regresia. Prv√° pon√∫ka predpoveƒè o bin√°rnej kateg√≥rii ("biela alebo nie biela"), zatiaƒæ ƒço druh√° je schopn√° predpoveda≈• kontinu√°lne hodnoty, napr√≠klad na z√°klade p√¥vodu tekvice a ƒçasu zberu, _ako veƒæmi sa zv√Ω≈°i jej cena_.

![Model klasifik√°cie tekv√≠c](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)

### In√© typy klasifik√°ci√≠

Existuj√∫ aj in√© typy logistickej regresie, vr√°tane multinomi√°lnej a ordin√°lnej:

- **Multinomi√°lna**, ktor√° zah≈ï≈àa viac ako jednu kateg√≥riu - "Oran≈æov√°, Biela a Pruhovan√°".
- **Ordin√°lna**, ktor√° zah≈ï≈àa usporiadan√© kateg√≥rie, u≈æitoƒçn√©, ak by sme chceli usporiada≈• na≈°e v√Ωsledky logicky, ako na≈°e tekvice, ktor√© s√∫ usporiadan√© podƒæa koneƒçn√©ho poƒçtu veƒækost√≠ (mini, sm, med, lg, xl, xxl).

![Multinomi√°lna vs ordin√°lna regresia](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Premenn√© NEMUSIA korelova≈•

Pam√§t√°te si, ako line√°rna regresia fungovala lep≈°ie s viac korelovan√Ωmi premenn√Ωmi? Logistick√° regresia je opakom - premenn√© nemusia by≈• v s√∫lade. To funguje pre tieto d√°ta, ktor√© maj√∫ pomerne slab√© korel√°cie.

### Potrebujete veƒæa ƒçist√Ωch d√°t

Logistick√° regresia poskytne presnej≈°ie v√Ωsledky, ak pou≈æijete viac d√°t; na≈°a mal√° d√°tov√° sada nie je optim√°lna pre t√∫to √∫lohu, tak≈æe to majte na pam√§ti.

[![Strojov√© uƒçenie pre zaƒçiatoƒçn√≠kov - Anal√Ωza a pr√≠prava d√°t pre logistick√∫ regresiu](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "Strojov√© uƒçenie pre zaƒçiatoƒçn√≠kov - Anal√Ωza a pr√≠prava d√°t pre logistick√∫ regresiu")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o pr√≠prave d√°t pre line√°rnu regresiu.

‚úÖ Prem√Ω≈°ƒæajte o typoch d√°t, ktor√© by sa hodili pre logistick√∫ regresiu.

## Cviƒçenie - upravte d√°ta

Najprv trochu upravte d√°ta, odstr√°≈àte nulov√© hodnoty a vyberte len niektor√© stƒ∫pce:

1. Pridajte nasleduj√∫ci k√≥d:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    V≈ædy sa m√¥≈æete pozrie≈• na svoj nov√Ω dataframe:

    ```python
    pumpkins.info
    ```

### Vizualiz√°cia - kateg√≥ri√°lny graf

Teraz ste naƒç√≠tali [≈°tartovac√≠ notebook](../../../../2-Regression/4-Logistic/notebook.ipynb) s d√°tami o tekviciach a upravili ho tak, aby obsahoval d√°tov√∫ sadu s niekoƒæk√Ωmi premenn√Ωmi vr√°tane `Farba`. Vizualizujme dataframe v notebooku pomocou inej kni≈ænice: [Seaborn](https://seaborn.pydata.org/index.html), ktor√° je postaven√° na Matplotlib, ktor√Ω sme pou≈æili sk√¥r.

Seaborn pon√∫ka niekoƒæko zauj√≠mav√Ωch sp√¥sobov vizualiz√°cie va≈°ich d√°t. Napr√≠klad m√¥≈æete porovna≈• distrib√∫cie d√°t pre ka≈æd√∫ `Variety` a `Farba` v kateg√≥ri√°lnom grafe.

1. Vytvorte tak√Ωto graf pomocou funkcie `catplot`, pou≈æite na≈°e d√°ta o tekviciach `pumpkins` a ≈°pecifikujte farebn√© mapovanie pre ka≈æd√∫ kateg√≥riu tekv√≠c (oran≈æov√° alebo biela):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Mrie≈æka vizualizovan√Ωch d√°t](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Pozorovan√≠m d√°t m√¥≈æete vidie≈•, ako sa d√°ta o farbe vz≈•ahuj√∫ na odrodu.

    ‚úÖ Na z√°klade tohto kateg√≥ri√°lneho grafu, ak√© zauj√≠mav√© sk√∫mania si dok√°≈æete predstavi≈•?

### Predspracovanie d√°t: k√≥dovanie vlastnost√≠ a ≈°t√≠tkov
Na≈°a d√°tov√° sada o tekviciach obsahuje textov√© hodnoty vo v≈°etk√Ωch svojich stƒ∫pcoch. Pr√°ca s kateg√≥ri√°lnymi d√°tami je intuit√≠vna pre ƒæud√≠, ale nie pre stroje. Algoritmy strojov√©ho uƒçenia funguj√∫ dobre s ƒç√≠slami. Preto je k√≥dovanie veƒæmi d√¥le≈æit√Ωm krokom vo f√°ze predspracovania d√°t, preto≈æe n√°m umo≈æ≈àuje premeni≈• kateg√≥ri√°lne d√°ta na ƒç√≠seln√© d√°ta bez straty inform√°ci√≠. Dobr√© k√≥dovanie vedie k vytvoreniu dobr√©ho modelu.

Pre k√≥dovanie vlastnost√≠ existuj√∫ dva hlavn√© typy k√≥derov:

1. Ordin√°lny k√≥der: hod√≠ sa pre ordin√°lne premenn√©, ktor√© s√∫ kateg√≥ri√°lne premenn√©, kde ich d√°ta nasleduj√∫ logick√© usporiadanie, ako stƒ∫pec `Item Size` v na≈°ej d√°tovej sade. Vytv√°ra mapovanie, kde ka≈æd√° kateg√≥ria je reprezentovan√° ƒç√≠slom, ktor√© je poradie kateg√≥rie v stƒ∫pci.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kateg√≥ri√°lny k√≥der: hod√≠ sa pre nomin√°lne premenn√©, ktor√© s√∫ kateg√≥ri√°lne premenn√©, kde ich d√°ta nenasleduj√∫ logick√© usporiadanie, ako v≈°etky vlastnosti odli≈°n√© od `Item Size` v na≈°ej d√°tovej sade. Ide o k√≥dovanie typu one-hot, ƒço znamen√°, ≈æe ka≈æd√° kateg√≥ria je reprezentovan√° bin√°rnym stƒ∫pcom: k√≥dovan√° premenn√° je rovn√° 1, ak tekvica patr√≠ do tejto odrody, a 0 inak.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Potom sa na kombin√°ciu viacer√Ωch k√≥derov do jedn√©ho kroku a ich aplik√°ciu na pr√≠slu≈°n√© stƒ∫pce pou≈æ√≠va `ColumnTransformer`.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
Na druhej strane, na k√≥dovanie ≈°t√≠tku pou≈æ√≠vame triedu `LabelEncoder` zo scikit-learn, ktor√° je pomocnou triedou na normaliz√°ciu ≈°t√≠tkov tak, aby obsahovali iba hodnoty medzi 0 a n_classes-1 (tu 0 a 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
Keƒè sme zak√≥dovali vlastnosti a ≈°t√≠tok, m√¥≈æeme ich zl√∫ƒçi≈• do nov√©ho dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
‚úÖ Ak√© s√∫ v√Ωhody pou≈æitia ordin√°lneho k√≥dera pre stƒ∫pec `Item Size`?

### Anal√Ωza vz≈•ahov medzi premenn√Ωmi

Teraz, keƒè sme predspracovali na≈°e d√°ta, m√¥≈æeme analyzova≈• vz≈•ahy medzi vlastnos≈•ami a ≈°t√≠tkom, aby sme z√≠skali predstavu o tom, ako dobre bude model schopn√Ω predpoveda≈• ≈°t√≠tok na z√°klade vlastnost√≠.
Najlep≈°√≠ sp√¥sob, ako vykona≈• tento druh anal√Ωzy, je vizualiz√°cia d√°t. Op√§≈• pou≈æijeme funkciu `catplot` zo Seaborn na vizualiz√°ciu vz≈•ahov medzi `Item Size`, `Variety` a `Farba` v kateg√≥ri√°lnom grafe. Na lep≈°iu vizualiz√°ciu d√°t pou≈æijeme zak√≥dovan√Ω stƒ∫pec `Item Size` a nezak√≥dovan√Ω stƒ∫pec `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![Kateg√≥ri√°lny graf vizualizovan√Ωch d√°t](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Pou≈æitie swarm grafu

Keƒè≈æe Farba je bin√°rna kateg√≥ria (Biela alebo Nie), potrebuje '≈°pecializovan√Ω pr√≠stup [k vizualiz√°cii](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar)'. Existuj√∫ aj in√© sp√¥soby vizualiz√°cie vz≈•ahu tejto kateg√≥rie s in√Ωmi premenn√Ωmi.

Premenn√© m√¥≈æete vizualizova≈• vedƒæa seba pomocou grafov Seaborn.

1. Sk√∫ste 'swarm' graf na zobrazenie distrib√∫cie hodn√¥t:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Swarm graf vizualizovan√Ωch d√°t](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Pozor**: vy≈°≈°ie uveden√Ω k√≥d m√¥≈æe generova≈• varovanie, preto≈æe Seaborn nedok√°≈æe reprezentova≈• tak√© mno≈æstvo d√°tov√Ωch bodov v swarm grafe. Mo≈æn√Ωm rie≈°en√≠m je zmen≈°enie veƒækosti znaƒçky pomocou parametra 'size'. Buƒète v≈°ak opatrn√≠, preto≈æe to ovplyv≈àuje ƒçitateƒænos≈• grafu.

> **üßÆ Uk√°≈æte mi matematiku**
>
> Logistick√° regresia sa opiera o koncept 'maxim√°lnej pravdepodobnosti' pomocou [sigmoidn√Ωch funkci√≠](https://wikipedia.org/wiki/Sigmoid_function). 'Sigmoidn√° funkcia' na grafe vyzer√° ako tvar 'S'. Berie hodnotu a mapuje ju na nieƒço medzi 0 a 1. Jej krivka sa tie≈æ naz√Ωva 'logistick√° krivka'. Jej vzorec vyzer√° takto:
>
> ![logistick√° funkcia](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> kde stred sigmoidnej funkcie sa nach√°dza na 0 bode x, L je maxim√°lna hodnota krivky a k je strmos≈• krivky. Ak je v√Ωsledok funkcie viac ako 0.5, dan√Ω ≈°t√≠tok bude priraden√Ω do triedy '1' bin√°rnej voƒæby. Ak nie, bude klasifikovan√Ω ako '0'.

## Vytvorte svoj model

Vytvorenie modelu na n√°jdenie t√Ωchto bin√°rnych klasifik√°ci√≠ je prekvapivo jednoduch√© v Scikit-learn.

[![Strojov√© uƒçenie pre zaƒçiatoƒçn√≠kov - Logistick√° regresia pre klasifik√°ciu d√°t](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "Strojov√© uƒçenie pre zaƒçiatoƒçn√≠kov - Logistick√° regresia pre klasifik√°ciu d√°t")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o vytv√°ran√≠ modelu line√°rnej regresie.

1. Vyberte premenn√©, ktor√© chcete pou≈æi≈• vo svojom klasifikaƒçnom modeli, a rozdeƒæte tr√©ningov√© a testovacie sady pomocou `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Teraz m√¥≈æete tr√©nova≈• svoj model, zavolan√≠m `fit()` s va≈°imi tr√©ningov√Ωmi d√°tami, a vytlaƒçi≈• jeho v√Ωsledok:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Pozrite sa na sk√≥re v√°≈°ho modelu. Nie je to zl√©, vzhƒæadom na to, ≈æe m√°te len asi 1000 riadkov d√°t:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Lep≈°ie pochopenie pomocou matice zm√§tku

Zatiaƒæ ƒço m√¥≈æete z√≠ska≈• spr√°vu o sk√≥re [term√≠ny](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) vytlaƒçen√≠m vy≈°≈°ie uveden√Ωch polo≈æiek, m√¥≈æete svoj model lep≈°ie pochopi≈• pomocou [matice zm√§tku](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), ktor√° n√°m pom√°ha pochopi≈•, ako model funguje.

> üéì '[Matica zm√§tku](https://wikipedia.org/wiki/Confusion_matrix)' (alebo 'matica ch√Ωb') je tabuƒæka, ktor√° vyjadruje skutoƒçn√© vs. falo≈°n√© pozit√≠va a negat√≠va v√°≈°ho modelu, ƒç√≠m hodnot√≠ presnos≈• predpoved√≠.

1. Na pou≈æitie matice zm√§tku zavolajte `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Pozrite sa na maticu zm√§tku v√°≈°ho modelu:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

V Scikit-learn, riadky (os 0) s√∫ skutoƒçn√© ≈°t√≠tky a stƒ∫pce (os 1) s√∫ predpovedan√© ≈°t√≠tky.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

ƒåo sa tu deje? Povedzme, ≈æe n√°≈° model je po≈æiadan√Ω klasifikova≈• tekvice medzi dvoma bin√°rnymi kateg√≥riami, kateg√≥riou 'biela' a kateg√≥riou 'nie biela'.

- Ak v√°≈° model predpoved√° tekvicu ako nie bielu a v skutoƒçnosti patr√≠ do kateg√≥rie 'nie biela', naz√Ωvame to prav√Ω negat√≠vny, zobrazen√Ω horn√Ωm ƒæav√Ωm ƒç√≠slom.
- Ak v√°≈° model predpoved√° tekvicu ako bielu a v skutoƒçnosti patr√≠ do kateg√≥rie 'nie biela', naz√Ωvame to falo≈°n√Ω negat√≠vny, zobrazen√Ω doln√Ωm ƒæav√Ωm ƒç√≠slom.
- Ak v√°≈° model predpoved√° tekvicu ako nie bielu a v skutoƒçnosti patr√≠ do kateg√≥rie 'biela', naz√Ωvame to falo≈°n√Ω pozit√≠vny, zobrazen√Ω horn√Ωm prav√Ωm ƒç√≠slom.
- Ak v√°≈° model predpoved√° tekvicu ako bielu a v skutoƒçnosti patr√≠ do kateg√≥rie 'biela', naz√Ωvame to prav√Ω pozit√≠vny, zobrazen√Ω doln√Ωm prav√Ωm ƒç√≠slom.

Ako ste mohli uh√°dnu≈•, je preferovan√© ma≈• v√§ƒç≈°√≠ poƒçet prav√Ωch pozit√≠vnych a prav√Ωch negat√≠vnych a ni≈æ≈°√≠ poƒçet falo≈°n√Ωch pozit√≠vnych a falo≈°n√Ωch negat√≠vnych, ƒço naznaƒçuje, ≈æe model funguje lep≈°ie.
Ako s√∫vis√≠ matica z√°mien s presnos≈•ou a odvolan√≠m? Pam√§tajte, ≈æe klasifikaƒçn√° spr√°va uveden√° vy≈°≈°ie uk√°zala presnos≈• (0,85) a odvolanie (0,67).

Presnos≈• = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

Odvolanie = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

‚úÖ Ot√°zka: Podƒæa matice z√°mien, ako si model viedol? Odpoveƒè: Nie zle; je tu dobr√Ω poƒçet spr√°vne negat√≠vnych, ale aj niekoƒæko nespr√°vne negat√≠vnych.

Poƒème si znova prejs≈• pojmy, ktor√© sme videli sk√¥r, s pomocou mapovania TP/TN a FP/FN v matici z√°mien:

üéì Presnos≈•: TP/(TP + FP) Podiel relevantn√Ωch pr√≠padov medzi z√≠skan√Ωmi pr√≠padmi (napr. ktor√© ≈°t√≠tky boli spr√°vne oznaƒçen√©)

üéì Odvolanie: TP/(TP + FN) Podiel relevantn√Ωch pr√≠padov, ktor√© boli z√≠skan√©, ƒçi u≈æ spr√°vne oznaƒçen√© alebo nie

üéì f1-sk√≥re: (2 * presnos≈• * odvolanie)/(presnos≈• + odvolanie) V√°≈æen√Ω priemer presnosti a odvolania, priƒçom najlep≈°ie je 1 a najhor≈°ie 0

üéì Podpora: Poƒçet v√Ωskytov ka≈æd√©ho z√≠skan√©ho ≈°t√≠tku

üéì Presnos≈•: (TP + TN)/(TP + TN + FP + FN) Percento ≈°t√≠tkov predpovedan√Ωch spr√°vne pre vzorku.

üéì Makro priemer: V√Ωpoƒçet nevyv√°≈æen√©ho priemeru metr√≠k pre ka≈æd√Ω ≈°t√≠tok, bez ohƒæadu na nerovnov√°hu ≈°t√≠tkov.

üéì V√°≈æen√Ω priemer: V√Ωpoƒçet priemeru metr√≠k pre ka≈æd√Ω ≈°t√≠tok, priƒçom sa berie do √∫vahy nerovnov√°ha ≈°t√≠tkov v√°≈æen√≠m podƒæa ich podpory (poƒçet skutoƒçn√Ωch pr√≠padov pre ka≈æd√Ω ≈°t√≠tok).

‚úÖ Dok√°≈æete si predstavi≈•, ktor√∫ metriku by ste mali sledova≈•, ak chcete, aby v√°≈° model zn√≠≈æil poƒçet nespr√°vne negat√≠vnych?

## Vizualiz√°cia ROC krivky tohto modelu

[![ML pre zaƒçiatoƒçn√≠kov - Anal√Ωza v√Ωkonu logistickej regresie pomocou ROC kriviek](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML pre zaƒçiatoƒçn√≠kov - Anal√Ωza v√Ωkonu logistickej regresie pomocou ROC kriviek")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad ROC kriviek

Poƒème urobi≈• e≈°te jednu vizualiz√°ciu, aby sme videli tzv. 'ROC' krivku:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Pomocou Matplotlibu vykreslite [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) alebo ROC modelu. ROC krivky sa ƒçasto pou≈æ√≠vaj√∫ na zobrazenie v√Ωstupu klasifik√°tora z hƒæadiska jeho spr√°vne vs. nespr√°vne pozit√≠vnych. "ROC krivky zvyƒçajne zobrazuj√∫ mieru spr√°vne pozit√≠vnych na osi Y a mieru nespr√°vne pozit√≠vnych na osi X." Preto z√°le≈æ√≠ na strmosti krivky a priestore medzi stredovou ƒçiarou a krivkou: chcete krivku, ktor√° r√Ωchlo st√∫pa a prech√°dza nad ƒçiaru. V na≈°om pr√≠pade s√∫ na zaƒçiatku nespr√°vne pozit√≠vne, a potom krivka spr√°vne st√∫pa a prech√°dza nad ƒçiaru:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Nakoniec pou≈æite Scikit-learn [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) na v√Ωpoƒçet skutoƒçnej 'Plochy pod krivkou' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
V√Ωsledok je `0.9749908725812341`. Keƒè≈æe AUC sa pohybuje od 0 do 1, chcete vysok√© sk√≥re, preto≈æe model, ktor√Ω je 100% spr√°vny vo svojich predpovediach, bude ma≈• AUC 1; v tomto pr√≠pade je model _celkom dobr√Ω_. 

V bud√∫cich lekci√°ch o klasifik√°ci√°ch sa nauƒç√≠te, ako iterova≈• na zlep≈°enie sk√≥re v√°≈°ho modelu. Ale zatiaƒæ gratulujeme! Dokonƒçili ste tieto lekcie o regresii!

---
## üöÄV√Ωzva

Logistick√° regresia m√° oveƒæa viac, ƒço sa d√° presk√∫ma≈•! Ale najlep≈°√≠ sp√¥sob, ako sa uƒçi≈•, je experimentova≈•. N√°jdite dataset, ktor√Ω sa hod√≠ na tento typ anal√Ωzy, a vytvorte s n√≠m model. ƒåo ste sa nauƒçili? tip: sk√∫ste [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) pre zauj√≠mav√© datasety.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ml/)

## Prehƒæad a samostatn√© ≈°t√∫dium

Preƒç√≠tajte si prv√© str√°nky [tohto dokumentu zo Stanfordu](https://web.stanford.edu/~jurafsky/slp3/5.pdf) o niektor√Ωch praktick√Ωch vyu≈æitiach logistickej regresie. Prem√Ω≈°ƒæajte o √∫loh√°ch, ktor√© s√∫ lep≈°ie vhodn√© pre jeden alebo druh√Ω typ regresn√Ωch √∫loh, ktor√© sme doteraz ≈°tudovali. ƒåo by fungovalo najlep≈°ie?

## Zadanie 

[Opakovanie tejto regresie](assignment.md)

---

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nie sme zodpovedn√≠ za ak√©koƒævek nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.