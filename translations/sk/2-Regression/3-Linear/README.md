<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T15:11:10+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "sk"
}
-->
# Vytvorenie regresn√©ho modelu pomocou Scikit-learn: ≈°tyri sp√¥soby regresie

![Infografika line√°rna vs polynomi√°lna regresia](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Kv√≠z pred predn√°≈°kou](https://ff-quizzes.netlify.app/en/ml/)

> ### [T√°to lekcia je dostupn√° v R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### √övod 

Doteraz ste presk√∫mali, ƒço je regresia, na vzorov√Ωch √∫dajoch zo s√∫boru √∫dajov o cen√°ch tekv√≠c, ktor√Ω budeme pou≈æ√≠va≈• poƒças celej tejto lekcie. Vizualizovali ste ich pomocou Matplotlibu.

Teraz ste pripraven√≠ ponori≈• sa hlb≈°ie do regresie pre strojov√© uƒçenie. Zatiaƒæ ƒço vizualiz√°cia v√°m umo≈æ≈àuje pochopi≈• √∫daje, skutoƒçn√° sila strojov√©ho uƒçenia spoƒç√≠va v _tr√©ningu modelov_. Modely s√∫ tr√©novan√© na historick√Ωch √∫dajoch, aby automaticky zachytili z√°vislosti medzi √∫dajmi, a umo≈æ≈àuj√∫ v√°m predpoveda≈• v√Ωsledky pre nov√© √∫daje, ktor√© model predt√Ωm nevidel.

V tejto lekcii sa dozviete viac o dvoch typoch regresie: _z√°kladn√° line√°rna regresia_ a _polynomi√°lna regresia_, spolu s niektor√Ωmi matematick√Ωmi z√°kladmi t√Ωchto techn√≠k. Tieto modely n√°m umo≈ænia predpoveda≈• ceny tekv√≠c na z√°klade r√¥znych vstupn√Ωch √∫dajov.

[![ML pre zaƒçiatoƒçn√≠kov - Pochopenie line√°rnej regresie](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pre zaƒçiatoƒçn√≠kov - Pochopenie line√°rnej regresie")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o line√°rnej regresii.

> Poƒças cel√©ho kurzu predpoklad√°me minim√°lne znalosti matematiky a sna≈æ√≠me sa spr√≠stupni≈• obsah ≈°tudentom z in√Ωch odborov, preto sledujte pozn√°mky, üßÆ v√Ωpoƒçty, diagramy a ƒèal≈°ie n√°stroje na uƒçenie, ktor√© v√°m pom√¥≈æu pochopi≈• obsah.

### Predpoklady

Teraz by ste mali by≈• obozn√°men√≠ so ≈°trukt√∫rou √∫dajov o tekviciach, ktor√© sk√∫mame. N√°jdete ich prednahran√© a predƒçisten√© v s√∫bore _notebook.ipynb_ tejto lekcie. V s√∫bore je cena tekv√≠c zobrazen√° za bu≈°el v novom d√°tovom r√°mci. Uistite sa, ≈æe dok√°≈æete spusti≈• tieto notebooky v jadr√°ch vo Visual Studio Code.

### Pr√≠prava

Pripome≈àme si, ≈æe tieto √∫daje naƒç√≠tavate, aby ste mohli kl√°s≈• ot√°zky:

- Kedy je najlep≈°√≠ ƒças na k√∫pu tekv√≠c? 
- Ak√∫ cenu m√¥≈æem oƒçak√°va≈• za balenie miniat√∫rnych tekv√≠c?
- M√°m ich k√∫pi≈• v poloviƒçn√Ωch bu≈°lov√Ωch ko≈°och alebo v 1 1/9 bu≈°lov√Ωch ≈°katuliach?
Poƒème sa hlb≈°ie pozrie≈• na tieto √∫daje.

V predch√°dzaj√∫cej lekcii ste vytvorili Pandas d√°tov√Ω r√°mec a naplnili ho ƒças≈•ou p√¥vodn√©ho s√∫boru √∫dajov, ≈°tandardizuj√∫c ceny podƒæa bu≈°lu. T√Ωmto sp√¥sobom ste v≈°ak dok√°zali zhroma≈ædi≈• iba pribli≈æne 400 d√°tov√Ωch bodov a iba pre jesenn√© mesiace.

Pozrite sa na √∫daje, ktor√© sme prednahrali v notebooku tejto lekcie. √ödaje s√∫ prednahran√© a √∫vodn√Ω bodov√Ω graf je vytvoren√Ω na zobrazenie √∫dajov podƒæa mesiacov. Mo≈æno m√¥≈æeme z√≠ska≈• trochu viac detailov o povahe √∫dajov ich ƒèal≈°√≠m ƒçisten√≠m.

## Line√°rna regresn√° ƒçiara

Ako ste sa nauƒçili v Lekcii 1, cieƒæom cviƒçenia line√°rnej regresie je by≈• schopn√Ω nakresli≈• ƒçiaru na:

- **Uk√°zanie vz≈•ahov medzi premenn√Ωmi**. Uk√°za≈• vz≈•ah medzi premenn√Ωmi
- **Predpovedanie**. Urobi≈• presn√© predpovede, kde by nov√Ω d√°tov√Ω bod spadol vo vz≈•ahu k tejto ƒçiare. 
 
Typick√© pre **regresiu met√≥dou najmen≈°√≠ch ≈°tvorcov** je nakresli≈• tento typ ƒçiary. Term√≠n 'najmen≈°ie ≈°tvorce' znamen√°, ≈æe v≈°etky d√°tov√© body obklopuj√∫ce regresn√∫ ƒçiaru s√∫ umocnen√© na druh√∫ a potom sƒç√≠tan√©. Ide√°lne je, aby tento koneƒçn√Ω s√∫ƒçet bol ƒço najmen≈°√≠, preto≈æe chceme n√≠zky poƒçet ch√Ωb, alebo `najmen≈°ie ≈°tvorce`. 

Rob√≠me to, preto≈æe chceme modelova≈• ƒçiaru, ktor√° m√° najmen≈°iu kumulat√≠vnu vzdialenos≈• od v≈°etk√Ωch na≈°ich d√°tov√Ωch bodov. Tie≈æ umoc≈àujeme hodnoty na druh√∫ pred ich sƒç√≠tan√≠m, preto≈æe n√°s zauj√≠ma ich veƒækos≈•, nie ich smer.

> **üßÆ Uk√°≈æte mi matematiku** 
> 
> T√°to ƒçiara, naz√Ωvan√° _ƒçiara najlep≈°ieho prisp√¥sobenia_, m√¥≈æe by≈• vyjadren√° [rovnicou](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'vysvetƒæuj√∫ca premenn√°'. `Y` je 'z√°visl√° premenn√°'. Sklon ƒçiary je `b` a `a` je y-prieseƒçn√≠k, ktor√Ω odkazuje na hodnotu `Y`, keƒè `X = 0`. 
>
>![v√Ωpoƒçet sklonu](../../../../2-Regression/3-Linear/images/slope.png)
>
> Najprv vypoƒç√≠tajte sklon `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> In√Ωmi slovami, a odkazuj√∫c na p√¥vodn√∫ ot√°zku o √∫dajoch o tekviciach: "predpovedajte cenu tekvice za bu≈°el podƒæa mesiaca", `X` by odkazovalo na cenu a `Y` by odkazovalo na mesiac predaja. 
>
>![dokonƒçenie rovnice](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Vypoƒç√≠tajte hodnotu Y. Ak plat√≠te okolo $4, mus√≠ by≈• apr√≠l! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika, ktor√° vypoƒç√≠tava ƒçiaru, mus√≠ demon≈°trova≈• sklon ƒçiary, ktor√Ω tie≈æ z√°vis√≠ od prieseƒçn√≠ka, alebo kde sa `Y` nach√°dza, keƒè `X = 0`.
>
> Met√≥du v√Ωpoƒçtu t√Ωchto hodn√¥t si m√¥≈æete pozrie≈• na webovej str√°nke [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Nav≈°t√≠vte tie≈æ [tento kalkul√°tor met√≥dy najmen≈°√≠ch ≈°tvorcov](https://www.mathsisfun.com/data/least-squares-calculator.html), aby ste videli, ako hodnoty ƒç√≠sel ovplyv≈àuj√∫ ƒçiaru.

## Korel√°cia

ƒéal≈°√≠ term√≠n, ktor√Ω je potrebn√© pochopi≈•, je **koeficient korel√°cie** medzi dan√Ωmi premenn√Ωmi X a Y. Pomocou bodov√©ho grafu m√¥≈æete r√Ωchlo vizualizova≈• tento koeficient. Graf s d√°tov√Ωmi bodmi rozmiestnen√Ωmi v √∫hƒæadnej ƒçiare m√° vysok√∫ korel√°ciu, ale graf s d√°tov√Ωmi bodmi rozmiestnen√Ωmi v≈°ade medzi X a Y m√° n√≠zku korel√°ciu.

Dobr√Ω model line√°rnej regresie bude tak√Ω, ktor√Ω m√° vysok√Ω (bli≈æ≈°ie k 1 ako k 0) koeficient korel√°cie pomocou met√≥dy najmen≈°√≠ch ≈°tvorcov s regresnou ƒçiarou.

‚úÖ Spustite notebook, ktor√Ω sprev√°dza t√∫to lekciu, a pozrite sa na bodov√Ω graf Mesiac vs Cena. Zd√° sa, ≈æe √∫daje sp√°jaj√∫ce Mesiac s Cenou za predaj tekv√≠c maj√∫ podƒæa va≈°ej vizu√°lnej interpret√°cie bodov√©ho grafu vysok√∫ alebo n√≠zku korel√°ciu? Zmen√≠ sa to, ak pou≈æijete jemnej≈°ie meranie namiesto `Mesiac`, napr. *de≈à v roku* (t. j. poƒçet dn√≠ od zaƒçiatku roka)?

V nasleduj√∫com k√≥de predpoklad√°me, ≈æe sme vyƒçistili √∫daje a z√≠skali d√°tov√Ω r√°mec nazvan√Ω `new_pumpkins`, podobn√Ω nasleduj√∫cemu:

ID | Mesiac | De≈àVroku | Typ | Mesto | Balenie | N√≠zka cena | Vysok√° cena | Cena
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bu≈°lov√© kart√≥ny | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bu≈°lov√© kart√≥ny | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bu≈°lov√© kart√≥ny | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bu≈°lov√© kart√≥ny | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bu≈°lov√© kart√≥ny | 15.0 | 15.0 | 13.636364

> K√≥d na ƒçistenie √∫dajov je dostupn√Ω v [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Vykonali sme rovnak√© kroky ƒçistenia ako v predch√°dzaj√∫cej lekcii a vypoƒç√≠tali sme stƒ∫pec `De≈àVroku` pomocou nasleduj√∫ceho v√Ωrazu: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Teraz, keƒè rozumiete matematike za line√°rnou regresiou, poƒème vytvori≈• regresn√Ω model, aby sme zistili, ktor√Ω bal√≠k tekv√≠c bude ma≈• najlep≈°ie ceny tekv√≠c. Niekto, kto kupuje tekvice na sviatoƒçn√∫ tekvicov√∫ z√°hradu, by mohol chcie≈• tieto inform√°cie, aby optimalizoval svoje n√°kupy bal√≠kov tekv√≠c pre z√°hradu.

## Hƒæadanie korel√°cie

[![ML pre zaƒçiatoƒçn√≠kov - Hƒæadanie korel√°cie: Kƒæ√∫ƒç k line√°rnej regresii](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pre zaƒçiatoƒçn√≠kov - Hƒæadanie korel√°cie: Kƒæ√∫ƒç k line√°rnej regresii")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o korel√°cii.

Z predch√°dzaj√∫cej lekcie ste pravdepodobne videli, ≈æe priemern√° cena za r√¥zne mesiace vyzer√° takto:

<img alt="Priemern√° cena podƒæa mesiaca" src="../2-Data/images/barchart.png" width="50%"/>

To naznaƒçuje, ≈æe by mala existova≈• nejak√° korel√°cia, a m√¥≈æeme sk√∫si≈• tr√©nova≈• model line√°rnej regresie na predpovedanie vz≈•ahu medzi `Mesiac` a `Cena`, alebo medzi `De≈àVroku` a `Cena`. Tu je bodov√Ω graf, ktor√Ω ukazuje druh√Ω vz≈•ah:

<img alt="Bodov√Ω graf Cena vs. De≈à v roku" src="images/scatter-dayofyear.png" width="50%" /> 

Pozrime sa, ƒçi existuje korel√°cia pomocou funkcie `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Zd√° sa, ≈æe korel√°cia je pomerne mal√°, -0.15 podƒæa `Mesiac` a -0.17 podƒæa `De≈àVroku`, ale mohol by existova≈• in√Ω d√¥le≈æit√Ω vz≈•ah. Zd√° sa, ≈æe existuj√∫ r√¥zne zhluky cien zodpovedaj√∫ce r√¥znym odrod√°m tekv√≠c. Na potvrdenie tejto hypot√©zy nakreslime ka≈æd√∫ kateg√≥riu tekv√≠c pomocou inej farby. Pri prechode parametra `ax` do funkcie `scatter` m√¥≈æeme nakresli≈• v≈°etky body na rovnak√Ω graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Bodov√Ω graf Cena vs. De≈à v roku" src="images/scatter-dayofyear-color.png" width="50%" /> 

Na≈°e vy≈°etrovanie naznaƒçuje, ≈æe odroda m√° v√§ƒç≈°√≠ vplyv na celkov√∫ cenu ako skutoƒçn√Ω d√°tum predaja. M√¥≈æeme to vidie≈• na stƒ∫pcovom grafe:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Stƒ∫pcov√Ω graf cena vs odroda" src="images/price-by-variety.png" width="50%" /> 

Zamerajme sa na chv√≠ƒæu iba na jednu odrodu tekv√≠c, 'pie type', a pozrime sa, ak√Ω vplyv m√° d√°tum na cenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Bodov√Ω graf Cena vs. De≈à v roku" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Ak teraz vypoƒç√≠tame korel√°ciu medzi `Cena` a `De≈àVroku` pomocou funkcie `corr`, dostaneme hodnotu okolo `-0.27` - ƒço znamen√°, ≈æe tr√©novanie predikt√≠vneho modelu m√° zmysel.

> Pred tr√©novan√≠m modelu line√°rnej regresie je d√¥le≈æit√© zabezpeƒçi≈•, aby na≈°e √∫daje boli ƒçist√©. Line√°rna regresia nefunguje dobre s ch√Ωbaj√∫cimi hodnotami, preto m√° zmysel zbavi≈• sa v≈°etk√Ωch pr√°zdnych buniek:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

ƒéal≈°√≠m pr√≠stupom by bolo vyplni≈• tieto pr√°zdne hodnoty priemern√Ωmi hodnotami z pr√≠slu≈°n√©ho stƒ∫pca.

## Jednoduch√° line√°rna regresia

[![ML pre zaƒçiatoƒçn√≠kov - Line√°rna a polynomi√°lna regresia pomocou Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pre zaƒçiatoƒçn√≠kov - Line√°rna a polynomi√°lna regresia pomocou Scikit-learn")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o line√°rnej a polynomi√°lnej regresii.

Na tr√©novanie n√°≈°ho modelu line√°rnej regresie pou≈æijeme kni≈ænicu **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Zaƒç√≠name oddelen√≠m vstupn√Ωch hodn√¥t (pr√≠znakov) a oƒçak√°van√©ho v√Ωstupu (oznaƒçenia) do samostatn√Ωch numpy pol√≠:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> V≈°imnite si, ≈æe sme museli vykona≈• `reshape` na vstupn√Ωch √∫dajoch, aby ich bal√≠k line√°rnej regresie spr√°vne pochopil. Line√°rna regresia oƒçak√°va 2D pole ako vstup, kde ka≈æd√Ω riadok poƒæa zodpoved√° vektoru vstupn√Ωch pr√≠znakov. V na≈°om pr√≠pade, keƒè≈æe m√°me iba jeden vstup, potrebujeme pole s tvarom N√ó1, kde N je veƒækos≈• s√∫boru √∫dajov.

Potom mus√≠me rozdeli≈• √∫daje na tr√©novac√≠ a testovac√≠ s√∫bor √∫dajov, aby sme mohli po tr√©ningu overi≈• n√°≈° model:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Nakoniec, samotn√© tr√©novanie modelu line√°rnej regresie trv√° iba dva riadky k√≥du. Definujeme objekt `LinearRegression` a prisp√¥sob√≠me ho na≈°im √∫dajom pomocou met√≥dy `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po prisp√¥soben√≠ obsahuje v≈°etky koeficienty regresie, ku ktor√Ωm je mo≈æn√© pristupova≈• pomocou vlastnosti `.coef_`. V na≈°om pr√≠pade existuje iba jeden koeficient, ktor√Ω by mal by≈• okolo `-0.017`. To znamen√°, ≈æe ceny sa zdaj√∫ klesa≈• trochu s ƒçasom, ale nie pr√≠li≈°, pribli≈æne o 2 centy za de≈à. M√¥≈æeme tie≈æ pristupova≈• k prieseƒçn√≠ku regresie s Y-osou pomocou `lin_reg.intercept_` - bude to okolo `21` v na≈°om pr√≠pade, ƒço naznaƒçuje cenu na zaƒçiatku roka.

Aby sme videli, ak√Ω presn√Ω je n√°≈° model, m√¥≈æeme predpoveda≈• ceny na testovacom s√∫bore √∫dajov a potom zmera≈•, ako bl√≠zko s√∫ na≈°e predpovede k oƒçak√°van√Ωm hodnot√°m. To sa d√° urobi≈• pomocou metriky strednej kvadratickej chyby (MSE), ktor√° je priemerom v≈°etk√Ωch kvadratick√Ωch rozdielov medzi oƒçak√°vanou a predpovedanou hodnotou.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Na≈°a chyba sa zd√° by≈• okolo 2 bodov, ƒço je ~17 %. Nie je to pr√≠li≈° dobr√©. ƒéal≈°√≠m indik√°torom kvality modelu je **koeficient determin√°cie**, ktor√Ω m√¥≈æeme z√≠ska≈• takto:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Ak je hodnota 0, znamen√° to, ≈æe model neberie do √∫vahy vstupn√© √∫daje a funguje ako *najhor≈°√≠ line√°rny prediktor*, ƒço je jednoducho priemern√° hodnota v√Ωsledku. Hodnota 1 znamen√°, ≈æe m√¥≈æeme dokonale predpoveda≈• v≈°etky oƒçak√°van√© v√Ωstupy. V na≈°om pr√≠pade je koeficient okolo 0.06, ƒço je pomerne n√≠zke.

M√¥≈æeme tie≈æ vykresli≈• testovacie √∫daje spolu s regresnou ƒçiarou, aby sme lep≈°ie videli, ako regresia funguje v na≈°om pr√≠pade:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="Line√°rna regresia" src="images/linear-results.png" width="50%" />

## Polynomick√° regresia

ƒéal≈°√≠m typom line√°rnej regresie je polynomick√° regresia. Zatiaƒæ ƒço niekedy existuje line√°rny vz≈•ah medzi premenn√Ωmi ‚Äì ƒç√≠m v√§ƒç≈°√≠ objem tekvice, t√Ωm vy≈°≈°ia cena ‚Äì niekedy tieto vz≈•ahy nemo≈æno vykresli≈• ako rovinu alebo priamku.

‚úÖ Tu s√∫ [niektor√© ƒèal≈°ie pr√≠klady](https://online.stat.psu.edu/stat501/lesson/9/9.8) √∫dajov, ktor√© by mohli vyu≈æi≈• polynomick√∫ regresiu.

Pozrite sa znova na vz≈•ah medzi d√°tumom a cenou. Zd√° sa, ≈æe tento bodov√Ω graf by mal by≈• nevyhnutne analyzovan√Ω priamkou? Nem√¥≈æu ceny kol√≠sa≈•? V tomto pr√≠pade m√¥≈æete vysk√∫≈°a≈• polynomick√∫ regresiu.

‚úÖ Polyn√≥my s√∫ matematick√© v√Ωrazy, ktor√© m√¥≈æu pozost√°va≈• z jednej alebo viacer√Ωch premenn√Ωch a koeficientov.

Polynomick√° regresia vytv√°ra zakriven√∫ ƒçiaru, ktor√° lep≈°ie zodpoved√° neline√°rnym √∫dajom. V na≈°om pr√≠pade, ak do vstupn√Ωch √∫dajov zahrnieme premenn√∫ `DayOfYear` na druh√∫, mali by sme by≈• schopn√≠ prisp√¥sobi≈• na≈°e √∫daje parabolickou krivkou, ktor√° bude ma≈• minimum v urƒçitom bode poƒças roka.

Scikit-learn obsahuje u≈æitoƒçn√© [API pre pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), ktor√© kombinuje r√¥zne kroky spracovania √∫dajov. **Pipeline** je re≈•az **odhadovateƒæov**. V na≈°om pr√≠pade vytvor√≠me pipeline, ktor√° najsk√¥r prid√° polynomick√© prvky do n√°≈°ho modelu a potom tr√©nuje regresiu:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

Pou≈æitie `PolynomialFeatures(2)` znamen√°, ≈æe zahrnieme v≈°etky polyn√≥my druh√©ho stup≈àa zo vstupn√Ωch √∫dajov. V na≈°om pr√≠pade to bude znamena≈• len `DayOfYear`<sup>2</sup>, ale pri dvoch vstupn√Ωch premenn√Ωch X a Y to prid√° X<sup>2</sup>, XY a Y<sup>2</sup>. M√¥≈æeme tie≈æ pou≈æi≈• polyn√≥my vy≈°≈°ieho stup≈àa, ak chceme.

Pipeline m√¥≈æeme pou≈æ√≠va≈• rovnak√Ωm sp√¥sobom ako p√¥vodn√Ω objekt `LinearRegression`, t.j. m√¥≈æeme pipeline `fit` a potom pou≈æi≈• `predict` na z√≠skanie v√Ωsledkov predikcie. Tu je graf zobrazuj√∫ci testovacie √∫daje a aproximaƒçn√∫ krivku:

<img alt="Polynomick√° regresia" src="images/poly-results.png" width="50%" />

Pou≈æit√≠m polynomickej regresie m√¥≈æeme dosiahnu≈• mierne ni≈æ≈°ie MSE a vy≈°≈°iu determin√°ciu, ale nie v√Ωznamne. Mus√≠me zohƒæadni≈• ƒèal≈°ie prvky!

> Vid√≠te, ≈æe minim√°lne ceny tekv√≠c s√∫ pozorovan√© niekde okolo Halloweenu. Ako to m√¥≈æete vysvetli≈•?

üéÉ Gratulujeme, pr√°ve ste vytvorili model, ktor√Ω m√¥≈æe pom√¥c≈• predpoveda≈• cenu tekv√≠c na kol√°ƒçe. Pravdepodobne m√¥≈æete zopakova≈• rovnak√Ω postup pre v≈°etky typy tekv√≠c, ale to by bolo zdƒ∫hav√©. Poƒème sa teraz nauƒçi≈•, ako zohƒæadni≈• odrodu tekv√≠c v na≈°om modeli!

## Kategorick√© prvky

V ide√°lnom svete chceme by≈• schopn√≠ predpoveda≈• ceny pre r√¥zne odrody tekv√≠c pomocou rovnak√©ho modelu. Stƒ∫pec `Variety` je v≈°ak trochu odli≈°n√Ω od stƒ∫pcov ako `Month`, preto≈æe obsahuje nenumerick√© hodnoty. Tak√©to stƒ∫pce sa naz√Ωvaj√∫ **kategorick√©**.

[![ML pre zaƒçiatoƒçn√≠kov - Predikcia kategorick√Ωch prvkov pomocou line√°rnej regresie](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pre zaƒçiatoƒçn√≠kov - Predikcia kategorick√Ωch prvkov pomocou line√°rnej regresie")

> üé• Kliknite na obr√°zok vy≈°≈°ie pre kr√°tky video prehƒæad o pou≈æ√≠van√≠ kategorick√Ωch prvkov.

Tu m√¥≈æete vidie≈•, ako priemern√° cena z√°vis√≠ od odrody:

<img alt="Priemern√° cena podƒæa odrody" src="images/price-by-variety.png" width="50%" />

Aby sme zohƒæadnili odrodu, mus√≠me ju najsk√¥r previes≈• na numerick√∫ formu, alebo ju **zak√≥dova≈•**. Existuje niekoƒæko sp√¥sobov, ako to m√¥≈æeme urobi≈•:

* Jednoduch√© **numerick√© k√≥dovanie** vytvor√≠ tabuƒæku r√¥znych odr√¥d a potom nahrad√≠ n√°zov odrody indexom v tejto tabuƒæke. Toto nie je najlep≈°√≠ n√°pad pre line√°rnu regresiu, preto≈æe line√°rna regresia berie skutoƒçn√∫ numerick√∫ hodnotu indexu a prid√°va ju k v√Ωsledku, n√°sobiac ju nejak√Ωm koeficientom. V na≈°om pr√≠pade je vz≈•ah medzi ƒç√≠slom indexu a cenou jasne neline√°rny, aj keƒè zabezpeƒç√≠me, ≈æe indexy s√∫ usporiadan√© urƒçit√Ωm sp√¥sobom.
* **One-hot k√≥dovanie** nahrad√≠ stƒ∫pec `Variety` ≈°tyrmi r√¥znymi stƒ∫pcami, jeden pre ka≈æd√∫ odrodu. Ka≈æd√Ω stƒ∫pec bude obsahova≈• `1`, ak pr√≠slu≈°n√Ω riadok patr√≠ danej odrode, a `0` inak. To znamen√°, ≈æe v line√°rnej regresii bud√∫ ≈°tyri koeficienty, jeden pre ka≈æd√∫ odrodu tekv√≠c, zodpovedn√© za "v√Ωchodiskov√∫ cenu" (alebo sk√¥r "dodatoƒçn√∫ cenu") pre dan√∫ odrodu.

Ni≈æ≈°ie uveden√Ω k√≥d ukazuje, ako m√¥≈æeme zak√≥dova≈• odrodu pomocou one-hot k√≥dovania:

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

Na tr√©novanie line√°rnej regresie pomocou one-hot zak√≥dovanej odrody ako vstupu staƒç√≠ spr√°vne inicializova≈• √∫daje `X` a `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

Zvy≈°ok k√≥du je rovnak√Ω ako ten, ktor√Ω sme pou≈æili vy≈°≈°ie na tr√©novanie line√°rnej regresie. Ak to vysk√∫≈°ate, uvid√≠te, ≈æe stredn√° kvadratick√° chyba je pribli≈æne rovnak√°, ale z√≠skame oveƒæa vy≈°≈°√≠ koeficient determin√°cie (~77 %). Na z√≠skanie e≈°te presnej≈°√≠ch predpoved√≠ m√¥≈æeme zohƒæadni≈• viac kategorick√Ωch prvkov, ako aj numerick√© prvky, ako `Month` alebo `DayOfYear`. Na z√≠skanie jedn√©ho veƒæk√©ho poƒæa prvkov m√¥≈æeme pou≈æi≈• `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

Tu tie≈æ zohƒæad≈àujeme `City` a typ balenia `Package`, ƒço n√°m d√°va MSE 2.84 (10 %) a determin√°ciu 0.94!

## Spojenie v≈°etk√©ho dohromady

Na vytvorenie najlep≈°ieho modelu m√¥≈æeme pou≈æi≈• kombinovan√© (one-hot zak√≥dovan√© kategorick√© + numerick√©) √∫daje z vy≈°≈°ie uveden√©ho pr√≠kladu spolu s polynomickou regresiou. Tu je kompletn√Ω k√≥d pre va≈°e pohodlie:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Toto by n√°m malo da≈• najlep≈°√≠ koeficient determin√°cie takmer 97 % a MSE=2.23 (~8 % predikƒçn√° chyba).

| Model | MSE | Determin√°cia |  
|-------|-----|-------------|  
| `DayOfYear` Line√°rny | 2.77 (17.2 %) | 0.07 |  
| `DayOfYear` Polynomick√Ω | 2.73 (17.0 %) | 0.08 |  
| `Variety` Line√°rny | 5.24 (19.7 %) | 0.77 |  
| V≈°etky prvky Line√°rny | 2.84 (10.5 %) | 0.94 |  
| V≈°etky prvky Polynomick√Ω | 2.23 (8.25 %) | 0.97 |  

üèÜ V√Ωborne! Vytvorili ste ≈°tyri regresn√© modely v jednej lekcii a zlep≈°ili kvalitu modelu na 97 %. V poslednej ƒçasti o regresii sa nauƒç√≠te o logistickej regresii na urƒçenie kateg√≥ri√≠.

---

## üöÄV√Ωzva

Otestujte niekoƒæko r√¥znych premenn√Ωch v tomto notebooku, aby ste videli, ako korel√°cia zodpoved√° presnosti modelu.

## [Kv√≠z po predn√°≈°ke](https://ff-quizzes.netlify.app/en/ml/)

## Prehƒæad a samo≈°t√∫dium

V tejto lekcii sme sa nauƒçili o line√°rnej regresii. Existuj√∫ aj ƒèal≈°ie d√¥le≈æit√© typy regresie. Preƒç√≠tajte si o technik√°ch Stepwise, Ridge, Lasso a Elasticnet. Dobr√Ω kurz na ≈°t√∫dium je [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Zadanie

[Postavte model](assignment.md)

---

**Upozornenie**:  
Tento dokument bol prelo≈æen√Ω pomocou slu≈æby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa sna≈æ√≠me o presnos≈•, pros√≠m, berte na vedomie, ≈æe automatizovan√© preklady m√¥≈æu obsahova≈• chyby alebo nepresnosti. P√¥vodn√Ω dokument v jeho rodnom jazyku by mal by≈• pova≈æovan√Ω za autoritat√≠vny zdroj. Pre kritick√© inform√°cie sa odpor√∫ƒça profesion√°lny ƒæudsk√Ω preklad. Nie sme zodpovedn√≠ za ≈æiadne nedorozumenia alebo nespr√°vne interpret√°cie vypl√Ωvaj√∫ce z pou≈æitia tohto prekladu.