<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T15:11:10+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "sk"
}
-->
# Vytvorenie regresnÃ©ho modelu pomocou Scikit-learn: Å¡tyri spÃ´soby regresie

![Infografika lineÃ¡rna vs polynomiÃ¡lna regresia](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [KvÃ­z pred prednÃ¡Å¡kou](https://ff-quizzes.netlify.app/en/ml/)

> ### [TÃ¡to lekcia je dostupnÃ¡ v R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Ãšvod 

Doteraz ste preskÃºmali, Äo je regresia, na vzorovÃ½ch Ãºdajoch zo sÃºboru Ãºdajov o cenÃ¡ch tekvÃ­c, ktorÃ½ budeme pouÅ¾Ã­vaÅ¥ poÄas celej tejto lekcie. Vizualizovali ste ich pomocou Matplotlibu.

Teraz ste pripravenÃ­ ponoriÅ¥ sa hlbÅ¡ie do regresie pre strojovÃ© uÄenie. ZatiaÄ¾ Äo vizualizÃ¡cia vÃ¡m umoÅ¾Åˆuje pochopiÅ¥ Ãºdaje, skutoÄnÃ¡ sila strojovÃ©ho uÄenia spoÄÃ­va v _trÃ©ningu modelov_. Modely sÃº trÃ©novanÃ© na historickÃ½ch Ãºdajoch, aby automaticky zachytili zÃ¡vislosti medzi Ãºdajmi, a umoÅ¾ÅˆujÃº vÃ¡m predpovedaÅ¥ vÃ½sledky pre novÃ© Ãºdaje, ktorÃ© model predtÃ½m nevidel.

V tejto lekcii sa dozviete viac o dvoch typoch regresie: _zÃ¡kladnÃ¡ lineÃ¡rna regresia_ a _polynomiÃ¡lna regresia_, spolu s niektorÃ½mi matematickÃ½mi zÃ¡kladmi tÃ½chto technÃ­k. Tieto modely nÃ¡m umoÅ¾nia predpovedaÅ¥ ceny tekvÃ­c na zÃ¡klade rÃ´znych vstupnÃ½ch Ãºdajov.

[![ML pre zaÄiatoÄnÃ­kov - Pochopenie lineÃ¡rnej regresie](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pre zaÄiatoÄnÃ­kov - Pochopenie lineÃ¡rnej regresie")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o lineÃ¡rnej regresii.

> PoÄas celÃ©ho kurzu predpokladÃ¡me minimÃ¡lne znalosti matematiky a snaÅ¾Ã­me sa sprÃ­stupniÅ¥ obsah Å¡tudentom z inÃ½ch odborov, preto sledujte poznÃ¡mky, ğŸ§® vÃ½poÄty, diagramy a ÄalÅ¡ie nÃ¡stroje na uÄenie, ktorÃ© vÃ¡m pomÃ´Å¾u pochopiÅ¥ obsah.

### Predpoklady

Teraz by ste mali byÅ¥ oboznÃ¡menÃ­ so Å¡truktÃºrou Ãºdajov o tekviciach, ktorÃ© skÃºmame. NÃ¡jdete ich prednahranÃ© a predÄistenÃ© v sÃºbore _notebook.ipynb_ tejto lekcie. V sÃºbore je cena tekvÃ­c zobrazenÃ¡ za buÅ¡el v novom dÃ¡tovom rÃ¡mci. Uistite sa, Å¾e dokÃ¡Å¾ete spustiÅ¥ tieto notebooky v jadrÃ¡ch vo Visual Studio Code.

### PrÃ­prava

PripomeÅˆme si, Å¾e tieto Ãºdaje naÄÃ­tavate, aby ste mohli klÃ¡sÅ¥ otÃ¡zky:

- Kedy je najlepÅ¡Ã­ Äas na kÃºpu tekvÃ­c? 
- AkÃº cenu mÃ´Å¾em oÄakÃ¡vaÅ¥ za balenie miniatÃºrnych tekvÃ­c?
- MÃ¡m ich kÃºpiÅ¥ v poloviÄnÃ½ch buÅ¡lovÃ½ch koÅ¡och alebo v 1 1/9 buÅ¡lovÃ½ch Å¡katuliach?
PoÄme sa hlbÅ¡ie pozrieÅ¥ na tieto Ãºdaje.

V predchÃ¡dzajÃºcej lekcii ste vytvorili Pandas dÃ¡tovÃ½ rÃ¡mec a naplnili ho ÄasÅ¥ou pÃ´vodnÃ©ho sÃºboru Ãºdajov, Å¡tandardizujÃºc ceny podÄ¾a buÅ¡lu. TÃ½mto spÃ´sobom ste vÅ¡ak dokÃ¡zali zhromaÅ¾diÅ¥ iba pribliÅ¾ne 400 dÃ¡tovÃ½ch bodov a iba pre jesennÃ© mesiace.

Pozrite sa na Ãºdaje, ktorÃ© sme prednahrali v notebooku tejto lekcie. Ãšdaje sÃº prednahranÃ© a ÃºvodnÃ½ bodovÃ½ graf je vytvorenÃ½ na zobrazenie Ãºdajov podÄ¾a mesiacov. MoÅ¾no mÃ´Å¾eme zÃ­skaÅ¥ trochu viac detailov o povahe Ãºdajov ich ÄalÅ¡Ã­m ÄistenÃ­m.

## LineÃ¡rna regresnÃ¡ Äiara

Ako ste sa nauÄili v Lekcii 1, cieÄ¾om cviÄenia lineÃ¡rnej regresie je byÅ¥ schopnÃ½ nakresliÅ¥ Äiaru na:

- **UkÃ¡zanie vzÅ¥ahov medzi premennÃ½mi**. UkÃ¡zaÅ¥ vzÅ¥ah medzi premennÃ½mi
- **Predpovedanie**. UrobiÅ¥ presnÃ© predpovede, kde by novÃ½ dÃ¡tovÃ½ bod spadol vo vzÅ¥ahu k tejto Äiare. 
 
TypickÃ© pre **regresiu metÃ³dou najmenÅ¡Ã­ch Å¡tvorcov** je nakresliÅ¥ tento typ Äiary. TermÃ­n 'najmenÅ¡ie Å¡tvorce' znamenÃ¡, Å¾e vÅ¡etky dÃ¡tovÃ© body obklopujÃºce regresnÃº Äiaru sÃº umocnenÃ© na druhÃº a potom sÄÃ­tanÃ©. IdeÃ¡lne je, aby tento koneÄnÃ½ sÃºÄet bol Äo najmenÅ¡Ã­, pretoÅ¾e chceme nÃ­zky poÄet chÃ½b, alebo `najmenÅ¡ie Å¡tvorce`. 

RobÃ­me to, pretoÅ¾e chceme modelovaÅ¥ Äiaru, ktorÃ¡ mÃ¡ najmenÅ¡iu kumulatÃ­vnu vzdialenosÅ¥ od vÅ¡etkÃ½ch naÅ¡ich dÃ¡tovÃ½ch bodov. TieÅ¾ umocÅˆujeme hodnoty na druhÃº pred ich sÄÃ­tanÃ­m, pretoÅ¾e nÃ¡s zaujÃ­ma ich veÄ¾kosÅ¥, nie ich smer.

> **ğŸ§® UkÃ¡Å¾te mi matematiku** 
> 
> TÃ¡to Äiara, nazÃ½vanÃ¡ _Äiara najlepÅ¡ieho prispÃ´sobenia_, mÃ´Å¾e byÅ¥ vyjadrenÃ¡ [rovnicou](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'vysvetÄ¾ujÃºca premennÃ¡'. `Y` je 'zÃ¡vislÃ¡ premennÃ¡'. Sklon Äiary je `b` a `a` je y-prieseÄnÃ­k, ktorÃ½ odkazuje na hodnotu `Y`, keÄ `X = 0`. 
>
>![vÃ½poÄet sklonu](../../../../2-Regression/3-Linear/images/slope.png)
>
> Najprv vypoÄÃ­tajte sklon `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> InÃ½mi slovami, a odkazujÃºc na pÃ´vodnÃº otÃ¡zku o Ãºdajoch o tekviciach: "predpovedajte cenu tekvice za buÅ¡el podÄ¾a mesiaca", `X` by odkazovalo na cenu a `Y` by odkazovalo na mesiac predaja. 
>
>![dokonÄenie rovnice](../../../../2-Regression/3-Linear/images/calculation.png)
>
> VypoÄÃ­tajte hodnotu Y. Ak platÃ­te okolo $4, musÃ­ byÅ¥ aprÃ­l! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika, ktorÃ¡ vypoÄÃ­tava Äiaru, musÃ­ demonÅ¡trovaÅ¥ sklon Äiary, ktorÃ½ tieÅ¾ zÃ¡visÃ­ od prieseÄnÃ­ka, alebo kde sa `Y` nachÃ¡dza, keÄ `X = 0`.
>
> MetÃ³du vÃ½poÄtu tÃ½chto hodnÃ´t si mÃ´Å¾ete pozrieÅ¥ na webovej strÃ¡nke [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). NavÅ¡tÃ­vte tieÅ¾ [tento kalkulÃ¡tor metÃ³dy najmenÅ¡Ã­ch Å¡tvorcov](https://www.mathsisfun.com/data/least-squares-calculator.html), aby ste videli, ako hodnoty ÄÃ­sel ovplyvÅˆujÃº Äiaru.

## KorelÃ¡cia

ÄalÅ¡Ã­ termÃ­n, ktorÃ½ je potrebnÃ© pochopiÅ¥, je **koeficient korelÃ¡cie** medzi danÃ½mi premennÃ½mi X a Y. Pomocou bodovÃ©ho grafu mÃ´Å¾ete rÃ½chlo vizualizovaÅ¥ tento koeficient. Graf s dÃ¡tovÃ½mi bodmi rozmiestnenÃ½mi v ÃºhÄ¾adnej Äiare mÃ¡ vysokÃº korelÃ¡ciu, ale graf s dÃ¡tovÃ½mi bodmi rozmiestnenÃ½mi vÅ¡ade medzi X a Y mÃ¡ nÃ­zku korelÃ¡ciu.

DobrÃ½ model lineÃ¡rnej regresie bude takÃ½, ktorÃ½ mÃ¡ vysokÃ½ (bliÅ¾Å¡ie k 1 ako k 0) koeficient korelÃ¡cie pomocou metÃ³dy najmenÅ¡Ã­ch Å¡tvorcov s regresnou Äiarou.

âœ… Spustite notebook, ktorÃ½ sprevÃ¡dza tÃºto lekciu, a pozrite sa na bodovÃ½ graf Mesiac vs Cena. ZdÃ¡ sa, Å¾e Ãºdaje spÃ¡jajÃºce Mesiac s Cenou za predaj tekvÃ­c majÃº podÄ¾a vaÅ¡ej vizuÃ¡lnej interpretÃ¡cie bodovÃ©ho grafu vysokÃº alebo nÃ­zku korelÃ¡ciu? ZmenÃ­ sa to, ak pouÅ¾ijete jemnejÅ¡ie meranie namiesto `Mesiac`, napr. *deÅˆ v roku* (t. j. poÄet dnÃ­ od zaÄiatku roka)?

V nasledujÃºcom kÃ³de predpokladÃ¡me, Å¾e sme vyÄistili Ãºdaje a zÃ­skali dÃ¡tovÃ½ rÃ¡mec nazvanÃ½ `new_pumpkins`, podobnÃ½ nasledujÃºcemu:

ID | Mesiac | DeÅˆVroku | Typ | Mesto | Balenie | NÃ­zka cena | VysokÃ¡ cena | Cena
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡lovÃ© kartÃ³ny | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡lovÃ© kartÃ³ny | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡lovÃ© kartÃ³ny | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡lovÃ© kartÃ³ny | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 buÅ¡lovÃ© kartÃ³ny | 15.0 | 15.0 | 13.636364

> KÃ³d na Äistenie Ãºdajov je dostupnÃ½ v [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Vykonali sme rovnakÃ© kroky Äistenia ako v predchÃ¡dzajÃºcej lekcii a vypoÄÃ­tali sme stÄºpec `DeÅˆVroku` pomocou nasledujÃºceho vÃ½razu: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Teraz, keÄ rozumiete matematike za lineÃ¡rnou regresiou, poÄme vytvoriÅ¥ regresnÃ½ model, aby sme zistili, ktorÃ½ balÃ­k tekvÃ­c bude maÅ¥ najlepÅ¡ie ceny tekvÃ­c. Niekto, kto kupuje tekvice na sviatoÄnÃº tekvicovÃº zÃ¡hradu, by mohol chcieÅ¥ tieto informÃ¡cie, aby optimalizoval svoje nÃ¡kupy balÃ­kov tekvÃ­c pre zÃ¡hradu.

## HÄ¾adanie korelÃ¡cie

[![ML pre zaÄiatoÄnÃ­kov - HÄ¾adanie korelÃ¡cie: KÄ¾ÃºÄ k lineÃ¡rnej regresii](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pre zaÄiatoÄnÃ­kov - HÄ¾adanie korelÃ¡cie: KÄ¾ÃºÄ k lineÃ¡rnej regresii")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o korelÃ¡cii.

Z predchÃ¡dzajÃºcej lekcie ste pravdepodobne videli, Å¾e priemernÃ¡ cena za rÃ´zne mesiace vyzerÃ¡ takto:

<img alt="PriemernÃ¡ cena podÄ¾a mesiaca" src="../2-Data/images/barchart.png" width="50%"/>

To naznaÄuje, Å¾e by mala existovaÅ¥ nejakÃ¡ korelÃ¡cia, a mÃ´Å¾eme skÃºsiÅ¥ trÃ©novaÅ¥ model lineÃ¡rnej regresie na predpovedanie vzÅ¥ahu medzi `Mesiac` a `Cena`, alebo medzi `DeÅˆVroku` a `Cena`. Tu je bodovÃ½ graf, ktorÃ½ ukazuje druhÃ½ vzÅ¥ah:

<img alt="BodovÃ½ graf Cena vs. DeÅˆ v roku" src="images/scatter-dayofyear.png" width="50%" /> 

Pozrime sa, Äi existuje korelÃ¡cia pomocou funkcie `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

ZdÃ¡ sa, Å¾e korelÃ¡cia je pomerne malÃ¡, -0.15 podÄ¾a `Mesiac` a -0.17 podÄ¾a `DeÅˆVroku`, ale mohol by existovaÅ¥ inÃ½ dÃ´leÅ¾itÃ½ vzÅ¥ah. ZdÃ¡ sa, Å¾e existujÃº rÃ´zne zhluky cien zodpovedajÃºce rÃ´znym odrodÃ¡m tekvÃ­c. Na potvrdenie tejto hypotÃ©zy nakreslime kaÅ¾dÃº kategÃ³riu tekvÃ­c pomocou inej farby. Pri prechode parametra `ax` do funkcie `scatter` mÃ´Å¾eme nakresliÅ¥ vÅ¡etky body na rovnakÃ½ graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="BodovÃ½ graf Cena vs. DeÅˆ v roku" src="images/scatter-dayofyear-color.png" width="50%" /> 

NaÅ¡e vyÅ¡etrovanie naznaÄuje, Å¾e odroda mÃ¡ vÃ¤ÄÅ¡Ã­ vplyv na celkovÃº cenu ako skutoÄnÃ½ dÃ¡tum predaja. MÃ´Å¾eme to vidieÅ¥ na stÄºpcovom grafe:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="StÄºpcovÃ½ graf cena vs odroda" src="images/price-by-variety.png" width="50%" /> 

Zamerajme sa na chvÃ­Ä¾u iba na jednu odrodu tekvÃ­c, 'pie type', a pozrime sa, akÃ½ vplyv mÃ¡ dÃ¡tum na cenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="BodovÃ½ graf Cena vs. DeÅˆ v roku" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Ak teraz vypoÄÃ­tame korelÃ¡ciu medzi `Cena` a `DeÅˆVroku` pomocou funkcie `corr`, dostaneme hodnotu okolo `-0.27` - Äo znamenÃ¡, Å¾e trÃ©novanie prediktÃ­vneho modelu mÃ¡ zmysel.

> Pred trÃ©novanÃ­m modelu lineÃ¡rnej regresie je dÃ´leÅ¾itÃ© zabezpeÄiÅ¥, aby naÅ¡e Ãºdaje boli ÄistÃ©. LineÃ¡rna regresia nefunguje dobre s chÃ½bajÃºcimi hodnotami, preto mÃ¡ zmysel zbaviÅ¥ sa vÅ¡etkÃ½ch prÃ¡zdnych buniek:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

ÄalÅ¡Ã­m prÃ­stupom by bolo vyplniÅ¥ tieto prÃ¡zdne hodnoty priemernÃ½mi hodnotami z prÃ­sluÅ¡nÃ©ho stÄºpca.

## JednoduchÃ¡ lineÃ¡rna regresia

[![ML pre zaÄiatoÄnÃ­kov - LineÃ¡rna a polynomiÃ¡lna regresia pomocou Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pre zaÄiatoÄnÃ­kov - LineÃ¡rna a polynomiÃ¡lna regresia pomocou Scikit-learn")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o lineÃ¡rnej a polynomiÃ¡lnej regresii.

Na trÃ©novanie nÃ¡Å¡ho modelu lineÃ¡rnej regresie pouÅ¾ijeme kniÅ¾nicu **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZaÄÃ­name oddelenÃ­m vstupnÃ½ch hodnÃ´t (prÃ­znakov) a oÄakÃ¡vanÃ©ho vÃ½stupu (oznaÄenia) do samostatnÃ½ch numpy polÃ­:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> VÅ¡imnite si, Å¾e sme museli vykonaÅ¥ `reshape` na vstupnÃ½ch Ãºdajoch, aby ich balÃ­k lineÃ¡rnej regresie sprÃ¡vne pochopil. LineÃ¡rna regresia oÄakÃ¡va 2D pole ako vstup, kde kaÅ¾dÃ½ riadok poÄ¾a zodpovedÃ¡ vektoru vstupnÃ½ch prÃ­znakov. V naÅ¡om prÃ­pade, keÄÅ¾e mÃ¡me iba jeden vstup, potrebujeme pole s tvarom NÃ—1, kde N je veÄ¾kosÅ¥ sÃºboru Ãºdajov.

Potom musÃ­me rozdeliÅ¥ Ãºdaje na trÃ©novacÃ­ a testovacÃ­ sÃºbor Ãºdajov, aby sme mohli po trÃ©ningu overiÅ¥ nÃ¡Å¡ model:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Nakoniec, samotnÃ© trÃ©novanie modelu lineÃ¡rnej regresie trvÃ¡ iba dva riadky kÃ³du. Definujeme objekt `LinearRegression` a prispÃ´sobÃ­me ho naÅ¡im Ãºdajom pomocou metÃ³dy `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po prispÃ´sobenÃ­ obsahuje vÅ¡etky koeficienty regresie, ku ktorÃ½m je moÅ¾nÃ© pristupovaÅ¥ pomocou vlastnosti `.coef_`. V naÅ¡om prÃ­pade existuje iba jeden koeficient, ktorÃ½ by mal byÅ¥ okolo `-0.017`. To znamenÃ¡, Å¾e ceny sa zdajÃº klesaÅ¥ trochu s Äasom, ale nie prÃ­liÅ¡, pribliÅ¾ne o 2 centy za deÅˆ. MÃ´Å¾eme tieÅ¾ pristupovaÅ¥ k prieseÄnÃ­ku regresie s Y-osou pomocou `lin_reg.intercept_` - bude to okolo `21` v naÅ¡om prÃ­pade, Äo naznaÄuje cenu na zaÄiatku roka.

Aby sme videli, akÃ½ presnÃ½ je nÃ¡Å¡ model, mÃ´Å¾eme predpovedaÅ¥ ceny na testovacom sÃºbore Ãºdajov a potom zmeraÅ¥, ako blÃ­zko sÃº naÅ¡e predpovede k oÄakÃ¡vanÃ½m hodnotÃ¡m. To sa dÃ¡ urobiÅ¥ pomocou metriky strednej kvadratickej chyby (MSE), ktorÃ¡ je priemerom vÅ¡etkÃ½ch kvadratickÃ½ch rozdielov medzi oÄakÃ¡vanou a predpovedanou hodnotou.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
NaÅ¡a chyba sa zdÃ¡ byÅ¥ okolo 2 bodov, Äo je ~17 %. Nie je to prÃ­liÅ¡ dobrÃ©. ÄalÅ¡Ã­m indikÃ¡torom kvality modelu je **koeficient determinÃ¡cie**, ktorÃ½ mÃ´Å¾eme zÃ­skaÅ¥ takto:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Ak je hodnota 0, znamenÃ¡ to, Å¾e model neberie do Ãºvahy vstupnÃ© Ãºdaje a funguje ako *najhorÅ¡Ã­ lineÃ¡rny prediktor*, Äo je jednoducho priemernÃ¡ hodnota vÃ½sledku. Hodnota 1 znamenÃ¡, Å¾e mÃ´Å¾eme dokonale predpovedaÅ¥ vÅ¡etky oÄakÃ¡vanÃ© vÃ½stupy. V naÅ¡om prÃ­pade je koeficient okolo 0.06, Äo je pomerne nÃ­zke.

MÃ´Å¾eme tieÅ¾ vykresliÅ¥ testovacie Ãºdaje spolu s regresnou Äiarou, aby sme lepÅ¡ie videli, ako regresia funguje v naÅ¡om prÃ­pade:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="LineÃ¡rna regresia" src="images/linear-results.png" width="50%" />

## PolynomickÃ¡ regresia

ÄalÅ¡Ã­m typom lineÃ¡rnej regresie je polynomickÃ¡ regresia. ZatiaÄ¾ Äo niekedy existuje lineÃ¡rny vzÅ¥ah medzi premennÃ½mi â€“ ÄÃ­m vÃ¤ÄÅ¡Ã­ objem tekvice, tÃ½m vyÅ¡Å¡ia cena â€“ niekedy tieto vzÅ¥ahy nemoÅ¾no vykresliÅ¥ ako rovinu alebo priamku.

âœ… Tu sÃº [niektorÃ© ÄalÅ¡ie prÃ­klady](https://online.stat.psu.edu/stat501/lesson/9/9.8) Ãºdajov, ktorÃ© by mohli vyuÅ¾iÅ¥ polynomickÃº regresiu.

Pozrite sa znova na vzÅ¥ah medzi dÃ¡tumom a cenou. ZdÃ¡ sa, Å¾e tento bodovÃ½ graf by mal byÅ¥ nevyhnutne analyzovanÃ½ priamkou? NemÃ´Å¾u ceny kolÃ­saÅ¥? V tomto prÃ­pade mÃ´Å¾ete vyskÃºÅ¡aÅ¥ polynomickÃº regresiu.

âœ… PolynÃ³my sÃº matematickÃ© vÃ½razy, ktorÃ© mÃ´Å¾u pozostÃ¡vaÅ¥ z jednej alebo viacerÃ½ch premennÃ½ch a koeficientov.

PolynomickÃ¡ regresia vytvÃ¡ra zakrivenÃº Äiaru, ktorÃ¡ lepÅ¡ie zodpovedÃ¡ nelineÃ¡rnym Ãºdajom. V naÅ¡om prÃ­pade, ak do vstupnÃ½ch Ãºdajov zahrnieme premennÃº `DayOfYear` na druhÃº, mali by sme byÅ¥ schopnÃ­ prispÃ´sobiÅ¥ naÅ¡e Ãºdaje parabolickou krivkou, ktorÃ¡ bude maÅ¥ minimum v urÄitom bode poÄas roka.

Scikit-learn obsahuje uÅ¾itoÄnÃ© [API pre pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), ktorÃ© kombinuje rÃ´zne kroky spracovania Ãºdajov. **Pipeline** je reÅ¥az **odhadovateÄ¾ov**. V naÅ¡om prÃ­pade vytvorÃ­me pipeline, ktorÃ¡ najskÃ´r pridÃ¡ polynomickÃ© prvky do nÃ¡Å¡ho modelu a potom trÃ©nuje regresiu:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

PouÅ¾itie `PolynomialFeatures(2)` znamenÃ¡, Å¾e zahrnieme vÅ¡etky polynÃ³my druhÃ©ho stupÅˆa zo vstupnÃ½ch Ãºdajov. V naÅ¡om prÃ­pade to bude znamenaÅ¥ len `DayOfYear`<sup>2</sup>, ale pri dvoch vstupnÃ½ch premennÃ½ch X a Y to pridÃ¡ X<sup>2</sup>, XY a Y<sup>2</sup>. MÃ´Å¾eme tieÅ¾ pouÅ¾iÅ¥ polynÃ³my vyÅ¡Å¡ieho stupÅˆa, ak chceme.

Pipeline mÃ´Å¾eme pouÅ¾Ã­vaÅ¥ rovnakÃ½m spÃ´sobom ako pÃ´vodnÃ½ objekt `LinearRegression`, t.j. mÃ´Å¾eme pipeline `fit` a potom pouÅ¾iÅ¥ `predict` na zÃ­skanie vÃ½sledkov predikcie. Tu je graf zobrazujÃºci testovacie Ãºdaje a aproximaÄnÃº krivku:

<img alt="PolynomickÃ¡ regresia" src="images/poly-results.png" width="50%" />

PouÅ¾itÃ­m polynomickej regresie mÃ´Å¾eme dosiahnuÅ¥ mierne niÅ¾Å¡ie MSE a vyÅ¡Å¡iu determinÃ¡ciu, ale nie vÃ½znamne. MusÃ­me zohÄ¾adniÅ¥ ÄalÅ¡ie prvky!

> VidÃ­te, Å¾e minimÃ¡lne ceny tekvÃ­c sÃº pozorovanÃ© niekde okolo Halloweenu. Ako to mÃ´Å¾ete vysvetliÅ¥?

ğŸƒ Gratulujeme, prÃ¡ve ste vytvorili model, ktorÃ½ mÃ´Å¾e pomÃ´cÅ¥ predpovedaÅ¥ cenu tekvÃ­c na kolÃ¡Äe. Pravdepodobne mÃ´Å¾ete zopakovaÅ¥ rovnakÃ½ postup pre vÅ¡etky typy tekvÃ­c, ale to by bolo zdÄºhavÃ©. PoÄme sa teraz nauÄiÅ¥, ako zohÄ¾adniÅ¥ odrodu tekvÃ­c v naÅ¡om modeli!

## KategorickÃ© prvky

V ideÃ¡lnom svete chceme byÅ¥ schopnÃ­ predpovedaÅ¥ ceny pre rÃ´zne odrody tekvÃ­c pomocou rovnakÃ©ho modelu. StÄºpec `Variety` je vÅ¡ak trochu odliÅ¡nÃ½ od stÄºpcov ako `Month`, pretoÅ¾e obsahuje nenumerickÃ© hodnoty. TakÃ©to stÄºpce sa nazÃ½vajÃº **kategorickÃ©**.

[![ML pre zaÄiatoÄnÃ­kov - Predikcia kategorickÃ½ch prvkov pomocou lineÃ¡rnej regresie](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pre zaÄiatoÄnÃ­kov - Predikcia kategorickÃ½ch prvkov pomocou lineÃ¡rnej regresie")

> ğŸ¥ Kliknite na obrÃ¡zok vyÅ¡Å¡ie pre krÃ¡tky video prehÄ¾ad o pouÅ¾Ã­vanÃ­ kategorickÃ½ch prvkov.

Tu mÃ´Å¾ete vidieÅ¥, ako priemernÃ¡ cena zÃ¡visÃ­ od odrody:

<img alt="PriemernÃ¡ cena podÄ¾a odrody" src="images/price-by-variety.png" width="50%" />

Aby sme zohÄ¾adnili odrodu, musÃ­me ju najskÃ´r previesÅ¥ na numerickÃº formu, alebo ju **zakÃ³dovaÅ¥**. Existuje niekoÄ¾ko spÃ´sobov, ako to mÃ´Å¾eme urobiÅ¥:

* JednoduchÃ© **numerickÃ© kÃ³dovanie** vytvorÃ­ tabuÄ¾ku rÃ´znych odrÃ´d a potom nahradÃ­ nÃ¡zov odrody indexom v tejto tabuÄ¾ke. Toto nie je najlepÅ¡Ã­ nÃ¡pad pre lineÃ¡rnu regresiu, pretoÅ¾e lineÃ¡rna regresia berie skutoÄnÃº numerickÃº hodnotu indexu a pridÃ¡va ju k vÃ½sledku, nÃ¡sobiac ju nejakÃ½m koeficientom. V naÅ¡om prÃ­pade je vzÅ¥ah medzi ÄÃ­slom indexu a cenou jasne nelineÃ¡rny, aj keÄ zabezpeÄÃ­me, Å¾e indexy sÃº usporiadanÃ© urÄitÃ½m spÃ´sobom.
* **One-hot kÃ³dovanie** nahradÃ­ stÄºpec `Variety` Å¡tyrmi rÃ´znymi stÄºpcami, jeden pre kaÅ¾dÃº odrodu. KaÅ¾dÃ½ stÄºpec bude obsahovaÅ¥ `1`, ak prÃ­sluÅ¡nÃ½ riadok patrÃ­ danej odrode, a `0` inak. To znamenÃ¡, Å¾e v lineÃ¡rnej regresii budÃº Å¡tyri koeficienty, jeden pre kaÅ¾dÃº odrodu tekvÃ­c, zodpovednÃ© za "vÃ½chodiskovÃº cenu" (alebo skÃ´r "dodatoÄnÃº cenu") pre danÃº odrodu.

NiÅ¾Å¡ie uvedenÃ½ kÃ³d ukazuje, ako mÃ´Å¾eme zakÃ³dovaÅ¥ odrodu pomocou one-hot kÃ³dovania:

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

Na trÃ©novanie lineÃ¡rnej regresie pomocou one-hot zakÃ³dovanej odrody ako vstupu staÄÃ­ sprÃ¡vne inicializovaÅ¥ Ãºdaje `X` a `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

ZvyÅ¡ok kÃ³du je rovnakÃ½ ako ten, ktorÃ½ sme pouÅ¾ili vyÅ¡Å¡ie na trÃ©novanie lineÃ¡rnej regresie. Ak to vyskÃºÅ¡ate, uvidÃ­te, Å¾e strednÃ¡ kvadratickÃ¡ chyba je pribliÅ¾ne rovnakÃ¡, ale zÃ­skame oveÄ¾a vyÅ¡Å¡Ã­ koeficient determinÃ¡cie (~77 %). Na zÃ­skanie eÅ¡te presnejÅ¡Ã­ch predpovedÃ­ mÃ´Å¾eme zohÄ¾adniÅ¥ viac kategorickÃ½ch prvkov, ako aj numerickÃ© prvky, ako `Month` alebo `DayOfYear`. Na zÃ­skanie jednÃ©ho veÄ¾kÃ©ho poÄ¾a prvkov mÃ´Å¾eme pouÅ¾iÅ¥ `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

Tu tieÅ¾ zohÄ¾adÅˆujeme `City` a typ balenia `Package`, Äo nÃ¡m dÃ¡va MSE 2.84 (10 %) a determinÃ¡ciu 0.94!

## Spojenie vÅ¡etkÃ©ho dohromady

Na vytvorenie najlepÅ¡ieho modelu mÃ´Å¾eme pouÅ¾iÅ¥ kombinovanÃ© (one-hot zakÃ³dovanÃ© kategorickÃ© + numerickÃ©) Ãºdaje z vyÅ¡Å¡ie uvedenÃ©ho prÃ­kladu spolu s polynomickou regresiou. Tu je kompletnÃ½ kÃ³d pre vaÅ¡e pohodlie:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Toto by nÃ¡m malo daÅ¥ najlepÅ¡Ã­ koeficient determinÃ¡cie takmer 97 % a MSE=2.23 (~8 % predikÄnÃ¡ chyba).

| Model | MSE | DeterminÃ¡cia |  
|-------|-----|-------------|  
| `DayOfYear` LineÃ¡rny | 2.77 (17.2 %) | 0.07 |  
| `DayOfYear` PolynomickÃ½ | 2.73 (17.0 %) | 0.08 |  
| `Variety` LineÃ¡rny | 5.24 (19.7 %) | 0.77 |  
| VÅ¡etky prvky LineÃ¡rny | 2.84 (10.5 %) | 0.94 |  
| VÅ¡etky prvky PolynomickÃ½ | 2.23 (8.25 %) | 0.97 |  

ğŸ† VÃ½borne! Vytvorili ste Å¡tyri regresnÃ© modely v jednej lekcii a zlepÅ¡ili kvalitu modelu na 97 %. V poslednej Äasti o regresii sa nauÄÃ­te o logistickej regresii na urÄenie kategÃ³riÃ­.

---

## ğŸš€VÃ½zva

Otestujte niekoÄ¾ko rÃ´znych premennÃ½ch v tomto notebooku, aby ste videli, ako korelÃ¡cia zodpovedÃ¡ presnosti modelu.

## [KvÃ­z po prednÃ¡Å¡ke](https://ff-quizzes.netlify.app/en/ml/)

## PrehÄ¾ad a samoÅ¡tÃºdium

V tejto lekcii sme sa nauÄili o lineÃ¡rnej regresii. ExistujÃº aj ÄalÅ¡ie dÃ´leÅ¾itÃ© typy regresie. PreÄÃ­tajte si o technikÃ¡ch Stepwise, Ridge, Lasso a Elasticnet. DobrÃ½ kurz na Å¡tÃºdium je [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Zadanie

[Postavte model](assignment.md)

---

**Upozornenie**:  
Tento dokument bol preloÅ¾enÃ½ pomocou sluÅ¾by AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snaÅ¾Ã­me o presnosÅ¥, prosÃ­m, berte na vedomie, Å¾e automatizovanÃ© preklady mÃ´Å¾u obsahovaÅ¥ chyby alebo nepresnosti. PÃ´vodnÃ½ dokument v jeho rodnom jazyku by mal byÅ¥ povaÅ¾ovanÃ½ za autoritatÃ­vny zdroj. Pre kritickÃ© informÃ¡cie sa odporÃºÄa profesionÃ¡lny Ä¾udskÃ½ preklad. Nie sme zodpovednÃ­ za Å¾iadne nedorozumenia alebo nesprÃ¡vne interpretÃ¡cie vyplÃ½vajÃºce z pouÅ¾itia tohto prekladu.