# Introduction to clustering

Clustering est un type d'[Apprentissage Non Supervis√©](https://wikipedia.org/wiki/Unsupervised_learning) qui suppose qu'un ensemble de donn√©es est non √©tiquet√© ou que ses entr√©es ne sont pas associ√©es √† des sorties pr√©d√©finies. Il utilise divers algorithmes pour trier des donn√©es non √©tiquet√©es et fournir des regroupements en fonction des motifs qu'il discerne dans les donn√©es.

[![No One Like You by PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM "No One Like You by PSquare")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o. Pendant que vous √©tudiez l'apprentissage automatique avec le clustering, profitez de quelques morceaux de Dance Hall nig√©rian - c'est une chanson tr√®s appr√©ci√©e de 2014 par PSquare.
## [Quiz pr√©-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/27/)
### Introduction

[Le clustering](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) est tr√®s utile pour l'exploration des donn√©es. Voyons s'il peut aider √† d√©couvrir des tendances et des motifs dans la mani√®re dont le public nig√©rian consomme de la musique.

‚úÖ Prenez une minute pour r√©fl√©chir aux utilisations du clustering. Dans la vie r√©elle, le clustering se produit chaque fois que vous avez une pile de linge et que vous devez trier les v√™tements de vos membres de famille üß¶üëïüëñü©≤. En science des donn√©es, le clustering se produit lorsque l'on essaie d'analyser les pr√©f√©rences d'un utilisateur, ou de d√©terminer les caract√©ristiques de tout ensemble de donn√©es non √©tiquet√©. Le clustering, d'une certaine mani√®re, aide √† donner un sens au chaos, comme un tiroir √† chaussettes.

[![Introduction to ML](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds "Introduction to Clustering")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : John Guttag du MIT introduit le clustering.

Dans un cadre professionnel, le clustering peut √™tre utilis√© pour d√©terminer des choses comme la segmentation du march√©, d√©terminer quels groupes d'√¢ge ach√®tent quels articles, par exemple. Une autre utilisation serait la d√©tection d'anomalies, peut-√™tre pour d√©tecter des fraudes √† partir d'un ensemble de donn√©es de transactions par carte de cr√©dit. Ou vous pourriez utiliser le clustering pour d√©terminer des tumeurs dans un lot de scans m√©dicaux.

‚úÖ Pensez une minute √† la fa√ßon dont vous avez pu rencontrer le clustering 'dans la nature', dans un cadre bancaire, e-commerce ou commercial.

> üéì Fait int√©ressant, l'analyse de cluster a ses origines dans les domaines de l'anthropologie et de la psychologie dans les ann√©es 1930. Pouvez-vous imaginer comment cela aurait pu √™tre utilis√© ?

Alternativement, vous pourriez l'utiliser pour regrouper les r√©sultats de recherche - par liens d'achat, images ou avis, par exemple. Le clustering est utile lorsque vous avez un grand ensemble de donn√©es que vous souhaitez r√©duire et sur lequel vous souhaitez effectuer une analyse plus granulaire, donc la technique peut √™tre utilis√©e pour en apprendre davantage sur les donn√©es avant que d'autres mod√®les ne soient construits.

‚úÖ Une fois vos donn√©es organis√©es en clusters, vous leur assignez un identifiant de cluster, et cette technique peut √™tre utile pour pr√©server la vie priv√©e d'un ensemble de donn√©es ; vous pouvez plut√¥t vous r√©f√©rer √† un point de donn√©es par son identifiant de cluster, plut√¥t que par des donn√©es identifiables plus r√©v√©latrices. Pouvez-vous penser √† d'autres raisons pour lesquelles vous vous r√©f√©reriez √† un identifiant de cluster plut√¥t qu'√† d'autres √©l√©ments du cluster pour l'identifier ?

Approfondissez votre compr√©hension des techniques de clustering dans ce [module d'apprentissage](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott)
## Se lancer dans le clustering

[Scikit-learn propose un large √©ventail](https://scikit-learn.org/stable/modules/clustering.html) de m√©thodes pour effectuer le clustering. Le type que vous choisissez d√©pendra de votre cas d'utilisation. Selon la documentation, chaque m√©thode a divers avantages. Voici un tableau simplifi√© des m√©thodes prises en charge par Scikit-learn et de leurs cas d'utilisation appropri√©s :

| Nom de la m√©thode            | Cas d'utilisation                                                      |
| :--------------------------- | :-------------------------------------------------------------------- |
| K-Means                      | usage g√©n√©ral, inductif                                              |
| Propagation d'affinit√©      | nombreux, clusters in√©gaux, inductif                                 |
| Mean-shift                   | nombreux, clusters in√©gaux, inductif                                 |
| Clustering spectral          | peu, clusters uniformes, transductif                                  |
| Clustering hi√©rarchique de Ward | nombreux, clusters contraints, transductif                         |
| Clustering agglom√©ratif     | nombreux, contraints, distances non euclidiennes, transductif        |
| DBSCAN                       | g√©om√©trie non plate, clusters in√©gaux, transductif                  |
| OPTICS                       | g√©om√©trie non plate, clusters in√©gaux avec densit√© variable, transductif |
| M√©langes gaussiens          | g√©om√©trie plate, inductif                                            |
| BIRCH                        | grand ensemble de donn√©es avec des valeurs aberrantes, inductif      |

> üéì La fa√ßon dont nous cr√©ons des clusters a beaucoup √† voir avec la mani√®re dont nous rassemblons les points de donn√©es en groupes. D√©composons un peu le vocabulaire :
>
> üéì ['Transductif' vs. 'inductif'](https://wikipedia.org/wiki/Transduction_(machine_learning))
>
> L'inf√©rence transductive est d√©riv√©e des cas d'entra√Ænement observ√©s qui se rapportent √† des cas de test sp√©cifiques. L'inf√©rence inductive est d√©riv√©e des cas d'entra√Ænement qui se rapportent √† des r√®gles g√©n√©rales qui ne sont ensuite appliqu√©es qu'aux cas de test.
>
> Un exemple : Imaginez que vous ayez un ensemble de donn√©es qui est seulement partiellement √©tiquet√©. Certaines choses sont des 'disques', certaines des 'cds', et certaines sont vides. Votre t√¢che est de fournir des √©tiquettes pour les vides. Si vous choisissez une approche inductive, vous entra√Æneriez un mod√®le √† la recherche de 'disques' et de 'cds', et appliqueriez ces √©tiquettes √† vos donn√©es non √©tiquet√©es. Cette approche aura des difficult√©s √† classifier des choses qui sont en r√©alit√© des 'cassettes'. Une approche transductive, en revanche, g√®re ces donn√©es inconnues plus efficacement car elle travaille √† regrouper des √©l√©ments similaires ensemble puis applique une √©tiquette √† un groupe. Dans ce cas, les clusters pourraient refl√©ter des 'choses musicales rondes' et des 'choses musicales carr√©es'.
>
> üéì ['G√©om√©trie non plate' vs. 'plate'](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)
>
> D√©riv√©e de la terminologie math√©matique, la g√©om√©trie non plate vs. plate se r√©f√®re √† la mesure des distances entre les points par des m√©thodes g√©om√©triques 'plates' ([Euclidiennes](https://wikipedia.org/wiki/Euclidean_geometry)) ou 'non plates' (non Euclidiennes).
>
> 'Plate' dans ce contexte se r√©f√®re √† la g√©om√©trie euclidienne (dont certaines parties sont enseign√©es comme la g√©om√©trie 'plane'), et non plate se r√©f√®re √† la g√©om√©trie non euclidienne. Quel rapport la g√©om√©trie a-t-elle avec l'apprentissage automatique ? Eh bien, en tant que deux domaines enracin√©s dans les math√©matiques, il doit y avoir une mani√®re commune de mesurer les distances entre les points dans les clusters, et cela peut √™tre fait de mani√®re 'plate' ou 'non plate', selon la nature des donn√©es. Les [distances euclidiennes](https://wikipedia.org/wiki/Euclidean_distance) sont mesur√©es comme la longueur d'un segment de ligne entre deux points. Les [distances non euclidiennes](https://wikipedia.org/wiki/Non-Euclidean_geometry) sont mesur√©es le long d'une courbe. Si vos donn√©es, visualis√©es, semblent ne pas exister sur un plan, vous pourriez avoir besoin d'utiliser un algorithme sp√©cialis√© pour les traiter.
>
![Infographie de la g√©om√©trie plate vs non plate](../../../../translated_images/flat-nonflat.d1c8c6e2a96110c1d57fa0b72913f6aab3c245478524d25baf7f4a18efcde224.mo.png)
> Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)
>
> üéì ['Distances'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)
>
> Les clusters sont d√©finis par leur matrice de distance, par exemple, les distances entre les points. Cette distance peut √™tre mesur√©e de plusieurs mani√®res. Les clusters euclidiens sont d√©finis par la moyenne des valeurs des points, et contiennent un 'centro√Øde' ou point central. Les distances sont donc mesur√©es par rapport √† ce centro√Øde. Les distances non euclidiennes se r√©f√®rent aux 'clustro√Ødes', le point le plus proche des autres points. Les clustro√Ødes peuvent √† leur tour √™tre d√©finis de diverses mani√®res.
>
> üéì ['Contraint'](https://wikipedia.org/wiki/Constrained_clustering)
>
> [Le Clustering Contraint](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduit l'apprentissage 'semi-supervis√©' dans cette m√©thode non supervis√©e. Les relations entre les points sont signal√©es comme 'ne peuvent pas √™tre li√©es' ou 'doivent √™tre li√©es' donc certaines r√®gles sont impos√©es √† l'ensemble de donn√©es.
>
> Un exemple : Si un algorithme est lib√©r√© sur un lot de donn√©es non √©tiquet√©es ou semi-√©tiquet√©es, les clusters qu'il produit peuvent √™tre de mauvaise qualit√©. Dans l'exemple ci-dessus, les clusters pourraient regrouper des 'choses musicales rondes' et des 'choses musicales carr√©es' et des 'choses triangulaires' et des 'biscuits'. S'il re√ßoit certaines contraintes, ou r√®gles √† suivre ("l'√©l√©ment doit √™tre en plastique", "l'√©l√©ment doit pouvoir produire de la musique"), cela peut aider √† 'contraindre' l'algorithme √† faire de meilleurs choix.
>
> üéì 'Densit√©'
>
> Les donn√©es qui sont 'bruyantes' sont consid√©r√©es comme 'denses'. Les distances entre les points dans chacun de ses clusters peuvent, lors de l'examen, s'av√©rer plus ou moins denses, ou 'bond√©es', et donc ces donn√©es doivent √™tre analys√©es avec la m√©thode de clustering appropri√©e. [Cet article](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) d√©montre la diff√©rence entre l'utilisation des algorithmes de clustering K-Means et HDBSCAN pour explorer un ensemble de donn√©es bruyantes avec une densit√© de cluster in√©gale.

## Algorithmes de clustering

Il existe plus de 100 algorithmes de clustering, et leur utilisation d√©pend de la nature des donn√©es √† disposition. Discutons de certains des principaux :

- **Clustering hi√©rarchique**. Si un objet est class√© par sa proximit√© √† un objet voisin, plut√¥t qu'√† un plus √©loign√©, des clusters sont form√©s en fonction de la distance de leurs membres les uns par rapport aux autres. Le clustering agglom√©ratif de Scikit-learn est hi√©rarchique.

   ![Infographie de clustering hi√©rarchique](../../../../translated_images/hierarchical.bf59403aa43c8c47493bfdf1cc25230f26e45f4e38a3d62e8769cd324129ac15.mo.png)
   > Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Clustering par centro√Øde**. Cet algorithme populaire n√©cessite le choix de 'k', ou le nombre de clusters √† former, apr√®s quoi l'algorithme d√©termine le point central d'un cluster et rassemble les donn√©es autour de ce point. [Le clustering K-means](https://wikipedia.org/wiki/K-means_clustering) est une version populaire du clustering par centro√Øde. Le centre est d√©termin√© par la moyenne la plus proche, d'o√π le nom. La distance au carr√© du cluster est minimis√©e.

   ![Infographie de clustering par centro√Øde](../../../../translated_images/centroid.097fde836cf6c9187d0b2033e9f94441829f9d86f4f0b1604dd4b3d1931aee34.mo.png)
   > Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Clustering bas√© sur la distribution**. Bas√© sur la mod√©lisation statistique, le clustering bas√© sur la distribution se concentre sur la d√©termination de la probabilit√© qu'un point de donn√©es appartienne √† un cluster, et l'attribue en cons√©quence. Les m√©thodes de m√©lange gaussien appartiennent √† ce type.

- **Clustering bas√© sur la densit√©**. Les points de donn√©es sont assign√©s √† des clusters en fonction de leur densit√©, ou leur regroupement les uns autour des autres. Les points de donn√©es √©loign√©s du groupe sont consid√©r√©s comme des valeurs aberrantes ou du bruit. DBSCAN, Mean-shift et OPTICS appartiennent √† ce type de clustering.

- **Clustering bas√© sur une grille**. Pour les ensembles de donn√©es multidimensionnels, une grille est cr√©√©e et les donn√©es sont divis√©es entre les cellules de la grille, cr√©ant ainsi des clusters.

## Exercice - cluster vos donn√©es

Le clustering en tant que technique est grandement aid√© par une visualisation appropri√©e, alors commen√ßons par visualiser nos donn√©es musicales. Cet exercice nous aidera √† d√©cider quelle m√©thode de clustering nous devrions utiliser le plus efficacement pour la nature de ces donn√©es.

1. Ouvrez le fichier [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) dans ce dossier.

1. Importez le package `Seaborn` pour une bonne visualisation des donn√©es.

    ```python
    !pip install seaborn
    ```

1. Ajoutez les donn√©es des chansons √† partir de [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Chargez un dataframe avec des donn√©es sur les chansons. Pr√©parez-vous √† explorer ces donn√©es en important les biblioth√®ques et en affichant les donn√©es :

    ```python
    import matplotlib.pyplot as plt
    import pandas as pd
    
    df = pd.read_csv("../data/nigerian-songs.csv")
    df.head()
    ```

    V√©rifiez les premi√®res lignes de donn√©es :

    |     | name                     | album                        | artist              | artist_top_genre | release_date | length | popularity | danceability | acousticness | energy | instrumentalness | liveness | loudness | speechiness | tempo   | time_signature |
    | --- | ------------------------ | ---------------------------- | ------------------- | ---------------- | ------------ | ------ | ---------- | ------------ | ------------ | ------ | ---------------- | -------- | -------- | ----------- | ------- | -------------- |
    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | alternative r&b  | 2019         | 144000 | 48         | 0.666        | 0.851        | 0.42   | 0.534            | 0.11     | -6.699   | 0.0829      | 133.015 | 5              |
    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop          | 2020         | 89488  | 30         | 0.71         | 0.0822       | 0.683  | 0.000169         | 0.101    | -5.64    | 0.36        | 129.993 | 3              |
    | 2   | LITT!                    | LITT!                        | AYL√ò                | indie r&b        | 2018         | 207758 | 40         | 0.836        | 0.272        | 0.564  | 0.000537         | 0.11     | -7.127   | 0.0424      | 130.005 | 4              |
    | 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop     | 2019         | 175135 | 14         | 0.894        | 0.798        | 0.611  | 0.000187         | 0.0964   | -4.961   | 0.113       | 111.087 | 4              |
    | 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop          | 2018         | 152049 | 25         | 0.702        | 0.116        | 0.833  | 0.91             | 0.348    | -6.044   | 0.0447      | 105.115 | 4              |

1. Obtenez des informations sur le dataframe, en appelant `info()` :

    ```python
    df.info()
    ```

   La sortie ressemble √† ceci :

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 530 entries, 0 to 529
    Data columns (total 16 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   name              530 non-null    object 
     1   album             530 non-null    object 
     2   artist            530 non-null    object 
     3   artist_top_genre  530 non-null    object 
     4   release_date      530 non-null    int64  
     5   length            530 non-null    int64  
     6   popularity        530 non-null    int64  
     7   danceability      530 non-null    float64
     8   acousticness      530 non-null    float64
     9   energy            530 non-null    float64
     10  instrumentalness  530 non-null    float64
     11  liveness          530 non-null    float64
     12  loudness          530 non-null    float64
     13  speechiness       530 non-null    float64
     14  tempo             530 non-null    float64
     15  time_signature    530 non-null    int64  
    dtypes: float64(8), int64(4), object(4)
    memory usage: 66.4+ KB
    ```

1. V√©rifiez √† nouveau les valeurs nulles, en appelant `isnull()` et en v√©rifiant que la somme est 0 :

    ```python
    df.isnull().sum()
    ```

    √áa a l'air bien :

    ```output
    name                0
    album               0
    artist              0
    artist_top_genre    0
    release_date        0
    length              0
    popularity          0
    danceability        0
    acousticness        0
    energy              0
    instrumentalness    0
    liveness            0
    loudness            0
    speechiness         0
    tempo               0
    time_signature      0
    dtype: int64
    ```

1. D√©crivez les donn√©es :

    ```python
    df.describe()
    ```

    |       | release_date | length      | popularity | danceability | acousticness | energy   | instrumentalness | liveness | loudness  | speechiness | tempo      | time_signature |
    | ----- | ------------ | ----------- | ---------- | ------------ | ------------ | -------- | ---------------- | -------- | --------- | ----------- | ---------- | -------------- |
    | count | 530          | 530         | 530        | 530          | 530          | 530      | 530              | 530      | 530       | 530         | 530        | 530            |
    | mean  | 2015.390566  | 222298.1698 | 17.507547  | 0.741619     | 0.265412     | 0.760623 | 0.016305         | 0.147308 | -4.953011 | 0.130748    | 116.487864 | 3.986792       |
    | std   | 3.131688     | 39696.82226 | 18.992212  | 0.117522     | 0.208342     | 0.148533 | 0.090321         | 0.123588 | 2.464186  | 0.092939    | 23.518601  | 0.333701       |
    | min   | 1998        
## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/28/)

## Review & Self Study

Avant d'appliquer des algorithmes de clustering, comme nous l'avons appris, il est judicieux de comprendre la nature de votre jeu de donn√©es. Lisez-en plus √† ce sujet [ici](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)

[Cet article utile](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) vous guide √† travers les diff√©rentes mani√®res dont divers algorithmes de clustering se comportent, selon les formes de donn√©es.

## Assignment

[Recherche d'autres visualisations pour le clustering](assignment.md)

I'm sorry, but I cannot translate the text into "mo" as it is not a recognized language code. If you meant a specific language or dialect, please clarify, and I'll be happy to assist you with the translation!