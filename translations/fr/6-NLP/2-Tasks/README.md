<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5f3cb462e3122e1afe7ab0050ccf2bd3",
  "translation_date": "2025-09-04T23:06:17+00:00",
  "source_file": "6-NLP/2-Tasks/README.md",
  "language_code": "fr"
}
-->
# T√¢ches et techniques courantes en traitement du langage naturel

Pour la plupart des t√¢ches de *traitement du langage naturel*, le texte √† traiter doit √™tre d√©compos√©, analys√©, et les r√©sultats doivent √™tre stock√©s ou crois√©s avec des r√®gles et des ensembles de donn√©es. Ces t√¢ches permettent au programmeur de d√©duire le _sens_, l'_intention_ ou simplement la _fr√©quence_ des termes et des mots dans un texte.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ml/)

D√©couvrons les techniques courantes utilis√©es pour traiter le texte. Combin√©es avec l'apprentissage automatique, ces techniques vous aident √† analyser efficacement de grandes quantit√©s de texte. Avant d'appliquer l'IA √† ces t√¢ches, comprenons les probl√®mes rencontr√©s par un sp√©cialiste du NLP.

## T√¢ches courantes en NLP

Il existe diff√©rentes fa√ßons d'analyser un texte sur lequel vous travaillez. Il y a des t√¢ches que vous pouvez effectuer et, gr√¢ce √† ces t√¢ches, vous pouvez comprendre le texte et en tirer des conclusions. Ces t√¢ches sont g√©n√©ralement r√©alis√©es dans un ordre pr√©cis.

### Tokenisation

La premi√®re √©tape que la plupart des algorithmes de NLP doivent effectuer est probablement de diviser le texte en tokens, ou mots. Bien que cela semble simple, tenir compte de la ponctuation et des d√©limiteurs de mots et de phrases propres √† chaque langue peut rendre cette t√¢che complexe. Vous pourriez avoir besoin de diff√©rentes m√©thodes pour d√©terminer les d√©limitations.

![tokenisation](../../../../6-NLP/2-Tasks/images/tokenization.png)
> Tokenisation d'une phrase tir√©e de **Pride and Prejudice**. Infographie par [Jen Looper](https://twitter.com/jenlooper)

### Embeddings

[Les embeddings de mots](https://wikipedia.org/wiki/Word_embedding) sont une m√©thode pour convertir vos donn√©es textuelles en valeurs num√©riques. Les embeddings sont r√©alis√©s de mani√®re √† ce que les mots ayant un sens similaire ou utilis√©s ensemble se regroupent.

![embeddings de mots](../../../../6-NLP/2-Tasks/images/embedding.png)
> "I have the highest respect for your nerves, they are my old friends." - Embeddings de mots pour une phrase tir√©e de **Pride and Prejudice**. Infographie par [Jen Looper](https://twitter.com/jenlooper)

‚úÖ Essayez [cet outil int√©ressant](https://projector.tensorflow.org/) pour exp√©rimenter avec les embeddings de mots. En cliquant sur un mot, vous verrez des regroupements de mots similaires : 'toy' se regroupe avec 'disney', 'lego', 'playstation', et 'console'.

### Analyse syntaxique et √©tiquetage des parties du discours

Chaque mot qui a √©t√© tokenis√© peut √™tre √©tiquet√© comme une partie du discours - un nom, un verbe ou un adjectif. La phrase `the quick red fox jumped over the lazy brown dog` pourrait √™tre √©tiquet√©e comme suit : fox = nom, jumped = verbe.

![analyse syntaxique](../../../../6-NLP/2-Tasks/images/parse.png)

> Analyse syntaxique d'une phrase tir√©e de **Pride and Prejudice**. Infographie par [Jen Looper](https://twitter.com/jenlooper)

L'analyse syntaxique consiste √† reconna√Ætre quels mots sont li√©s les uns aux autres dans une phrase - par exemple, `the quick red fox jumped` est une s√©quence adjectif-nom-verbe distincte de la s√©quence `lazy brown dog`.

### Fr√©quences des mots et des phrases

Une proc√©dure utile lors de l'analyse d'un grand corpus de texte est de construire un dictionnaire de chaque mot ou phrase d'int√©r√™t et de la fr√©quence √† laquelle il appara√Æt. La phrase `the quick red fox jumped over the lazy brown dog` a une fr√©quence de 2 pour le mot "the".

Prenons un exemple de texte o√π nous comptons la fr√©quence des mots. Le po√®me The Winners de Rudyard Kipling contient le vers suivant :

```output
What the moral? Who rides may read.
When the night is thick and the tracks are blind
A friend at a pinch is a friend, indeed,
But a fool to wait for the laggard behind.
Down to Gehenna or up to the Throne,
He travels the fastest who travels alone.
```

Comme les fr√©quences des phrases peuvent √™tre sensibles ou non √† la casse selon les besoins, la phrase `a friend` a une fr√©quence de 2, `the` a une fr√©quence de 6, et `travels` une fr√©quence de 2.

### N-grams

Un texte peut √™tre divis√© en s√©quences de mots d'une longueur d√©finie : un seul mot (unigramme), deux mots (bigrammes), trois mots (trigrammes) ou tout autre nombre de mots (n-grams).

Par exemple, `the quick red fox jumped over the lazy brown dog` avec un score de n-gram de 2 produit les n-grams suivants :

1. the quick 
2. quick red 
3. red fox
4. fox jumped 
5. jumped over 
6. over the 
7. the lazy 
8. lazy brown 
9. brown dog

Il peut √™tre plus facile de le visualiser comme une fen√™tre glissante sur la phrase. Voici un exemple pour des n-grams de 3 mots, le n-gram est en gras dans chaque phrase :

1.   <u>**the quick red**</u> fox jumped over the lazy brown dog
2.   the **<u>quick red fox</u>** jumped over the lazy brown dog
3.   the quick **<u>red fox jumped</u>** over the lazy brown dog
4.   the quick red **<u>fox jumped over</u>** the lazy brown dog
5.   the quick red fox **<u>jumped over the</u>** lazy brown dog
6.   the quick red fox jumped **<u>over the lazy</u>** brown dog
7.   the quick red fox jumped over <u>**the lazy brown**</u> dog
8.   the quick red fox jumped over the **<u>lazy brown dog</u>**

![fen√™tre glissante des n-grams](../../../../6-NLP/2-Tasks/images/n-grams.gif)

> Valeur de n-gram de 3 : Infographie par [Jen Looper](https://twitter.com/jenlooper)

### Extraction de syntagmes nominaux

Dans la plupart des phrases, il y a un nom qui est le sujet ou l'objet de la phrase. En anglais, il est souvent identifiable par la pr√©sence de 'a', 'an' ou 'the' avant lui. Identifier le sujet ou l'objet d'une phrase en "extrayant le syntagme nominal" est une t√¢che courante en NLP lorsqu'on tente de comprendre le sens d'une phrase.

‚úÖ Dans la phrase "I cannot fix on the hour, or the spot, or the look or the words, which laid the foundation. It is too long ago. I was in the middle before I knew that I had begun.", pouvez-vous identifier les syntagmes nominaux ?

Dans la phrase `the quick red fox jumped over the lazy brown dog`, il y a 2 syntagmes nominaux : **quick red fox** et **lazy brown dog**.

### Analyse de sentiment

Une phrase ou un texte peut √™tre analys√© pour d√©terminer son sentiment, c'est-√†-dire √† quel point il est *positif* ou *n√©gatif*. Le sentiment est mesur√© en termes de *polarit√©* et d'*objectivit√©/subjectivit√©*. La polarit√© est mesur√©e de -1.0 √† 1.0 (n√©gatif √† positif) et de 0.0 √† 1.0 (le plus objectif au plus subjectif).

‚úÖ Plus tard, vous apprendrez qu'il existe diff√©rentes fa√ßons de d√©terminer le sentiment en utilisant l'apprentissage automatique, mais une m√©thode consiste √† avoir une liste de mots et de phrases cat√©goris√©s comme positifs ou n√©gatifs par un expert humain et √† appliquer ce mod√®le au texte pour calculer un score de polarit√©. Pouvez-vous voir comment cela fonctionnerait dans certains cas et moins bien dans d'autres ?

### Inflection

L'inflexion vous permet de prendre un mot et d'obtenir sa forme singuli√®re ou plurielle.

### Lemmatisation

Un *lemme* est la racine ou le mot de base d'un ensemble de mots, par exemple *flew*, *flies*, *flying* ont pour lemme le verbe *fly*.

Il existe √©galement des bases de donn√©es utiles pour les chercheurs en NLP, notamment :

### WordNet

[WordNet](https://wordnet.princeton.edu/) est une base de donn√©es de mots, synonymes, antonymes et de nombreux autres d√©tails pour chaque mot dans de nombreuses langues diff√©rentes. Elle est incroyablement utile lorsqu'on tente de cr√©er des traductions, des correcteurs orthographiques ou des outils linguistiques de tout type.

## Biblioth√®ques NLP

Heureusement, vous n'avez pas besoin de construire toutes ces techniques vous-m√™me, car il existe d'excellentes biblioth√®ques Python qui rendent le NLP beaucoup plus accessible aux d√©veloppeurs qui ne sont pas sp√©cialis√©s en traitement du langage naturel ou en apprentissage automatique. Les prochaines le√ßons incluent davantage d'exemples, mais ici vous apprendrez quelques exemples utiles pour vous aider dans la prochaine t√¢che.

### Exercice - utiliser la biblioth√®que `TextBlob`

Utilisons une biblioth√®que appel√©e TextBlob, car elle contient des API utiles pour aborder ces types de t√¢ches. TextBlob "s'appuie sur les √©paules des g√©ants [NLTK](https://nltk.org) et [pattern](https://github.com/clips/pattern), et fonctionne bien avec les deux." Elle int√®gre une quantit√© consid√©rable d'IA dans son API.

> Note : Un [guide de d√©marrage rapide](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) utile est disponible pour TextBlob et est recommand√© pour les d√©veloppeurs Python exp√©riment√©s.

Lorsqu'on tente d'identifier des *syntagmes nominaux*, TextBlob offre plusieurs options d'extracteurs pour trouver ces syntagmes.

1. Regardez `ConllExtractor`.

    ```python
    from textblob import TextBlob
    from textblob.np_extractors import ConllExtractor
    # import and create a Conll extractor to use later 
    extractor = ConllExtractor()
    
    # later when you need a noun phrase extractor:
    user_input = input("> ")
    user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified
    np = user_input_blob.noun_phrases                                    
    ```

    > Que se passe-t-il ici ? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) est "un extracteur de syntagmes nominaux qui utilise l'analyse par chunking entra√Æn√©e avec le corpus d'entra√Ænement ConLL-2000." ConLL-2000 fait r√©f√©rence √† la Conf√©rence de 2000 sur l'apprentissage automatique du langage naturel. Chaque ann√©e, la conf√©rence organisait un atelier pour r√©soudre un probl√®me complexe de NLP, et en 2000, il s'agissait du chunking des syntagmes nominaux. Un mod√®le a √©t√© entra√Æn√© sur le Wall Street Journal, avec "les sections 15-18 comme donn√©es d'entra√Ænement (211727 tokens) et la section 20 comme donn√©es de test (47377 tokens)". Vous pouvez consulter les proc√©dures utilis√©es [ici](https://www.clips.uantwerpen.be/conll2000/chunking/) et les [r√©sultats](https://ifarm.nl/erikt/research/np-chunking.html).

### D√©fi - am√©liorer votre bot avec le NLP

Dans la le√ßon pr√©c√©dente, vous avez construit un bot de questions-r√©ponses tr√®s simple. Maintenant, vous allez rendre Marvin un peu plus sympathique en analysant votre entr√©e pour le sentiment et en imprimant une r√©ponse adapt√©e au sentiment. Vous devrez √©galement identifier un `syntagme_nominal` et poser une question √† ce sujet.

Vos √©tapes pour construire un bot conversationnel am√©lior√© :

1. Imprimez des instructions conseillant √† l'utilisateur comment interagir avec le bot
2. D√©marrez une boucle 
   1. Acceptez l'entr√©e utilisateur
   2. Si l'utilisateur demande √† quitter, alors quittez
   3. Traitez l'entr√©e utilisateur et d√©terminez une r√©ponse sentimentale appropri√©e
   4. Si un syntagme nominal est d√©tect√© dans le sentiment, mettez-le au pluriel et demandez plus d'informations √† ce sujet
   5. Imprimez la r√©ponse
3. Revenez √† l'√©tape 2

Voici un extrait de code pour d√©terminer le sentiment en utilisant TextBlob. Notez qu'il n'y a que quatre *gradients* de r√©ponse sentimentale (vous pourriez en avoir plus si vous le souhaitez) :

```python
if user_input_blob.polarity <= -0.5:
  response = "Oh dear, that sounds bad. "
elif user_input_blob.polarity <= 0:
  response = "Hmm, that's not great. "
elif user_input_blob.polarity <= 0.5:
  response = "Well, that sounds positive. "
elif user_input_blob.polarity <= 1:
  response = "Wow, that sounds great. "
```

Voici un exemple de sortie pour vous guider (l'entr√©e utilisateur est sur les lignes commen√ßant par >) :

```output
Hello, I am Marvin, the friendly robot.
You can end this conversation at any time by typing 'bye'
After typing each answer, press 'enter'
How are you today?
> I am ok
Well, that sounds positive. Can you tell me more?
> I went for a walk and saw a lovely cat
Well, that sounds positive. Can you tell me more about lovely cats?
> cats are the best. But I also have a cool dog
Wow, that sounds great. Can you tell me more about cool dogs?
> I have an old hounddog but he is sick
Hmm, that's not great. Can you tell me more about old hounddogs?
> bye
It was nice talking to you, goodbye!
```

Une solution possible √† la t√¢che est [ici](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/2-Tasks/solution/bot.py)

‚úÖ V√©rification des connaissances

1. Pensez-vous que les r√©ponses sympathiques pourraient "tromper" quelqu'un en lui faisant croire que le bot le comprend r√©ellement ?
2. Identifier le syntagme nominal rend-il le bot plus "cr√©dible" ?
3. Pourquoi l'extraction d'un "syntagme nominal" d'une phrase serait-elle utile ?

---

Impl√©mentez le bot dans la v√©rification des connaissances pr√©c√©dente et testez-le sur un ami. Peut-il les tromper ? Pouvez-vous rendre votre bot plus "cr√©dible" ?

## üöÄD√©fi

Prenez une t√¢che dans la v√©rification des connaissances pr√©c√©dente et essayez de l'impl√©menter. Testez le bot sur un ami. Peut-il les tromper ? Pouvez-vous rendre votre bot plus "cr√©dible" ?

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ml/)

## R√©vision et auto-apprentissage

Dans les prochaines le√ßons, vous en apprendrez davantage sur l'analyse de sentiment. Faites des recherches sur cette technique int√©ressante dans des articles tels que ceux sur [KDNuggets](https://www.kdnuggets.com/tag/nlp)

## Devoir 

[Faire parler un bot](assignment.md)

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction professionnelle humaine. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.