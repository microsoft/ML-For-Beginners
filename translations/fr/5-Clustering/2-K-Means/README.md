<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-04T22:57:44+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "fr"
}
-->
# Regroupement K-Means

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ml/)

Dans cette le√ßon, vous apprendrez √† cr√©er des regroupements en utilisant Scikit-learn et le dataset de musique nig√©riane que vous avez import√© pr√©c√©demment. Nous couvrirons les bases du K-Means pour le regroupement. Gardez √† l'esprit que, comme vous l'avez appris dans la le√ßon pr√©c√©dente, il existe de nombreuses fa√ßons de travailler avec des regroupements, et la m√©thode que vous utilisez d√©pend de vos donn√©es. Nous allons essayer le K-Means car c'est la technique de regroupement la plus courante. Commen√ßons !

Termes que vous allez d√©couvrir :

- Score de silhouette
- M√©thode du coude
- Inertie
- Variance

## Introduction

Le [regroupement K-Means](https://wikipedia.org/wiki/K-means_clustering) est une m√©thode issue du domaine du traitement du signal. Elle est utilis√©e pour diviser et partitionner des groupes de donn√©es en 'k' regroupements √† l'aide d'une s√©rie d'observations. Chaque observation travaille √† regrouper un point de donn√©es donn√© le plus proche de son 'moyen' le plus proche, ou du point central d'un regroupement.

Les regroupements peuvent √™tre visualis√©s sous forme de [diagrammes de Vorono√Ø](https://wikipedia.org/wiki/Voronoi_diagram), qui incluent un point (ou 'graine') et sa r√©gion correspondante.

![diagramme de vorono√Ø](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infographie par [Jen Looper](https://twitter.com/jenlooper)

Le processus de regroupement K-Means [s'ex√©cute en trois √©tapes](https://scikit-learn.org/stable/modules/clustering.html#k-means) :

1. L'algorithme s√©lectionne un nombre k de points centraux en √©chantillonnant √† partir du dataset. Ensuite, il boucle :
    1. Il attribue chaque √©chantillon au centro√Øde le plus proche.
    2. Il cr√©e de nouveaux centro√Ødes en prenant la valeur moyenne de tous les √©chantillons attribu√©s aux centro√Ødes pr√©c√©dents.
    3. Ensuite, il calcule la diff√©rence entre les nouveaux et les anciens centro√Ødes et r√©p√®te jusqu'√† ce que les centro√Ødes se stabilisent.

Un inconv√©nient du K-Means est que vous devez d√©finir 'k', c'est-√†-dire le nombre de centro√Ødes. Heureusement, la 'm√©thode du coude' aide √† estimer une bonne valeur de d√©part pour 'k'. Vous allez l'essayer dans un instant.

## Pr√©requis

Vous travaillerez dans le fichier [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) de cette le√ßon, qui inclut l'importation des donn√©es et le nettoyage pr√©liminaire que vous avez effectu√© dans la le√ßon pr√©c√©dente.

## Exercice - pr√©paration

Commencez par examiner √† nouveau les donn√©es des chansons.

1. Cr√©ez un boxplot en appelant `boxplot()` pour chaque colonne :

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Ces donn√©es sont un peu bruyantes : en observant chaque colonne sous forme de boxplot, vous pouvez voir des valeurs aberrantes.

    ![valeurs aberrantes](../../../../5-Clustering/2-K-Means/images/boxplots.png)

Vous pourriez parcourir le dataset et supprimer ces valeurs aberrantes, mais cela rendrait les donn√©es assez minimales.

1. Pour l'instant, choisissez les colonnes que vous utiliserez pour votre exercice de regroupement. S√©lectionnez celles avec des plages similaires et encodez la colonne `artist_top_genre` en donn√©es num√©riques :

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Maintenant, vous devez choisir combien de regroupements cibler. Vous savez qu'il y a 3 genres musicaux que nous avons extraits du dataset, alors essayons 3 :

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Vous voyez un tableau imprim√© avec des regroupements pr√©dits (0, 1 ou 2) pour chaque ligne du dataframe.

1. Utilisez ce tableau pour calculer un 'score de silhouette' :

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Score de silhouette

Cherchez un score de silhouette proche de 1. Ce score varie de -1 √† 1, et si le score est 1, le regroupement est dense et bien s√©par√© des autres regroupements. Une valeur proche de 0 repr√©sente des regroupements qui se chevauchent avec des √©chantillons tr√®s proches de la fronti√®re de d√©cision des regroupements voisins. [(Source)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Notre score est **0,53**, donc en plein milieu. Cela indique que nos donn√©es ne sont pas particuli√®rement adapt√©es √† ce type de regroupement, mais continuons.

### Exercice - construire un mod√®le

1. Importez `KMeans` et commencez le processus de regroupement.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Il y a quelques parties ici qui m√©ritent une explication.

    > üéì range : Ce sont les it√©rations du processus de regroupement.

    > üéì random_state : "D√©termine la g√©n√©ration de nombres al√©atoires pour l'initialisation des centro√Ødes." [Source](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS : "somme des carr√©s intra-regroupement" mesure la distance moyenne au carr√© de tous les points au sein d'un regroupement par rapport au centro√Øde du regroupement. [Source](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì Inertie : Les algorithmes K-Means tentent de choisir des centro√Ødes pour minimiser l'inertie, "une mesure de la coh√©rence interne des regroupements." [Source](https://scikit-learn.org/stable/modules/clustering.html). La valeur est ajout√©e √† la variable wcss √† chaque it√©ration.

    > üéì k-means++ : Dans [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means), vous pouvez utiliser l'optimisation 'k-means++', qui "initialise les centro√Ødes pour qu'ils soient (g√©n√©ralement) √©loign√©s les uns des autres, conduisant probablement √† de meilleurs r√©sultats que l'initialisation al√©atoire."

### M√©thode du coude

Pr√©c√©demment, vous avez suppos√© que, parce que vous avez cibl√© 3 genres musicaux, vous devriez choisir 3 regroupements. Mais est-ce le cas ?

1. Utilisez la 'm√©thode du coude' pour en √™tre s√ªr.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Utilisez la variable `wcss` que vous avez construite √† l'√©tape pr√©c√©dente pour cr√©er un graphique montrant o√π se trouve le 'pli' dans le coude, ce qui indique le nombre optimal de regroupements. Peut-√™tre que c'est **bien** 3 !

    ![m√©thode du coude](../../../../5-Clustering/2-K-Means/images/elbow.png)

## Exercice - afficher les regroupements

1. Essayez √† nouveau le processus, cette fois en d√©finissant trois regroupements, et affichez les regroupements sous forme de diagramme de dispersion :

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. V√©rifiez la pr√©cision du mod√®le :

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    La pr√©cision de ce mod√®le n'est pas tr√®s bonne, et la forme des regroupements vous donne un indice sur la raison.

    ![regroupements](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Ces donn√©es sont trop d√©s√©quilibr√©es, trop peu corr√©l√©es et il y a trop de variance entre les valeurs des colonnes pour bien regrouper. En fait, les regroupements qui se forment sont probablement fortement influenc√©s ou biais√©s par les trois cat√©gories de genres que nous avons d√©finies ci-dessus. C'√©tait un processus d'apprentissage !

    Dans la documentation de Scikit-learn, vous pouvez voir qu'un mod√®le comme celui-ci, avec des regroupements pas tr√®s bien d√©limit√©s, a un probl√®me de 'variance' :

    ![mod√®les probl√©matiques](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infographie de Scikit-learn

## Variance

La variance est d√©finie comme "la moyenne des carr√©s des √©carts par rapport √† la moyenne" [(Source)](https://www.mathsisfun.com/data/standard-deviation.html). Dans le contexte de ce probl√®me de regroupement, cela fait r√©f√©rence √† des donn√©es o√π les nombres de notre dataset ont tendance √† diverger un peu trop de la moyenne.

‚úÖ C'est un excellent moment pour r√©fl√©chir √† toutes les fa√ßons dont vous pourriez corriger ce probl√®me. Modifier un peu les donn√©es ? Utiliser diff√©rentes colonnes ? Utiliser un autre algorithme ? Astuce : Essayez de [normaliser vos donn√©es](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) pour les mettre √† l'√©chelle et tester d'autres colonnes.

> Essayez ce '[calculateur de variance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' pour mieux comprendre le concept.

---

## üöÄD√©fi

Passez du temps avec ce notebook en ajustant les param√®tres. Pouvez-vous am√©liorer la pr√©cision du mod√®le en nettoyant davantage les donn√©es (en supprimant les valeurs aberrantes, par exemple) ? Vous pouvez utiliser des poids pour donner plus d'importance √† certains √©chantillons de donn√©es. Que pouvez-vous faire d'autre pour cr√©er de meilleurs regroupements ?

Astuce : Essayez de mettre vos donn√©es √† l'√©chelle. Il y a du code comment√© dans le notebook qui ajoute une mise √† l'√©chelle standard pour que les colonnes de donn√©es se ressemblent davantage en termes de plage. Vous constaterez que, bien que le score de silhouette diminue, le 'pli' dans le graphique du coude devient plus lisse. Cela est d√ª au fait que laisser les donn√©es non mises √† l'√©chelle permet aux donn√©es avec moins de variance de peser davantage. Lisez un peu plus sur ce probl√®me [ici](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ml/)

## R√©vision et auto-apprentissage

Jetez un ≈ìil √† un simulateur K-Means [comme celui-ci](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Vous pouvez utiliser cet outil pour visualiser des points de donn√©es d'√©chantillon et d√©terminer leurs centro√Ødes. Vous pouvez modifier l'al√©atoire des donn√©es, le nombre de regroupements et le nombre de centro√Ødes. Cela vous aide-t-il √† comprendre comment les donn√©es peuvent √™tre regroup√©es ?

Consultez √©galement [ce document sur le K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de Stanford.

## Devoir

[Essayez diff√©rentes m√©thodes de regroupement](assignment.md)

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.