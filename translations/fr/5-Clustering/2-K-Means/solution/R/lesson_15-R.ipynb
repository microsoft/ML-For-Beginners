{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  },
  "colab": {
   "name": "lesson_14.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "coopTranslator": {
   "original_hash": "ad65fb4aad0a156b42216e4929f490fc",
   "translation_date": "2025-09-04T02:13:59+00:00",
   "source_file": "5-Clustering/2-K-Means/solution/R/lesson_15-R.ipynb",
   "language_code": "fr"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GULATlQXLXyR"
   },
   "source": [
    "## Explorer le clustering K-Means avec R et les principes des donn√©es Tidy\n",
    "\n",
    "### [**Quiz avant le cours**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)\n",
    "\n",
    "Dans cette le√ßon, vous apprendrez √† cr√©er des clusters en utilisant le package Tidymodels et d'autres packages de l'√©cosyst√®me R (que nous appellerons des amis üßë‚Äçü§ù‚Äçüßë), ainsi que le jeu de donn√©es sur la musique nig√©riane que vous avez import√© pr√©c√©demment. Nous couvrirons les bases du K-Means pour le clustering. Gardez √† l'esprit que, comme vous l'avez appris dans la le√ßon pr√©c√©dente, il existe de nombreuses fa√ßons de travailler avec des clusters, et la m√©thode que vous utilisez d√©pend de vos donn√©es. Nous essayerons le K-Means car c'est la technique de clustering la plus courante. Allons-y !\n",
    "\n",
    "Termes que vous allez d√©couvrir :\n",
    "\n",
    "-   Scoring silhouette\n",
    "\n",
    "-   M√©thode du coude\n",
    "\n",
    "-   Inertie\n",
    "\n",
    "-   Variance\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "[Le clustering K-Means](https://wikipedia.org/wiki/K-means_clustering) est une m√©thode issue du domaine du traitement du signal. Elle est utilis√©e pour diviser et partitionner des groupes de donn√©es en `k clusters` en fonction des similarit√©s de leurs caract√©ristiques.\n",
    "\n",
    "Les clusters peuvent √™tre visualis√©s sous forme de [diagrammes de Vorono√Ø](https://wikipedia.org/wiki/Voronoi_diagram), qui incluent un point (ou \"graine\") et sa r√©gion correspondante.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/voronoi.png\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Infographie par Jen Looper</figcaption>\n",
    "\n",
    "Le clustering K-Means suit les √©tapes suivantes :\n",
    "\n",
    "1.  Le data scientist commence par sp√©cifier le nombre de clusters souhait√©s.\n",
    "\n",
    "2.  Ensuite, l'algorithme s√©lectionne al√©atoirement K observations du jeu de donn√©es pour servir de centres initiaux des clusters (c'est-√†-dire des centro√Ødes).\n",
    "\n",
    "3.  Chaque observation restante est ensuite assign√©e √† son centro√Øde le plus proche.\n",
    "\n",
    "4.  Les nouvelles moyennes de chaque cluster sont calcul√©es, et le centro√Øde est d√©plac√© vers cette moyenne.\n",
    "\n",
    "5.  Maintenant que les centres ont √©t√© recalcul√©s, chaque observation est √† nouveau v√©rifi√©e pour voir si elle pourrait √™tre plus proche d'un autre cluster. Tous les objets sont r√©assign√©s en utilisant les moyennes des clusters mises √† jour. Les √©tapes d'assignation des clusters et de mise √† jour des centro√Ødes sont r√©p√©t√©es de mani√®re it√©rative jusqu'√† ce que les assignations de clusters cessent de changer (c'est-√†-dire lorsque la convergence est atteinte). En g√©n√©ral, l'algorithme s'arr√™te lorsque chaque nouvelle it√©ration entra√Æne un d√©placement n√©gligeable des centro√Ødes et que les clusters deviennent statiques.\n",
    "\n",
    "<div>\n",
    "\n",
    "> Notez qu'en raison de la randomisation des k observations initiales utilis√©es comme centro√Ødes de d√©part, nous pouvons obtenir des r√©sultats l√©g√®rement diff√©rents √† chaque application de la proc√©dure. Pour cette raison, la plupart des algorithmes utilisent plusieurs *d√©marrages al√©atoires* et choisissent l'it√©ration avec le WCSS le plus faible. Il est donc fortement recommand√© d'ex√©cuter K-Means avec plusieurs valeurs de *nstart* pour √©viter un *optimum local ind√©sirable.*\n",
    "\n",
    "</div>\n",
    "\n",
    "Cette courte animation utilisant les [illustrations](https://github.com/allisonhorst/stats-illustrations) d'Allison Horst explique le processus de clustering :\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/kmeans.gif\"\n",
    "   width=\"550\"/>\n",
    "   <figcaption>Illustration par @allison_horst</figcaption>\n",
    "\n",
    "Une question fondamentale qui se pose dans le clustering est la suivante : comment savoir en combien de clusters diviser vos donn√©es ? Un inconv√©nient du K-Means est que vous devez d√©finir `k`, c'est-√†-dire le nombre de `centro√Ødes`. Heureusement, la `m√©thode du coude` aide √† estimer une bonne valeur de d√©part pour `k`. Vous allez l'essayer dans un instant.\n",
    "\n",
    "### \n",
    "\n",
    "**Pr√©requis**\n",
    "\n",
    "Nous reprendrons l√† o√π nous nous sommes arr√™t√©s dans la [le√ßon pr√©c√©dente](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb), o√π nous avons analys√© le jeu de donn√©es, r√©alis√© de nombreuses visualisations et filtr√© le jeu de donn√©es pour des observations d'int√©r√™t. Assurez-vous de la consulter !\n",
    "\n",
    "Nous aurons besoin de quelques packages pour compl√©ter ce module. Vous pouvez les installer avec : `install.packages(c('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork'))`\n",
    "\n",
    "Alternativement, le script ci-dessous v√©rifie si vous avez les packages n√©cessaires pour compl√©ter ce module et les installe pour vous si certains manquent.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ah_tBi58LXyi"
   },
   "source": [
    "suppressWarnings(if(!require(\"pacman\")) install.packages(\"pacman\"))\n",
    "\n",
    "pacman::p_load('tidyverse', 'tidymodels', 'cluster', 'summarytools', 'plotly', 'paletteer', 'factoextra', 'patchwork')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e--UCUTLXym"
   },
   "source": [
    "Allons-y !\n",
    "\n",
    "## 1. Une danse avec les donn√©es : R√©duire aux 3 genres musicaux les plus populaires\n",
    "\n",
    "Voici un r√©capitulatif de ce que nous avons fait dans la le√ßon pr√©c√©dente. D√©cortiquons un peu les donn√©es !\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ycamx7GGLXyn"
   },
   "source": [
    "# Load the core tidyverse and make it available in your current R session\n",
    "library(tidyverse)\n",
    "\n",
    "# Import the data into a tibble\n",
    "df <- read_csv(file = \"https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/5-Clustering/data/nigerian-songs.csv\", show_col_types = FALSE)\n",
    "\n",
    "# Narrow down to top 3 popular genres\n",
    "nigerian_songs <- df %>% \n",
    "  # Concentrate on top 3 genres\n",
    "  filter(artist_top_genre %in% c(\"afro dancehall\", \"afropop\",\"nigerian pop\")) %>% \n",
    "  # Remove unclassified observations\n",
    "  filter(popularity != 0)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize popular genres using bar plots\n",
    "theme_set(theme_light())\n",
    "nigerian_songs %>%\n",
    "  count(artist_top_genre) %>%\n",
    "  ggplot(mapping = aes(x = artist_top_genre, y = n,\n",
    "                       fill = artist_top_genre)) +\n",
    "  geom_col(alpha = 0.8) +\n",
    "  paletteer::scale_fill_paletteer_d(\"ggsci::category10_d3\") +\n",
    "  ggtitle(\"Top genres\") +\n",
    "  theme(plot.title = element_text(hjust = 0.5))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5h5zmkPLXyp"
   },
   "source": [
    "ü§© √áa s'est bien pass√© !\n",
    "\n",
    "## 2. Exploration suppl√©mentaire des donn√©es.\n",
    "\n",
    "√Ä quel point ces donn√©es sont-elles propres ? V√©rifions la pr√©sence de valeurs aberrantes √† l'aide de diagrammes en bo√Æte. Nous allons nous concentrer sur les colonnes num√©riques avec moins de valeurs aberrantes (bien que vous puissiez nettoyer les valeurs aberrantes). Les diagrammes en bo√Æte peuvent montrer l'√©tendue des donn√©es et aider √† choisir les colonnes √† utiliser. Notez que les diagrammes en bo√Æte ne montrent pas la variance, un √©l√©ment important pour des donn√©es bien adapt√©es au clustering. Veuillez consulter [cette discussion](https://stats.stackexchange.com/questions/91536/deduce-variance-from-boxplot) pour en savoir plus.\n",
    "\n",
    "Les [diagrammes en bo√Æte](https://en.wikipedia.org/wiki/Box_plot) sont utilis√©s pour repr√©senter graphiquement la distribution des donn√©es `num√©riques`, alors commen√ßons par *s√©lectionner* toutes les colonnes num√©riques ainsi que les genres musicaux populaires.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HhNreJKLLXyq"
   },
   "source": [
    "# Select top genre column and all other numeric columns\n",
    "df_numeric <- nigerian_songs %>% \n",
    "  select(artist_top_genre, where(is.numeric)) \n",
    "\n",
    "# Display the data\n",
    "df_numeric %>% \n",
    "  slice_head(n = 5)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYXrwJRaLXyq"
   },
   "source": [
    "Voyez comme l'assistant de s√©lection `where` rend cela facile üíÅ ? D√©couvrez d'autres fonctions similaires [ici](https://tidyselect.r-lib.org/).\n",
    "\n",
    "Puisque nous allons cr√©er un boxplot pour chaque caract√©ristique num√©rique et que nous voulons √©viter d'utiliser des boucles, reformons nos donn√©es dans un format *plus long* qui nous permettra de tirer parti des `facets` - des sous-graphiques affichant chacun un sous-ensemble des donn√©es.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gd5bR3f8LXys"
   },
   "source": [
    "# Pivot data from wide to long\n",
    "df_numeric_long <- df_numeric %>% \n",
    "  pivot_longer(!artist_top_genre, names_to = \"feature_names\", values_to = \"values\") \n",
    "\n",
    "# Print out data\n",
    "df_numeric_long %>% \n",
    "  slice_head(n = 15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7tE1swnLXyv"
   },
   "source": [
    "Beaucoup plus long ! Maintenant, il est temps de passer √† quelques `ggplots` ! Alors, quel `geom` allons-nous utiliser ?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r88bIsyuLXyy"
   },
   "source": [
    "# Make a box plot\n",
    "df_numeric_long %>% \n",
    "  ggplot(mapping = aes(x = feature_names, y = values, fill = feature_names)) +\n",
    "  geom_boxplot() +\n",
    "  facet_wrap(~ feature_names, ncol = 4, scales = \"free\") +\n",
    "  theme(legend.position = \"none\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYVyKIUELXyz"
   },
   "source": [
    "Facile-gg !\n",
    "\n",
    "Nous pouvons maintenant constater que ces donn√©es sont un peu bruyantes : en observant chaque colonne sous forme de bo√Æte √† moustaches, on peut rep√©rer des valeurs aberrantes. Vous pourriez parcourir l'ensemble de donn√©es et supprimer ces valeurs aberrantes, mais cela rendrait les donn√©es assez limit√©es.\n",
    "\n",
    "Pour l'instant, choisissons les colonnes que nous utiliserons pour notre exercice de regroupement. Prenons les colonnes num√©riques ayant des plages similaires. Nous pourrions encoder `artist_top_genre` en num√©rique, mais nous allons l'ignorer pour le moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wkpINyZLXy0"
   },
   "source": [
    "# Select variables with similar ranges\n",
    "df_numeric_select <- df_numeric %>% \n",
    "  select(popularity, danceability, acousticness, loudness, energy) \n",
    "\n",
    "# Normalize data\n",
    "# df_numeric_select <- scale(df_numeric_select)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7dLzgpqLXy1"
   },
   "source": [
    "## 3. Calcul des clusters k-means dans R\n",
    "\n",
    "Nous pouvons calculer les k-means dans R avec la fonction int√©gr√©e `kmeans`, voir `help(\"kmeans()\")`. La fonction `kmeans()` accepte un tableau de donn√©es avec toutes les colonnes num√©riques comme argument principal.\n",
    "\n",
    "La premi√®re √©tape lors de l'utilisation du clustering k-means est de sp√©cifier le nombre de clusters (k) qui seront g√©n√©r√©s dans la solution finale. Nous savons qu'il y a 3 genres musicaux que nous avons extraits du jeu de donn√©es, alors essayons avec 3 :\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uC4EQ5w7LXy5"
   },
   "source": [
    "set.seed(2056)\n",
    "# Kmeans clustering for 3 clusters\n",
    "kclust <- kmeans(\n",
    "  df_numeric_select,\n",
    "  # Specify the number of clusters\n",
    "  centers = 3,\n",
    "  # How many random initial configurations\n",
    "  nstart = 25\n",
    ")\n",
    "\n",
    "# Display clustering object\n",
    "kclust\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzfhscWrLXy-"
   },
   "source": [
    "L'objet kmeans contient plusieurs informations bien expliqu√©es dans `help(\"kmeans()\")`. Pour l'instant, concentrons-nous sur quelques-unes. Nous voyons que les donn√©es ont √©t√© regroup√©es en 3 clusters de tailles 65, 110, 111. La sortie contient √©galement les centres des clusters (moyennes) pour les 3 groupes sur les 5 variables.\n",
    "\n",
    "Le vecteur de clustering correspond √† l'affectation de cluster pour chaque observation. Utilisons la fonction `augment` pour ajouter l'affectation de cluster au jeu de donn√©es original.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0XwwpFGQLXy_"
   },
   "source": [
    "# Add predicted cluster assignment to data set\n",
    "augment(kclust, df_numeric_select) %>% \n",
    "  relocate(.cluster) %>% \n",
    "  slice_head(n = 10)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXIVXXACLXzA"
   },
   "source": [
    "Parfait, nous venons de diviser notre ensemble de donn√©es en 3 groupes. Alors, √† quel point notre regroupement est-il bon ü§∑ ? Jetons un coup d'≈ìil au `Silhouette score`.\n",
    "\n",
    "### **Score de silhouette**\n",
    "\n",
    "[L'analyse de silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering)) peut √™tre utilis√©e pour √©tudier la distance de s√©paration entre les clusters obtenus. Ce score varie de -1 √† 1, et si le score est proche de 1, cela signifie que le cluster est dense et bien s√©par√© des autres clusters. Une valeur proche de 0 repr√©sente des clusters qui se chevauchent, avec des √©chantillons tr√®s proches de la fronti√®re de d√©cision des clusters voisins. [source](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam).\n",
    "\n",
    "La m√©thode de silhouette moyenne calcule la silhouette moyenne des observations pour diff√©rentes valeurs de *k*. Un score moyen de silhouette √©lev√© indique un bon regroupement.\n",
    "\n",
    "La fonction `silhouette` du package cluster permet de calculer la largeur moyenne de silhouette.\n",
    "\n",
    "> La silhouette peut √™tre calcul√©e avec n'importe quelle [distance](https://en.wikipedia.org/wiki/Distance \"Distance\"), comme la [distance euclidienne](https://en.wikipedia.org/wiki/Euclidean_distance \"Euclidean distance\") ou la [distance de Manhattan](https://en.wikipedia.org/wiki/Manhattan_distance \"Manhattan distance\") que nous avons abord√©es dans la [le√ßon pr√©c√©dente](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14-R.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jn0McL28LXzB"
   },
   "source": [
    "# Load cluster package\n",
    "library(cluster)\n",
    "\n",
    "# Compute average silhouette score\n",
    "ss <- silhouette(kclust$cluster,\n",
    "                 # Compute euclidean distance\n",
    "                 dist = dist(df_numeric_select))\n",
    "mean(ss[, 3])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyQRn97nLXzC"
   },
   "source": [
    "Notre score est **0,549**, ce qui le place en plein milieu. Cela indique que nos donn√©es ne sont pas particuli√®rement adapt√©es √† ce type de regroupement. Voyons si nous pouvons confirmer cette intuition visuellement. Le [package factoextra](https://rpkgs.datanovia.com/factoextra/index.html) fournit des fonctions (`fviz_cluster()`) pour visualiser les regroupements.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7a6Km1_FLXzD"
   },
   "source": [
    "library(factoextra)\n",
    "\n",
    "# Visualize clustering results\n",
    "fviz_cluster(kclust, df_numeric_select)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwCWt-0LXzD"
   },
   "source": [
    "Le chevauchement des clusters indique que nos donn√©es ne sont pas particuli√®rement adapt√©es √† ce type de regroupement, mais poursuivons tout de m√™me.\n",
    "\n",
    "## 4. D√©terminer le nombre optimal de clusters\n",
    "\n",
    "Une question fondamentale qui se pose souvent dans le cadre du clustering K-Means est la suivante : sans √©tiquettes de classe connues, comment savoir en combien de clusters diviser vos donn√©es ?\n",
    "\n",
    "Une m√©thode pour tenter de le d√©terminer consiste √† utiliser un √©chantillon de donn√©es pour `cr√©er une s√©rie de mod√®les de clustering` avec un nombre croissant de clusters (par exemple de 1 √† 10), et √† √©valuer des m√©triques de clustering telles que le **score de Silhouette.**\n",
    "\n",
    "D√©terminons le nombre optimal de clusters en appliquant l'algorithme de clustering pour diff√©rentes valeurs de *k* et en √©valuant la **somme des carr√©s intra-cluster** (WCSS). La somme totale des carr√©s intra-cluster (WCSS) mesure la compacit√© du regroupement, et nous souhaitons qu'elle soit aussi faible que possible, des valeurs plus basses signifiant que les points de donn√©es sont plus proches les uns des autres.\n",
    "\n",
    "Explorons l'effet de diff√©rents choix de `k`, de 1 √† 10, sur ce regroupement.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hSeIiylDLXzE"
   },
   "source": [
    "# Create a series of clustering models\n",
    "kclusts <- tibble(k = 1:10) %>% \n",
    "  # Perform kmeans clustering for 1,2,3 ... ,10 clusters\n",
    "  mutate(model = map(k, ~ kmeans(df_numeric_select, centers = .x, nstart = 25)),\n",
    "  # Farm out clustering metrics eg WCSS\n",
    "         glanced = map(model, ~ glance(.x))) %>% \n",
    "  unnest(cols = glanced)\n",
    "  \n",
    "\n",
    "# View clustering rsulsts\n",
    "kclusts\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7rS2U1eLXzE"
   },
   "source": [
    "Maintenant que nous avons la somme totale des carr√©s intra-clusters (tot.withinss) pour chaque algorithme de regroupement avec un centre *k*, nous utilisons la [m√©thode du coude](https://fr.wikipedia.org/wiki/M%C3%A9thode_du_coude_(regroupement)) pour d√©terminer le nombre optimal de clusters. Cette m√©thode consiste √† tracer la WCSS en fonction du nombre de clusters, puis √† choisir le [coude de la courbe](https://fr.wikipedia.org/wiki/Coude_de_la_courbe \"Coude de la courbe\") comme le nombre de clusters √† utiliser.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o_DjHGItLXzF"
   },
   "source": [
    "set.seed(2056)\n",
    "# Use elbow method to determine optimum number of clusters\n",
    "kclusts %>% \n",
    "  ggplot(mapping = aes(x = k, y = tot.withinss)) +\n",
    "  geom_line(size = 1.2, alpha = 0.8, color = \"#FF7F0EFF\") +\n",
    "  geom_point(size = 2, color = \"#FF7F0EFF\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLYyt5XSLXzG"
   },
   "source": [
    "Le graphique montre une grande r√©duction de WCSS (donc une *compacit√©* accrue) lorsque le nombre de clusters passe de un √† deux, et une r√©duction encore notable de deux √† trois clusters. Apr√®s cela, la r√©duction est moins marqu√©e, ce qui cr√©e un `coude` üí™ dans le graphique autour de trois clusters. Cela indique clairement qu'il y a deux √† trois groupes de points de donn√©es raisonnablement bien s√©par√©s.\n",
    "\n",
    "Nous pouvons maintenant proc√©der √† l'extraction du mod√®le de clustering o√π `k = 3` :\n",
    "\n",
    "> `pull()`: utilis√© pour extraire une seule colonne\n",
    ">\n",
    "> `pluck()`: utilis√© pour indexer des structures de donn√©es telles que des listes\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JP_JPKBILXzG"
   },
   "source": [
    "# Extract k = 3 clustering\n",
    "final_kmeans <- kclusts %>% \n",
    "  filter(k == 3) %>% \n",
    "  pull(model) %>% \n",
    "  pluck(1)\n",
    "\n",
    "\n",
    "final_kmeans\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_PDTu8tLXzI"
   },
   "source": [
    "Super ! Visualisons les clusters obtenus. Envie d'un peu d'interactivit√© avec `plotly` ?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dNcleFe-LXzJ"
   },
   "source": [
    "# Add predicted cluster assignment to data set\n",
    "results <-  augment(final_kmeans, df_numeric_select) %>% \n",
    "  bind_cols(df_numeric %>% select(artist_top_genre)) \n",
    "\n",
    "# Plot cluster assignments\n",
    "clust_plt <- results %>% \n",
    "  ggplot(mapping = aes(x = popularity, y = danceability, color = .cluster, shape = artist_top_genre)) +\n",
    "  geom_point(size = 2, alpha = 0.8) +\n",
    "  paletteer::scale_color_paletteer_d(\"ggthemes::Tableau_10\")\n",
    "\n",
    "ggplotly(clust_plt)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JUM_51VLXzK"
   },
   "source": [
    "Peut-√™tre aurions-nous pu nous attendre √† ce que chaque cluster (repr√©sent√© par des couleurs diff√©rentes) ait des genres distincts (repr√©sent√©s par des formes diff√©rentes).\n",
    "\n",
    "Examinons la pr√©cision du mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HdIMUGq7LXzL"
   },
   "source": [
    "# Assign genres to predefined integers\n",
    "label_count <- results %>% \n",
    "  group_by(artist_top_genre) %>% \n",
    "  mutate(id = cur_group_id()) %>% \n",
    "  ungroup() %>% \n",
    "  summarise(correct_labels = sum(.cluster == id))\n",
    "\n",
    "\n",
    "# Print results  \n",
    "cat(\"Result:\", label_count$correct_labels, \"out of\", nrow(results), \"samples were correctly labeled.\")\n",
    "\n",
    "cat(\"\\nAccuracy score:\", label_count$correct_labels/nrow(results))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C50wvaAOLXzM"
   },
   "source": [
    "La pr√©cision de ce mod√®le n'est pas mauvaise, mais elle n'est pas excellente non plus. Il se peut que les donn√©es ne se pr√™tent pas bien au clustering par K-Means. Ces donn√©es sont trop d√©s√©quilibr√©es, trop peu corr√©l√©es et il y a trop de variance entre les valeurs des colonnes pour permettre un bon regroupement. En fait, les clusters qui se forment sont probablement fortement influenc√©s ou biais√©s par les trois cat√©gories de genres que nous avons d√©finies plus haut.\n",
    "\n",
    "N√©anmoins, ce fut un processus d'apprentissage tr√®s enrichissant !\n",
    "\n",
    "Dans la documentation de Scikit-learn, vous pouvez voir qu'un mod√®le comme celui-ci, avec des clusters peu bien d√©limit√©s, pr√©sente un probl√®me de \"variance\" :\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/problems.png\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Infographie tir√©e de Scikit-learn</figcaption>\n",
    "\n",
    "\n",
    "\n",
    "## **Variance**\n",
    "\n",
    "La variance est d√©finie comme \"la moyenne des carr√©s des √©carts par rapport √† la moyenne\" [source](https://www.mathsisfun.com/data/standard-deviation.html). Dans le contexte de ce probl√®me de clustering, cela fait r√©f√©rence au fait que les chiffres de notre jeu de donn√©es ont tendance √† s'√©carter un peu trop de la moyenne.\n",
    "\n",
    "‚úÖ C'est un excellent moment pour r√©fl√©chir √† toutes les fa√ßons dont vous pourriez corriger ce probl√®me. Ajuster un peu plus les donn√©es ? Utiliser d'autres colonnes ? Employer un algorithme diff√©rent ? Indice : Essayez de [normaliser vos donn√©es](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) pour les mettre √† l'√©chelle et testez d'autres colonnes.\n",
    "\n",
    "> Essayez ce '[calculateur de variance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' pour mieux comprendre le concept.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **üöÄD√©fi**\n",
    "\n",
    "Passez un peu de temps avec ce notebook en ajustant les param√®tres. Pouvez-vous am√©liorer la pr√©cision du mod√®le en nettoyant davantage les donn√©es (en supprimant les valeurs aberrantes, par exemple) ? Vous pouvez utiliser des pond√©rations pour donner plus d'importance √† certains √©chantillons de donn√©es. Que pouvez-vous faire d'autre pour cr√©er de meilleurs clusters ?\n",
    "\n",
    "Indice : Essayez de mettre vos donn√©es √† l'√©chelle. Il y a du code comment√© dans le notebook qui ajoute une mise √† l'√©chelle standard pour que les colonnes de donn√©es se ressemblent davantage en termes de plage. Vous constaterez que, bien que le score de silhouette diminue, le \"coude\" dans le graphique du coude devient plus lisse. Cela s'explique par le fait que laisser les donn√©es non mises √† l'√©chelle permet aux donn√©es avec moins de variance d'avoir plus de poids. Lisez un peu plus sur ce probl√®me [ici](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).\n",
    "\n",
    "## [**Quiz post-cours**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)\n",
    "\n",
    "## **R√©vision et auto-apprentissage**\n",
    "\n",
    "-   Jetez un ≈ìil √† un simulateur de K-Means [comme celui-ci](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Vous pouvez utiliser cet outil pour visualiser des points de donn√©es d'exemple et d√©terminer leurs centro√Ødes. Vous pouvez modifier l'al√©atoire des donn√©es, le nombre de clusters et le nombre de centro√Ødes. Cela vous aide-t-il √† mieux comprendre comment les donn√©es peuvent √™tre regroup√©es ?\n",
    "\n",
    "-   Consultez √©galement [ce document sur K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de Stanford.\n",
    "\n",
    "Envie de tester vos nouvelles comp√©tences en clustering sur des ensembles de donn√©es qui se pr√™tent bien au clustering par K-Means ? Consultez :\n",
    "\n",
    "-   [Entra√Æner et √©valuer des mod√®les de clustering](https://rpubs.com/eR_ic/clustering) en utilisant Tidymodels et ses outils associ√©s\n",
    "\n",
    "-   [Analyse de clusters K-Means](https://uc-r.github.io/kmeans_clustering), Guide de programmation R pour l'analyse commerciale de l'UC\n",
    "\n",
    "- [Clustering K-Means avec les principes des donn√©es tidy](https://www.tidymodels.org/learn/statistics/k-means/)\n",
    "\n",
    "## **Devoir**\n",
    "\n",
    "[Essayez diff√©rentes m√©thodes de clustering](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/assignment.md)\n",
    "\n",
    "## REMERCIEMENTS √Ä :\n",
    "\n",
    "[Jen Looper](https://www.twitter.com/jenlooper) pour avoir cr√©√© la version Python originale de ce module ‚ô•Ô∏è\n",
    "\n",
    "[`Allison Horst`](https://twitter.com/allison_horst/) pour ses illustrations incroyables qui rendent R plus accueillant et engageant. Retrouvez plus d'illustrations dans sa [galerie](https://www.google.com/url?q=https://github.com/allisonhorst/stats-illustrations&sa=D&source=editors&ust=1626380772530000&usg=AOvVaw3zcfyCizFQZpkSLzxiiQEM).\n",
    "\n",
    "Bon apprentissage,\n",
    "\n",
    "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/r_learners_sm.jpeg\"\n",
    "   width=\"500\"/>\n",
    "   <figcaption>Illustration par @allison_horst</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.\n"
   ]
  }
 ]
}