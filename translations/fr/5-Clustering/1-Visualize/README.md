<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "730225ea274c9174fe688b21d421539d",
  "translation_date": "2025-09-04T22:56:33+00:00",
  "source_file": "5-Clustering/1-Visualize/README.md",
  "language_code": "fr"
}
-->
# Introduction √† la classification par regroupement

La classification par regroupement est un type d'[apprentissage non supervis√©](https://wikipedia.org/wiki/Unsupervised_learning) qui suppose qu'un ensemble de donn√©es est non √©tiquet√© ou que ses entr√©es ne sont pas associ√©es √† des sorties pr√©d√©finies. Elle utilise divers algorithmes pour trier les donn√©es non √©tiquet√©es et fournir des regroupements en fonction des motifs qu'elle discerne dans les donn√©es.

[![No One Like You par PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM "No One Like You par PSquare")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o. Pendant que vous √©tudiez l'apprentissage automatique avec la classification par regroupement, profitez de quelques morceaux de Dance Hall nig√©rian - voici une chanson tr√®s appr√©ci√©e de 2014 par PSquare.

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ml/)

### Introduction

[La classification par regroupement](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) est tr√®s utile pour explorer les donn√©es. Voyons si elle peut aider √† d√©couvrir des tendances et des motifs dans la mani√®re dont les audiences nig√©rianes consomment de la musique.

‚úÖ Prenez une minute pour r√©fl√©chir aux utilisations de la classification par regroupement. Dans la vie quotidienne, cela se produit chaque fois que vous avez une pile de linge √† trier selon les v√™tements des membres de votre famille üß¶üëïüëñü©≤. En science des donn√©es, cela se produit lorsqu'on essaie d'analyser les pr√©f√©rences d'un utilisateur ou de d√©terminer les caract√©ristiques d'un ensemble de donn√©es non √©tiquet√©. En quelque sorte, la classification par regroupement aide √† donner un sens au chaos, comme un tiroir √† chaussettes.

[![Introduction √† l'apprentissage automatique](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds "Introduction √† la classification par regroupement")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : John Guttag du MIT introduit la classification par regroupement.

Dans un cadre professionnel, la classification par regroupement peut √™tre utilis√©e pour d√©terminer des segments de march√©, comme identifier quels groupes d'√¢ge ach√®tent quels articles, par exemple. Une autre utilisation serait la d√©tection d'anomalies, peut-√™tre pour d√©tecter des fraudes dans un ensemble de donn√©es de transactions par carte de cr√©dit. Ou encore, vous pourriez l'utiliser pour identifier des tumeurs dans un lot de scans m√©dicaux.

‚úÖ Prenez une minute pour r√©fl√©chir √† la mani√®re dont vous avez pu rencontrer la classification par regroupement "dans la nature", dans un contexte bancaire, de commerce √©lectronique ou d'affaires.

> üéì Fait int√©ressant, l'analyse par regroupement a vu le jour dans les domaines de l'anthropologie et de la psychologie dans les ann√©es 1930. Pouvez-vous imaginer comment elle aurait pu √™tre utilis√©e ?

Alternativement, vous pourriez l'utiliser pour regrouper des r√©sultats de recherche - par liens d'achat, images ou avis, par exemple. La classification par regroupement est utile lorsque vous avez un grand ensemble de donn√©es que vous souhaitez r√©duire et sur lequel vous voulez effectuer une analyse plus d√©taill√©e. Cette technique peut donc √™tre utilis√©e pour mieux comprendre les donn√©es avant de construire d'autres mod√®les.

‚úÖ Une fois vos donn√©es organis√©es en groupes, vous leur attribuez un identifiant de groupe. Cette technique peut √™tre utile pour pr√©server la confidentialit√© d'un ensemble de donn√©es ; vous pouvez alors vous r√©f√©rer √† un point de donn√©es par son identifiant de groupe plut√¥t que par des donn√©es identifiables plus r√©v√©latrices. Pouvez-vous penser √† d'autres raisons pour lesquelles vous pr√©f√©reriez utiliser un identifiant de groupe plut√¥t que d'autres √©l√©ments du groupe pour l'identifier ?

Approfondissez votre compr√©hension des techniques de classification par regroupement dans ce [module d'apprentissage](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott).

## Commencer avec la classification par regroupement

[Scikit-learn propose une large gamme](https://scikit-learn.org/stable/modules/clustering.html) de m√©thodes pour effectuer la classification par regroupement. Le type que vous choisissez d√©pendra de votre cas d'utilisation. Selon la documentation, chaque m√©thode pr√©sente divers avantages. Voici un tableau simplifi√© des m√©thodes prises en charge par Scikit-learn et leurs cas d'utilisation appropri√©s :

| Nom de la m√©thode            | Cas d'utilisation                                                    |
| :--------------------------- | :------------------------------------------------------------------- |
| K-Means                      | usage g√©n√©ral, inductif                                              |
| Propagation d'affinit√©       | nombreux groupes, in√©gaux, inductif                                  |
| Mean-shift                   | nombreux groupes, in√©gaux, inductif                                  |
| Classification spectrale     | quelques groupes, √©gaux, transductif                                |
| Classification hi√©rarchique Ward | nombreux groupes, contraints, transductif                        |
| Classification agglom√©rative | nombreux groupes, contraints, distances non euclidiennes, transductif |
| DBSCAN                       | g√©om√©trie non plate, groupes in√©gaux, transductif                    |
| OPTICS                       | g√©om√©trie non plate, groupes in√©gaux avec densit√© variable, transductif |
| M√©langes gaussiens           | g√©om√©trie plate, inductif                                            |
| BIRCH                        | grand ensemble de donn√©es avec des valeurs aberrantes, inductif      |

> üéì La mani√®re dont nous cr√©ons des groupes d√©pend beaucoup de la fa√ßon dont nous rassemblons les points de donn√©es en groupes. D√©composons quelques termes :
>
> üéì ['Transductif' vs. 'inductif'](https://wikipedia.org/wiki/Transduction_(machine_learning))
> 
> L'inf√©rence transductive est d√©riv√©e des cas d'entra√Ænement observ√©s qui correspondent √† des cas de test sp√©cifiques. L'inf√©rence inductive est d√©riv√©e des cas d'entra√Ænement qui m√®nent √† des r√®gles g√©n√©rales, appliqu√©es ensuite aux cas de test.
> 
> Un exemple : Imaginez que vous avez un ensemble de donn√©es partiellement √©tiquet√©. Certains √©l√©ments sont des 'disques', d'autres des 'CD', et certains sont vides. Votre t√¢che est de fournir des √©tiquettes pour les √©l√©ments vides. Si vous choisissez une approche inductive, vous entra√Æneriez un mod√®le en recherchant des 'disques' et des 'CD', et appliqueriez ces √©tiquettes aux donn√©es non √©tiquet√©es. Cette approche aurait du mal √† classer des √©l√©ments qui sont en r√©alit√© des 'cassettes'. Une approche transductive, en revanche, g√®re ces donn√©es inconnues plus efficacement en regroupant des √©l√©ments similaires et en appliquant une √©tiquette √† un groupe. Dans ce cas, les groupes pourraient refl√©ter 'objets musicaux ronds' et 'objets musicaux carr√©s'.
> 
> üéì ['G√©om√©trie non plate' vs. 'plate'](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)
> 
> Tir√© de la terminologie math√©matique, la g√©om√©trie non plate vs. plate fait r√©f√©rence √† la mesure des distances entre les points par des m√©thodes g√©om√©triques 'plates' ([euclidiennes](https://wikipedia.org/wiki/Euclidean_geometry)) ou 'non plates' (non euclidiennes).
>
>'Plate' dans ce contexte fait r√©f√©rence √† la g√©om√©trie euclidienne (dont certaines parties sont enseign√©es comme g√©om√©trie 'plane'), et 'non plate' fait r√©f√©rence √† la g√©om√©trie non euclidienne. Quel rapport avec l'apprentissage automatique ? Eh bien, en tant que deux domaines enracin√©s dans les math√©matiques, il doit y avoir une mani√®re commune de mesurer les distances entre les points dans les groupes, et cela peut √™tre fait de mani√®re 'plate' ou 'non plate', selon la nature des donn√©es. Les [distances euclidiennes](https://wikipedia.org/wiki/Euclidean_distance) sont mesur√©es comme la longueur d'un segment de ligne entre deux points. Les [distances non euclidiennes](https://wikipedia.org/wiki/Non-Euclidean_geometry) sont mesur√©es le long d'une courbe. Si vos donn√©es, visualis√©es, semblent ne pas exister sur un plan, vous pourriez avoir besoin d'un algorithme sp√©cialis√© pour les traiter.
>
![Infographie G√©om√©trie Plate vs Non Plate](../../../../5-Clustering/1-Visualize/images/flat-nonflat.png)
> Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)
> 
> üéì ['Distances'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)
> 
> Les groupes sont d√©finis par leur matrice de distances, c'est-√†-dire les distances entre les points. Cette distance peut √™tre mesur√©e de plusieurs fa√ßons. Les groupes euclidiens sont d√©finis par la moyenne des valeurs des points et contiennent un 'centro√Øde' ou point central. Les distances sont donc mesur√©es par rapport √† ce centro√Øde. Les distances non euclidiennes font r√©f√©rence aux 'clustro√Ødes', le point le plus proche des autres points. Les clustro√Ødes peuvent √† leur tour √™tre d√©finis de diverses mani√®res.
> 
> üéì ['Contraint'](https://wikipedia.org/wiki/Constrained_clustering)
> 
> [La classification par regroupement contrainte](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduit l'apprentissage 'semi-supervis√©' dans cette m√©thode non supervis√©e. Les relations entre les points sont marqu√©es comme 'ne peut pas lier' ou 'doit lier', ce qui impose certaines r√®gles √† l'ensemble de donn√©es.
>
>Un exemple : Si un algorithme est laiss√© libre sur un lot de donn√©es non √©tiquet√©es ou semi-√©tiquet√©es, les groupes qu'il produit peuvent √™tre de mauvaise qualit√©. Dans l'exemple ci-dessus, les groupes pourraient regrouper 'objets musicaux ronds', 'objets musicaux carr√©s', 'objets triangulaires' et 'biscuits'. Si on lui donne des contraintes ou des r√®gles √† suivre ("l'objet doit √™tre en plastique", "l'objet doit pouvoir produire de la musique"), cela peut aider √† 'contraindre' l'algorithme √† faire de meilleurs choix.
> 
> üéì 'Densit√©'
> 
> Les donn√©es 'bruyantes' sont consid√©r√©es comme 'denses'. Les distances entre les points dans chacun de ses groupes peuvent s'av√©rer, apr√®s examen, plus ou moins denses, ou 'concentr√©es', et ces donn√©es doivent donc √™tre analys√©es avec la m√©thode de regroupement appropri√©e. [Cet article](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) montre la diff√©rence entre l'utilisation de la classification par regroupement K-Means et les algorithmes HDBSCAN pour explorer un ensemble de donn√©es bruyant avec une densit√© de groupe in√©gale.

## Algorithmes de classification par regroupement

Il existe plus de 100 algorithmes de classification par regroupement, et leur utilisation d√©pend de la nature des donn√©es en question. Discutons de quelques-uns des principaux :

- **Classification hi√©rarchique**. Si un objet est class√© par sa proximit√© avec un objet voisin, plut√¥t qu'avec un objet plus √©loign√©, les groupes sont form√©s en fonction de la distance de leurs membres par rapport aux autres objets. La classification agglom√©rative de Scikit-learn est hi√©rarchique.

   ![Infographie Classification Hi√©rarchique](../../../../5-Clustering/1-Visualize/images/hierarchical.png)
   > Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Classification par centro√Øde**. Cet algorithme populaire n√©cessite le choix de 'k', ou le nombre de groupes √† former, apr√®s quoi l'algorithme d√©termine le point central d'un groupe et rassemble les donn√©es autour de ce point. [La classification K-means](https://wikipedia.org/wiki/K-means_clustering) est une version populaire de la classification par centro√Øde. Le centre est d√©termin√© par la moyenne la plus proche, d'o√π son nom. La distance au carr√© par rapport au groupe est minimis√©e.

   ![Infographie Classification par Centro√Øde](../../../../5-Clustering/1-Visualize/images/centroid.png)
   > Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Classification bas√©e sur la distribution**. Bas√©e sur la mod√©lisation statistique, cette classification se concentre sur la d√©termination de la probabilit√© qu'un point de donn√©es appartienne √† un groupe, et l'y attribue en cons√©quence. Les m√©thodes de m√©lange gaussien appartiennent √† ce type.

- **Classification bas√©e sur la densit√©**. Les points de donn√©es sont attribu√©s √† des groupes en fonction de leur densit√©, ou de leur regroupement autour les uns des autres. Les points de donn√©es √©loign√©s du groupe sont consid√©r√©s comme des valeurs aberrantes ou du bruit. DBSCAN, Mean-shift et OPTICS appartiennent √† ce type de classification.

- **Classification bas√©e sur une grille**. Pour les ensembles de donn√©es multidimensionnels, une grille est cr√©√©e et les donn√©es sont r√©parties entre les cellules de la grille, cr√©ant ainsi des groupes.

## Exercice - regroupez vos donn√©es

La classification par regroupement en tant que technique est grandement facilit√©e par une bonne visualisation, alors commen√ßons par visualiser nos donn√©es musicales. Cet exercice nous aidera √† d√©cider quelle m√©thode de classification par regroupement utiliser le plus efficacement en fonction de la nature de ces donn√©es.

1. Ouvrez le fichier [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) dans ce dossier.

1. Importez le package `Seaborn` pour une bonne visualisation des donn√©es.

    ```python
    !pip install seaborn
    ```

1. Ajoutez les donn√©es des chansons √† partir de [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Chargez un dataframe avec des donn√©es sur les chansons. Pr√©parez-vous √† explorer ces donn√©es en important les biblioth√®ques et en affichant les donn√©es :

    ```python
    import matplotlib.pyplot as plt
    import pandas as pd
    
    df = pd.read_csv("../data/nigerian-songs.csv")
    df.head()
    ```

    V√©rifiez les premi√®res lignes de donn√©es :

    |     | nom                     | album                        | artiste             | genre_principal_artiste | date_sortie | dur√©e | popularit√© | dansabilit√© | acoustique | √©nergie | instrumentalit√© | vivacit√© | volume   | parole      | tempo   | signature_temps |
    | --- | ------------------------ | ---------------------------- | ------------------- | ----------------------- | ------------ | ------ | ---------- | ----------- | ---------- | ------ | --------------- | -------- | -------- | ----------- | ------- | -------------- |
    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | r&b alternatif          | 2019         | 144000 | 48         | 0.666       | 0.851      | 0.42   | 0.534           | 0.11     | -6.699   | 0.0829      | 133.015 | 5              |
    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop                 | 2020         | 89488  | 30         | 0.71        | 0.0822     | 0.683  | 0.000169        | 0.101    | -5.64    | 0.36        | 129.993 | 3              |
| 2   | LITT!                    | LITT!                        | AYL√ò                | indie r&b        | 2018         | 207758 | 40         | 0.836        | 0.272        | 0.564  | 0.000537         | 0.11     | -7.127   | 0.0424      | 130.005 | 4              |
| 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop     | 2019         | 175135 | 14         | 0.894        | 0.798        | 0.611  | 0.000187         | 0.0964   | -4.961   | 0.113       | 111.087 | 4              |
| 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop          | 2018         | 152049 | 25         | 0.702        | 0.116        | 0.833  | 0.91             | 0.348    | -6.044   | 0.0447      | 105.115 | 4              |

1. Obtenez des informations sur le dataframe en appelant `info()` :

    ```python
    df.info()
    ```

   Le r√©sultat ressemble √† ceci :

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 530 entries, 0 to 529
    Data columns (total 16 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   name              530 non-null    object 
     1   album             530 non-null    object 
     2   artist            530 non-null    object 
     3   artist_top_genre  530 non-null    object 
     4   release_date      530 non-null    int64  
     5   length            530 non-null    int64  
     6   popularity        530 non-null    int64  
     7   danceability      530 non-null    float64
     8   acousticness      530 non-null    float64
     9   energy            530 non-null    float64
     10  instrumentalness  530 non-null    float64
     11  liveness          530 non-null    float64
     12  loudness          530 non-null    float64
     13  speechiness       530 non-null    float64
     14  tempo             530 non-null    float64
     15  time_signature    530 non-null    int64  
    dtypes: float64(8), int64(4), object(4)
    memory usage: 66.4+ KB
    ```

1. V√©rifiez les valeurs nulles en appelant `isnull()` et en confirmant que la somme est √©gale √† 0 :

    ```python
    df.isnull().sum()
    ```

    Tout semble correct :

    ```output
    name                0
    album               0
    artist              0
    artist_top_genre    0
    release_date        0
    length              0
    popularity          0
    danceability        0
    acousticness        0
    energy              0
    instrumentalness    0
    liveness            0
    loudness            0
    speechiness         0
    tempo               0
    time_signature      0
    dtype: int64
    ```

1. D√©crivez les donn√©es :

    ```python
    df.describe()
    ```

    |       | release_date | length      | popularity | danceability | acousticness | energy   | instrumentalness | liveness | loudness  | speechiness | tempo      | time_signature |
    | ----- | ------------ | ----------- | ---------- | ------------ | ------------ | -------- | ---------------- | -------- | --------- | ----------- | ---------- | -------------- |
    | count | 530          | 530         | 530        | 530          | 530          | 530      | 530              | 530      | 530       | 530         | 530        | 530            |
    | mean  | 2015.390566  | 222298.1698 | 17.507547  | 0.741619     | 0.265412     | 0.760623 | 0.016305         | 0.147308 | -4.953011 | 0.130748    | 116.487864 | 3.986792       |
    | std   | 3.131688     | 39696.82226 | 18.992212  | 0.117522     | 0.208342     | 0.148533 | 0.090321         | 0.123588 | 2.464186  | 0.092939    | 23.518601  | 0.333701       |
    | min   | 1998         | 89488       | 0          | 0.255        | 0.000665     | 0.111    | 0                | 0.0283   | -19.362   | 0.0278      | 61.695     | 3              |
    | 25%   | 2014         | 199305      | 0          | 0.681        | 0.089525     | 0.669    | 0                | 0.07565  | -6.29875  | 0.0591      | 102.96125  | 4              |
    | 50%   | 2016         | 218509      | 13         | 0.761        | 0.2205       | 0.7845   | 0.000004         | 0.1035   | -4.5585   | 0.09795     | 112.7145   | 4              |
    | 75%   | 2017         | 242098.5    | 31         | 0.8295       | 0.403        | 0.87575  | 0.000234         | 0.164    | -3.331    | 0.177       | 125.03925  | 4              |
    | max   | 2020         | 511738      | 73         | 0.966        | 0.954        | 0.995    | 0.91             | 0.811    | 0.582     | 0.514       | 206.007    | 5              |

> ü§î Si nous travaillons avec le clustering, une m√©thode non supervis√©e qui ne n√©cessite pas de donn√©es √©tiquet√©es, pourquoi montrons-nous ces donn√©es avec des √©tiquettes ? Lors de la phase d'exploration des donn√©es, elles sont utiles, mais elles ne sont pas n√©cessaires pour que les algorithmes de clustering fonctionnent. Vous pourriez tout aussi bien supprimer les en-t√™tes de colonnes et vous r√©f√©rer aux donn√©es par num√©ro de colonne.

Regardez les valeurs g√©n√©rales des donn√©es. Notez que la popularit√© peut √™tre '0', ce qui montre des chansons sans classement. Supprimons ces donn√©es prochainement.

1. Utilisez un barplot pour d√©couvrir les genres les plus populaires :

    ```python
    import seaborn as sns
    
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top[:5].index,y=top[:5].values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

    ![most popular](../../../../5-Clustering/1-Visualize/images/popular.png)

‚úÖ Si vous souhaitez voir plus de valeurs en haut du classement, modifiez le top `[:5]` pour une valeur plus grande ou supprimez-le pour voir tout.

Notez que lorsque le genre principal est d√©crit comme 'Missing', cela signifie que Spotify ne l'a pas classifi√©. Supprimons-le.

1. Supprimez les donn√©es manquantes en les filtrant :

    ```python
    df = df[df['artist_top_genre'] != 'Missing']
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top.index,y=top.values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

    Re-v√©rifiez maintenant les genres :

    ![most popular](../../../../5-Clustering/1-Visualize/images/all-genres.png)

1. De loin, les trois genres principaux dominent ce dataset. Concentrons-nous sur `afro dancehall`, `afropop` et `nigerian pop`, et filtrons √©galement le dataset pour supprimer tout ce qui a une valeur de popularit√© √©gale √† 0 (ce qui signifie qu'il n'a pas √©t√© class√© avec une popularit√© dans le dataset et peut √™tre consid√©r√© comme du bruit pour nos objectifs) :

    ```python
    df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
    df = df[(df['popularity'] > 0)]
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top.index,y=top.values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

1. Faites un test rapide pour voir si les donn√©es pr√©sentent une corr√©lation particuli√®rement forte :

    ```python
    corrmat = df.corr(numeric_only=True)
    f, ax = plt.subplots(figsize=(12, 9))
    sns.heatmap(corrmat, vmax=.8, square=True)
    ```

    ![correlations](../../../../5-Clustering/1-Visualize/images/correlation.png)

    La seule corr√©lation forte est entre `energy` et `loudness`, ce qui n'est pas trop surprenant, √©tant donn√© que la musique forte est g√©n√©ralement assez √©nergique. Sinon, les corr√©lations sont relativement faibles. Il sera int√©ressant de voir ce qu'un algorithme de clustering peut tirer de ces donn√©es.

    > üéì Notez que la corr√©lation n'implique pas la causalit√© ! Nous avons une preuve de corr√©lation mais aucune preuve de causalit√©. Un [site web amusant](https://tylervigen.com/spurious-correlations) propose des visuels qui mettent en √©vidence ce point.

Y a-t-il une convergence dans ce dataset autour de la popularit√© per√ßue d'une chanson et de sa capacit√© √† faire danser ? Une FacetGrid montre qu'il existe des cercles concentriques qui s'alignent, quel que soit le genre. Serait-il possible que les go√ªts nig√©rians convergent √† un certain niveau de capacit√© √† faire danser pour ce genre ?  

‚úÖ Essayez diff√©rents points de donn√©es (√©nergie, loudness, speechiness) et plus ou diff√©rents genres musicaux. Que pouvez-vous d√©couvrir ? Consultez le tableau `df.describe()` pour voir la r√©partition g√©n√©rale des points de donn√©es.

### Exercice - distribution des donn√©es

Ces trois genres sont-ils significativement diff√©rents dans la perception de leur capacit√© √† faire danser, en fonction de leur popularit√© ?

1. Examinez la distribution des donn√©es de nos trois genres principaux pour la popularit√© et la capacit√© √† faire danser le long d'un axe x et y donn√©.

    ```python
    sns.set_theme(style="ticks")
    
    g = sns.jointplot(
        data=df,
        x="popularity", y="danceability", hue="artist_top_genre",
        kind="kde",
    )
    ```

    Vous pouvez d√©couvrir des cercles concentriques autour d'un point de convergence g√©n√©ral, montrant la distribution des points.

    > üéì Notez que cet exemple utilise un graphique KDE (Kernel Density Estimate) qui repr√©sente les donn√©es √† l'aide d'une courbe de densit√© de probabilit√© continue. Cela nous permet d'interpr√©ter les donn√©es lorsque nous travaillons avec plusieurs distributions.

    En g√©n√©ral, les trois genres s'alignent vaguement en termes de popularit√© et de capacit√© √† faire danser. D√©terminer des clusters dans ces donn√©es faiblement align√©es sera un d√©fi :

    ![distribution](../../../../5-Clustering/1-Visualize/images/distribution.png)

1. Cr√©ez un scatter plot :

    ```python
    sns.FacetGrid(df, hue="artist_top_genre", height=5) \
       .map(plt.scatter, "popularity", "danceability") \
       .add_legend()
    ```

    Un scatter plot des m√™mes axes montre un sch√©ma similaire de convergence.

    ![Facetgrid](../../../../5-Clustering/1-Visualize/images/facetgrid.png)

En g√©n√©ral, pour le clustering, vous pouvez utiliser des scatter plots pour montrer des clusters de donn√©es, donc ma√Ætriser ce type de visualisation est tr√®s utile. Dans la prochaine le√ßon, nous utiliserons ces donn√©es filtr√©es et appliquerons le clustering k-means pour d√©couvrir des groupes dans ces donn√©es qui semblent se chevaucher de mani√®re int√©ressante.

---

## üöÄD√©fi

En pr√©paration de la prochaine le√ßon, cr√©ez un tableau sur les diff√©rents algorithmes de clustering que vous pourriez d√©couvrir et utiliser dans un environnement de production. Quels types de probl√®mes le clustering cherche-t-il √† r√©soudre ?

## [Quiz post-lecture](https://ff-quizzes.netlify.app/en/ml/)

## R√©vision & Auto-√©tude

Avant d'appliquer des algorithmes de clustering, comme nous l'avons appris, il est judicieux de comprendre la nature de votre dataset. Lisez davantage sur ce sujet [ici](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)

[Cet article utile](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) vous guide √† travers les diff√©rentes fa√ßons dont divers algorithmes de clustering se comportent, en fonction des formes des donn√©es.

## Devoir

[Recherchez d'autres visualisations pour le clustering](assignment.md)

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.