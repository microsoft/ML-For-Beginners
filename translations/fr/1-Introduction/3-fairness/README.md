<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-04T22:59:38+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "fr"
}
-->
# Construire des solutions de Machine Learning avec une IA responsable

![R√©sum√© de l'IA responsable dans le Machine Learning sous forme de sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote par [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz avant le cours](https://ff-quizzes.netlify.app/en/ml/)

## Introduction

Dans ce programme, vous commencerez √† d√©couvrir comment le machine learning influence et impacte nos vies quotidiennes. Aujourd'hui, des syst√®mes et mod√®les participent √† des t√¢ches de prise de d√©cision quotidienne, comme les diagnostics m√©dicaux, les approbations de pr√™ts ou la d√©tection de fraudes. Il est donc crucial que ces mod√®les fonctionnent correctement pour fournir des r√©sultats fiables. Comme tout logiciel, les syst√®mes d'IA peuvent ne pas r√©pondre aux attentes ou produire des r√©sultats ind√©sirables. C'est pourquoi il est essentiel de comprendre et d'expliquer le comportement d'un mod√®le d'IA.

Imaginez ce qui peut arriver lorsque les donn√©es utilis√©es pour construire ces mod√®les manquent de repr√©sentations de certains groupes d√©mographiques, comme la race, le genre, les opinions politiques ou la religion, ou lorsqu'elles surrepr√©sentent certains groupes. Que se passe-t-il lorsque les r√©sultats du mod√®le favorisent un groupe d√©mographique particulier ? Quelles sont les cons√©quences pour l'application ? De plus, que se passe-t-il lorsque le mod√®le produit un r√©sultat n√©gatif et nuit aux personnes ? Qui est responsable du comportement des syst√®mes d'IA ? Ce sont quelques-unes des questions que nous explorerons dans ce programme.

Dans cette le√ßon, vous allez :

- Prendre conscience de l'importance de l'√©quit√© dans le machine learning et des pr√©judices li√©s √† l'injustice.
- Vous familiariser avec la pratique d'exploration des cas atypiques et des sc√©narios inhabituels pour garantir fiabilit√© et s√©curit√©.
- Comprendre la n√©cessit√© de concevoir des syst√®mes inclusifs pour autonomiser tout le monde.
- Explorer l'importance de prot√©ger la vie priv√©e et la s√©curit√© des donn√©es et des individus.
- D√©couvrir l'importance d'une approche transparente pour expliquer le comportement des mod√®les d'IA.
- √ätre attentif √† l'importance de la responsabilit√© pour instaurer la confiance dans les syst√®mes d'IA.

## Pr√©requis

En guise de pr√©requis, veuillez suivre le parcours d'apprentissage "Principes de l'IA responsable" et regarder la vid√©o ci-dessous sur le sujet :

Apprenez-en davantage sur l'IA responsable en suivant ce [parcours d'apprentissage](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Approche de Microsoft sur l'IA responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Approche de Microsoft sur l'IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : Approche de Microsoft sur l'IA responsable

## √âquit√©

Les syst√®mes d'IA doivent traiter tout le monde de mani√®re √©quitable et √©viter d'affecter diff√©remment des groupes similaires. Par exemple, lorsque les syst√®mes d'IA fournissent des recommandations sur les traitements m√©dicaux, les demandes de pr√™t ou l'emploi, ils doivent faire les m√™mes recommandations √† tous ceux qui pr√©sentent des sympt√¥mes, des circonstances financi√®res ou des qualifications professionnelles similaires. Chacun de nous, en tant qu'humain, porte des biais h√©rit√©s qui influencent nos d√©cisions et actions. Ces biais peuvent se refl√©ter dans les donn√©es utilis√©es pour entra√Æner les syst√®mes d'IA. Ces manipulations peuvent parfois se produire involontairement. Il est souvent difficile de savoir consciemment quand on introduit des biais dans les donn√©es.

**L'‚Äúinjustice‚Äù** englobe les impacts n√©gatifs, ou ‚Äúpr√©judices‚Äù, pour un groupe de personnes, comme ceux d√©finis en termes de race, genre, √¢ge ou statut de handicap. Les principaux pr√©judices li√©s √† l'√©quit√© peuvent √™tre class√©s comme suit :

- **Allocation**, lorsque, par exemple, un genre ou une ethnie est favoris√© par rapport √† un autre.
- **Qualit√© du service**. Si vous entra√Ænez les donn√©es pour un sc√©nario sp√©cifique alors que la r√©alit√© est bien plus complexe, cela conduit √† un service de mauvaise qualit√©. Par exemple, un distributeur de savon qui ne semble pas d√©tecter les personnes √† la peau fonc√©e. [R√©f√©rence](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **D√©nigrement**. Critiquer ou √©tiqueter injustement quelque chose ou quelqu'un. Par exemple, une technologie de reconnaissance d'images a tristement mal identifi√© des images de personnes √† la peau fonc√©e comme des gorilles.
- **Sur- ou sous-repr√©sentation**. L'id√©e qu'un certain groupe n'est pas repr√©sent√© dans une profession donn√©e, et tout service ou fonction qui continue de promouvoir cela contribue au pr√©judice.
- **St√©r√©otypage**. Associer un groupe donn√© √† des attributs pr√©assign√©s. Par exemple, un syst√®me de traduction entre l'anglais et le turc peut pr√©senter des inexactitudes dues √† des associations st√©r√©otyp√©es li√©es au genre.

![traduction en turc](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> traduction en turc

![traduction en anglais](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> traduction en anglais

Lors de la conception et des tests des syst√®mes d'IA, nous devons nous assurer que l'IA est √©quitable et qu'elle n'est pas programm√©e pour prendre des d√©cisions biais√©es ou discriminatoires, que les √™tres humains sont √©galement interdits de prendre. Garantir l'√©quit√© dans l'IA et le machine learning reste un d√©fi sociotechnique complexe.

### Fiabilit√© et s√©curit√©

Pour instaurer la confiance, les syst√®mes d'IA doivent √™tre fiables, s√ªrs et coh√©rents dans des conditions normales et inattendues. Il est important de savoir comment les syst√®mes d'IA se comporteront dans une vari√©t√© de situations, en particulier lorsqu'il s'agit de cas atypiques. Lors de la cr√©ation de solutions d'IA, il est n√©cessaire de se concentrer sur la mani√®re de g√©rer une grande vari√©t√© de circonstances que les solutions d'IA pourraient rencontrer. Par exemple, une voiture autonome doit donner la priorit√© √† la s√©curit√© des personnes. En cons√©quence, l'IA qui alimente la voiture doit prendre en compte tous les sc√©narios possibles que la voiture pourrait rencontrer, comme la nuit, les orages ou les temp√™tes de neige, les enfants traversant la rue, les animaux domestiques, les travaux routiers, etc. La capacit√© d'un syst√®me d'IA √† g√©rer de mani√®re fiable et s√ªre une large gamme de conditions refl√®te le niveau d'anticipation que le data scientist ou le d√©veloppeur d'IA a pris en compte lors de la conception ou des tests du syst√®me.

> [üé• Cliquez ici pour une vid√©o : ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusion

Les syst√®mes d'IA doivent √™tre con√ßus pour engager et autonomiser tout le monde. Lors de la conception et de la mise en ≈ìuvre des syst√®mes d'IA, les data scientists et les d√©veloppeurs d'IA identifient et abordent les obstacles potentiels dans le syst√®me qui pourraient exclure involontairement des personnes. Par exemple, il y a 1 milliard de personnes handicap√©es dans le monde. Avec les avanc√©es de l'IA, elles peuvent acc√©der plus facilement √† une large gamme d'informations et d'opportunit√©s dans leur vie quotidienne. En abordant ces obstacles, cela cr√©e des opportunit√©s d'innover et de d√©velopper des produits d'IA avec de meilleures exp√©riences qui profitent √† tout le monde.

> [üé• Cliquez ici pour une vid√©o : inclusion dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### S√©curit√© et confidentialit√©

Les syst√®mes d'IA doivent √™tre s√ªrs et respecter la vie priv√©e des individus. Les gens ont moins confiance dans les syst√®mes qui mettent leur vie priv√©e, leurs informations ou leur s√©curit√© en danger. Lors de l'entra√Ænement des mod√®les de machine learning, nous nous appuyons sur les donn√©es pour produire les meilleurs r√©sultats. Ce faisant, l'origine des donn√©es et leur int√©grit√© doivent √™tre prises en compte. Par exemple, les donn√©es ont-elles √©t√© soumises par les utilisateurs ou sont-elles disponibles publiquement ? Ensuite, en travaillant avec les donn√©es, il est crucial de d√©velopper des syst√®mes d'IA capables de prot√©ger les informations confidentielles et de r√©sister aux attaques. √Ä mesure que l'IA devient plus r√©pandue, prot√©ger la vie priv√©e et s√©curiser les informations personnelles et professionnelles importantes devient de plus en plus critique et complexe. Les questions de confidentialit√© et de s√©curit√© des donn√©es n√©cessitent une attention particuli√®re pour l'IA, car l'acc√®s aux donn√©es est essentiel pour que les syst√®mes d'IA fassent des pr√©dictions et des d√©cisions pr√©cises et √©clair√©es sur les individus.

> [üé• Cliquez ici pour une vid√©o : s√©curit√© dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- En tant qu'industrie, nous avons r√©alis√© des avanc√©es significatives en mati√®re de confidentialit√© et de s√©curit√©, largement aliment√©es par des r√©glementations comme le RGPD (R√®glement G√©n√©ral sur la Protection des Donn√©es).
- Cependant, avec les syst√®mes d'IA, nous devons reconna√Ætre la tension entre le besoin de donn√©es personnelles pour rendre les syst√®mes plus efficaces et la protection de la vie priv√©e.
- Tout comme avec la naissance des ordinateurs connect√©s √† Internet, nous observons √©galement une augmentation significative des probl√®mes de s√©curit√© li√©s √† l'IA.
- En m√™me temps, nous avons vu l'IA √™tre utilis√©e pour am√©liorer la s√©curit√©. Par exemple, la plupart des scanners antivirus modernes sont aujourd'hui aliment√©s par des heuristiques d'IA.
- Nous devons nous assurer que nos processus de Data Science s'harmonisent avec les derni√®res pratiques en mati√®re de confidentialit√© et de s√©curit√©.

### Transparence

Les syst√®mes d'IA doivent √™tre compr√©hensibles. Une partie essentielle de la transparence consiste √† expliquer le comportement des syst√®mes d'IA et de leurs composants. Am√©liorer la compr√©hension des syst√®mes d'IA n√©cessite que les parties prenantes comprennent comment et pourquoi ils fonctionnent afin qu'elles puissent identifier les probl√®mes de performance potentiels, les pr√©occupations en mati√®re de s√©curit√© et de confidentialit√©, les biais, les pratiques d'exclusion ou les r√©sultats involontaires. Nous pensons √©galement que ceux qui utilisent les syst√®mes d'IA doivent √™tre honn√™tes et transparents sur le moment, les raisons et la mani√®re dont ils choisissent de les d√©ployer, ainsi que sur les limites des syst√®mes qu'ils utilisent. Par exemple, si une banque utilise un syst√®me d'IA pour soutenir ses d√©cisions de pr√™t aux consommateurs, il est important d'examiner les r√©sultats et de comprendre quelles donn√©es influencent les recommandations du syst√®me. Les gouvernements commencent √† r√©glementer l'IA dans divers secteurs, donc les data scientists et les organisations doivent expliquer si un syst√®me d'IA r√©pond aux exigences r√©glementaires, en particulier lorsqu'il y a un r√©sultat ind√©sirable.

> [üé• Cliquez ici pour une vid√©o : transparence dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Parce que les syst√®mes d'IA sont si complexes, il est difficile de comprendre leur fonctionnement et d'interpr√©ter les r√©sultats.
- Ce manque de compr√©hension affecte la mani√®re dont ces syst√®mes sont g√©r√©s, op√©rationnalis√©s et document√©s.
- Ce manque de compr√©hension affecte surtout les d√©cisions prises en utilisant les r√©sultats produits par ces syst√®mes.

### Responsabilit√©

Les personnes qui con√ßoivent et d√©ploient des syst√®mes d'IA doivent √™tre responsables de leur fonctionnement. La n√©cessit√© de responsabilit√© est particuli√®rement cruciale avec les technologies sensibles comme la reconnaissance faciale. R√©cemment, la demande pour la technologie de reconnaissance faciale a augment√©, notamment de la part des organisations de maintien de l'ordre qui voient le potentiel de cette technologie pour des usages comme retrouver des enfants disparus. Cependant, ces technologies pourraient √™tre utilis√©es par un gouvernement pour mettre en danger les libert√©s fondamentales de ses citoyens, par exemple en permettant une surveillance continue de certains individus. Ainsi, les data scientists et les organisations doivent √™tre responsables de l'impact de leur syst√®me d'IA sur les individus ou la soci√©t√©.

[![Un chercheur en IA met en garde contre la surveillance de masse via la reconnaissance faciale](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Approche de Microsoft sur l'IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : Mises en garde contre la surveillance de masse via la reconnaissance faciale

En fin de compte, l'une des plus grandes questions pour notre g√©n√©ration, en tant que premi√®re g√©n√©ration √† introduire l'IA dans la soci√©t√©, est de savoir comment garantir que les ordinateurs resteront responsables envers les humains et comment s'assurer que les personnes qui con√ßoivent les ordinateurs restent responsables envers tout le monde.

## √âvaluation de l'impact

Avant d'entra√Æner un mod√®le de machine learning, il est important de r√©aliser une √©valuation de l'impact pour comprendre l'objectif du syst√®me d'IA, son utilisation pr√©vue, son lieu de d√©ploiement et les personnes qui interagiront avec le syst√®me. Ces √©l√©ments sont utiles pour les examinateurs ou testeurs qui √©valuent le syst√®me afin de savoir quels facteurs prendre en compte pour identifier les risques potentiels et les cons√©quences attendues.

Les domaines suivants sont √† examiner lors de l'√©valuation de l'impact :

* **Impact n√©gatif sur les individus**. √ätre conscient de toute restriction ou exigence, utilisation non prise en charge ou toute limitation connue entravant les performances du syst√®me est essentiel pour garantir que le syst√®me n'est pas utilis√© d'une mani√®re qui pourrait nuire aux individus.
* **Exigences en mati√®re de donn√©es**. Comprendre comment et o√π le syst√®me utilisera les donn√©es permet aux examinateurs d'explorer les exigences en mati√®re de donn√©es dont vous devez tenir compte (par exemple, les r√©glementations RGPD ou HIPAA). En outre, examinez si la source ou la quantit√© de donn√©es est suffisante pour l'entra√Ænement.
* **R√©sum√© de l'impact**. Rassemblez une liste des pr√©judices potentiels qui pourraient d√©couler de l'utilisation du syst√®me. Tout au long du cycle de vie du ML, v√©rifiez si les probl√®mes identifi√©s sont att√©nu√©s ou r√©solus.
* **Objectifs applicables** pour chacun des six principes fondamentaux. √âvaluez si les objectifs de chaque principe sont atteints et s'il existe des lacunes.

## D√©bogage avec une IA responsable

Tout comme le d√©bogage d'une application logicielle, le d√©bogage d'un syst√®me d'IA est un processus n√©cessaire pour identifier et r√©soudre les probl√®mes dans le syst√®me. De nombreux facteurs peuvent affecter les performances d'un mod√®le et l'emp√™cher d'agir de mani√®re responsable. La plupart des m√©triques traditionnelles de performance des mod√®les sont des agr√©gats quantitatifs des performances d'un mod√®le, ce qui n'est pas suffisant pour analyser comment un mod√®le viole les principes de l'IA responsable. De plus, un mod√®le de machine learning est une bo√Æte noire, ce qui rend difficile de comprendre ce qui motive ses r√©sultats ou d'expliquer ses erreurs. Plus tard dans ce cours, nous apprendrons √† utiliser le tableau de bord de l'IA responsable pour aider √† d√©boguer les syst√®mes d'IA. Ce tableau de bord fournit un outil holistique pour les data scientists et les d√©veloppeurs d'IA afin de r√©aliser :

* **Analyse des erreurs**. Identifier la distribution des erreurs du mod√®le qui peut affecter l'√©quit√© ou la fiabilit√© du syst√®me.
* **Vue d'ensemble du mod√®le**. D√©couvrir o√π il existe des disparit√©s dans les performances du mod√®le √† travers les cohortes de donn√©es.
* **Analyse des donn√©es**. Comprendre la distribution des donn√©es et identifier tout biais potentiel dans les donn√©es pouvant entra√Æner des probl√®mes d'√©quit√©, d'inclusion et de fiabilit√©.
* **Interpr√©tabilit√© du mod√®le**. Comprendre ce qui affecte ou influence les pr√©dictions du mod√®le. Cela aide √† expliquer le comportement du mod√®le, ce qui est important pour la transparence et la responsabilit√©.

## üöÄ D√©fi

Pour √©viter que des pr√©judices ne soient introduits d√®s le d√©part, nous devrions :

- avoir une diversit√© de parcours et de perspectives parmi les personnes travaillant sur les syst√®mes
- investir dans des ensembles de donn√©es qui refl√®tent la diversit√© de notre soci√©t√©
- d√©velopper de meilleures m√©thodes tout au long du cycle de vie du machine learning pour d√©tecter et corriger les probl√®mes li√©s √† l'IA responsable lorsqu'ils surviennent

R√©fl√©chissez √† des sc√©narios r√©els o√π le manque de fiabilit√© d'un mod√®le est √©vident dans sa cr√©ation et son utilisation. Que devrions-nous consid√©rer d'autre ?

## [Quiz apr√®s le cours](https://ff-quizzes.netlify.app/en/ml/)

## R√©vision et √©tude personnelle

Dans cette le√ßon, vous avez appris les bases des concepts d'√©quit√© et d'injustice dans le machine learning.
Regardez cet atelier pour approfondir les sujets abord√©s :

- √Ä la recherche d'une IA responsable : Mettre les principes en pratique par Besmira Nushi, Mehrnoosh Sameki et Amit Sharma

[![Responsible AI Toolbox : Un cadre open-source pour construire une IA responsable](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox : Un cadre open-source pour construire une IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : RAI Toolbox : Un cadre open-source pour construire une IA responsable par Besmira Nushi, Mehrnoosh Sameki et Amit Sharma

√Ä lire √©galement :

- Centre de ressources RAI de Microsoft : [Ressources sur l'IA responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Groupe de recherche FATE de Microsoft : [FATE : √âquit√©, Responsabilit√©, Transparence et √âthique dans l'IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox :

- [D√©p√¥t GitHub de Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

D√©couvrez les outils d'Azure Machine Learning pour garantir l'√©quit√© :

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Devoir

[Explorez RAI Toolbox](assignment.md)

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.