# Pr√©vision de s√©ries temporelles avec le Support Vector Regressor

Dans la le√ßon pr√©c√©dente, vous avez appris √† utiliser le mod√®le ARIMA pour faire des pr√©visions de s√©ries temporelles. Maintenant, vous allez vous int√©resser au mod√®le Support Vector Regressor, qui est un mod√®le de r√©gression utilis√© pour pr√©dire des donn√©es continues.

## [Quiz pr√©-lecture](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/51/)

## Introduction

Dans cette le√ßon, vous d√©couvrirez une m√©thode sp√©cifique pour construire des mod√®les avec [**SVM** : **S**upport **V**ector **M**achine](https://en.wikipedia.org/wiki/Support-vector_machine) pour la r√©gression, ou **SVR : Support Vector Regressor**.

### SVR dans le contexte des s√©ries temporelles [^1]

Avant de comprendre l'importance de SVR dans la pr√©vision des s√©ries temporelles, voici quelques concepts importants que vous devez conna√Ætre :

- **R√©gression :** Technique d'apprentissage supervis√© pour pr√©dire des valeurs continues √† partir d'un ensemble donn√© d'entr√©es. L'id√©e est d'ajuster une courbe (ou une ligne) dans l'espace des caract√©ristiques qui a le maximum de points de donn√©es. [Cliquez ici](https://en.wikipedia.org/wiki/Regression_analysis) pour plus d'informations.
- **Support Vector Machine (SVM) :** Un type de mod√®le d'apprentissage automatique supervis√© utilis√© pour la classification, la r√©gression et la d√©tection d'outliers. Le mod√®le est un hyperplan dans l'espace des caract√©ristiques, qui dans le cas de la classification agit comme une fronti√®re, et dans le cas de la r√©gression agit comme la ligne de meilleur ajustement. Dans SVM, une fonction noyau est g√©n√©ralement utilis√©e pour transformer le jeu de donn√©es dans un espace de dimensions sup√©rieures, afin qu'ils puissent √™tre facilement s√©parables. [Cliquez ici](https://en.wikipedia.org/wiki/Support-vector_machine) pour plus d'informations sur les SVM.
- **Support Vector Regressor (SVR) :** Un type de SVM, pour trouver la ligne de meilleur ajustement (qui dans le cas de SVM est un hyperplan) qui a le maximum de points de donn√©es.

### Pourquoi SVR ? [^1]

Dans la derni√®re le√ßon, vous avez appris sur ARIMA, qui est une m√©thode statistique lin√©aire tr√®s r√©ussie pour pr√©voir des donn√©es de s√©ries temporelles. Cependant, dans de nombreux cas, les donn√©es de s√©ries temporelles pr√©sentent *une non-lin√©arit√©*, qui ne peut pas √™tre mod√©lis√©e par des mod√®les lin√©aires. Dans de tels cas, la capacit√© de SVM √† prendre en compte la non-lin√©arit√© dans les donn√©es pour les t√¢ches de r√©gression rend SVR efficace pour la pr√©vision de s√©ries temporelles.

## Exercice - construire un mod√®le SVR

Les premi√®res √©tapes de pr√©paration des donn√©es sont les m√™mes que celles de la le√ßon pr√©c√©dente sur [ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA).

Ouvrez le dossier [_/working_](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/3-SVR/working) de cette le√ßon et trouvez le fichier [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb). [^2]

1. Ex√©cutez le notebook et importez les biblioth√®ques n√©cessaires :  [^2]

   ```python
   import sys
   sys.path.append('../../')
   ```

   ```python
   import os
   import warnings
   import matplotlib.pyplot as plt
   import numpy as np
   import pandas as pd
   import datetime as dt
   import math
   
   from sklearn.svm import SVR
   from sklearn.preprocessing import MinMaxScaler
   from common.utils import load_data, mape
   ```

2. Chargez les donn√©es √† partir du fichier `/data/energy.csv` dans un dataframe Pandas et jetez un ≈ìil :  [^2]

   ```python
   energy = load_data('../../data')[['load']]
   ```

3. Tracez toutes les donn√©es √©nerg√©tiques disponibles de janvier 2012 √† d√©cembre 2014 : [^2]

   ```python
   energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)
   plt.xlabel('timestamp', fontsize=12)
   plt.ylabel('load', fontsize=12)
   plt.show()
   ```

   ![full data](../../../../translated_images/full-data.a82ec9957e580e976f651a4fc38f280b9229c6efdbe3cfe7c60abaa9486d2cbe.fr.png)

   Maintenant, construisons notre mod√®le SVR.

### Cr√©er des ensembles de donn√©es d'entra√Ænement et de test

Maintenant que vos donn√©es sont charg√©es, vous pouvez les s√©parer en ensembles d'entra√Ænement et de test. Ensuite, vous allez remodeler les donn√©es pour cr√©er un ensemble de donn√©es bas√© sur les √©tapes temporelles, ce qui sera n√©cessaire pour le SVR. Vous allez entra√Æner votre mod√®le sur l'ensemble d'entra√Ænement. Apr√®s que le mod√®le ait termin√© l'entra√Ænement, vous √©valuerez sa pr√©cision sur l'ensemble d'entra√Ænement, l'ensemble de test, puis sur l'ensemble de donn√©es complet pour voir la performance globale. Vous devez vous assurer que l'ensemble de test couvre une p√©riode ult√©rieure par rapport √† l'ensemble d'entra√Ænement pour garantir que le mod√®le ne tire pas d'informations des p√©riodes futures [^2] (une situation connue sous le nom de *Surapprentissage*).

1. Allouez une p√©riode de deux mois du 1er septembre au 31 octobre 2014 √† l'ensemble d'entra√Ænement. L'ensemble de test comprendra la p√©riode de deux mois du 1er novembre au 31 d√©cembre 2014 : [^2]

   ```python
   train_start_dt = '2014-11-01 00:00:00'
   test_start_dt = '2014-12-30 00:00:00'
   ```

2. Visualisez les diff√©rences : [^2]

   ```python
   energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \
       .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \
       .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)
   plt.xlabel('timestamp', fontsize=12)
   plt.ylabel('load', fontsize=12)
   plt.show()
   ```

   ![training and testing data](../../../../translated_images/train-test.ead0cecbfc341921d4875eccf25fed5eefbb860cdbb69cabcc2276c49e4b33e5.fr.png)

### Pr√©parer les donn√©es pour l'entra√Ænement

Maintenant, vous devez pr√©parer les donn√©es pour l'entra√Ænement en effectuant un filtrage et un redimensionnement de vos donn√©es. Filtrez votre ensemble de donn√©es pour n'inclure que les p√©riodes et colonnes n√©cessaires, et redimensionnez pour garantir que les donn√©es sont projet√©es dans l'intervalle 0,1.

1. Filtrez l'ensemble de donn√©es original pour inclure uniquement les p√©riodes mentionn√©es par ensemble et n'incluez que la colonne n√©cessaire 'load' ainsi que la date : [^2]

   ```python
   train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]
   test = energy.copy()[energy.index >= test_start_dt][['load']]
   
   print('Training data shape: ', train.shape)
   print('Test data shape: ', test.shape)
   ```

   ```output
   Training data shape:  (1416, 1)
   Test data shape:  (48, 1)
   ```
   
2. Redimensionnez les donn√©es d'entra√Ænement pour qu'elles soient dans l'intervalle (0, 1) : [^2]

   ```python
   scaler = MinMaxScaler()
   train['load'] = scaler.fit_transform(train)
   ```
   
4. Maintenant, vous redimensionnez les donn√©es de test : [^2]

   ```python
   test['load'] = scaler.transform(test)
   ```

### Cr√©er des donn√©es avec des √©tapes temporelles [^1]

Pour le SVR, vous transformez les donn√©es d'entr√©e pour qu'elles soient de la forme `[batch, timesteps]`. So, you reshape the existing `train_data` and `test_data` de sorte qu'il y ait une nouvelle dimension qui fait r√©f√©rence aux √©tapes temporelles.

```python
# Converting to numpy arrays
train_data = train.values
test_data = test.values
```

Pour cet exemple, nous prenons `timesteps = 5`. Ainsi, les entr√©es du mod√®le sont les donn√©es pour les 4 premi√®res √©tapes temporelles, et la sortie sera les donn√©es pour la 5√®me √©tape temporelle.

```python
timesteps=5
```

Conversion des donn√©es d'entra√Ænement en tenseur 2D √† l'aide de la compr√©hension de liste imbriqu√©e :

```python
train_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]
train_data_timesteps.shape
```

```output
(1412, 5)
```

Conversion des donn√©es de test en tenseur 2D :

```python
test_data_timesteps=np.array([[j for j in test_data[i:i+timesteps]] for i in range(0,len(test_data)-timesteps+1)])[:,:,0]
test_data_timesteps.shape
```

```output
(44, 5)
```

S√©lection des entr√©es et sorties √† partir des donn√©es d'entra√Ænement et de test :

```python
x_train, y_train = train_data_timesteps[:,:timesteps-1],train_data_timesteps[:,[timesteps-1]]
x_test, y_test = test_data_timesteps[:,:timesteps-1],test_data_timesteps[:,[timesteps-1]]

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
```

```output
(1412, 4) (1412, 1)
(44, 4) (44, 1)
```

### Impl√©menter SVR [^1]

Maintenant, il est temps d'impl√©menter SVR. Pour en savoir plus sur cette impl√©mentation, vous pouvez consulter [cette documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Pour notre impl√©mentation, nous suivons ces √©tapes :

  1. D√©finir le mod√®le en appelant la fonction `SVR()` and passing in the model hyperparameters: kernel, gamma, c and epsilon
  2. Prepare the model for the training data by calling the `fit()` function
  3. Make predictions calling the `predict()`

Maintenant, nous cr√©ons un mod√®le SVR. Ici, nous utilisons le [noyau RBF](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel) et fixons les hyperparam√®tres gamma, C et epsilon respectivement √† 0.5, 10 et 0.05.

```python
model = SVR(kernel='rbf',gamma=0.5, C=10, epsilon = 0.05)
```

#### Ajuster le mod√®le sur les donn√©es d'entra√Ænement [^1]

```python
model.fit(x_train, y_train[:,0])
```

```output
SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.05, gamma=0.5,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
```

#### Faire des pr√©dictions avec le mod√®le [^1]

```python
y_train_pred = model.predict(x_train).reshape(-1,1)
y_test_pred = model.predict(x_test).reshape(-1,1)

print(y_train_pred.shape, y_test_pred.shape)
```

```output
(1412, 1) (44, 1)
```

Vous avez construit votre SVR ! Maintenant, nous devons l'√©valuer.

### √âvaluer votre mod√®le [^1]

Pour l'√©valuation, nous allons d'abord redimensionner les donn√©es √† notre √©chelle originale. Ensuite, pour v√©rifier la performance, nous tracerons le graphique des s√©ries temporelles originales et pr√©dites, et nous imprimerons √©galement le r√©sultat MAPE.

Redimensionnez la sortie pr√©dite et originale :

```python
# Scaling the predictions
y_train_pred = scaler.inverse_transform(y_train_pred)
y_test_pred = scaler.inverse_transform(y_test_pred)

print(len(y_train_pred), len(y_test_pred))
```

```python
# Scaling the original values
y_train = scaler.inverse_transform(y_train)
y_test = scaler.inverse_transform(y_test)

print(len(y_train), len(y_test))
```

#### V√©rifier la performance du mod√®le sur les donn√©es d'entra√Ænement et de test [^1]

Nous extrayons les horodatages de l'ensemble de donn√©es pour les afficher sur l'axe des x de notre graphique. Notez que nous utilisons les premi√®res ```timesteps-1``` valeurs comme entr√©e pour la premi√®re sortie, donc les horodatages pour la sortie commenceront apr√®s cela.

```python
train_timestamps = energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)].index[timesteps-1:]
test_timestamps = energy[test_start_dt:].index[timesteps-1:]

print(len(train_timestamps), len(test_timestamps))
```

```output
1412 44
```

Tracez les pr√©dictions pour les donn√©es d'entra√Ænement :

```python
plt.figure(figsize=(25,6))
plt.plot(train_timestamps, y_train, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(train_timestamps, y_train_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.title("Training data prediction")
plt.show()
```

![training data prediction](../../../../translated_images/train-data-predict.3c4ef4e78553104ffdd53d47a4c06414007947ea328e9261ddf48d3eafdefbbf.fr.png)

Imprimez le MAPE pour les donn√©es d'entra√Ænement

```python
print('MAPE for training data: ', mape(y_train_pred, y_train)*100, '%')
```

```output
MAPE for training data: 1.7195710200875551 %
```

Tracez les pr√©dictions pour les donn√©es de test

```python
plt.figure(figsize=(10,3))
plt.plot(test_timestamps, y_test, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(test_timestamps, y_test_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

![testing data prediction](../../../../translated_images/test-data-predict.8afc47ee7e52874f514ebdda4a798647e9ecf44a97cc927c535246fcf7a28aa9.fr.png)

Imprimez le MAPE pour les donn√©es de test

```python
print('MAPE for testing data: ', mape(y_test_pred, y_test)*100, '%')
```

```output
MAPE for testing data:  1.2623790187854018 %
```

üèÜ Vous avez obtenu un tr√®s bon r√©sultat sur l'ensemble de donn√©es de test !

### V√©rifier la performance du mod√®le sur l'ensemble de donn√©es complet [^1]

```python
# Extracting load values as numpy array
data = energy.copy().values

# Scaling
data = scaler.transform(data)

# Transforming to 2D tensor as per model input requirement
data_timesteps=np.array([[j for j in data[i:i+timesteps]] for i in range(0,len(data)-timesteps+1)])[:,:,0]
print("Tensor shape: ", data_timesteps.shape)

# Selecting inputs and outputs from data
X, Y = data_timesteps[:,:timesteps-1],data_timesteps[:,[timesteps-1]]
print("X shape: ", X.shape,"\nY shape: ", Y.shape)
```

```output
Tensor shape:  (26300, 5)
X shape:  (26300, 4) 
Y shape:  (26300, 1)
```

```python
# Make model predictions
Y_pred = model.predict(X).reshape(-1,1)

# Inverse scale and reshape
Y_pred = scaler.inverse_transform(Y_pred)
Y = scaler.inverse_transform(Y)
```

```python
plt.figure(figsize=(30,8))
plt.plot(Y, color = 'red', linewidth=2.0, alpha = 0.6)
plt.plot(Y_pred, color = 'blue', linewidth=0.8)
plt.legend(['Actual','Predicted'])
plt.xlabel('Timestamp')
plt.show()
```

![full data prediction](../../../../translated_images/full-data-predict.4f0fed16a131c8f3bcc57a3060039dc7f2f714a05b07b68c513e0fe7fb3d8964.fr.png)

```python
print('MAPE: ', mape(Y_pred, Y)*100, '%')
```

```output
MAPE:  2.0572089029888656 %
```

üèÜ De tr√®s beaux graphiques, montrant un mod√®le avec une bonne pr√©cision. Bien jou√© !

---

## üöÄD√©fi

- Essayez d'ajuster les hyperparam√®tres (gamma, C, epsilon) lors de la cr√©ation du mod√®le et √©valuez-les sur les donn√©es pour voir quel ensemble d'hyperparam√®tres donne les meilleurs r√©sultats sur les donn√©es de test. Pour en savoir plus sur ces hyperparam√®tres, vous pouvez consulter le document [ici](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel).
- Essayez d'utiliser diff√©rentes fonctions noyau pour le mod√®le et analysez leurs performances sur l'ensemble de donn√©es. Un document utile peut √™tre trouv√© [ici](https://scikit-learn.org/stable/modules/svm.html#kernel-functions).
- Essayez d'utiliser diff√©rentes valeurs pour `timesteps` afin que le mod√®le puisse remonter dans le temps pour faire des pr√©dictions.

## [Quiz post-lecture](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/52/)

## Revue et auto-apprentissage

Cette le√ßon avait pour but d'introduire l'application de SVR pour la pr√©vision de s√©ries temporelles. Pour en savoir plus sur SVR, vous pouvez consulter [ce blog](https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/). Cette [documentation sur scikit-learn](https://scikit-learn.org/stable/modules/svm.html) fournit une explication plus compl√®te sur les SVM en g√©n√©ral, [les SVR](https://scikit-learn.org/stable/modules/svm.html#regression) et √©galement d'autres d√©tails d'impl√©mentation tels que les diff√©rentes [fonctions noyau](https://scikit-learn.org/stable/modules/svm.html#kernel-functions) qui peuvent √™tre utilis√©es, ainsi que leurs param√®tres.

## Devoir

[Un nouveau mod√®le SVR](assignment.md)

## Cr√©dits

[^1]: Le texte, le code et la sortie de cette section ont √©t√© contribu√© par [@AnirbanMukherjeeXD](https://github.com/AnirbanMukherjeeXD)  
[^2]: Le texte, le code et la sortie de cette section ont √©t√© pris de [ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous visons √† l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoris√©e. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.