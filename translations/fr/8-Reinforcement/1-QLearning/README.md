<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0ffe994d1cc881bdeb49226a064116e5",
  "translation_date": "2025-09-04T00:15:37+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "fr"
}
-->
# Introduction √† l'apprentissage par renforcement et au Q-Learning

![R√©sum√© du renforcement dans l'apprentissage automatique sous forme de sketchnote](../../../../translated_images/ml-reinforcement.94024374d63348dbb3571c343ca7ddabef72adac0b8086d47164b769ba3a8a1d.fr.png)
> Sketchnote par [Tomomi Imura](https://www.twitter.com/girlie_mac)

L'apprentissage par renforcement repose sur trois concepts importants : l'agent, des √©tats, et un ensemble d'actions par √©tat. En ex√©cutant une action dans un √©tat donn√©, l'agent re√ßoit une r√©compense. Imaginez √† nouveau le jeu vid√©o Super Mario. Vous √™tes Mario, dans un niveau de jeu, debout pr√®s du bord d'une falaise. Au-dessus de vous se trouve une pi√®ce. Vous, √©tant Mario, dans un niveau de jeu, √† une position sp√©cifique... c'est votre √©tat. Faire un pas vers la droite (une action) vous ferait tomber dans le vide, ce qui vous donnerait un score num√©rique faible. Cependant, appuyer sur le bouton de saut vous permettrait de marquer un point et de rester en vie. C'est un r√©sultat positif qui devrait vous attribuer un score num√©rique positif.

En utilisant l'apprentissage par renforcement et un simulateur (le jeu), vous pouvez apprendre √† jouer au jeu pour maximiser la r√©compense, c'est-√†-dire rester en vie et marquer autant de points que possible.

[![Introduction √† l'apprentissage par renforcement](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Cliquez sur l'image ci-dessus pour √©couter Dmitry parler de l'apprentissage par renforcement

## [Quiz avant la le√ßon](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/)

## Pr√©requis et configuration

Dans cette le√ßon, nous allons exp√©rimenter avec du code en Python. Vous devriez √™tre en mesure d'ex√©cuter le code du Jupyter Notebook de cette le√ßon, soit sur votre ordinateur, soit dans le cloud.

Vous pouvez ouvrir [le notebook de la le√ßon](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) et suivre cette le√ßon pour construire.

> **Note :** Si vous ouvrez ce code depuis le cloud, vous devez √©galement r√©cup√©rer le fichier [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), qui est utilis√© dans le code du notebook. Ajoutez-le au m√™me r√©pertoire que le notebook.

## Introduction

Dans cette le√ßon, nous allons explorer le monde de **[Pierre et le Loup](https://fr.wikipedia.org/wiki/Pierre_et_le_Loup)**, inspir√© d'un conte musical du compositeur russe [Sergei Prokofiev](https://fr.wikipedia.org/wiki/Sergei_Prokofiev). Nous utiliserons **l'apprentissage par renforcement** pour permettre √† Pierre d'explorer son environnement, de collecter de d√©licieuses pommes et d'√©viter de rencontrer le loup.

**L'apprentissage par renforcement** (RL) est une technique d'apprentissage qui nous permet d'apprendre un comportement optimal d'un **agent** dans un **environnement** en r√©alisant de nombreuses exp√©riences. Un agent dans cet environnement doit avoir un **objectif**, d√©fini par une **fonction de r√©compense**.

## L'environnement

Pour simplifier, consid√©rons le monde de Pierre comme un plateau carr√© de taille `largeur` x `hauteur`, comme ceci :

![Environnement de Pierre](../../../../translated_images/environment.40ba3cb66256c93fa7e92f6f7214e1d1f588aafa97d266c11d108c5c5d101b6c.fr.png)

Chaque case de ce plateau peut √™tre :

* **sol**, sur lequel Pierre et d'autres cr√©atures peuvent marcher.
* **eau**, sur laquelle il est √©videmment impossible de marcher.
* un **arbre** ou de l'**herbe**, un endroit o√π l'on peut se reposer.
* une **pomme**, que Pierre serait ravi de trouver pour se nourrir.
* un **loup**, qui est dangereux et doit √™tre √©vit√©.

Il existe un module Python s√©par√©, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), qui contient le code pour travailler avec cet environnement. Comme ce code n'est pas essentiel pour comprendre nos concepts, nous allons importer le module et l'utiliser pour cr√©er le plateau d'exemple (bloc de code 1) :

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Ce code devrait afficher une image de l'environnement similaire √† celle ci-dessus.

## Actions et politique

Dans notre exemple, l'objectif de Pierre serait de trouver une pomme tout en √©vitant le loup et les autres obstacles. Pour ce faire, il peut essentiellement se d√©placer jusqu'√† ce qu'il trouve une pomme.

Ainsi, √† n'importe quelle position, il peut choisir entre l'une des actions suivantes : haut, bas, gauche et droite.

Nous allons d√©finir ces actions sous forme de dictionnaire et les associer √† des paires de changements de coordonn√©es correspondants. Par exemple, se d√©placer √† droite (`R`) correspondrait √† une paire `(1,0)`. (bloc de code 2) :

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Pour r√©sumer, la strat√©gie et l'objectif de ce sc√©nario sont les suivants :

- **La strat√©gie** de notre agent (Pierre) est d√©finie par ce qu'on appelle une **politique**. Une politique est une fonction qui retourne l'action √† effectuer dans un √©tat donn√©. Dans notre cas, l'√©tat du probl√®me est repr√©sent√© par le plateau, y compris la position actuelle du joueur.

- **L'objectif** de l'apprentissage par renforcement est d'apprendre une bonne politique qui nous permettra de r√©soudre le probl√®me efficacement. Cependant, comme point de d√©part, consid√©rons la politique la plus simple appel√©e **marche al√©atoire**.

## Marche al√©atoire

R√©solvons d'abord notre probl√®me en impl√©mentant une strat√©gie de marche al√©atoire. Avec la marche al√©atoire, nous choisirons al√©atoirement la prochaine action parmi les actions autoris√©es, jusqu'√† ce que nous atteignions la pomme (bloc de code 3).

1. Impl√©mentez la marche al√©atoire avec le code ci-dessous :

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    L'appel √† `walk` devrait retourner la longueur du chemin correspondant, qui peut varier d'une ex√©cution √† l'autre.

1. Ex√©cutez l'exp√©rience de marche plusieurs fois (disons, 100), et affichez les statistiques r√©sultantes (bloc de code 4) :

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Notez que la longueur moyenne d'un chemin est d'environ 30-40 √©tapes, ce qui est assez √©lev√©, √©tant donn√© que la distance moyenne jusqu'√† la pomme la plus proche est d'environ 5-6 √©tapes.

    Vous pouvez √©galement voir √† quoi ressemble le mouvement de Pierre pendant la marche al√©atoire :

    ![Marche al√©atoire de Pierre](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Fonction de r√©compense

Pour rendre notre politique plus intelligente, nous devons comprendre quels mouvements sont "meilleurs" que d'autres. Pour ce faire, nous devons d√©finir notre objectif.

L'objectif peut √™tre d√©fini en termes de **fonction de r√©compense**, qui retournera une valeur de score pour chaque √©tat. Plus le nombre est √©lev√©, meilleure est la fonction de r√©compense. (bloc de code 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Un aspect int√©ressant des fonctions de r√©compense est que dans la plupart des cas, *nous ne recevons une r√©compense substantielle qu'√† la fin du jeu*. Cela signifie que notre algorithme doit d'une certaine mani√®re se souvenir des "bons" pas qui m√®nent √† une r√©compense positive √† la fin, et augmenter leur importance. De m√™me, tous les mouvements qui m√®nent √† de mauvais r√©sultats doivent √™tre d√©courag√©s.

## Q-Learning

L'algorithme que nous allons discuter ici s'appelle **Q-Learning**. Dans cet algorithme, la politique est d√©finie par une fonction (ou une structure de donn√©es) appel√©e **Q-Table**. Elle enregistre la "qualit√©" de chaque action dans un √©tat donn√©.

On l'appelle Q-Table car il est souvent pratique de la repr√©senter sous forme de tableau ou de tableau multidimensionnel. √âtant donn√© que notre plateau a des dimensions `largeur` x `hauteur`, nous pouvons repr√©senter la Q-Table √† l'aide d'un tableau numpy de forme `largeur` x `hauteur` x `len(actions)` : (bloc de code 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Notez que nous initialisons toutes les valeurs de la Q-Table avec une valeur √©gale, dans notre cas - 0.25. Cela correspond √† la politique de "marche al√©atoire", car tous les mouvements dans chaque √©tat sont √©galement bons. Nous pouvons passer la Q-Table √† la fonction `plot` afin de visualiser la table sur le plateau : `m.plot(Q)`.

![Environnement de Pierre](../../../../translated_images/env_init.04e8f26d2d60089e128f21d22e5fef57d580e559f0d5937b06c689e5e7cdd438.fr.png)

Au centre de chaque case, il y a une "fl√®che" qui indique la direction pr√©f√©r√©e du mouvement. √âtant donn√© que toutes les directions sont √©gales, un point est affich√©.

Nous devons maintenant ex√©cuter la simulation, explorer notre environnement, et apprendre une meilleure distribution des valeurs de la Q-Table, ce qui nous permettra de trouver le chemin vers la pomme beaucoup plus rapidement.

## Essence du Q-Learning : √âquation de Bellman

Une fois que nous commen√ßons √† nous d√©placer, chaque action aura une r√©compense correspondante, c'est-√†-dire que nous pourrions th√©oriquement s√©lectionner la prochaine action en fonction de la r√©compense imm√©diate la plus √©lev√©e. Cependant, dans la plupart des √©tats, le mouvement n'atteindra pas notre objectif de trouver la pomme, et nous ne pouvons donc pas imm√©diatement d√©cider quelle direction est meilleure.

> Rappelez-vous que ce n'est pas le r√©sultat imm√©diat qui compte, mais plut√¥t le r√©sultat final, que nous obtiendrons √† la fin de la simulation.

Pour tenir compte de cette r√©compense diff√©r√©e, nous devons utiliser les principes de la **[programmation dynamique](https://fr.wikipedia.org/wiki/Programmation_dynamique)**, qui nous permettent de r√©fl√©chir √† notre probl√®me de mani√®re r√©cursive.

Supposons que nous sommes maintenant dans l'√©tat *s*, et que nous voulons passer √† l'√©tat suivant *s'*. En le faisant, nous recevrons la r√©compense imm√©diate *r(s,a)*, d√©finie par la fonction de r√©compense, plus une certaine r√©compense future. Si nous supposons que notre Q-Table refl√®te correctement l'"attractivit√©" de chaque action, alors dans l'√©tat *s'*, nous choisirons une action *a* qui correspond √† la valeur maximale de *Q(s',a')*. Ainsi, la meilleure r√©compense future possible que nous pourrions obtenir dans l'√©tat *s* sera d√©finie comme `max`

## V√©rification de la politique

√âtant donn√© que la Q-Table liste l'"attractivit√©" de chaque action dans chaque √©tat, il est assez facile de l'utiliser pour d√©finir une navigation efficace dans notre monde. Dans le cas le plus simple, nous pouvons s√©lectionner l'action correspondant √† la valeur la plus √©lev√©e de la Q-Table : (bloc de code 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Si vous essayez le code ci-dessus plusieurs fois, vous remarquerez peut-√™tre qu'il "bloque" parfois, et vous devez appuyer sur le bouton STOP dans le notebook pour l'interrompre. Cela se produit parce qu'il peut y avoir des situations o√π deux √©tats "pointent" l'un vers l'autre en termes de valeur Q optimale, ce qui am√®ne l'agent √† se d√©placer ind√©finiment entre ces √©tats.

## üöÄD√©fi

> **T√¢che 1 :** Modifiez la fonction `walk` pour limiter la longueur maximale du chemin √† un certain nombre de pas (par exemple, 100), et observez le code ci-dessus retourner cette valeur de temps en temps.

> **T√¢che 2 :** Modifiez la fonction `walk` afin qu'elle ne retourne pas aux endroits o√π elle est d√©j√† pass√©e auparavant. Cela emp√™chera `walk` de boucler, cependant, l'agent peut toujours se retrouver "pi√©g√©" dans un endroit dont il ne peut pas s'√©chapper.

## Navigation

Une meilleure politique de navigation serait celle que nous avons utilis√©e pendant l'entra√Ænement, qui combine exploitation et exploration. Dans cette politique, nous s√©lectionnerons chaque action avec une certaine probabilit√©, proportionnelle aux valeurs dans la Q-Table. Cette strat√©gie peut encore amener l'agent √† retourner √† une position qu'il a d√©j√† explor√©e, mais, comme vous pouvez le voir dans le code ci-dessous, elle aboutit √† un chemin moyen tr√®s court vers l'emplacement souhait√© (rappelez-vous que `print_statistics` ex√©cute la simulation 100 fois) : (bloc de code 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Apr√®s avoir ex√©cut√© ce code, vous devriez obtenir une longueur moyenne de chemin beaucoup plus petite qu'auparavant, dans une plage de 3 √† 6.

## √âtudier le processus d'apprentissage

Comme nous l'avons mentionn√©, le processus d'apprentissage est un √©quilibre entre l'exploration et l'exploitation des connaissances acquises sur la structure de l'espace probl√®me. Nous avons vu que les r√©sultats de l'apprentissage (la capacit√© √† aider un agent √† trouver un chemin court vers l'objectif) se sont am√©lior√©s, mais il est √©galement int√©ressant d'observer comment la longueur moyenne du chemin √©volue pendant le processus d'apprentissage :

## Les apprentissages peuvent √™tre r√©sum√©s comme suit :

- **La longueur moyenne du chemin augmente**. Ce que nous observons ici, c'est qu'au d√©but, la longueur moyenne du chemin augmente. Cela est probablement d√ª au fait que lorsque nous ne savons rien de l'environnement, nous sommes susceptibles de nous retrouver pi√©g√©s dans des √©tats d√©favorables, comme l'eau ou le loup. √Ä mesure que nous apprenons davantage et commen√ßons √† utiliser ces connaissances, nous pouvons explorer l'environnement plus longtemps, mais nous ne savons toujours pas tr√®s bien o√π se trouvent les pommes.

- **La longueur du chemin diminue √† mesure que nous apprenons davantage**. Une fois que nous avons suffisamment appris, il devient plus facile pour l'agent d'atteindre l'objectif, et la longueur du chemin commence √† diminuer. Cependant, nous restons ouverts √† l'exploration, ce qui nous am√®ne souvent √† nous √©loigner du meilleur chemin et √† explorer de nouvelles options, rendant le chemin plus long que l'optimal.

- **Augmentation brusque de la longueur**. Ce que nous observons √©galement sur ce graphique, c'est qu'√† un certain moment, la longueur a augment√© brusquement. Cela indique la nature stochastique du processus, et qu'√† un moment donn√©, nous pouvons "alt√©rer" les coefficients de la Q-Table en les rempla√ßant par de nouvelles valeurs. Cela devrait id√©alement √™tre minimis√© en diminuant le taux d'apprentissage (par exemple, vers la fin de l'entra√Ænement, nous ajustons les valeurs de la Q-Table uniquement par une petite valeur).

Dans l'ensemble, il est important de se rappeler que le succ√®s et la qualit√© du processus d'apprentissage d√©pendent significativement des param√®tres, tels que le taux d'apprentissage, la d√©croissance du taux d'apprentissage et le facteur de r√©duction. Ceux-ci sont souvent appel√©s **hyperparam√®tres**, pour les distinguer des **param√®tres**, que nous optimisons pendant l'entra√Ænement (par exemple, les coefficients de la Q-Table). Le processus de recherche des meilleures valeurs d'hyperparam√®tres est appel√© **optimisation des hyperparam√®tres**, et il m√©rite un sujet √† part enti√®re.

## [Quiz post-lecture](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Devoir 
[Un monde plus r√©aliste](assignment.md)

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle effectu√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.