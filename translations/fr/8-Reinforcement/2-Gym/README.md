<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9660fbd80845c59c15715cb418cd6e23",
  "translation_date": "2025-09-04T00:25:46+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "fr"
}
-->
## Pr√©requis

Dans cette le√ßon, nous utiliserons une biblioth√®que appel√©e **OpenAI Gym** pour simuler diff√©rents **environnements**. Vous pouvez ex√©cuter le code de cette le√ßon localement (par exemple, depuis Visual Studio Code), auquel cas la simulation s'ouvrira dans une nouvelle fen√™tre. Si vous ex√©cutez le code en ligne, vous devrez peut-√™tre apporter quelques ajustements au code, comme d√©crit [ici](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Dans la le√ßon pr√©c√©dente, les r√®gles du jeu et l'√©tat √©taient d√©finis par la classe `Board` que nous avons cr√©√©e nous-m√™mes. Ici, nous utiliserons un **environnement de simulation** sp√©cial, qui simulera la physique derri√®re le balancement du poteau. L'un des environnements de simulation les plus populaires pour entra√Æner des algorithmes d'apprentissage par renforcement est appel√© [Gym](https://gym.openai.com/), maintenu par [OpenAI](https://openai.com/). Gr√¢ce √† ce gym, nous pouvons cr√©er diff√©rents **environnements**, allant de la simulation de CartPole aux jeux Atari.

> **Note** : Vous pouvez voir les autres environnements disponibles dans OpenAI Gym [ici](https://gym.openai.com/envs/#classic_control).

Tout d'abord, installons le gym et importons les biblioth√®ques n√©cessaires (bloc de code 1) :

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Exercice - initialiser un environnement CartPole

Pour travailler sur le probl√®me d'√©quilibrage du CartPole, nous devons initialiser l'environnement correspondant. Chaque environnement est associ√© √† :

- **Observation space** qui d√©finit la structure des informations que nous recevons de l'environnement. Pour le probl√®me CartPole, nous recevons la position du poteau, la vitesse et quelques autres valeurs.

- **Action space** qui d√©finit les actions possibles. Dans notre cas, l'espace d'action est discret et se compose de deux actions : **gauche** et **droite**. (bloc de code 2)

1. Pour initialiser, tapez le code suivant :

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Pour voir comment fonctionne l'environnement, ex√©cutons une courte simulation de 100 √©tapes. √Ä chaque √©tape, nous fournissons une action √† effectuer - dans cette simulation, nous s√©lectionnons simplement une action au hasard dans `action_space`.

1. Ex√©cutez le code ci-dessous et observez le r√©sultat.

    ‚úÖ N'oubliez pas qu'il est pr√©f√©rable d'ex√©cuter ce code sur une installation locale de Python ! (bloc de code 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Vous devriez voir quelque chose de similaire √† cette image :

    ![CartPole sans √©quilibre](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Pendant la simulation, nous devons obtenir des observations pour d√©cider comment agir. En fait, la fonction step retourne les observations actuelles, une fonction de r√©compense et un indicateur `done` qui indique s'il est pertinent de continuer la simulation ou non : (bloc de code 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Vous verrez quelque chose comme ceci dans la sortie du notebook :

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Le vecteur d'observation retourn√© √† chaque √©tape de la simulation contient les valeurs suivantes :
    - Position du chariot
    - Vitesse du chariot
    - Angle du poteau
    - Taux de rotation du poteau

1. Obtenez les valeurs minimales et maximales de ces nombres : (bloc de code 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Vous remarquerez √©galement que la valeur de r√©compense √† chaque √©tape de la simulation est toujours 1. Cela s'explique par le fait que notre objectif est de survivre le plus longtemps possible, c'est-√†-dire de maintenir le poteau dans une position raisonnablement verticale pendant la p√©riode la plus longue possible.

    ‚úÖ En fait, la simulation CartPole est consid√©r√©e comme r√©solue si nous parvenons √† obtenir une r√©compense moyenne de 195 sur 100 essais cons√©cutifs.

## Discr√©tisation de l'√©tat

Dans le Q-Learning, nous devons construire une Q-Table qui d√©finit quoi faire √† chaque √©tat. Pour ce faire, l'√©tat doit √™tre **discret**, plus pr√©cis√©ment, il doit contenir un nombre fini de valeurs discr√®tes. Ainsi, nous devons d'une mani√®re ou d'une autre **discr√©tiser** nos observations, en les mappant √† un ensemble fini d'√©tats.

Il existe plusieurs fa√ßons de proc√©der :

- **Diviser en intervalles**. Si nous connaissons l'intervalle d'une certaine valeur, nous pouvons diviser cet intervalle en un certain nombre d'**intervalles**, puis remplacer la valeur par le num√©ro de l'intervalle auquel elle appartient. Cela peut √™tre fait en utilisant la m√©thode [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) de numpy. Dans ce cas, nous conna√Ætrons pr√©cis√©ment la taille de l'√©tat, car elle d√©pendra du nombre d'intervalles que nous s√©lectionnons pour la digitalisation.

‚úÖ Nous pouvons utiliser l'interpolation lin√©aire pour ramener les valeurs √† un certain intervalle fini (par exemple, de -20 √† 20), puis convertir les nombres en entiers en les arrondissant. Cela nous donne un peu moins de contr√¥le sur la taille de l'√©tat, surtout si nous ne connaissons pas les plages exactes des valeurs d'entr√©e. Par exemple, dans notre cas, 2 des 4 valeurs n'ont pas de limites sup√©rieures/inf√©rieures, ce qui peut entra√Æner un nombre infini d'√©tats.

Dans notre exemple, nous opterons pour la deuxi√®me approche. Comme vous le remarquerez plus tard, malgr√© l'absence de limites sup√©rieures/inf√©rieures d√©finies, ces valeurs prennent rarement des valeurs en dehors de certains intervalles finis, donc ces √©tats avec des valeurs extr√™mes seront tr√®s rares.

1. Voici la fonction qui prendra l'observation de notre mod√®le et produira un tuple de 4 valeurs enti√®res : (bloc de code 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Explorons √©galement une autre m√©thode de discr√©tisation utilisant des intervalles : (bloc de code 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Ex√©cutons maintenant une courte simulation et observons ces valeurs discr√®tes de l'environnement. N'h√©sitez pas √† essayer `discretize` et `discretize_bins` et √† voir s'il y a une diff√©rence.

    ‚úÖ `discretize_bins` retourne le num√©ro de l'intervalle, qui commence √† 0. Ainsi, pour les valeurs de la variable d'entr√©e autour de 0, il retourne le num√©ro du milieu de l'intervalle (10). Dans `discretize`, nous ne nous sommes pas souci√©s de la plage des valeurs de sortie, leur permettant d'√™tre n√©gatives, donc les valeurs d'√©tat ne sont pas d√©cal√©es, et 0 correspond √† 0. (bloc de code 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ D√©commentez la ligne commen√ßant par env.render si vous souhaitez voir comment l'environnement s'ex√©cute. Sinon, vous pouvez l'ex√©cuter en arri√®re-plan, ce qui est plus rapide. Nous utiliserons cette ex√©cution "invisible" pendant notre processus de Q-Learning.

## La structure de la Q-Table

Dans notre le√ßon pr√©c√©dente, l'√©tat √©tait une simple paire de nombres allant de 0 √† 8, et il √©tait donc pratique de repr√©senter la Q-Table par un tenseur numpy de forme 8x8x2. Si nous utilisons la discr√©tisation par intervalles, la taille de notre vecteur d'√©tat est √©galement connue, nous pouvons donc utiliser la m√™me approche et repr√©senter l'√©tat par un tableau de forme 20x20x10x10x2 (ici 2 est la dimension de l'espace d'action, et les premi√®res dimensions correspondent au nombre d'intervalles que nous avons choisi d'utiliser pour chacun des param√®tres dans l'espace d'observation).

Cependant, parfois, les dimensions pr√©cises de l'espace d'observation ne sont pas connues. Dans le cas de la fonction `discretize`, nous ne pouvons jamais √™tre s√ªrs que notre √©tat reste dans certaines limites, car certaines des valeurs originales ne sont pas born√©es. Ainsi, nous utiliserons une approche l√©g√®rement diff√©rente et repr√©senterons la Q-Table par un dictionnaire.

1. Utilisez la paire *(state,action)* comme cl√© du dictionnaire, et la valeur correspondra √† la valeur de l'entr√©e de la Q-Table. (bloc de code 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Ici, nous d√©finissons √©galement une fonction `qvalues()`, qui retourne une liste de valeurs de la Q-Table pour un √©tat donn√© correspondant √† toutes les actions possibles. Si l'entr√©e n'est pas pr√©sente dans la Q-Table, nous retournerons 0 par d√©faut.

## Commen√ßons le Q-Learning

Nous sommes maintenant pr√™ts √† apprendre √† Peter √† maintenir l'√©quilibre !

1. Tout d'abord, d√©finissons quelques hyperparam√®tres : (bloc de code 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Ici, `alpha` est le **taux d'apprentissage** qui d√©finit dans quelle mesure nous devons ajuster les valeurs actuelles de la Q-Table √† chaque √©tape. Dans la le√ßon pr√©c√©dente, nous avons commenc√© avec 1, puis diminu√© `alpha` √† des valeurs plus faibles pendant l'entra√Ænement. Dans cet exemple, nous le garderons constant pour simplifier, et vous pourrez exp√©rimenter avec l'ajustement des valeurs de `alpha` plus tard.

    `gamma` est le **facteur d'actualisation** qui montre dans quelle mesure nous devons privil√©gier la r√©compense future par rapport √† la r√©compense actuelle.

    `epsilon` est le **facteur d'exploration/exploitation** qui d√©termine si nous devons pr√©f√©rer l'exploration √† l'exploitation ou vice versa. Dans notre algorithme, nous s√©lectionnerons dans `epsilon` pourcentage des cas la prochaine action selon les valeurs de la Q-Table, et dans le reste des cas, nous ex√©cuterons une action al√©atoire. Cela nous permettra d'explorer des zones de l'espace de recherche que nous n'avons jamais vues auparavant.

    ‚úÖ En termes d'√©quilibrage - choisir une action al√©atoire (exploration) agirait comme un coup al√©atoire dans la mauvaise direction, et le poteau devrait apprendre √† r√©cup√©rer l'√©quilibre √† partir de ces "erreurs".

### Am√©liorer l'algorithme

Nous pouvons √©galement apporter deux am√©liorations √† notre algorithme de la le√ßon pr√©c√©dente :

- **Calculer la r√©compense cumulative moyenne**, sur un certain nombre de simulations. Nous imprimerons les progr√®s tous les 5000 it√©rations, et nous ferons la moyenne de notre r√©compense cumulative sur cette p√©riode. Cela signifie que si nous obtenons plus de 195 points, nous pouvons consid√©rer le probl√®me comme r√©solu, avec une qualit√© encore sup√©rieure √† celle requise.

- **Calculer le r√©sultat cumulatif moyen maximum**, `Qmax`, et nous stockerons la Q-Table correspondant √† ce r√©sultat. Lorsque vous ex√©cutez l'entra√Ænement, vous remarquerez que parfois le r√©sultat cumulatif moyen commence √† diminuer, et nous voulons conserver les valeurs de la Q-Table qui correspondent au meilleur mod√®le observ√© pendant l'entra√Ænement.

1. Collectez toutes les r√©compenses cumulatives √† chaque simulation dans le vecteur `rewards` pour un futur trac√©. (bloc de code 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Ce que vous pouvez remarquer √† partir de ces r√©sultats :

- **Proche de notre objectif**. Nous sommes tr√®s proches d'atteindre l'objectif de 195 r√©compenses cumulatives sur 100+ ex√©cutions cons√©cutives de la simulation, ou nous l'avons peut-√™tre d√©j√† atteint ! M√™me si nous obtenons des nombres plus faibles, nous ne le savons pas, car nous faisons la moyenne sur 5000 ex√©cutions, et seuls 100 ex√©cutions sont n√©cessaires dans les crit√®res formels.

- **La r√©compense commence √† diminuer**. Parfois, la r√©compense commence √† diminuer, ce qui signifie que nous pouvons "d√©truire" les valeurs d√©j√† apprises dans la Q-Table avec celles qui aggravent la situation.

Cette observation est plus clairement visible si nous tra√ßons les progr√®s de l'entra√Ænement.

## Tracer les progr√®s de l'entra√Ænement

Pendant l'entra√Ænement, nous avons collect√© la valeur de la r√©compense cumulative √† chaque it√©ration dans le vecteur `rewards`. Voici √† quoi cela ressemble lorsque nous le tra√ßons par rapport au num√©ro d'it√©ration :

```python
plt.plot(rewards)
```

![progr√®s brut](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.fr.png)

√Ä partir de ce graphique, il n'est pas possible de tirer des conclusions, car en raison de la nature du processus d'entra√Ænement stochastique, la dur√©e des sessions d'entra√Ænement varie consid√©rablement. Pour donner plus de sens √† ce graphique, nous pouvons calculer la **moyenne mobile** sur une s√©rie d'exp√©riences, disons 100. Cela peut √™tre fait facilement en utilisant `np.convolve` : (bloc de code 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progr√®s de l'entra√Ænement](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.fr.png)

## Variation des hyperparam√®tres

Pour rendre l'apprentissage plus stable, il est judicieux d'ajuster certains de nos hyperparam√®tres pendant l'entra√Ænement. En particulier :

- **Pour le taux d'apprentissage**, `alpha`, nous pouvons commencer avec des valeurs proches de 1, puis diminuer progressivement le param√®tre. Avec le temps, nous obtiendrons de bonnes probabilit√©s dans la Q-Table, et nous devrions donc les ajuster l√©g√®rement, et non les √©craser compl√®tement avec de nouvelles valeurs.

- **Augmenter epsilon**. Nous pouvons vouloir augmenter lentement `epsilon`, afin d'explorer moins et d'exploiter davantage. Il est probablement judicieux de commencer avec une valeur plus faible de `epsilon`, et de monter jusqu'√† presque 1.
> **T√¢che 1** : Exp√©rimentez avec les valeurs des hyperparam√®tres et voyez si vous pouvez obtenir une r√©compense cumulative plus √©lev√©e. Atteignez-vous plus de 195 ?
> **T√¢che 2** : Pour r√©soudre formellement le probl√®me, vous devez atteindre une r√©compense moyenne de 195 sur 100 ex√©cutions cons√©cutives. Mesurez cela pendant l'entra√Ænement et assurez-vous d'avoir r√©solu le probl√®me de mani√®re formelle !

## Voir le r√©sultat en action

Il serait int√©ressant de voir comment le mod√®le entra√Æn√© se comporte r√©ellement. Lan√ßons la simulation et suivons la m√™me strat√©gie de s√©lection d'actions qu'au cours de l'entra√Ænement, en √©chantillonnant selon la distribution de probabilit√© dans la Q-Table : (bloc de code 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Vous devriez voir quelque chose comme ceci :

![un chariot en √©quilibre](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄD√©fi

> **T√¢che 3** : Ici, nous utilisions la copie finale de la Q-Table, qui n'est peut-√™tre pas la meilleure. Rappelez-vous que nous avons stock√© la Q-Table la plus performante dans la variable `Qbest` ! Essayez le m√™me exemple avec la Q-Table la plus performante en copiant `Qbest` dans `Q` et voyez si vous remarquez une diff√©rence.

> **T√¢che 4** : Ici, nous ne s√©lectionnions pas la meilleure action √† chaque √©tape, mais plut√¥t en √©chantillonnant avec la distribution de probabilit√© correspondante. Serait-il plus logique de toujours s√©lectionner la meilleure action, celle avec la valeur la plus √©lev√©e dans la Q-Table ? Cela peut √™tre fait en utilisant la fonction `np.argmax` pour trouver le num√©ro de l'action correspondant √† la valeur la plus √©lev√©e dans la Q-Table. Impl√©mentez cette strat√©gie et voyez si cela am√©liore l'√©quilibre.

## [Quiz post-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## Devoir
[Entra√Æner une voiture de montagne](assignment.md)

## Conclusion

Nous avons maintenant appris √† entra√Æner des agents pour obtenir de bons r√©sultats simplement en leur fournissant une fonction de r√©compense qui d√©finit l'√©tat souhait√© du jeu, et en leur donnant l'opportunit√© d'explorer intelligemment l'espace de recherche. Nous avons appliqu√© avec succ√®s l'algorithme de Q-Learning dans des environnements discrets et continus, mais avec des actions discr√®tes.

Il est √©galement important d'√©tudier des situations o√π l'√©tat des actions est continu, et o√π l'espace d'observation est beaucoup plus complexe, comme l'image de l'√©cran d'un jeu Atari. Dans ces probl√®mes, nous devons souvent utiliser des techniques d'apprentissage automatique plus puissantes, telles que les r√©seaux neuronaux, pour obtenir de bons r√©sultats. Ces sujets plus avanc√©s seront abord√©s dans notre prochain cours d'IA plus avanc√©.

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.