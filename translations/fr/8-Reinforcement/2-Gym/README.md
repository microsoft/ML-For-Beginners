# Patinage CartPole

Le probl√®me que nous avons r√©solu dans la le√ßon pr√©c√©dente peut sembler √™tre un probl√®me trivial, pas vraiment applicable √† des sc√©narios de la vie r√©elle. Ce n'est pas le cas, car de nombreux probl√®mes du monde r√©el partagent √©galement ce sc√©nario - y compris le jeu d'√©checs ou de go. Ils sont similaires, car nous avons √©galement un plateau avec des r√®gles donn√©es et un **√©tat discret**.

## [Quiz pr√©-lecture](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)

## Introduction

Dans cette le√ßon, nous appliquerons les m√™mes principes de Q-Learning √† un probl√®me avec un **√©tat continu**, c'est-√†-dire un √©tat qui est d√©fini par un ou plusieurs nombres r√©els. Nous allons traiter le probl√®me suivant :

> **Probl√®me** : Si Peter veut √©chapper au loup, il doit √™tre capable de se d√©placer plus vite. Nous verrons comment Peter peut apprendre √† patiner, en particulier, √† garder son √©quilibre, en utilisant le Q-Learning.

![La grande √©vasion !](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.fr.png)

> Peter et ses amis font preuve de cr√©ativit√© pour √©chapper au loup ! Image par [Jen Looper](https://twitter.com/jenlooper)

Nous utiliserons une version simplifi√©e de l'√©quilibre connue sous le nom de probl√®me **CartPole**. Dans le monde de cartpole, nous avons un curseur horizontal qui peut se d√©placer √† gauche ou √† droite, et l'objectif est de maintenir un poteau vertical au sommet du curseur.

## Pr√©requis

Dans cette le√ßon, nous utiliserons une biblioth√®que appel√©e **OpenAI Gym** pour simuler diff√©rents **environnements**. Vous pouvez ex√©cuter le code de cette le√ßon localement (par exemple, depuis Visual Studio Code), auquel cas la simulation s'ouvrira dans une nouvelle fen√™tre. Lorsque vous ex√©cutez le code en ligne, vous devrez peut-√™tre apporter quelques modifications au code, comme d√©crit [ici](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Dans la le√ßon pr√©c√©dente, les r√®gles du jeu et l'√©tat √©taient donn√©s par la classe `Board` que nous avons d√©finie nous-m√™mes. Ici, nous utiliserons un **environnement de simulation** sp√©cial, qui simulera la physique derri√®re l'√©quilibre du poteau. L'un des environnements de simulation les plus populaires pour entra√Æner des algorithmes d'apprentissage par renforcement est appel√© [Gym](https://gym.openai.com/), qui est maintenu par [OpenAI](https://openai.com/). En utilisant ce gym, nous pouvons cr√©er diff√©rents **environnements**, allant de la simulation de cartpole aux jeux Atari.

> **Note** : Vous pouvez voir d'autres environnements disponibles dans OpenAI Gym [ici](https://gym.openai.com/envs/#classic_control).

Tout d'abord, installons le gym et importons les biblioth√®ques n√©cessaires (bloc de code 1) :

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Exercice - initialiser un environnement cartpole

Pour travailler avec un probl√®me d'√©quilibre de cartpole, nous devons initialiser l'environnement correspondant. Chaque environnement est associ√© √† un :

- **Espace d'observation** qui d√©finit la structure des informations que nous recevons de l'environnement. Pour le probl√®me cartpole, nous recevons la position du poteau, la vitesse et d'autres valeurs.

- **Espace d'action** qui d√©finit les actions possibles. Dans notre cas, l'espace d'action est discret et se compose de deux actions - **gauche** et **droite**. (bloc de code 2)

1. Pour initialiser, tapez le code suivant :

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Pour voir comment l'environnement fonctionne, ex√©cutons une courte simulation pendant 100 √©tapes. √Ä chaque √©tape, nous fournissons l'une des actions √† effectuer - dans cette simulation, nous s√©lectionnons simplement une action au hasard dans `action_space`.

1. Ex√©cutez le code ci-dessous et voyez ce que cela donne.

    ‚úÖ Rappelez-vous qu'il est pr√©f√©rable d'ex√©cuter ce code sur une installation Python locale ! (bloc de code 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Vous devriez voir quelque chose de similaire √† cette image :

    ![cartpole non √©quilibr√©](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Pendant la simulation, nous devons obtenir des observations afin de d√©cider comment agir. En fait, la fonction d'√©tape renvoie les observations actuelles, une fonction de r√©compense et le drapeau done qui indique s'il est judicieux de continuer la simulation ou non : (bloc de code 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Vous finirez par voir quelque chose comme ceci dans la sortie du notebook :

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Le vecteur d'observation qui est renvoy√© √† chaque √©tape de la simulation contient les valeurs suivantes :
    - Position du chariot
    - Vitesse du chariot
    - Angle du poteau
    - Taux de rotation du poteau

1. Obtenez la valeur minimale et maximale de ces nombres : (bloc de code 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Vous remarquerez √©galement que la valeur de la r√©compense √† chaque √©tape de simulation est toujours 1. Cela est d√ª au fait que notre objectif est de survivre le plus longtemps possible, c'est-√†-dire de maintenir le poteau dans une position raisonnablement verticale pendant la plus longue p√©riode de temps.

    ‚úÖ En fait, la simulation CartPole est consid√©r√©e comme r√©solue si nous parvenons √† obtenir une r√©compense moyenne de 195 sur 100 essais cons√©cutifs.

## Discr√©tisation de l'√©tat

Dans le Q-Learning, nous devons construire une Q-Table qui d√©finit quoi faire √† chaque √©tat. Pour pouvoir le faire, nous avons besoin que l'√©tat soit **discret**, plus pr√©cis√©ment, il doit contenir un nombre fini de valeurs discr√®tes. Ainsi, nous devons d'une mani√®re ou d'une autre **discr√©tiser** nos observations, en les mappant √† un ensemble fini d'√©tats.

Il existe plusieurs fa√ßons de proc√©der :

- **Diviser en bacs**. Si nous connaissons l'intervalle d'une certaine valeur, nous pouvons diviser cet intervalle en un certain nombre de **bacs**, puis remplacer la valeur par le num√©ro du bac auquel elle appartient. Cela peut √™tre fait en utilisant la m√©thode numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). Dans ce cas, nous conna√Ætrons pr√©cis√©ment la taille de l'√©tat, car elle d√©pendra du nombre de bacs que nous s√©lectionnons pour la num√©risation.

‚úÖ Nous pouvons utiliser l'interpolation lin√©aire pour amener les valeurs √† un certain intervalle fini (disons, de -20 √† 20), puis convertir les nombres en entiers en les arrondissant. Cela nous donne un peu moins de contr√¥le sur la taille de l'√©tat, surtout si nous ne connaissons pas les plages exactes des valeurs d'entr√©e. Par exemple, dans notre cas, 2 des 4 valeurs n'ont pas de limites sup√©rieures/inf√©rieures, ce qui peut entra√Æner un nombre infini d'√©tats.

Dans notre exemple, nous allons opter pour la deuxi√®me approche. Comme vous le remarquerez plus tard, malgr√© l'absence de limites sup√©rieures/inf√©rieures, ces valeurs prennent rarement des valeurs en dehors de certains intervalles finis, donc ces √©tats avec des valeurs extr√™mes seront tr√®s rares.

1. Voici la fonction qui prendra l'observation de notre mod√®le et produira un tuple de 4 valeurs enti√®res : (bloc de code 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Explorons √©galement une autre m√©thode de discr√©tisation utilisant des bacs : (bloc de code 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Ex√©cutons maintenant une courte simulation et observons ces valeurs d'environnement discr√®tes. N'h√©sitez pas √† essayer √† la fois `discretize` and `discretize_bins` et voir s'il y a une diff√©rence.

    ‚úÖ discretize_bins renvoie le num√©ro du bac, qui est bas√© sur 0. Ainsi, pour des valeurs de variable d'entr√©e autour de 0, cela renvoie le num√©ro du milieu de l'intervalle (10). Dans discretize, nous ne nous sommes pas souci√©s de l'intervalle des valeurs de sortie, leur permettant d'√™tre n√©gatives, donc les valeurs d'√©tat ne sont pas d√©cal√©es, et 0 correspond √† 0. (bloc de code 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ D√©commentez la ligne commen√ßant par env.render si vous voulez voir comment l'environnement s'ex√©cute. Sinon, vous pouvez l'ex√©cuter en arri√®re-plan, ce qui est plus rapide. Nous utiliserons cette ex√©cution "invisible" lors de notre processus de Q-Learning.

## La structure de la Q-Table

Dans notre le√ßon pr√©c√©dente, l'√©tat √©tait une simple paire de nombres de 0 √† 8, et il √©tait donc pratique de repr√©senter la Q-Table par un tenseur numpy de forme 8x8x2. Si nous utilisons la discr√©tisation par bacs, la taille de notre vecteur d'√©tat est √©galement connue, donc nous pouvons utiliser la m√™me approche et repr√©senter l'√©tat par un tableau de forme 20x20x10x10x2 (ici 2 est la dimension de l'espace d'action, et les premi√®res dimensions correspondent au nombre de bacs que nous avons s√©lectionn√©s pour chacun des param√®tres de l'espace d'observation).

Cependant, parfois, les dimensions pr√©cises de l'espace d'observation ne sont pas connues. Dans le cas de la fonction `discretize`, nous ne pouvons jamais √™tre s√ªrs que notre √©tat reste dans certaines limites, car certaines des valeurs d'origine ne sont pas born√©es. Ainsi, nous utiliserons une approche l√©g√®rement diff√©rente et repr√©senterons la Q-Table par un dictionnaire.

1. Utilisez la paire *(√©tat, action)* comme cl√© du dictionnaire, et la valeur correspondra √† la valeur d'entr√©e de la Q-Table. (bloc de code 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Ici, nous d√©finissons √©galement une fonction `qvalues()`, qui renvoie une liste des valeurs de la Q-Table pour un √©tat donn√© qui correspond √† toutes les actions possibles. Si l'entr√©e n'est pas pr√©sente dans la Q-Table, nous renverrons 0 par d√©faut.

## Commen√ßons le Q-Learning

Maintenant, nous sommes pr√™ts √† apprendre √† Peter √† √©quilibrer !

1. Tout d'abord, d√©finissons quelques hyperparam√®tres : (bloc de code 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Ici, `alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.

    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.

    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. 

    ‚úÖ In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those "mistakes"

### Improve the algorithm

We can also make two improvements to our algorithm from the previous lesson:

- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.
  
- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.

1. Collect all cumulative rewards at each simulation at `rewards` vecteur pour un tra√ßage ult√©rieur. (bloc de code 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Ce que vous pouvez remarquer √† partir de ces r√©sultats :

- **Proche de notre objectif**. Nous sommes tr√®s proches d'atteindre l'objectif d'obtenir 195 r√©compenses cumul√©es sur 100+ ex√©cutions cons√©cutives de la simulation, ou nous l'avons peut-√™tre d√©j√† atteint ! M√™me si nous obtenons des chiffres plus petits, nous ne le savons toujours pas, car nous faisons la moyenne sur 5000 ex√©cutions, et seulement 100 ex√©cutions sont requises dans les crit√®res formels.

- **La r√©compense commence √† diminuer**. Parfois, la r√©compense commence √† diminuer, ce qui signifie que nous pouvons "d√©truire" les valeurs d√©j√† apprises dans la Q-Table avec celles qui aggravent la situation.

Cette observation est plus clairement visible si nous tra√ßons les progr√®s de l'entra√Ænement.

## Tra√ßage des progr√®s de l'entra√Ænement

Pendant l'entra√Ænement, nous avons collect√© la valeur de la r√©compense cumul√©e √† chacune des it√©rations dans le vecteur `rewards`. Voici √† quoi cela ressemble lorsque nous le tra√ßons par rapport au num√©ro d'it√©ration :

```python
plt.plot(rewards)
```

![progr√®s brut](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.fr.png)

√Ä partir de ce graphique, il n'est pas possible de dire quoi que ce soit, car en raison de la nature du processus d'entra√Ænement stochastique, la dur√©e des sessions d'entra√Ænement varie consid√©rablement. Pour donner plus de sens √† ce graphique, nous pouvons calculer la **moyenne mobile** sur une s√©rie d'exp√©riences, disons 100. Cela peut √™tre fait facilement en utilisant `np.convolve` : (bloc de code 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progr√®s de l'entra√Ænement](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.fr.png)

## Variation des hyperparam√®tres

Pour rendre l'apprentissage plus stable, il est judicieux d'ajuster certains de nos hyperparam√®tres pendant l'entra√Ænement. En particulier :

- **Pour le taux d'apprentissage**, `alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.

- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon`, et passez √† presque 1.

> **T√¢che 1** : Jouez avec les valeurs des hyperparam√®tres et voyez si vous pouvez atteindre une r√©compense cumulative plus √©lev√©e. Obtenez-vous plus de 195 ?

> **T√¢che 2** : Pour r√©soudre formellement le probl√®me, vous devez obtenir une r√©compense moyenne de 195 sur 100 ex√©cutions cons√©cutives. Mesurez cela pendant l'entra√Ænement et assurez-vous que vous avez formellement r√©solu le probl√®me !

## Voir le r√©sultat en action

Il serait int√©ressant de voir comment le mod√®le entra√Æn√© se comporte. Ex√©cutons la simulation et suivons la m√™me strat√©gie de s√©lection d'actions que pendant l'entra√Ænement, en √©chantillonnant selon la distribution de probabilit√© dans la Q-Table : (bloc de code 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Vous devriez voir quelque chose comme ceci :

![un cartpole √©quilibr√©](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄD√©fi

> **T√¢che 3** : Ici, nous utilisions la copie finale de la Q-Table, qui peut ne pas √™tre la meilleure. N'oubliez pas que nous avons stock√© la Q-Table la plus performante dans `Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.

> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax` fonction pour d√©couvrir le num√©ro d'action correspondant √† la valeur la plus √©lev√©e de la Q-Table. Impl√©mentez cette strat√©gie et voyez si cela am√©liore l'√©quilibre.

## [Quiz post-lecture](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## Devoir
[Entra√Ænez une voiture de montagne](assignment.md)

## Conclusion

Nous avons maintenant appris comment entra√Æner des agents pour obtenir de bons r√©sultats simplement en leur fournissant une fonction de r√©compense qui d√©finit l'√©tat souhait√© du jeu, et en leur donnant l'occasion d'explorer intelligemment l'espace de recherche. Nous avons appliqu√© avec succ√®s l'algorithme Q-Learning dans les cas d'environnements discrets et continus, mais avec des actions discr√®tes.

Il est √©galement important d'√©tudier des situations o√π l'√©tat d'action est √©galement continu, et lorsque l'espace d'observation est beaucoup plus complexe, comme l'image de l'√©cran de jeu Atari. Dans ces probl√®mes, nous devons souvent utiliser des techniques d'apprentissage automatique plus puissantes, telles que les r√©seaux neuronaux, afin d'obtenir de bons r√©sultats. Ces sujets plus avanc√©s sont le sujet de notre prochain cours d'IA plus avanc√©.

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.