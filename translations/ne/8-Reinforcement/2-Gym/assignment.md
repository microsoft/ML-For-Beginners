<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-08-29T18:17:35+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "ne"
}
-->
# ट्रेन माउन्टेन कार

[OpenAI Gym](http://gym.openai.com) यसरी डिजाइन गरिएको छ कि सबै वातावरणहरूले एउटै API प्रदान गर्छन् - अर्थात् एउटै विधिहरू `reset`, `step` र `render`, र **action space** र **observation space** को एउटै संरचना। यसले गर्दा एउटै reinforcement learning एल्गोरिदमलाई विभिन्न वातावरणहरूमा न्यूनतम कोड परिवर्तनको साथ अनुकूलन गर्न सम्भव हुनुपर्छ।

## माउन्टेन कार वातावरण

[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) मा एउटा कार उपत्यकामा अड्किएको छ:

उपत्यकाबाट बाहिर निस्कन र झण्डा कब्जा गर्नको लागि, प्रत्येक चरणमा निम्न कार्यहरू मध्ये एक गर्नुपर्छ:

| मान | अर्थ |
|---|---|
| 0 | बायाँतिर गति बढाउनुहोस् |
| 1 | गति नबढाउनुहोस् |
| 2 | दायाँतिर गति बढाउनुहोस् |

तर, यस समस्याको मुख्य चुनौती भनेको कारको इन्जिन एक पटकमा पहाड चढ्न पर्याप्त बलियो छैन। त्यसैले, सफल हुनको लागि कारलाई अगाडि र पछाडि चलाएर गति बढाउनु नै एकमात्र उपाय हो।

Observation space मा केवल दुई मानहरू छन्:

| संख्या | अवलोकन  | न्यूनतम | अधिकतम |
|-----|--------------|-----|-----|
|  0  | कारको स्थिति | -1.2| 0.6 |
|  1  | कारको गति | -0.07 | 0.07 |

माउन्टेन कारको पुरस्कार प्रणाली अलि जटिल छ:

 * यदि एजेन्टले पहाडको टुप्पामा झण्डा (स्थिति = 0.5) पुगेको छ भने 0 को पुरस्कार दिइन्छ।
 * यदि एजेन्टको स्थिति 0.5 भन्दा कम छ भने -1 को पुरस्कार दिइन्छ।

एपिसोड समाप्त हुन्छ यदि कारको स्थिति 0.5 भन्दा बढी छ, वा एपिसोडको लम्बाइ 200 भन्दा बढी छ।
## निर्देशनहरू

हाम्रो reinforcement learning एल्गोरिदमलाई माउन्टेन कार समस्यालाई समाधान गर्न अनुकूलन गर्नुहोस्। [notebook.ipynb](notebook.ipynb) को विद्यमान कोडबाट सुरु गर्नुहोस्, नयाँ वातावरण प्रतिस्थापन गर्नुहोस्, state discretization functions परिवर्तन गर्नुहोस्, र विद्यमान एल्गोरिदमलाई न्यूनतम कोड परिवर्तनको साथ प्रशिक्षण दिन प्रयास गर्नुहोस्। हाइपरप्यारामिटरहरू समायोजन गरेर नतिजा अनुकूलन गर्नुहोस्।

> **Note**: एल्गोरिदमलाई सफल बनाउन हाइपरप्यारामिटरहरूको समायोजन आवश्यक हुन सक्छ। 
## मूल्यांकन मापदण्ड

| मापदण्ड | उत्कृष्ट | पर्याप्त | सुधार आवश्यक |
| -------- | --------- | -------- | ----------------- |
|          | Q-Learning एल्गोरिदम सफलतापूर्वक CartPole उदाहरणबाट अनुकूलित गरिएको छ, न्यूनतम कोड परिवर्तनको साथ, जसले 200 चरणभित्र झण्डा कब्जा गर्ने समस्या समाधान गर्न सक्षम छ। | नयाँ Q-Learning एल्गोरिदम इन्टरनेटबाट अपनाइएको छ, तर राम्रोसँग दस्तावेज गरिएको छ; वा विद्यमान एल्गोरिदम अपनाइएको छ, तर इच्छित नतिजा प्राप्त गर्दैन। | विद्यार्थीले कुनै एल्गोरिदम सफलतापूर्वक अपनाउन सकेन, तर समाधानतर्फ महत्वपूर्ण कदमहरू चालेको छ (state discretization, Q-Table डेटा संरचना, आदि कार्यान्वयन गरेको छ)। |

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।