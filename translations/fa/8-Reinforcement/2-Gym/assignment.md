<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-04T00:31:48+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "fa"
}
-->
# آموزش ماشین کوهستانی

[OpenAI Gym](http://gym.openai.com) به گونه‌ای طراحی شده است که تمام محیط‌ها یک API مشابه ارائه می‌دهند - یعنی همان متدهای `reset`، `step` و `render`، و همان انتزاعات **فضای عمل** و **فضای مشاهده**. بنابراین باید امکان پذیر باشد که همان الگوریتم‌های یادگیری تقویتی را با تغییرات کد حداقلی به محیط‌های مختلف تطبیق داد.

## محیط ماشین کوهستانی

[محیط ماشین کوهستانی](https://gym.openai.com/envs/MountainCar-v0/) شامل یک ماشین گیر افتاده در یک دره است:

هدف این است که از دره خارج شده و پرچم را بگیرید، با انجام یکی از اقدامات زیر در هر مرحله:

| مقدار | معنی |
|---|---|
| 0 | شتاب به سمت چپ |
| 1 | بدون شتاب |
| 2 | شتاب به سمت راست |

اما نکته اصلی این مسئله این است که موتور ماشین به اندازه کافی قوی نیست که در یک حرکت از کوه بالا برود. بنابراین، تنها راه موفقیت این است که ماشین را به جلو و عقب برانید تا شتاب لازم را ایجاد کنید.

فضای مشاهده فقط شامل دو مقدار است:

| شماره | مشاهده  | حداقل | حداکثر |
|-----|--------------|-----|-----|
|  0  | موقعیت ماشین | -1.2| 0.6 |
|  1  | سرعت ماشین | -0.07 | 0.07 |

سیستم پاداش برای ماشین کوهستانی کمی پیچیده است:

 * پاداش 0 زمانی داده می‌شود که عامل به پرچم (موقعیت = 0.5) در بالای کوه برسد.
 * پاداش -1 زمانی داده می‌شود که موقعیت عامل کمتر از 0.5 باشد.

قسمت زمانی پایان می‌یابد که موقعیت ماشین بیشتر از 0.5 شود، یا طول قسمت بیشتر از 200 باشد.

## دستورالعمل‌ها

الگوریتم یادگیری تقویتی ما را برای حل مسئله ماشین کوهستانی تطبیق دهید. با کد موجود در [notebook.ipynb](notebook.ipynb) شروع کنید، محیط جدید را جایگزین کنید، توابع گسسته‌سازی حالت را تغییر دهید، و سعی کنید الگوریتم موجود را با حداقل تغییرات کد آموزش دهید. نتیجه را با تنظیم هایپرپارامترها بهینه کنید.

> **توجه**: تنظیم هایپرپارامترها احتمالاً برای همگرایی الگوریتم لازم است.

## معیار ارزیابی

| معیار | عالی | کافی | نیاز به بهبود |
| -------- | --------- | -------- | ----------------- |
|          | الگوریتم Q-Learning با موفقیت از مثال CartPole تطبیق داده شده است، با حداقل تغییرات کد، که قادر است مسئله گرفتن پرچم را در کمتر از 200 مرحله حل کند. | یک الگوریتم جدید Q-Learning از اینترنت گرفته شده است، اما به خوبی مستند شده است؛ یا الگوریتم موجود تطبیق داده شده است، اما به نتایج مطلوب نمی‌رسد. | دانشجو نتوانسته است هیچ الگوریتمی را با موفقیت تطبیق دهد، اما گام‌های قابل توجهی به سمت حل مسئله برداشته است (گسسته‌سازی حالت، ساختار داده Q-Table، و غیره را پیاده‌سازی کرده است). |

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را رعایت کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادقتی‌هایی باشند. سند اصلی به زبان بومی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.