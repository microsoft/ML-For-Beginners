<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-04T22:39:13+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "fa"
}
-->
# ساخت راه‌حل‌های یادگیری ماشین با هوش مصنوعی مسئولانه

![خلاصه‌ای از هوش مصنوعی مسئولانه در یادگیری ماشین در یک طرح](../../../../sketchnotes/ml-fairness.png)
> طرح توسط [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [آزمون پیش از درس](https://ff-quizzes.netlify.app/en/ml/)

## مقدمه

در این دوره آموزشی، شما شروع به کشف خواهید کرد که چگونه یادگیری ماشین می‌تواند و در حال حاضر زندگی روزمره ما را تحت تأثیر قرار می‌دهد. حتی اکنون، سیستم‌ها و مدل‌ها در وظایف تصمیم‌گیری روزانه مانند تشخیص‌های پزشکی، تأیید وام یا شناسایی تقلب دخیل هستند. بنابراین، مهم است که این مدل‌ها به خوبی کار کنند تا نتایجی قابل اعتماد ارائه دهند. همانند هر برنامه نرم‌افزاری، سیستم‌های هوش مصنوعی ممکن است انتظارات را برآورده نکنند یا نتایج نامطلوبی داشته باشند. به همین دلیل ضروری است که بتوانیم رفتار یک مدل هوش مصنوعی را درک و توضیح دهیم.

تصور کنید چه اتفاقی می‌افتد وقتی داده‌هایی که برای ساخت این مدل‌ها استفاده می‌کنید فاقد برخی گروه‌های جمعیتی مانند نژاد، جنسیت، دیدگاه سیاسی، مذهب یا نمایندگی نامتناسب این گروه‌ها باشد. یا وقتی خروجی مدل به گونه‌ای تفسیر شود که به نفع برخی گروه‌های جمعیتی باشد. پیامد این امر برای برنامه چیست؟ علاوه بر این، وقتی مدل نتیجه‌ای نامطلوب داشته باشد و به افراد آسیب برساند، چه اتفاقی می‌افتد؟ چه کسی مسئول رفتار سیستم‌های هوش مصنوعی است؟ این‌ها برخی از سوالاتی هستند که در این دوره آموزشی بررسی خواهیم کرد.

در این درس، شما:

- اهمیت عدالت در یادگیری ماشین و آسیب‌های مرتبط با عدالت را درک خواهید کرد.
- با تمرین بررسی موارد استثنایی و سناریوهای غیرمعمول برای اطمینان از قابلیت اطمینان و ایمنی آشنا خواهید شد.
- نیاز به توانمندسازی همه افراد از طریق طراحی سیستم‌های فراگیر را درک خواهید کرد.
- اهمیت حفاظت از حریم خصوصی و امنیت داده‌ها و افراد را بررسی خواهید کرد.
- اهمیت داشتن رویکرد شفاف برای توضیح رفتار مدل‌های هوش مصنوعی را خواهید دید.
- به اهمیت مسئولیت‌پذیری برای ایجاد اعتماد در سیستم‌های هوش مصنوعی توجه خواهید کرد.

## پیش‌نیاز

به عنوان پیش‌نیاز، لطفاً مسیر یادگیری "اصول هوش مصنوعی مسئولانه" را طی کنید و ویدئوی زیر را در این موضوع مشاهده کنید:

اطلاعات بیشتر درباره هوش مصنوعی مسئولانه را با دنبال کردن این [مسیر یادگیری](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott) بیاموزید.

[![رویکرد مایکروسافت به هوش مصنوعی مسئولانه](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "رویکرد مایکروسافت به هوش مصنوعی مسئولانه")

> 🎥 روی تصویر بالا کلیک کنید برای ویدئو: رویکرد مایکروسافت به هوش مصنوعی مسئولانه

## عدالت

سیستم‌های هوش مصنوعی باید با همه افراد به طور عادلانه رفتار کنند و از تأثیرگذاری متفاوت بر گروه‌های مشابه اجتناب کنند. برای مثال، وقتی سیستم‌های هوش مصنوعی راهنمایی‌هایی درباره درمان پزشکی، درخواست‌های وام یا استخدام ارائه می‌دهند، باید توصیه‌های مشابهی به همه افراد با علائم مشابه، شرایط مالی مشابه یا صلاحیت‌های حرفه‌ای مشابه بدهند. هر یک از ما به عنوان انسان، تعصباتی را به ارث برده‌ایم که بر تصمیمات و اقدامات ما تأثیر می‌گذارد. این تعصبات می‌توانند در داده‌هایی که برای آموزش سیستم‌های هوش مصنوعی استفاده می‌کنیم، مشهود باشند. چنین دستکاری‌هایی گاهی به طور ناخواسته اتفاق می‌افتد. اغلب دشوار است که به طور آگاهانه بدانیم چه زمانی در داده‌ها تعصب وارد می‌کنیم.

**"بی‌عدالتی"** شامل تأثیرات منفی یا "آسیب‌ها" برای گروهی از افراد است، مانند گروه‌هایی که بر اساس نژاد، جنسیت، سن یا وضعیت معلولیت تعریف شده‌اند. آسیب‌های اصلی مرتبط با عدالت را می‌توان به صورت زیر طبقه‌بندی کرد:

- **تخصیص**، اگر برای مثال یک جنسیت یا قومیت بر دیگری ترجیح داده شود.
- **کیفیت خدمات**. اگر داده‌ها را برای یک سناریوی خاص آموزش دهید اما واقعیت بسیار پیچیده‌تر باشد، منجر به خدمات ضعیف می‌شود. برای مثال، یک دستگاه پخش صابون که به نظر نمی‌رسد بتواند افراد با پوست تیره را تشخیص دهد. [منبع](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **تحقیر**. انتقاد ناعادلانه و برچسب زدن به چیزی یا کسی. برای مثال، یک فناوری برچسب‌گذاری تصویر به طور بدنامی تصاویر افراد با پوست تیره را به عنوان گوریل‌ها اشتباه برچسب‌گذاری کرد.
- **نمایندگی بیش از حد یا کم**. ایده این است که یک گروه خاص در یک حرفه خاص دیده نمی‌شود، و هر خدمات یا عملکردی که به ترویج این امر ادامه دهد، به آسیب کمک می‌کند.
- **کلیشه‌سازی**. ارتباط دادن یک گروه خاص با ویژگی‌های از پیش تعیین‌شده. برای مثال، یک سیستم ترجمه زبان بین انگلیسی و ترکی ممکن است به دلیل کلمات با ارتباطات کلیشه‌ای به جنسیت، نادرستی‌هایی داشته باشد.

![ترجمه به ترکی](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> ترجمه به ترکی

![ترجمه به انگلیسی](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> ترجمه به انگلیسی

هنگام طراحی و آزمایش سیستم‌های هوش مصنوعی، باید اطمینان حاصل کنیم که هوش مصنوعی عادلانه است و به گونه‌ای برنامه‌ریزی نشده است که تصمیمات متعصبانه یا تبعیض‌آمیز بگیرد، تصمیماتی که انسان‌ها نیز از گرفتن آن‌ها منع شده‌اند. تضمین عدالت در هوش مصنوعی و یادگیری ماشین همچنان یک چالش پیچیده اجتماعی-فنی است.

### قابلیت اطمینان و ایمنی

برای ایجاد اعتماد، سیستم‌های هوش مصنوعی باید قابل اعتماد، ایمن و سازگار در شرایط عادی و غیرمنتظره باشند. مهم است بدانیم که سیستم‌های هوش مصنوعی در انواع مختلف شرایط چگونه رفتار خواهند کرد، به ویژه زمانی که موارد استثنایی هستند. هنگام ساخت راه‌حل‌های هوش مصنوعی، باید تمرکز قابل توجهی بر نحوه مدیریت انواع مختلف شرایطی که راه‌حل‌های هوش مصنوعی با آن‌ها مواجه می‌شوند، وجود داشته باشد. برای مثال، یک ماشین خودران باید ایمنی افراد را به عنوان اولویت اصلی قرار دهد. در نتیجه، هوش مصنوعی که ماشین را قدرت می‌بخشد باید تمام سناریوهای ممکن را که ماشین ممکن است با آن‌ها روبرو شود، مانند شب، طوفان‌های رعد و برق یا کولاک، کودکان در حال دویدن در خیابان، حیوانات خانگی، ساخت‌وسازهای جاده‌ای و غیره در نظر بگیرد. اینکه یک سیستم هوش مصنوعی چقدر می‌تواند طیف گسترده‌ای از شرایط را به طور قابل اعتماد و ایمن مدیریت کند، سطح پیش‌بینی‌پذیری را که دانشمند داده یا توسعه‌دهنده هوش مصنوعی در طراحی یا آزمایش سیستم در نظر گرفته است، منعکس می‌کند.

> [🎥 روی اینجا کلیک کنید برای ویدئو: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### فراگیری

سیستم‌های هوش مصنوعی باید به گونه‌ای طراحی شوند که همه افراد را درگیر و توانمند کنند. هنگام طراحی و اجرای سیستم‌های هوش مصنوعی، دانشمندان داده و توسعه‌دهندگان هوش مصنوعی موانع احتمالی در سیستم را شناسایی و برطرف می‌کنند که ممکن است به طور ناخواسته افراد را حذف کند. برای مثال، یک میلیارد نفر در سراسر جهان دارای معلولیت هستند. با پیشرفت هوش مصنوعی، آن‌ها می‌توانند به طیف گسترده‌ای از اطلاعات و فرصت‌ها در زندگی روزمره خود دسترسی آسان‌تری داشته باشند. با رفع موانع، فرصت‌هایی برای نوآوری و توسعه محصولات هوش مصنوعی با تجربیات بهتر ایجاد می‌شود که به نفع همه است.

> [🎥 روی اینجا کلیک کنید برای ویدئو: فراگیری در هوش مصنوعی](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### امنیت و حریم خصوصی

سیستم‌های هوش مصنوعی باید ایمن باشند و به حریم خصوصی افراد احترام بگذارند. افراد کمتر به سیستم‌هایی اعتماد دارند که حریم خصوصی، اطلاعات یا زندگی آن‌ها را به خطر می‌اندازند. هنگام آموزش مدل‌های یادگیری ماشین، ما به داده‌ها برای تولید بهترین نتایج متکی هستیم. در این فرآیند، منبع داده‌ها و یکپارچگی آن‌ها باید مورد توجه قرار گیرد. برای مثال، آیا داده‌ها توسط کاربر ارسال شده‌اند یا به صورت عمومی در دسترس هستند؟ سپس، هنگام کار با داده‌ها، ضروری است که سیستم‌های هوش مصنوعی توسعه داده شوند که بتوانند اطلاعات محرمانه را محافظت کنند و در برابر حملات مقاومت کنند. با گسترش هوش مصنوعی، حفاظت از حریم خصوصی و امنیت اطلاعات شخصی و تجاری مهم‌تر و پیچیده‌تر می‌شود. مسائل مربوط به حریم خصوصی و امنیت داده‌ها نیاز به توجه ویژه‌ای در هوش مصنوعی دارند زیرا دسترسی به داده‌ها برای سیستم‌های هوش مصنوعی ضروری است تا پیش‌بینی‌ها و تصمیمات دقیق و آگاهانه‌ای درباره افراد انجام دهند.

> [🎥 روی اینجا کلیک کنید برای ویدئو: امنیت در هوش مصنوعی](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- به عنوان یک صنعت، ما پیشرفت‌های قابل توجهی در حریم خصوصی و امنیت داشته‌ایم که به طور قابل توجهی توسط مقرراتی مانند GDPR (مقررات عمومی حفاظت از داده‌ها) تقویت شده است.
- با این حال، با سیستم‌های هوش مصنوعی باید تنش بین نیاز به داده‌های شخصی بیشتر برای شخصی‌تر و مؤثرتر کردن سیستم‌ها و حریم خصوصی را بپذیریم.
- همانند تولد کامپیوترهای متصل به اینترنت، ما همچنین شاهد افزایش قابل توجهی در تعداد مسائل امنیتی مرتبط با هوش مصنوعی هستیم.
- در عین حال، ما شاهد استفاده از هوش مصنوعی برای بهبود امنیت بوده‌ایم. به عنوان مثال، اکثر اسکنرهای ضد ویروس مدرن امروز توسط الگوریتم‌های هوش مصنوعی هدایت می‌شوند.
- ما باید اطمینان حاصل کنیم که فرآیندهای علم داده ما به طور هماهنگ با آخرین شیوه‌های حریم خصوصی و امنیت ترکیب شوند.

### شفافیت

سیستم‌های هوش مصنوعی باید قابل فهم باشند. بخش مهمی از شفافیت، توضیح رفتار سیستم‌های هوش مصنوعی و اجزای آن‌ها است. بهبود درک سیستم‌های هوش مصنوعی مستلزم آن است که ذینفعان نحوه و دلیل عملکرد آن‌ها را درک کنند تا بتوانند مسائل بالقوه عملکردی، نگرانی‌های ایمنی و حریم خصوصی، تعصبات، شیوه‌های حذف‌کننده یا نتایج ناخواسته را شناسایی کنند. ما همچنین معتقدیم که کسانی که از سیستم‌های هوش مصنوعی استفاده می‌کنند باید صادق و شفاف باشند که چه زمانی، چرا و چگونه تصمیم به استفاده از آن‌ها می‌گیرند. همچنین محدودیت‌های سیستم‌هایی که استفاده می‌کنند. برای مثال، اگر یک بانک از سیستم هوش مصنوعی برای حمایت از تصمیمات وام‌دهی مصرف‌کننده خود استفاده کند، مهم است که نتایج را بررسی کند و بفهمد کدام داده‌ها بر توصیه‌های سیستم تأثیر می‌گذارند. دولت‌ها شروع به تنظیم هوش مصنوعی در صنایع مختلف کرده‌اند، بنابراین دانشمندان داده و سازمان‌ها باید توضیح دهند که آیا یک سیستم هوش مصنوعی الزامات قانونی را برآورده می‌کند، به ویژه زمانی که نتیجه‌ای نامطلوب وجود دارد.

> [🎥 روی اینجا کلیک کنید برای ویدئو: شفافیت در هوش مصنوعی](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- از آنجا که سیستم‌های هوش مصنوعی بسیار پیچیده هستند، درک نحوه عملکرد آن‌ها و تفسیر نتایج دشوار است.
- این عدم درک بر نحوه مدیریت، عملیاتی شدن و مستندسازی این سیستم‌ها تأثیر می‌گذارد.
- این عدم درک مهم‌تر از همه بر تصمیماتی که با استفاده از نتایج تولید شده توسط این سیستم‌ها گرفته می‌شود، تأثیر می‌گذارد.

### مسئولیت‌پذیری

افرادی که سیستم‌های هوش مصنوعی را طراحی و اجرا می‌کنند باید مسئول نحوه عملکرد سیستم‌های خود باشند. نیاز به مسئولیت‌پذیری به ویژه در فناوری‌های حساس مانند تشخیص چهره بسیار مهم است. اخیراً، تقاضا برای فناوری تشخیص چهره افزایش یافته است، به ویژه از سوی سازمان‌های اجرای قانون که پتانسیل این فناوری را در استفاده‌هایی مانند یافتن کودکان گمشده می‌بینند. با این حال، این فناوری‌ها می‌توانند به طور بالقوه توسط یک دولت برای به خطر انداختن آزادی‌های اساسی شهروندان خود استفاده شوند، به عنوان مثال، با امکان نظارت مداوم بر افراد خاص. بنابراین، دانشمندان داده و سازمان‌ها باید مسئول تأثیر سیستم هوش مصنوعی خود بر افراد یا جامعه باشند.

[![هشدار محقق برجسته هوش مصنوعی درباره نظارت گسترده از طریق تشخیص چهره](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "رویکرد مایکروسافت به هوش مصنوعی مسئولانه")

> 🎥 روی تصویر بالا کلیک کنید برای ویدئو: هشدار درباره نظارت گسترده از طریق تشخیص چهره

در نهایت یکی از بزرگ‌ترین سوالات برای نسل ما، به عنوان اولین نسلی که هوش مصنوعی را به جامعه می‌آورد، این است که چگونه می‌توان اطمینان حاصل کرد که کامپیوترها همچنان مسئولیت‌پذیر باقی می‌مانند و چگونه می‌توان اطمینان حاصل کرد که افرادی که کامپیوترها را طراحی می‌کنند مسئولیت‌پذیر باقی می‌مانند.

## ارزیابی تأثیر

قبل از آموزش یک مدل یادگیری ماشین، مهم است که یک ارزیابی تأثیر انجام شود تا هدف سیستم هوش مصنوعی، استفاده مورد نظر، محل اجرا و افرادی که با سیستم تعامل خواهند داشت، درک شود. این موارد برای بازبین‌ها یا آزمایش‌کنندگان سیستم مفید هستند تا بدانند چه عواملی را باید هنگام شناسایی خطرات بالقوه و پیامدهای مورد انتظار در نظر بگیرند.

موارد زیر حوزه‌های تمرکز هنگام انجام ارزیابی تأثیر هستند:

* **تأثیر نامطلوب بر افراد**. آگاهی از هرگونه محدودیت یا الزامات، استفاده غیرپشتیبانی شده یا هر محدودیت شناخته شده‌ای که عملکرد سیستم را مختل می‌کند، برای اطمینان از اینکه سیستم به گونه‌ای استفاده نمی‌شود که به افراد آسیب برساند، حیاتی است.
* **نیازهای داده**. درک نحوه و محل استفاده سیستم از داده‌ها به بازبین‌ها امکان می‌دهد تا هرگونه نیاز داده‌ای را که باید به آن توجه شود (مانند مقررات داده GDPR یا HIPPA) بررسی کنند. علاوه بر این، بررسی کنید که آیا منبع یا مقدار داده برای آموزش کافی است.
* **خلاصه تأثیر**. فهرستی از آسیب‌های بالقوه‌ای که ممکن است از استفاده از سیستم ایجاد شود، جمع‌آوری کنید. در طول چرخه عمر یادگیری ماشین، بررسی کنید که آیا مسائل شناسایی شده کاهش یافته یا برطرف شده‌اند.
* **اهداف قابل اجرا** برای هر یک از شش اصل اصلی. ارزیابی کنید که آیا اهداف هر یک از اصول برآورده شده‌اند و آیا شکاف‌هایی وجود دارد.

## اشکال‌زدایی با هوش مصنوعی مسئولانه

مشابه اشکال‌زدایی یک برنامه نرم‌افزاری، اشکال‌زدایی یک سیستم هوش مصنوعی فرآیند ضروری شناسایی و حل مشکلات در سیستم است. عوامل زیادی وجود دارند که می‌توانند باعث شوند یک مدل به طور مورد انتظار یا مسئولانه عمل نکند. اکثر معیارهای عملکرد مدل‌های سنتی، تجمعات کمی از عملکرد مدل هستند که برای تحلیل نحوه نقض اصول هوش مصنوعی مسئولانه کافی نیستند. علاوه بر این، یک مدل یادگیری ماشین یک جعبه سیاه است که درک آنچه باعث نتیجه آن می‌شود یا ارائه توضیح زمانی که اشتباه می‌کند، دشوار است. در ادامه این دوره، یاد خواهیم گرفت که چگونه از داشبورد هوش مصنوعی مسئولانه برای کمک به اشکال‌زدایی سیستم‌های هوش مصنوعی استفاده کنیم. این داشبورد ابزار جامعی برای دانشمندان داده و توسعه‌دهندگان هوش مصنوعی فراهم می‌کند تا انجام دهند:

* **تحلیل خطا**. برای شناسایی توزیع خطاهای مدل که می‌تواند بر عدالت یا قابلیت اطمینان سیستم تأثیر بگذارد.
* **نمای کلی مدل**. برای کشف اینکه کجا در عملکرد مدل در میان گروه‌های داده تفاوت وجود دارد.
* **تحلیل داده‌ها**. برای درک توزیع داده‌ها و شناسایی هرگونه تعصب بالقوه در داده‌ها که می‌تواند منجر به مسائل عدالت، فراگیری و قابلیت اطمینان شود.
* **قابلیت تفسیر مدل**. برای درک اینکه چه چیزی بر پیش‌بینی‌های مدل تأثیر می‌گذارد یا آن‌ها را تحت تأثیر قرار می‌دهد. این امر به توضیح رفتار مدل کمک می‌کند که برای شفافیت و مسئولیت‌پذیری مهم است.

## 🚀 چالش

برای جلوگیری از معرفی آسیب‌ها در وهله اول، باید:

- تنوع در پیشینه‌ها و دیدگاه‌ها در میان افرادی که روی سیستم‌ها کار می‌کنند داشته باشیم.
- در مجموعه داده‌هایی که تنوع جامعه ما را منعکس می‌کنند سرمایه‌گذاری کنیم.
- روش‌های بهتری در طول چرخه
تماشای این کارگاه برای بررسی عمیق‌تر موضوعات:

- در جستجوی هوش مصنوعی مسئولانه: تبدیل اصول به عمل توسط بسیمرا نوشی، مهرنوش سامکی و آمیت شارما

[![جعبه‌ابزار هوش مصنوعی مسئولانه: یک چارچوب متن‌باز برای ساخت هوش مصنوعی مسئولانه](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: یک چارچوب متن‌باز برای ساخت هوش مصنوعی مسئولانه")

> 🎥 برای مشاهده ویدیو روی تصویر بالا کلیک کنید: جعبه‌ابزار هوش مصنوعی مسئولانه: یک چارچوب متن‌باز برای ساخت هوش مصنوعی مسئولانه توسط بسیمرا نوشی، مهرنوش سامکی و آمیت شارما

همچنین بخوانید:

- مرکز منابع هوش مصنوعی مسئولانه مایکروسافت: [منابع هوش مصنوعی مسئولانه – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- گروه تحقیقاتی FATE مایکروسافت: [FATE: عدالت، پاسخگویی، شفافیت و اخلاق در هوش مصنوعی - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

جعبه‌ابزار هوش مصنوعی مسئولانه:

- [مخزن GitHub جعبه‌ابزار هوش مصنوعی مسئولانه](https://github.com/microsoft/responsible-ai-toolbox)

درباره ابزارهای Azure Machine Learning برای تضمین عدالت بخوانید:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## تکلیف

[جعبه‌ابزار هوش مصنوعی مسئولانه را بررسی کنید](assignment.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.