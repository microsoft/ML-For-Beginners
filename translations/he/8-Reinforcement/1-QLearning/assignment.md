<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-09-05T20:17:14+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "he"
}
-->
# עולם מציאותי יותר

במצב שלנו, פיטר הצליח לנוע כמעט בלי להתעייף או להרגיש רעב. בעולם מציאותי יותר, הוא צריך לשבת ולנוח מדי פעם, וגם להאכיל את עצמו. בואו נעשה את העולם שלנו מציאותי יותר, על ידי יישום הכללים הבאים:

1. כאשר פיטר נע ממקום למקום, הוא מאבד **אנרגיה** וצובר **עייפות**.
2. פיטר יכול להחזיר לעצמו אנרגיה על ידי אכילת תפוחים.
3. פיטר יכול להיפטר מעייפות על ידי מנוחה מתחת לעץ או על הדשא (כלומר, הליכה למיקום בלוח שבו יש עץ או דשא - שדה ירוק).
4. פיטר צריך למצוא ולהרוג את הזאב.
5. כדי להרוג את הזאב, פיטר צריך להגיע לרמות מסוימות של אנרגיה ועייפות, אחרת הוא מפסיד בקרב.

## הוראות

השתמשו במחברת המקורית [notebook.ipynb](../../../../8-Reinforcement/1-QLearning/notebook.ipynb) כנקודת התחלה לפתרון שלכם.

שנו את פונקציית התגמול בהתאם לכללי המשחק, הריצו את אלגוריתם הלמידה החיזוקית כדי ללמוד את האסטרטגיה הטובה ביותר לנצח במשחק, והשוו את התוצאות של הליכה אקראית עם האלגוריתם שלכם מבחינת מספר המשחקים שניצחו והפסידו.

> **Note**: בעולם החדש שלכם, המצב מורכב יותר, ובנוסף למיקום האדם כולל גם רמות עייפות ואנרגיה. אתם יכולים לבחור לייצג את המצב כטופל (Board,energy,fatigue), או להגדיר מחלקה עבור המצב (ייתכן שתרצו גם להוריש אותה מ-`Board`), או אפילו לשנות את מחלקת `Board` המקורית בתוך [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py).

בפתרון שלכם, אנא שמרו על הקוד האחראי לאסטרטגיית ההליכה האקראית, והשוו את תוצאות האלגוריתם שלכם עם ההליכה האקראית בסוף.

> **Note**: ייתכן שתצטרכו להתאים את ההיפר-פרמטרים כדי לגרום לזה לעבוד, במיוחד את מספר האפוקים. מכיוון שהצלחה במשחק (הקרב עם הזאב) היא אירוע נדיר, אתם יכולים לצפות לזמן אימון ארוך יותר.

## קריטריונים להערכה

| קריטריון | מצטיין                                                                                                                                                                                                 | מספק                                                                                                                                                                                   | דורש שיפור                                                                                                                                |
| --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
|           | מוצגת מחברת עם הגדרת כללי עולם חדשים, אלגוריתם Q-Learning והסברים טקסטואליים. האלגוריתם מצליח לשפר משמעותית את התוצאות בהשוואה להליכה אקראית.                                                     | מוצגת מחברת, אלגוריתם Q-Learning מיושם ומשפר את התוצאות בהשוואה להליכה אקראית, אך לא באופן משמעותי; או שהמחברת מתועדת בצורה לקויה והקוד אינו מובנה היטב.                              | נעשה ניסיון להגדיר מחדש את כללי העולם, אך אלגוריתם Q-Learning אינו עובד, או שפונקציית התגמול אינה מוגדרת במלואה.                                                            |

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.