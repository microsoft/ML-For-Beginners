<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T19:31:10+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "he"
}
-->
# בניית פתרונות למידת מכונה עם AI אחראי

![סיכום של AI אחראי בלמידת מכונה בסקיצה](../../../../sketchnotes/ml-fairness.png)
> סקיצה מאת [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [שאלון לפני ההרצאה](https://ff-quizzes.netlify.app/en/ml/)

## מבוא

בתוכנית לימודים זו, תתחילו לגלות כיצד למידת מכונה משפיעה על חיי היומיום שלנו. כבר עכשיו, מערכות ומודלים מעורבים במשימות קבלת החלטות יומיומיות, כמו אבחנות רפואיות, אישורי הלוואות או זיהוי הונאות. לכן, חשוב שהמודלים הללו יפעלו בצורה טובה ויספקו תוצאות אמינות. כמו כל יישום תוכנה, מערכות AI עשויות לאכזב או להוביל לתוצאה לא רצויה. זו הסיבה שחשוב להבין ולהסביר את ההתנהגות של מודל AI.

דמיינו מה יכול לקרות כאשר הנתונים שבהם אתם משתמשים לבניית המודלים חסרים ייצוג של קבוצות דמוגרפיות מסוימות, כמו גזע, מגדר, השקפה פוליטית, דת, או מייצגים אותן באופן לא פרופורציונלי. ומה לגבי מצב שבו תוצאות המודל מפורשות כך שהן מעדיפות קבוצה דמוגרפית מסוימת? מה ההשלכות עבור היישום? בנוסף, מה קורה כאשר למודל יש תוצאה שלילית שפוגעת באנשים? מי אחראי להתנהגות של מערכות AI? אלו שאלות שנחקור בתוכנית לימודים זו.

בשיעור זה תלמדו:

- להעלות את המודעות לחשיבות ההוגנות בלמידת מכונה ולנזקים הקשורים להוגנות.
- להכיר את הפרקטיקה של חקר חריגים ותסריטים לא שגרתיים כדי להבטיח אמינות ובטיחות.
- להבין את הצורך להעצים את כולם על ידי עיצוב מערכות מכילות.
- לחקור את החשיבות של הגנה על פרטיות ואבטחת נתונים ואנשים.
- לראות את החשיבות של גישה שקופה להסבר התנהגות מודלים של AI.
- להיות מודעים לכך שאחריות היא חיונית לבניית אמון במערכות AI.

## דרישות מקדימות

כדרישה מקדימה, אנא עברו על מסלול הלמידה "עקרונות AI אחראי" וצפו בסרטון הבא בנושא:

למדו עוד על AI אחראי על ידי מעקב אחר [מסלול הלמידה](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![הגישה של Microsoft ל-AI אחראי](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "הגישה של Microsoft ל-AI אחראי")

> 🎥 לחצו על התמונה למעלה לצפייה בסרטון: הגישה של Microsoft ל-AI אחראי

## הוגנות

מערכות AI צריכות להתייחס לכולם באופן הוגן ולהימנע מהשפעה שונה על קבוצות דומות של אנשים. לדוגמה, כאשר מערכות AI מספקות המלצות לטיפול רפואי, בקשות להלוואות או תעסוקה, עליהן להציע את אותן המלצות לכל מי שיש לו סימפטומים, נסיבות פיננסיות או כישורים מקצועיים דומים. כל אחד מאיתנו נושא עמו הטיות מובנות שמשפיעות על ההחלטות והפעולות שלנו. הטיות אלו יכולות להיות ניכרות בנתונים שבהם אנו משתמשים לאימון מערכות AI. לעיתים, מניפולציה כזו מתרחשת באופן לא מכוון. קשה לעיתים לדעת באופן מודע מתי אנו מכניסים הטיה לנתונים.

**"חוסר הוגנות"** כולל השפעות שליליות, או "נזקים", לקבוצה של אנשים, כמו אלו המוגדרים לפי גזע, מגדר, גיל או מצב נכות. הנזקים העיקריים הקשורים להוגנות יכולים להיות מסווגים כ:

- **הקצאה**, אם מגדר או אתניות, לדוגמה, מועדפים על פני אחרים.
- **איכות השירות**. אם מאמנים את הנתונים לתרחיש ספציפי אך המציאות מורכבת יותר, זה מוביל לשירות בעל ביצועים ירודים. לדוגמה, מתקן סבון ידיים שלא הצליח לזהות אנשים עם עור כהה. [מקור](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **השמצה**. ביקורת לא הוגנת ותיוג של משהו או מישהו. לדוגמה, טכנולוגיית תיוג תמונות שגתה בתיוג תמונות של אנשים כהי עור כגורילות.
- **ייצוג יתר או חסר**. הרעיון הוא שקבוצה מסוימת אינה נראית במקצוע מסוים, וכל שירות או פונקציה שממשיכים לקדם זאת תורמים לנזק.
- **סטראוטיפים**. שיוך קבוצה מסוימת לתכונות שהוקצו מראש. לדוגמה, מערכת תרגום בין אנגלית לטורקית עשויה לכלול אי דיוקים בשל מילים עם שיוך סטראוטיפי למגדר.

![תרגום לטורקית](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> תרגום לטורקית

![תרגום חזרה לאנגלית](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> תרגום חזרה לאנגלית

בעת עיצוב ובדיקת מערכות AI, עלינו להבטיח שה-AI יהיה הוגן ולא יתוכנת לקבל החלטות מוטות או מפלות, שאסורות גם על בני אדם. הבטחת הוגנות ב-AI ובלמידת מכונה נותרת אתגר סוציוטכני מורכב.

### אמינות ובטיחות

כדי לבנות אמון, מערכות AI צריכות להיות אמינות, בטוחות ועקביות בתנאים רגילים ולא צפויים. חשוב לדעת כיצד מערכות AI יתנהגו במגוון מצבים, במיוחד כאשר מדובר בחריגים. בעת בניית פתרונות AI, יש להתמקד באופן משמעותי בטיפול במגוון רחב של נסיבות שהפתרונות עשויים להיתקל בהן. לדוגמה, רכב אוטונומי צריך לשים את בטיחות האנשים בראש סדר העדיפויות. כתוצאה מכך, ה-AI שמפעיל את הרכב צריך לשקול את כל התרחישים האפשריים שהרכב עשוי להיתקל בהם, כמו לילה, סופות רעמים או שלגים, ילדים שרצים ברחוב, חיות מחמד, עבודות בכביש וכו'. עד כמה מערכת AI יכולה להתמודד עם מגוון רחב של תנאים בצורה אמינה ובטוחה משקף את רמת הציפייה שהמדען נתונים או מפתח ה-AI לקחו בחשבון במהלך העיצוב או הבדיקה של המערכת.

> [🎥 לחצו כאן לצפייה בסרטון: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### הכלה

מערכות AI צריכות להיות מעוצבות כך שיתקשרו ויעצימו את כולם. בעת עיצוב ויישום מערכות AI, מדעני נתונים ומפתחי AI מזהים ומטפלים במחסומים פוטנציאליים במערכת שיכולים להוציא אנשים באופן לא מכוון. לדוגמה, ישנם מיליארד אנשים עם מוגבלויות ברחבי העולם. עם התקדמות ה-AI, הם יכולים לגשת למגוון רחב של מידע והזדמנויות בקלות רבה יותר בחיי היומיום שלהם. על ידי טיפול במחסומים, נוצרת הזדמנות לחדש ולפתח מוצרים AI עם חוויות טובות יותר שמועילות לכולם.

> [🎥 לחצו כאן לצפייה בסרטון: הכלה ב-AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### אבטחה ופרטיות

מערכות AI צריכות להיות בטוחות ולכבד את פרטיות האנשים. אנשים נותנים פחות אמון במערכות שמסכנות את פרטיותם, המידע שלהם או חייהם. בעת אימון מודלים של למידת מכונה, אנו מסתמכים על נתונים כדי להפיק את התוצאות הטובות ביותר. תוך כדי כך, יש לקחת בחשבון את מקור הנתונים ואת שלמותם. לדוגמה, האם הנתונים הוגשו על ידי משתמשים או היו זמינים לציבור? לאחר מכן, בעת עבודה עם הנתונים, חשוב לפתח מערכות AI שיכולות להגן על מידע חסוי ולהתנגד להתקפות. ככל שה-AI הופך לנפוץ יותר, הגנה על פרטיות ואבטחת מידע אישי ועסקי חשובים הופכת לקריטית ומורכבת יותר. סוגיות פרטיות ואבטחת נתונים דורשות תשומת לב מיוחדת עבור AI מכיוון שגישה לנתונים חיונית למערכות AI כדי לבצע תחזיות והחלטות מדויקות ומושכלות על אנשים.

> [🎥 לחצו כאן לצפייה בסרטון: אבטחה ב-AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- בתעשייה עשינו התקדמות משמעותית בפרטיות ואבטחה, מונעת באופן משמעותי על ידי רגולציות כמו ה-GDPR (General Data Protection Regulation).
- עם זאת, במערכות AI עלינו להכיר במתח בין הצורך ביותר נתונים אישיים כדי להפוך את המערכות ליותר אישיות ויעילות – לבין פרטיות.
- כמו עם לידת המחשבים המחוברים לאינטרנט, אנו גם רואים עלייה משמעותית במספר סוגיות האבטחה הקשורות ל-AI.
- באותו הזמן, ראינו שימוש ב-AI לשיפור האבטחה. לדוגמה, רוב סורקי האנטי-וירוס המודרניים מונעים על ידי AI היום.
- עלינו להבטיח שתהליכי מדע הנתונים שלנו ישתלבו בהרמוניה עם שיטות הפרטיות והאבטחה העדכניות ביותר.

### שקיפות

מערכות AI צריכות להיות מובנות. חלק קריטי בשקיפות הוא הסבר ההתנהגות של מערכות AI ושל רכיביהן. שיפור ההבנה של מערכות AI דורש שהגורמים המעורבים יבינו כיצד ולמה הן פועלות, כך שיוכלו לזהות בעיות ביצועים פוטנציאליות, חששות בטיחות ופרטיות, הטיות, פרקטיקות מפלות או תוצאות לא מכוונות. אנו גם מאמינים שמי שמשתמש במערכות AI צריך להיות כנה וגלוי לגבי מתי, למה ואיך הוא בוחר להפעיל אותן, כמו גם לגבי המגבלות של המערכות שהוא משתמש בהן. לדוגמה, אם בנק משתמש במערכת AI כדי לתמוך בהחלטות הלוואה לצרכנים, חשוב לבחון את התוצאות ולהבין אילו נתונים משפיעים על ההמלצות של המערכת. ממשלות מתחילות להסדיר את ה-AI בתעשיות שונות, ולכן מדעני נתונים וארגונים חייבים להסביר אם מערכת AI עומדת בדרישות הרגולציה, במיוחד כאשר יש תוצאה לא רצויה.

> [🎥 לחצו כאן לצפייה בסרטון: שקיפות ב-AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- מכיוון שמערכות AI כל כך מורכבות, קשה להבין כיצד הן פועלות ולפרש את התוצאות.
- חוסר הבנה זה משפיע על האופן שבו מערכות אלו מנוהלות, מופעלות ומתועדות.
- חוסר הבנה זה משפיע יותר מכל על ההחלטות שמתקבלות באמצעות התוצאות שמערכות אלו מפיקות.

### אחריות

האנשים שמעצבים ומפעילים מערכות AI חייבים להיות אחראים לאופן שבו המערכות שלהם פועלות. הצורך באחריות הוא קריטי במיוחד בטכנולוגיות רגישות כמו זיהוי פנים. לאחרונה, ישנה דרישה גוברת לטכנולוגיית זיהוי פנים, במיוחד מארגוני אכיפת חוק שרואים את הפוטנציאל של הטכנולוגיה בשימושים כמו מציאת ילדים נעדרים. עם זאת, טכנולוגיות אלו עשויות לשמש ממשלות כדי לסכן את חירויות היסוד של אזרחיהן, למשל, על ידי הפעלת מעקב מתמשך על אנשים מסוימים. לכן, מדעני נתונים וארגונים צריכים להיות אחראים לאופן שבו מערכת ה-AI שלהם משפיעה על אנשים או על החברה.

[![חוקר AI מוביל מזהיר מפני מעקב המוני באמצעות זיהוי פנים](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "הגישה של Microsoft ל-AI אחראי")

> 🎥 לחצו על התמונה למעלה לצפייה בסרטון: אזהרות מפני מעקב המוני באמצעות זיהוי פנים

בסופו של דבר, אחת השאלות הגדולות ביותר לדור שלנו, כראשון שמביא את ה-AI לחברה, היא כיצד להבטיח שמחשבים יישארו אחראים לאנשים וכיצד להבטיח שהאנשים שמעצבים מחשבים יישארו אחראים לכל השאר.

## הערכת השפעה

לפני אימון מודל למידת מכונה, חשוב לבצע הערכת השפעה כדי להבין את מטרת מערכת ה-AI; מה השימוש המיועד; היכן היא תופעל; ומי יתקשר עם המערכת. אלו מועילים לבוחנים או למבקרים שמעריכים את המערכת לדעת אילו גורמים יש לקחת בחשבון בעת זיהוי סיכונים פוטנציאליים ותוצאות צפויות.

התחומים הבאים הם מוקדי תשומת לב בעת ביצוע הערכת השפעה:

* **השפעה שלילית על אנשים**. מודעות לכל מגבלה או דרישה, שימוש לא נתמך או כל מגבלה ידועה שמפריעה לביצועי המערכת היא חיונית כדי להבטיח שהמערכת לא תופעל באופן שעלול לגרום נזק לאנשים.
* **דרישות נתונים**. הבנת האופן והיכן המערכת תשתמש בנתונים מאפשרת למבקרים לחקור כל דרישות נתונים שיש לקחת בחשבון (למשל, תקנות GDPR או HIPPA). בנוסף, יש לבחון האם מקור או כמות הנתונים מספקים לאימון.
* **סיכום השפעה**. איסוף רשימה של נזקים פוטנציאליים שעלולים להתעורר משימוש במערכת. לאורך מחזור החיים של למידת מכונה, יש לבדוק אם הבעיות שזוהו טופלו או נפתרו.
* **מטרות ישימות** לכל אחד מששת העקרונות המרכזיים. יש להעריך אם המטרות מכל אחד מהעקרונות הושגו ואם יש פערים.

## איתור באגים עם AI אחראי

בדומה לאיתור באגים ביישום תוכנה, איתור באגים במערכת AI הוא תהליך הכרחי לזיהוי ופתרון בעיות במערכת. ישנם גורמים רבים שיכולים להשפיע על כך שמודל לא יפעל כמצופה או באופן אחראי. רוב מדדי הביצועים המסורתיים של מודלים הם אגרגטים כמותיים של ביצועי המודל, שאינם מספיקים לניתוח כיצד מודל מפר את עקרונות ה-AI האחראי. יתרה מכך, מודל למידת מכונה הוא "קופסה שחורה" שמקשה להבין מה מניע את תוצאותיו או לספק הסבר כאשר הוא טועה. בהמשך הקורס, נלמד כיצד להשתמש בלוח המחוונים של AI אחראי כדי לעזור באיתור באגים במערכות AI. לוח המחוונים מספק כלי הוליסטי למדעני נתונים ומפתחי AI לבצע:

* **ניתוח שגיאות**. לזהות את התפלגות השגיאות של המודל שיכולה להשפיע על ההוגנות או האמינות של המערכת.
* **סקירת מודל**. לגלות היכן יש פערים בביצועי המודל בין קבוצות נתונים שונות.
* **ניתוח נתונים**. להבין את התפלגות הנתונים ולזהות כל הטיה פוטנציאלית בנתונים שעלולה להוביל לבעיות הוגנות, הכלה ואמינות.
* **הבנת מודל**. להבין מה משפיע או משפיע על תחזיות המודל. זה עוזר להסביר את התנהגות המודל, שחשובה לשקיפות ואחריות.

## 🚀 אתגר

כדי למנוע נזקים מלהיות מוכנסים מלכתחילה, עלינו:

- לכלול מגוון של רקעים ופרספקטיבות בקרב האנשים שעובדים על מערכות
- להשקיע במאגרי נתונים שמייצגים את המגוון של החברה שלנו
- לפתח שיטות טובות יותר לאורך מחזור החיים של למידת מכונה לזיהוי ותיקון AI אחראי כאשר הוא מתרחש

חשבו על תרחישים אמיתיים שבהם חוסר אמינות של מודל ניכר בבנייה ובשימוש במודל. מה עוד כדאי לקחת בחשבון?

## [שאלון לאחר ההרצאה](https://ff-quizzes.netlify.app/en/ml/)

## סקירה ולימוד עצמי

בשיעור זה, למדתם כמה יסודות של מושגי הוגנות וחוסר הוגנות בלמידת מכונה.
צפו בסדנה זו כדי להעמיק בנושאים:

- במרדף אחר בינה מלאכותית אחראית: יישום עקרונות בפועל מאת בסמירה נושי, מהרנוש סמקי ואמיט שארמה

[![Responsible AI Toolbox: מסגרת קוד פתוח לבניית בינה מלאכותית אחראית](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: מסגרת קוד פתוח לבניית בינה מלאכותית אחראית")

> 🎥 לחצו על התמונה למעלה לצפייה בסרטון: RAI Toolbox: מסגרת קוד פתוח לבניית בינה מלאכותית אחראית מאת בסמירה נושי, מהרנוש סמקי ואמיט שארמה

בנוסף, קראו:

- מרכז המשאבים של Microsoft בנושא בינה מלאכותית אחראית: [Responsible AI Resources – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- קבוצת המחקר FATE של Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [מאגר GitHub של Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

קראו על הכלים של Azure Machine Learning להבטחת הוגנות:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## משימה

[חקור את RAI Toolbox](assignment.md)

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.