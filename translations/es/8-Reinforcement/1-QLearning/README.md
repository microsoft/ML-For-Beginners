<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0ffe994d1cc881bdeb49226a064116e5",
  "translation_date": "2025-09-04T00:19:42+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "es"
}
-->
# Introducci칩n al Aprendizaje por Refuerzo y Q-Learning

![Resumen del refuerzo en el aprendizaje autom치tico en un sketchnote](../../../../translated_images/ml-reinforcement.94024374d63348dbb3571c343ca7ddabef72adac0b8086d47164b769ba3a8a1d.es.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

El aprendizaje por refuerzo implica tres conceptos importantes: el agente, algunos estados y un conjunto de acciones por estado. Al ejecutar una acci칩n en un estado espec칤fico, el agente recibe una recompensa. Imagina nuevamente el videojuego Super Mario. T칰 eres Mario, est치s en un nivel del juego, parado junto al borde de un acantilado. Sobre ti hay una moneda. T칰, siendo Mario, en un nivel del juego, en una posici칩n espec칤fica... ese es tu estado. Moverte un paso hacia la derecha (una acci칩n) te llevar치 al borde del acantilado, y eso te dar칤a una puntuaci칩n num칠rica baja. Sin embargo, presionar el bot칩n de salto te permitir칤a obtener un punto y seguir vivo. Ese es un resultado positivo y deber칤a otorgarte una puntuaci칩n num칠rica positiva.

Usando aprendizaje por refuerzo y un simulador (el juego), puedes aprender a jugar para maximizar la recompensa, que es mantenerse vivo y obtener la mayor cantidad de puntos posible.

[![Introducci칩n al Aprendizaje por Refuerzo](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 游꿘 Haz clic en la imagen de arriba para escuchar a Dmitry hablar sobre el Aprendizaje por Refuerzo

## [Cuestionario previo a la lecci칩n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/)

## Prerrequisitos y Configuraci칩n

En esta lecci칩n, experimentaremos con algo de c칩digo en Python. Deber칤as poder ejecutar el c칩digo del Jupyter Notebook de esta lecci칩n, ya sea en tu computadora o en la nube.

Puedes abrir [el notebook de la lecci칩n](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) y seguir esta lecci칩n para construir.

> **Nota:** Si est치s abriendo este c칩digo desde la nube, tambi칠n necesitas obtener el archivo [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), que se utiliza en el c칩digo del notebook. Agr칠galo al mismo directorio que el notebook.

## Introducci칩n

En esta lecci칩n, exploraremos el mundo de **[Pedro y el Lobo](https://es.wikipedia.org/wiki/Pedro_y_el_lobo)**, inspirado en un cuento musical de un compositor ruso, [Sergei Prokofiev](https://es.wikipedia.org/wiki/Sergu%C3%A9i_Prok%C3%B3fiev). Usaremos **Aprendizaje por Refuerzo** para permitir que Pedro explore su entorno, recoja manzanas deliciosas y evite encontrarse con el lobo.

El **Aprendizaje por Refuerzo** (RL) es una t칠cnica de aprendizaje que nos permite aprender un comportamiento 칩ptimo de un **agente** en alg칰n **entorno** mediante la ejecuci칩n de muchos experimentos. Un agente en este entorno debe tener alg칰n **objetivo**, definido por una **funci칩n de recompensa**.

## El entorno

Para simplificar, consideremos el mundo de Pedro como un tablero cuadrado de tama침o `ancho` x `alto`, como este:

![Entorno de Pedro](../../../../translated_images/environment.40ba3cb66256c93fa7e92f6f7214e1d1f588aafa97d266c11d108c5c5d101b6c.es.png)

Cada celda en este tablero puede ser:

* **suelo**, sobre el cual Pedro y otras criaturas pueden caminar.
* **agua**, sobre la cual obviamente no puedes caminar.
* un **치rbol** o **hierba**, un lugar donde puedes descansar.
* una **manzana**, que representa algo que Pedro estar칤a encantado de encontrar para alimentarse.
* un **lobo**, que es peligroso y debe evitarse.

Hay un m칩dulo de Python separado, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), que contiene el c칩digo para trabajar con este entorno. Dado que este c칩digo no es importante para entender nuestros conceptos, importaremos el m칩dulo y lo usaremos para crear el tablero de muestra (bloque de c칩digo 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Este c칩digo deber칤a imprimir una imagen del entorno similar a la anterior.

## Acciones y pol칤tica

En nuestro ejemplo, el objetivo de Pedro ser칤a encontrar una manzana, mientras evita al lobo y otros obst치culos. Para ello, esencialmente puede caminar por el tablero hasta encontrar una manzana.

Por lo tanto, en cualquier posici칩n, puede elegir entre una de las siguientes acciones: arriba, abajo, izquierda y derecha.

Definiremos esas acciones como un diccionario y las mapearemos a pares de cambios de coordenadas correspondientes. Por ejemplo, moverse a la derecha (`R`) corresponder칤a a un par `(1,0)`. (bloque de c칩digo 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

En resumen, la estrategia y el objetivo de este escenario son los siguientes:

- **La estrategia** de nuestro agente (Pedro) est치 definida por una llamada **pol칤tica**. Una pol칤tica es una funci칩n que devuelve la acci칩n en cualquier estado dado. En nuestro caso, el estado del problema est치 representado por el tablero, incluida la posici칩n actual del jugador.

- **El objetivo** del aprendizaje por refuerzo es eventualmente aprender una buena pol칤tica que nos permita resolver el problema de manera eficiente. Sin embargo, como l칤nea base, consideremos la pol칤tica m치s simple llamada **camino aleatorio**.

## Camino aleatorio

Primero resolvamos nuestro problema implementando una estrategia de camino aleatorio. Con el camino aleatorio, elegiremos aleatoriamente la siguiente acci칩n de las acciones permitidas, hasta que lleguemos a la manzana (bloque de c칩digo 3).

1. Implementa el camino aleatorio con el siguiente c칩digo:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    La llamada a `walk` deber칤a devolver la longitud del camino correspondiente, que puede variar de una ejecuci칩n a otra.

1. Ejecuta el experimento de camino varias veces (digamos, 100), y muestra las estad칤sticas resultantes (bloque de c칩digo 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Nota que la longitud promedio de un camino es de alrededor de 30-40 pasos, lo cual es bastante, dado que la distancia promedio a la manzana m치s cercana es de alrededor de 5-6 pasos.

    Tambi칠n puedes ver c칩mo se mueve Pedro durante el camino aleatorio:

    ![Camino aleatorio de Pedro](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funci칩n de recompensa

Para hacer nuestra pol칤tica m치s inteligente, necesitamos entender qu칠 movimientos son "mejores" que otros. Para ello, necesitamos definir nuestro objetivo.

El objetivo puede definirse en t칠rminos de una **funci칩n de recompensa**, que devolver치 alg칰n valor de puntuaci칩n para cada estado. Cuanto mayor sea el n칰mero, mejor ser치 la funci칩n de recompensa. (bloque de c칩digo 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Una cosa interesante sobre las funciones de recompensa es que en la mayor칤a de los casos, *solo se nos da una recompensa sustancial al final del juego*. Esto significa que nuestro algoritmo deber칤a recordar los pasos "buenos" que conducen a una recompensa positiva al final y aumentar su importancia. De manera similar, todos los movimientos que conducen a malos resultados deber칤an desalentarse.

## Q-Learning

El algoritmo que discutiremos aqu칤 se llama **Q-Learning**. En este algoritmo, la pol칤tica est치 definida por una funci칩n (o una estructura de datos) llamada **Q-Table**. Registra la "bondad" de cada una de las acciones en un estado dado.

Se llama Q-Table porque a menudo es conveniente representarla como una tabla o un arreglo multidimensional. Dado que nuestro tablero tiene dimensiones `ancho` x `alto`, podemos representar la Q-Table usando un arreglo numpy con forma `ancho` x `alto` x `len(actions)`: (bloque de c칩digo 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Nota que inicializamos todos los valores de la Q-Table con un valor igual, en nuestro caso - 0.25. Esto corresponde a la pol칤tica de "camino aleatorio", porque todos los movimientos en cada estado son igualmente buenos. Podemos pasar la Q-Table a la funci칩n `plot` para visualizar la tabla en el tablero: `m.plot(Q)`.

![Entorno de Pedro](../../../../translated_images/env_init.04e8f26d2d60089e128f21d22e5fef57d580e559f0d5937b06c689e5e7cdd438.es.png)

En el centro de cada celda hay una "flecha" que indica la direcci칩n preferida de movimiento. Dado que todas las direcciones son iguales, se muestra un punto.

Ahora necesitamos ejecutar la simulaci칩n, explorar nuestro entorno y aprender una mejor distribuci칩n de valores de la Q-Table, lo que nos permitir치 encontrar el camino hacia la manzana mucho m치s r치pido.

## Esencia del Q-Learning: Ecuaci칩n de Bellman

Una vez que comenzamos a movernos, cada acci칩n tendr치 una recompensa correspondiente, es decir, te칩ricamente podemos seleccionar la siguiente acci칩n basada en la recompensa inmediata m치s alta. Sin embargo, en la mayor칤a de los estados, el movimiento no lograr치 nuestro objetivo de alcanzar la manzana, y por lo tanto no podemos decidir inmediatamente qu칠 direcci칩n es mejor.

> Recuerda que no importa el resultado inmediato, sino el resultado final, que obtendremos al final de la simulaci칩n.

Para tener en cuenta esta recompensa diferida, necesitamos usar los principios de **[programaci칩n din치mica](https://es.wikipedia.org/wiki/Programaci%C3%B3n_din%C3%A1mica)**, que nos permiten pensar en nuestro problema de manera recursiva.

Supongamos que ahora estamos en el estado *s*, y queremos movernos al siguiente estado *s'*. Al hacerlo, recibiremos la recompensa inmediata *r(s,a)*, definida por la funci칩n de recompensa, m치s alguna recompensa futura. Si suponemos que nuestra Q-Table refleja correctamente la "atractividad" de cada acci칩n, entonces en el estado *s'* elegiremos una acci칩n *a* que corresponda al valor m치ximo de *Q(s',a')*. As칤, la mejor recompensa futura posible que podr칤amos obtener en el estado *s* se definir치 como `max`

## Verificando la pol칤tica

Dado que la Q-Table enumera la "atractividad" de cada acci칩n en cada estado, es bastante sencillo usarla para definir la navegaci칩n eficiente en nuestro mundo. En el caso m치s simple, podemos seleccionar la acci칩n correspondiente al valor m치s alto de la Q-Table: (bloque de c칩digo 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Si pruebas el c칩digo anterior varias veces, puede que notes que a veces se "queda colgado" y necesitas presionar el bot칩n STOP en el notebook para interrumpirlo. Esto ocurre porque podr칤a haber situaciones en las que dos estados "apuntan" el uno al otro en t칠rminos de valor 칩ptimo de Q, en cuyo caso el agente termina movi칠ndose entre esos estados indefinidamente.

## 游Desaf칤o

> **Tarea 1:** Modifica la funci칩n `walk` para limitar la longitud m치xima del camino a un cierto n칰mero de pasos (por ejemplo, 100), y observa c칩mo el c칩digo anterior devuelve este valor de vez en cuando.

> **Tarea 2:** Modifica la funci칩n `walk` para que no regrese a los lugares donde ya ha estado previamente. Esto evitar치 que `walk` entre en bucles, sin embargo, el agente a칰n puede terminar "atrapado" en una ubicaci칩n de la que no pueda escapar.

## Navegaci칩n

Una pol칤tica de navegaci칩n mejor ser칤a la que usamos durante el entrenamiento, que combina explotaci칩n y exploraci칩n. En esta pol칤tica, seleccionaremos cada acci칩n con cierta probabilidad, proporcional a los valores en la Q-Table. Esta estrategia a칰n puede resultar en que el agente regrese a una posici칩n que ya ha explorado, pero, como puedes ver en el c칩digo a continuaci칩n, resulta en un camino promedio muy corto hacia la ubicaci칩n deseada (recuerda que `print_statistics` ejecuta la simulaci칩n 100 veces): (bloque de c칩digo 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Despu칠s de ejecutar este c칩digo, deber칤as obtener una longitud promedio de camino mucho m치s peque침a que antes, en el rango de 3-6.

## Investigando el proceso de aprendizaje

Como hemos mencionado, el proceso de aprendizaje es un equilibrio entre la exploraci칩n y la explotaci칩n del conocimiento adquirido sobre la estructura del espacio del problema. Hemos visto que los resultados del aprendizaje (la capacidad de ayudar a un agente a encontrar un camino corto hacia el objetivo) han mejorado, pero tambi칠n es interesante observar c칩mo se comporta la longitud promedio del camino durante el proceso de aprendizaje:

## Los aprendizajes pueden resumirse como:

- **La longitud promedio del camino aumenta**. Lo que vemos aqu칤 es que al principio, la longitud promedio del camino aumenta. Esto probablemente se debe al hecho de que cuando no sabemos nada sobre el entorno, es m치s probable que quedemos atrapados en estados desfavorables, como agua o lobos. A medida que aprendemos m치s y comenzamos a usar este conocimiento, podemos explorar el entorno por m치s tiempo, pero a칰n no sabemos muy bien d칩nde est치n las manzanas.

- **La longitud del camino disminuye a medida que aprendemos m치s**. Una vez que aprendemos lo suficiente, se vuelve m치s f치cil para el agente alcanzar el objetivo, y la longitud del camino comienza a disminuir. Sin embargo, a칰n estamos abiertos a la exploraci칩n, por lo que a menudo nos desviamos del mejor camino y exploramos nuevas opciones, haciendo que el camino sea m치s largo de lo 칩ptimo.

- **La longitud aumenta abruptamente**. Lo que tambi칠n observamos en este gr치fico es que en alg칰n momento, la longitud aument칩 abruptamente. Esto indica la naturaleza estoc치stica del proceso, y que en alg칰n momento podemos "estropear" los coeficientes de la Q-Table sobrescribi칠ndolos con nuevos valores. Esto idealmente deber칤a minimizarse disminuyendo la tasa de aprendizaje (por ejemplo, hacia el final del entrenamiento, solo ajustamos los valores de la Q-Table por un peque침o valor).

En general, es importante recordar que el 칠xito y la calidad del proceso de aprendizaje dependen significativamente de par치metros como la tasa de aprendizaje, la disminuci칩n de la tasa de aprendizaje y el factor de descuento. A menudo se les llama **hiperpar치metros**, para distinguirlos de los **par치metros**, que optimizamos durante el entrenamiento (por ejemplo, los coeficientes de la Q-Table). El proceso de encontrar los mejores valores de hiperpar치metros se llama **optimizaci칩n de hiperpar치metros**, y merece un tema aparte.

## [Cuestionario post-lectura](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Asignaci칩n 
[Un Mundo M치s Realista](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci칩n autom치tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi칩n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci칩n cr칤tica, se recomienda una traducci칩n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err칩neas que puedan surgir del uso de esta traducci칩n.