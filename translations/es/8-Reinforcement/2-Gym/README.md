## CartPole Patinaje

El problema que hemos estado resolviendo en la lecci√≥n anterior podr√≠a parecer un problema de juguete, no realmente aplicable a escenarios de la vida real. Este no es el caso, porque muchos problemas del mundo real tambi√©n comparten este escenario, incluyendo jugar al Ajedrez o al Go. Son similares, porque tambi√©n tenemos un tablero con reglas dadas y un **estado discreto**.

## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)

## Introducci√≥n

En esta lecci√≥n aplicaremos los mismos principios de Q-Learning a un problema con **estado continuo**, es decir, un estado que se da por uno o m√°s n√∫meros reales. Abordaremos el siguiente problema:

> **Problema**: Si Peter quiere escapar del lobo, necesita poder moverse m√°s r√°pido. Veremos c√≥mo Peter puede aprender a patinar, en particular, a mantener el equilibrio, utilizando Q-Learning.

![¬°La gran escapada!](../../../../translated_images/escape.18862db9930337e3fce23a9b6a76a06445f229dadea2268e12a6f0a1fde12115.es.png)

> ¬°Peter y sus amigos se ponen creativos para escapar del lobo! Imagen de [Jen Looper](https://twitter.com/jenlooper)

Usaremos una versi√≥n simplificada del equilibrio conocida como el problema **CartPole**. En el mundo de CartPole, tenemos un deslizador horizontal que puede moverse hacia la izquierda o hacia la derecha, y el objetivo es equilibrar un poste vertical sobre el deslizador.

## Requisitos previos

En esta lecci√≥n, utilizaremos una biblioteca llamada **OpenAI Gym** para simular diferentes **entornos**. Puedes ejecutar el c√≥digo de esta lecci√≥n localmente (por ejemplo, desde Visual Studio Code), en cuyo caso la simulaci√≥n se abrir√° en una nueva ventana. Al ejecutar el c√≥digo en l√≠nea, es posible que necesites hacer algunos ajustes en el c√≥digo, como se describe [aqu√≠](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

En la lecci√≥n anterior, las reglas del juego y el estado fueron dados por la clase `Board` que definimos nosotros mismos. Aqu√≠ utilizaremos un **entorno de simulaci√≥n** especial, que simular√° la f√≠sica detr√°s del poste en equilibrio. Uno de los entornos de simulaci√≥n m√°s populares para entrenar algoritmos de aprendizaje por refuerzo se llama [Gym](https://gym.openai.com/), que es mantenido por [OpenAI](https://openai.com/). Usando este gym, podemos crear diferentes **entornos** desde una simulaci√≥n de CartPole hasta juegos de Atari.

> **Nota**: Puedes ver otros entornos disponibles en OpenAI Gym [aqu√≠](https://gym.openai.com/envs/#classic_control).

Primero, instalemos el gym e importemos las bibliotecas requeridas (bloque de c√≥digo 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Ejercicio - inicializar un entorno de CartPole

Para trabajar con un problema de equilibrio de CartPole, necesitamos inicializar el entorno correspondiente. Cada entorno est√° asociado con:

- **Espacio de observaci√≥n** que define la estructura de la informaci√≥n que recibimos del entorno. Para el problema de CartPole, recibimos la posici√≥n del poste, la velocidad y otros valores.

- **Espacio de acci√≥n** que define las posibles acciones. En nuestro caso, el espacio de acci√≥n es discreto y consta de dos acciones: **izquierda** y **derecha**. (bloque de c√≥digo 2)

1. Para inicializar, escribe el siguiente c√≥digo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Para ver c√≥mo funciona el entorno, ejecutemos una simulaci√≥n corta de 100 pasos. En cada paso, proporcionamos una de las acciones a realizar; en esta simulaci√≥n simplemente seleccionamos aleatoriamente una acci√≥n de `action_space`.

1. Ejecuta el c√≥digo a continuaci√≥n y observa qu√© sucede.

    ‚úÖ Recuerda que es preferible ejecutar este c√≥digo en una instalaci√≥n local de Python. (bloque de c√≥digo 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Deber√≠as estar viendo algo similar a esta imagen:

    ![CartPole sin equilibrio](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Durante la simulaci√≥n, necesitamos obtener observaciones para decidir c√≥mo actuar. De hecho, la funci√≥n step devuelve las observaciones actuales, una funci√≥n de recompensa y una bandera de finalizaci√≥n que indica si tiene sentido continuar la simulaci√≥n o no: (bloque de c√≥digo 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Terminar√°s viendo algo como esto en la salida del notebook:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    El vector de observaci√≥n que se devuelve en cada paso de la simulaci√≥n contiene los siguientes valores:
    - Posici√≥n del carrito
    - Velocidad del carrito
    - √Ångulo del poste
    - Tasa de rotaci√≥n del poste

1. Obt√©n el valor m√≠nimo y m√°ximo de esos n√∫meros: (bloque de c√≥digo 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Tambi√©n puedes notar que el valor de recompensa en cada paso de la simulaci√≥n es siempre 1. Esto se debe a que nuestro objetivo es sobrevivir el mayor tiempo posible, es decir, mantener el poste en una posici√≥n razonablemente vertical durante el mayor tiempo posible.

    ‚úÖ De hecho, la simulaci√≥n de CartPole se considera resuelta si logramos obtener una recompensa promedio de 195 durante 100 pruebas consecutivas.

## Discretizaci√≥n del estado

En Q-Learning, necesitamos construir una Q-Table que defina qu√© hacer en cada estado. Para poder hacer esto, necesitamos que el estado sea **discreto**, m√°s precisamente, debe contener un n√∫mero finito de valores discretos. Por lo tanto, necesitamos de alguna manera **discretizar** nuestras observaciones, mape√°ndolas a un conjunto finito de estados.

Hay algunas formas de hacer esto:

- **Dividir en contenedores**. Si conocemos el intervalo de un cierto valor, podemos dividir este intervalo en un n√∫mero de **contenedores**, y luego reemplazar el valor por el n√∫mero del contenedor al que pertenece. Esto se puede hacer utilizando el m√©todo [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) de numpy. En este caso, conoceremos precisamente el tama√±o del estado, porque depender√° del n√∫mero de contenedores que seleccionemos para la digitalizaci√≥n.

‚úÖ Podemos usar la interpolaci√≥n lineal para llevar los valores a un intervalo finito (digamos, de -20 a 20), y luego convertir los n√∫meros en enteros redonde√°ndolos. Esto nos da un poco menos de control sobre el tama√±o del estado, especialmente si no conocemos los rangos exactos de los valores de entrada. Por ejemplo, en nuestro caso, 2 de los 4 valores no tienen l√≠mites superiores/inferiores en sus valores, lo que puede resultar en un n√∫mero infinito de estados.

En nuestro ejemplo, utilizaremos el segundo enfoque. Como puedes notar m√°s adelante, a pesar de los l√≠mites superiores/inferiores indefinidos, esos valores rara vez toman valores fuera de ciertos intervalos finitos, por lo que esos estados con valores extremos ser√°n muy raros.

1. Aqu√≠ est√° la funci√≥n que tomar√° la observaci√≥n de nuestro modelo y producir√° una tupla de 4 valores enteros: (bloque de c√≥digo 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Exploremos tambi√©n otro m√©todo de discretizaci√≥n utilizando contenedores: (bloque de c√≥digo 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Ahora ejecutemos una simulaci√≥n corta y observemos esos valores discretos del entorno. Si√©ntete libre de probar ambos `discretize` and `discretize_bins` y ver si hay alguna diferencia.

    ‚úÖ discretize_bins devuelve el n√∫mero del contenedor, que es basado en 0. Por lo tanto, para valores de la variable de entrada alrededor de 0, devuelve el n√∫mero del medio del intervalo (10). En discretize, no nos preocupamos por el rango de valores de salida, permiti√©ndoles ser negativos, por lo que los valores del estado no est√°n desplazados, y 0 corresponde a 0. (bloque de c√≥digo 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Descomenta la l√≠nea que comienza con env.render si deseas ver c√≥mo se ejecuta el entorno. De lo contrario, puedes ejecutarlo en segundo plano, lo cual es m√°s r√°pido. Usaremos esta ejecuci√≥n "invisible" durante nuestro proceso de Q-Learning.

## La estructura de la Q-Table

En nuestra lecci√≥n anterior, el estado era un simple par de n√∫meros del 0 al 8, y por lo tanto era conveniente representar la Q-Table con un tensor numpy con una forma de 8x8x2. Si usamos la discretizaci√≥n de contenedores, el tama√±o de nuestro vector de estado tambi√©n es conocido, por lo que podemos usar el mismo enfoque y representar el estado con un array de forma 20x20x10x10x2 (aqu√≠ 2 es la dimensi√≥n del espacio de acci√≥n, y las primeras dimensiones corresponden al n√∫mero de contenedores que hemos seleccionado para usar para cada uno de los par√°metros en el espacio de observaci√≥n).

Sin embargo, a veces las dimensiones precisas del espacio de observaci√≥n no son conocidas. En el caso de la funci√≥n `discretize`, nunca podemos estar seguros de que nuestro estado se mantenga dentro de ciertos l√≠mites, porque algunos de los valores originales no est√°n limitados. Por lo tanto, usaremos un enfoque ligeramente diferente y representaremos la Q-Table con un diccionario.

1. Usa el par *(estado, acci√≥n)* como la clave del diccionario, y el valor corresponder√≠a al valor de entrada de la Q-Table. (bloque de c√≥digo 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aqu√≠ tambi√©n definimos una funci√≥n `qvalues()`, que devuelve una lista de valores de la Q-Table para un estado dado que corresponde a todas las posibles acciones. Si la entrada no est√° presente en la Q-Table, devolveremos 0 como valor predeterminado.

## Comencemos con el Q-Learning

¬°Ahora estamos listos para ense√±ar a Peter a mantener el equilibrio!

1. Primero, establezcamos algunos hiperpar√°metros: (bloque de c√≥digo 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aqu√≠, `alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.

    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.

    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. 

    ‚úÖ In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those "mistakes"

### Improve the algorithm

We can also make two improvements to our algorithm from the previous lesson:

- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.
  
- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.

1. Collect all cumulative rewards at each simulation at `rewards` vector para su posterior representaci√≥n gr√°fica. (bloque de c√≥digo  11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Lo que puedes notar de esos resultados:

- **Cerca de nuestro objetivo**. Estamos muy cerca de alcanzar el objetivo de obtener 195 recompensas acumuladas en m√°s de 100 ejecuciones consecutivas de la simulaci√≥n, ¬°o podr√≠amos haberlo logrado! Incluso si obtenemos n√∫meros m√°s peque√±os, a√∫n no lo sabemos, porque promediamos sobre 5000 ejecuciones, y solo se requieren 100 ejecuciones en el criterio formal.

- **La recompensa comienza a disminuir**. A veces la recompensa comienza a disminuir, lo que significa que podemos "destruir" valores ya aprendidos en la Q-Table con los que empeoran la situaci√≥n.

Esta observaci√≥n es m√°s claramente visible si graficamos el progreso del entrenamiento.

## Graficando el progreso del entrenamiento

Durante el entrenamiento, hemos recopilado el valor de recompensa acumulada en cada una de las iteraciones en el vector `rewards`. As√≠ es como se ve cuando lo graficamos contra el n√∫mero de iteraci√≥n:

```python
plt.plot(rewards)
```

![progreso bruto](../../../../translated_images/train_progress_raw.2adfdf2daea09c596fc786fa347a23e9aceffe1b463e2257d20a9505794823ec.es.png)

De este gr√°fico, no es posible decir nada, porque debido a la naturaleza del proceso de entrenamiento estoc√°stico, la duraci√≥n de las sesiones de entrenamiento var√≠a mucho. Para darle m√°s sentido a este gr√°fico, podemos calcular el **promedio m√≥vil** sobre una serie de experimentos, digamos 100. Esto se puede hacer convenientemente usando `np.convolve`: (bloque de c√≥digo 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progreso del entrenamiento](../../../../translated_images/train_progress_runav.c71694a8fa9ab35935aff6f109e5ecdfdbdf1b0ae265da49479a81b5fae8f0aa.es.png)

## Variando los hiperpar√°metros

Para hacer el aprendizaje m√°s estable, tiene sentido ajustar algunos de nuestros hiperpar√°metros durante el entrenamiento. En particular:

- **Para la tasa de aprendizaje**, `alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.

- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon`, y subir hasta casi 1.

> **Tarea 1**: Juega con los valores de los hiperpar√°metros y ve si puedes lograr una mayor recompensa acumulada. ¬øEst√°s obteniendo m√°s de 195?

> **Tarea 2**: Para resolver formalmente el problema, necesitas obtener una recompensa promedio de 195 en 100 ejecuciones consecutivas. Mide eso durante el entrenamiento y aseg√∫rate de haber resuelto formalmente el problema.

## Viendo el resultado en acci√≥n

Ser√≠a interesante ver c√≥mo se comporta el modelo entrenado. Ejecutemos la simulaci√≥n y sigamos la misma estrategia de selecci√≥n de acciones que durante el entrenamiento, muestreando seg√∫n la distribuci√≥n de probabilidad en la Q-Table: (bloque de c√≥digo 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Deber√≠as ver algo como esto:

![un CartPole equilibrado](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄDesaf√≠o

> **Tarea 3**: Aqu√≠, est√°bamos usando la copia final de la Q-Table, que puede no ser la mejor. Recuerda que hemos almacenado la Q-Table con mejor rendimiento en `Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.

> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax` para encontrar el n√∫mero de acci√≥n correspondiente al valor m√°s alto de la Q-Table. Implementa esta estrategia y ve si mejora el equilibrio.

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)

## Tarea
[Entrena un Mountain Car](assignment.md)

## Conclusi√≥n

Ahora hemos aprendido c√≥mo entrenar agentes para lograr buenos resultados simplemente proporcion√°ndoles una funci√≥n de recompensa que define el estado deseado del juego, y d√°ndoles la oportunidad de explorar inteligentemente el espacio de b√∫squeda. Hemos aplicado con √©xito el algoritmo de Q-Learning en los casos de entornos discretos y continuos, pero con acciones discretas.

Es importante tambi√©n estudiar situaciones donde el estado de la acci√≥n tambi√©n es continuo, y cuando el espacio de observaci√≥n es mucho m√°s complejo, como la imagen de la pantalla del juego de Atari. En esos problemas a menudo necesitamos usar t√©cnicas de aprendizaje autom√°tico m√°s poderosas, como redes neuronales, para lograr buenos resultados. Esos temas m√°s avanzados son el tema de nuestro pr√≥ximo curso m√°s avanzado de IA.

**Descargo de responsabilidad**:
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en inteligencia artificial. Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n humana profesional. No somos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.