<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-04T22:26:14+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "es"
}
-->
# CartPole Patinaje

El problema que resolvimos en la lecci√≥n anterior puede parecer un problema de juguete, sin mucha aplicaci√≥n en escenarios de la vida real. Sin embargo, este no es el caso, ya que muchos problemas del mundo real comparten caracter√≠sticas similares, como jugar al ajedrez o al Go. Son similares porque tambi√©n tenemos un tablero con reglas definidas y un **estado discreto**.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Introducci√≥n

En esta lecci√≥n aplicaremos los mismos principios de Q-Learning a un problema con un **estado continuo**, es decir, un estado definido por uno o m√°s n√∫meros reales. Abordaremos el siguiente problema:

> **Problema**: Si Pedro quiere escapar del lobo, necesita aprender a moverse m√°s r√°pido. Veremos c√≥mo Pedro puede aprender a patinar, en particular, a mantener el equilibrio, utilizando Q-Learning.

![¬°La gran escapada!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> ¬°Pedro y sus amigos se ponen creativos para escapar del lobo! Imagen de [Jen Looper](https://twitter.com/jenlooper)

Usaremos una versi√≥n simplificada del equilibrio conocida como el problema del **CartPole**. En el mundo del CartPole, tenemos un deslizador horizontal que puede moverse a la izquierda o a la derecha, y el objetivo es equilibrar un palo vertical sobre el deslizador.

## Prerrequisitos

En esta lecci√≥n, utilizaremos una biblioteca llamada **OpenAI Gym** para simular diferentes **entornos**. Puedes ejecutar el c√≥digo de esta lecci√≥n localmente (por ejemplo, desde Visual Studio Code), en cuyo caso la simulaci√≥n se abrir√° en una nueva ventana. Si ejecutas el c√≥digo en l√≠nea, es posible que necesites hacer algunos ajustes, como se describe [aqu√≠](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

En la lecci√≥n anterior, las reglas del juego y el estado estaban definidos por la clase `Board` que creamos nosotros mismos. Aqu√≠ utilizaremos un **entorno de simulaci√≥n** especial, que simular√° la f√≠sica detr√°s del palo en equilibrio. Uno de los entornos de simulaci√≥n m√°s populares para entrenar algoritmos de aprendizaje por refuerzo se llama [Gym](https://gym.openai.com/), mantenido por [OpenAI](https://openai.com/). Usando este Gym, podemos crear diferentes **entornos**, desde una simulaci√≥n de CartPole hasta juegos de Atari.

> **Nota**: Puedes ver otros entornos disponibles en OpenAI Gym [aqu√≠](https://gym.openai.com/envs/#classic_control).

Primero, instalemos Gym e importemos las bibliotecas necesarias (bloque de c√≥digo 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Ejercicio - inicializar un entorno de CartPole

Para trabajar con el problema de equilibrio de CartPole, necesitamos inicializar el entorno correspondiente. Cada entorno est√° asociado con:

- Un **espacio de observaci√≥n** que define la estructura de la informaci√≥n que recibimos del entorno. Para el problema de CartPole, recibimos la posici√≥n del palo, la velocidad y otros valores.

- Un **espacio de acci√≥n** que define las acciones posibles. En nuestro caso, el espacio de acci√≥n es discreto y consta de dos acciones: **izquierda** y **derecha**. (bloque de c√≥digo 2)

1. Para inicializar, escribe el siguiente c√≥digo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Para ver c√≥mo funciona el entorno, ejecutemos una simulaci√≥n corta de 100 pasos. En cada paso, proporcionamos una de las acciones a realizar; en esta simulaci√≥n simplemente seleccionamos una acci√≥n aleatoriamente del `action_space`.

1. Ejecuta el siguiente c√≥digo y observa el resultado.

    ‚úÖ Recuerda que es preferible ejecutar este c√≥digo en una instalaci√≥n local de Python. (bloque de c√≥digo 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Deber√≠as ver algo similar a esta imagen:

    ![CartPole sin equilibrio](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Durante la simulaci√≥n, necesitamos obtener observaciones para decidir c√≥mo actuar. De hecho, la funci√≥n `step` devuelve las observaciones actuales, una funci√≥n de recompensa y una bandera `done` que indica si tiene sentido continuar la simulaci√≥n o no: (bloque de c√≥digo 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Ver√°s algo como esto en la salida del notebook:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    El vector de observaci√≥n que se devuelve en cada paso de la simulaci√≥n contiene los siguientes valores:
    - Posici√≥n del carrito
    - Velocidad del carrito
    - √Ångulo del palo
    - Tasa de rotaci√≥n del palo

1. Obt√©n el valor m√≠nimo y m√°ximo de esos n√∫meros: (bloque de c√≥digo 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Tambi√©n notar√°s que el valor de recompensa en cada paso de la simulaci√≥n siempre es 1. Esto se debe a que nuestro objetivo es sobrevivir el mayor tiempo posible, es decir, mantener el palo en una posici√≥n razonablemente vertical durante el mayor tiempo posible.

    ‚úÖ De hecho, la simulaci√≥n de CartPole se considera resuelta si logramos obtener una recompensa promedio de 195 en 100 ensayos consecutivos.

## Discretizaci√≥n del estado

En Q-Learning, necesitamos construir una Q-Table que defina qu√© hacer en cada estado. Para poder hacer esto, el estado debe ser **discreto**, m√°s precisamente, debe contener un n√∫mero finito de valores discretos. Por lo tanto, necesitamos de alguna manera **discretizar** nuestras observaciones, mape√°ndolas a un conjunto finito de estados.

Hay algunas formas de hacer esto:

- **Dividir en intervalos**. Si conocemos el intervalo de un valor determinado, podemos dividir este intervalo en un n√∫mero de **intervalos**, y luego reemplazar el valor por el n√∫mero del intervalo al que pertenece. Esto se puede hacer utilizando el m√©todo [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) de numpy. En este caso, conoceremos con precisi√≥n el tama√±o del estado, ya que depender√° del n√∫mero de intervalos que seleccionemos para la digitalizaci√≥n.

‚úÖ Podemos usar interpolaci√≥n lineal para llevar los valores a un intervalo finito (por ejemplo, de -20 a 20), y luego convertir los n√∫meros a enteros redonde√°ndolos. Esto nos da un poco menos de control sobre el tama√±o del estado, especialmente si no conocemos los rangos exactos de los valores de entrada. Por ejemplo, en nuestro caso, 2 de los 4 valores no tienen l√≠mites superiores/inferiores, lo que puede resultar en un n√∫mero infinito de estados.

En nuestro ejemplo, utilizaremos el segundo enfoque. Como notar√°s m√°s adelante, a pesar de los l√≠mites superiores/inferiores indefinidos, esos valores rara vez toman valores fuera de ciertos intervalos finitos, por lo que esos estados con valores extremos ser√°n muy raros.

1. Aqu√≠ est√° la funci√≥n que tomar√° la observaci√≥n de nuestro modelo y producir√° una tupla de 4 valores enteros: (bloque de c√≥digo 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Exploremos tambi√©n otro m√©todo de discretizaci√≥n utilizando intervalos: (bloque de c√≥digo 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Ahora ejecutemos una simulaci√≥n corta y observemos esos valores discretos del entorno. Si√©ntete libre de probar tanto `discretize` como `discretize_bins` y observa si hay alguna diferencia.

    ‚úÖ `discretize_bins` devuelve el n√∫mero del intervalo, que comienza en 0. Por lo tanto, para valores de la variable de entrada cercanos a 0, devuelve el n√∫mero del medio del intervalo (10). En `discretize`, no nos preocupamos por el rango de los valores de salida, permitiendo que sean negativos, por lo que los valores del estado no est√°n desplazados, y 0 corresponde a 0. (bloque de c√≥digo 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Descomenta la l√≠nea que comienza con `env.render` si deseas ver c√≥mo se ejecuta el entorno. De lo contrario, puedes ejecutarlo en segundo plano, lo cual es m√°s r√°pido. Usaremos esta ejecuci√≥n "invisible" durante nuestro proceso de Q-Learning.

## La estructura de la Q-Table

En nuestra lecci√≥n anterior, el estado era un simple par de n√∫meros del 0 al 8, por lo que era conveniente representar la Q-Table con un tensor de numpy con una forma de 8x8x2. Si usamos la discretizaci√≥n por intervalos, el tama√±o de nuestro vector de estado tambi√©n es conocido, por lo que podemos usar el mismo enfoque y representar el estado con un array de forma 20x20x10x10x2 (aqu√≠ 2 es la dimensi√≥n del espacio de acci√≥n, y las primeras dimensiones corresponden al n√∫mero de intervalos que seleccionamos para cada uno de los par√°metros en el espacio de observaci√≥n).

Sin embargo, a veces las dimensiones precisas del espacio de observaci√≥n no son conocidas. En el caso de la funci√≥n `discretize`, nunca podemos estar seguros de que nuestro estado se mantenga dentro de ciertos l√≠mites, ya que algunos de los valores originales no est√°n acotados. Por lo tanto, utilizaremos un enfoque ligeramente diferente y representaremos la Q-Table con un diccionario.

1. Usa el par *(estado, acci√≥n)* como clave del diccionario, y el valor corresponder√° al valor de la entrada en la Q-Table. (bloque de c√≥digo 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Aqu√≠ tambi√©n definimos una funci√≥n `qvalues()`, que devuelve una lista de valores de la Q-Table para un estado dado que corresponde a todas las acciones posibles. Si la entrada no est√° presente en la Q-Table, devolveremos 0 como valor predeterminado.

## ¬°Comencemos con Q-Learning!

Ahora estamos listos para ense√±ar a Pedro a mantener el equilibrio.

1. Primero, definamos algunos hiperpar√°metros: (bloque de c√≥digo 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Aqu√≠, `alpha` es la **tasa de aprendizaje** que define en qu√© medida debemos ajustar los valores actuales de la Q-Table en cada paso. En la lecci√≥n anterior comenzamos con 1 y luego disminuimos `alpha` a valores m√°s bajos durante el entrenamiento. En este ejemplo lo mantendremos constante por simplicidad, pero puedes experimentar ajustando los valores de `alpha` m√°s adelante.

    `gamma` es el **factor de descuento** que muestra en qu√© medida debemos priorizar la recompensa futura sobre la recompensa actual.

    `epsilon` es el **factor de exploraci√≥n/explotaci√≥n** que determina si debemos preferir la exploraci√≥n o la explotaci√≥n. En nuestro algoritmo, en un porcentaje `epsilon` de los casos seleccionaremos la siguiente acci√≥n seg√∫n los valores de la Q-Table, y en el porcentaje restante ejecutaremos una acci√≥n aleatoria. Esto nos permitir√° explorar √°reas del espacio de b√∫squeda que nunca hemos visto antes.

    ‚úÖ En t√©rminos de equilibrio, elegir una acci√≥n aleatoria (exploraci√≥n) actuar√≠a como un empuj√≥n aleatorio en la direcci√≥n equivocada, y el palo tendr√≠a que aprender a recuperar el equilibrio de esos "errores".

### Mejorar el algoritmo

Podemos hacer dos mejoras a nuestro algoritmo de la lecci√≥n anterior:

- **Calcular la recompensa acumulativa promedio**, durante un n√∫mero de simulaciones. Imprimiremos el progreso cada 5000 iteraciones, y promediaremos nuestra recompensa acumulativa durante ese per√≠odo de tiempo. Esto significa que si obtenemos m√°s de 195 puntos, podemos considerar el problema resuelto, con una calidad incluso mayor a la requerida.

- **Calcular el resultado acumulativo promedio m√°ximo**, `Qmax`, y almacenaremos la Q-Table correspondiente a ese resultado. Cuando ejecutes el entrenamiento, notar√°s que a veces el resultado acumulativo promedio comienza a disminuir, y queremos conservar los valores de la Q-Table que corresponden al mejor modelo observado durante el entrenamiento.

1. Recopila todas las recompensas acumulativas en cada simulaci√≥n en el vector `rewards` para su posterior representaci√≥n gr√°fica. (bloque de c√≥digo 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Lo que puedes notar de estos resultados:

- **Cerca de nuestro objetivo**. Estamos muy cerca de alcanzar el objetivo de obtener 195 recompensas acumulativas en m√°s de 100 ejecuciones consecutivas de la simulaci√≥n, ¬°o incluso podemos haberlo logrado! Incluso si obtenemos n√∫meros m√°s bajos, no lo sabremos con certeza, ya que promediamos sobre 5000 ejecuciones, y solo se requieren 100 ejecuciones seg√∫n el criterio formal.

- **La recompensa comienza a disminuir**. A veces la recompensa comienza a disminuir, lo que significa que podemos "destruir" valores ya aprendidos en la Q-Table con otros que empeoran la situaci√≥n.

Esta observaci√≥n es m√°s clara si graficamos el progreso del entrenamiento.

## Graficar el progreso del entrenamiento

Durante el entrenamiento, hemos recopilado el valor de la recompensa acumulativa en cada una de las iteraciones en el vector `rewards`. As√≠ es como se ve cuando lo graficamos contra el n√∫mero de iteraci√≥n:

```python
plt.plot(rewards)
```

![progreso sin procesar](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

En este gr√°fico no es posible sacar conclusiones, ya que debido a la naturaleza del proceso de entrenamiento estoc√°stico, la duraci√≥n de las sesiones de entrenamiento var√≠a mucho. Para darle m√°s sentido a este gr√°fico, podemos calcular el **promedio m√≥vil** sobre una serie de experimentos, digamos 100. Esto se puede hacer convenientemente usando `np.convolve`: (bloque de c√≥digo 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![progreso del entrenamiento](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variar los hiperpar√°metros

Para hacer el aprendizaje m√°s estable, tiene sentido ajustar algunos de nuestros hiperpar√°metros durante el entrenamiento. En particular:

- **Para la tasa de aprendizaje**, `alpha`, podemos comenzar con valores cercanos a 1 y luego ir disminuyendo el par√°metro. Con el tiempo, obtendremos buenos valores de probabilidad en la Q-Table, y por lo tanto deber√≠amos ajustarlos ligeramente, y no sobrescribirlos completamente con nuevos valores.

- **Aumentar epsilon**. Podr√≠amos querer aumentar lentamente el valor de `epsilon`, para explorar menos y explotar m√°s. Probablemente tenga sentido comenzar con un valor bajo de `epsilon` y aumentarlo hasta casi 1.
> **Tarea 1**: Prueba con diferentes valores de hiperpar√°metros y observa si puedes lograr una recompensa acumulativa m√°s alta. ¬øEst√°s obteniendo m√°s de 195?
> **Tarea 2**: Para resolver formalmente el problema, necesitas alcanzar un promedio de recompensa de 195 a lo largo de 100 ejecuciones consecutivas. Mide eso durante el entrenamiento y aseg√∫rate de que has resuelto el problema formalmente.

## Ver el resultado en acci√≥n

Ser√≠a interesante ver c√≥mo se comporta el modelo entrenado. Vamos a ejecutar la simulaci√≥n y seguir la misma estrategia de selecci√≥n de acciones que durante el entrenamiento, muestreando seg√∫n la distribuci√≥n de probabilidad en la Q-Table: (bloque de c√≥digo 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Deber√≠as ver algo como esto:

![un carrito equilibrando](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄDesaf√≠o

> **Tarea 3**: Aqu√≠ estuvimos utilizando la copia final de la Q-Table, que puede no ser la mejor. Recuerda que hemos almacenado la Q-Table con mejor rendimiento en la variable `Qbest`. ¬°Prueba el mismo ejemplo con la Q-Table de mejor rendimiento copiando `Qbest` sobre `Q` y observa si notas alguna diferencia!

> **Tarea 4**: Aqu√≠ no est√°bamos seleccionando la mejor acci√≥n en cada paso, sino muestreando con la distribuci√≥n de probabilidad correspondiente. ¬øTendr√≠a m√°s sentido seleccionar siempre la mejor acci√≥n, con el valor m√°s alto en la Q-Table? Esto se puede hacer utilizando la funci√≥n `np.argmax` para encontrar el n√∫mero de acci√≥n correspondiente al valor m√°s alto en la Q-Table. Implementa esta estrategia y observa si mejora el equilibrio.

## [Cuestionario post-clase](https://ff-quizzes.netlify.app/en/ml/)

## Asignaci√≥n
[Entrena un Mountain Car](assignment.md)

## Conclusi√≥n

Ahora hemos aprendido c√≥mo entrenar agentes para lograr buenos resultados simplemente proporcionando una funci√≥n de recompensa que define el estado deseado del juego y d√°ndoles la oportunidad de explorar inteligentemente el espacio de b√∫squeda. Hemos aplicado con √©xito el algoritmo de Q-Learning en casos de entornos discretos y continuos, pero con acciones discretas.

Es importante tambi√©n estudiar situaciones donde el estado de las acciones sea continuo y cuando el espacio de observaci√≥n sea mucho m√°s complejo, como la imagen de la pantalla de un juego de Atari. En esos problemas, a menudo necesitamos usar t√©cnicas de aprendizaje autom√°tico m√°s poderosas, como redes neuronales, para lograr buenos resultados. Estos temas m√°s avanzados ser√°n el enfoque de nuestro pr√≥ximo curso avanzado de IA.

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.