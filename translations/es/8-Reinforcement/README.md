# Introducci贸n al aprendizaje por refuerzo

El aprendizaje por refuerzo, RL, es visto como uno de los paradigmas b谩sicos del aprendizaje autom谩tico, junto con el aprendizaje supervisado y el aprendizaje no supervisado. RL trata sobre decisiones: tomar las decisiones correctas o al menos aprender de ellas.

Imagina que tienes un entorno simulado como el mercado de valores. 驴Qu茅 pasa si impones una regulaci贸n determinada? 驴Tiene un efecto positivo o negativo? Si ocurre algo negativo, necesitas tomar este _refuerzo negativo_, aprender de 茅l y cambiar de rumbo. Si es un resultado positivo, necesitas construir sobre ese _refuerzo positivo_.

![Pedro y el lobo](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.es.png)

> 隆Pedro y sus amigos necesitan escapar del lobo hambriento! Imagen por [Jen Looper](https://twitter.com/jenlooper)

## Tema regional: Pedro y el lobo (Rusia)

[Pedro y el lobo](https://es.wikipedia.org/wiki/Pedro_y_el_lobo) es un cuento musical escrito por el compositor ruso [Sergei Prokofiev](https://es.wikipedia.org/wiki/Sergu茅i_Prok贸fiev). Es una historia sobre el joven pionero Pedro, que valientemente sale de su casa hacia el claro del bosque para perseguir al lobo. En esta secci贸n, entrenaremos algoritmos de aprendizaje autom谩tico que ayudar谩n a Pedro:

- **Explorar** el 谩rea circundante y construir un mapa de navegaci贸n 贸ptimo.
- **Aprender** a usar una patineta y equilibrarse en ella, para moverse m谩s r谩pido.

[![Pedro y el lobo](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

>  Haz clic en la imagen de arriba para escuchar Pedro y el lobo por Prokofiev

## Aprendizaje por refuerzo

En secciones anteriores, has visto dos ejemplos de problemas de aprendizaje autom谩tico:

- **Supervisado**, donde tenemos conjuntos de datos que sugieren soluciones de muestra al problema que queremos resolver. [Clasificaci贸n](../4-Classification/README.md) y [regresi贸n](../2-Regression/README.md) son tareas de aprendizaje supervisado.
- **No supervisado**, en el cual no tenemos datos de entrenamiento etiquetados. El principal ejemplo de aprendizaje no supervisado es [Agrupamiento](../5-Clustering/README.md).

En esta secci贸n, te presentaremos un nuevo tipo de problema de aprendizaje que no requiere datos de entrenamiento etiquetados. Hay varios tipos de estos problemas:

- **[Aprendizaje semi-supervisado](https://es.wikipedia.org/wiki/Aprendizaje_semi-supervisado)**, donde tenemos muchos datos no etiquetados que pueden usarse para pre-entrenar el modelo.
- **[Aprendizaje por refuerzo](https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo)**, en el cual un agente aprende c贸mo comportarse realizando experimentos en alg煤n entorno simulado.

### Ejemplo - juego de computadora

Supongamos que quieres ense帽ar a una computadora a jugar un juego, como el ajedrez, o [Super Mario](https://es.wikipedia.org/wiki/Super_Mario). Para que la computadora juegue un juego, necesitamos que prediga qu茅 movimiento hacer en cada uno de los estados del juego. Aunque esto pueda parecer un problema de clasificaci贸n, no lo es, porque no tenemos un conjunto de datos con estados y acciones correspondientes. Aunque podamos tener algunos datos como partidas de ajedrez existentes o grabaciones de jugadores jugando Super Mario, es probable que esos datos no cubran suficientemente un n煤mero grande de estados posibles.

En lugar de buscar datos de juego existentes, el **Aprendizaje por Refuerzo** (RL) se basa en la idea de *hacer que la computadora juegue* muchas veces y observar el resultado. As铆, para aplicar el Aprendizaje por Refuerzo, necesitamos dos cosas:

- **Un entorno** y **un simulador** que nos permitan jugar un juego muchas veces. Este simulador definir铆a todas las reglas del juego, as铆 como los posibles estados y acciones.

- **Una funci贸n de recompensa**, que nos dir铆a qu茅 tan bien lo hicimos durante cada movimiento o juego.

La principal diferencia entre otros tipos de aprendizaje autom谩tico y RL es que en RL t铆picamente no sabemos si ganamos o perdemos hasta que terminamos el juego. Por lo tanto, no podemos decir si un cierto movimiento solo es bueno o no - solo recibimos una recompensa al final del juego. Y nuestro objetivo es dise帽ar algoritmos que nos permitan entrenar un modelo bajo condiciones inciertas. Aprenderemos sobre un algoritmo de RL llamado **Q-learning**.

## Lecciones

1. [Introducci贸n al aprendizaje por refuerzo y Q-Learning](1-QLearning/README.md)
2. [Uso de un entorno de simulaci贸n de gimnasio](2-Gym/README.md)

## Cr茅ditos

"La Introducci贸n al Aprendizaje por Refuerzo" fue escrita con ワ por [Dmitry Soshnikov](http://soshnikov.com)

**Descargo de responsabilidad**:
Este documento ha sido traducido utilizando servicios de traducci贸n autom谩tica basados en inteligencia artificial. Aunque nos esforzamos por lograr precisi贸n, tenga en cuenta que las traducciones automatizadas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci贸n cr铆tica, se recomienda una traducci贸n profesional realizada por humanos. No nos hacemos responsables de ning煤n malentendido o interpretaci贸n err贸nea que surja del uso de esta traducci贸n.