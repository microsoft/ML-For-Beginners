<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2f88fbc741d792890ff2f1430fe0dae0",
  "translation_date": "2025-09-03T22:18:59+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "es"
}
-->
# Construir un modelo de regresi√≥n usando Scikit-learn: regresi√≥n de cuatro maneras

![Infograf√≠a de regresi√≥n lineal vs polin√≥mica](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.es.png)
> Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [¬°Esta lecci√≥n est√° disponible en R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introducci√≥n 

Hasta ahora has explorado qu√© es la regresi√≥n con datos de muestra obtenidos del conjunto de datos de precios de calabazas que utilizaremos a lo largo de esta lecci√≥n. Tambi√©n lo has visualizado usando Matplotlib.

Ahora est√°s listo para profundizar en la regresi√≥n para ML. Aunque la visualizaci√≥n te permite comprender los datos, el verdadero poder del aprendizaje autom√°tico proviene del _entrenamiento de modelos_. Los modelos se entrenan con datos hist√≥ricos para capturar autom√°ticamente las dependencias de los datos y permiten predecir resultados para nuevos datos que el modelo no ha visto antes.

En esta lecci√≥n, aprender√°s m√°s sobre dos tipos de regresi√≥n: _regresi√≥n lineal b√°sica_ y _regresi√≥n polin√≥mica_, junto con algunas matem√°ticas subyacentes a estas t√©cnicas. Estos modelos nos permitir√°n predecir los precios de las calabazas dependiendo de diferentes datos de entrada.

[![ML para principiantes - Entendiendo la regresi√≥n lineal](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para principiantes - Entendiendo la regresi√≥n lineal")

> üé• Haz clic en la imagen de arriba para ver un breve video sobre la regresi√≥n lineal.

> A lo largo de este curr√≠culo, asumimos un conocimiento m√≠nimo de matem√°ticas y buscamos hacerlo accesible para estudiantes de otros campos, as√≠ que presta atenci√≥n a notas, üßÆ llamados, diagramas y otras herramientas de aprendizaje para facilitar la comprensi√≥n.

### Prerrequisitos

A estas alturas deber√≠as estar familiarizado con la estructura de los datos de calabazas que estamos examinando. Puedes encontrarlo precargado y preprocesado en el archivo _notebook.ipynb_ de esta lecci√≥n. En el archivo, el precio de las calabazas se muestra por bushel en un nuevo marco de datos. Aseg√∫rate de poder ejecutar estos notebooks en kernels en Visual Studio Code.

### Preparaci√≥n

Como recordatorio, est√°s cargando estos datos para hacer preguntas sobre ellos.

- ¬øCu√°ndo es el mejor momento para comprar calabazas? 
- ¬øQu√© precio puedo esperar por un caso de calabazas miniatura?
- ¬øDeber√≠a comprarlas en cestas de medio bushel o en cajas de 1 1/9 bushel?
Sigamos profundizando en estos datos.

En la lecci√≥n anterior, creaste un marco de datos de Pandas y lo llenaste con parte del conjunto de datos original, estandarizando los precios por bushel. Sin embargo, al hacer eso, solo pudiste recopilar alrededor de 400 puntos de datos y solo para los meses de oto√±o.

Echa un vistazo a los datos que hemos precargado en el notebook que acompa√±a esta lecci√≥n. Los datos est√°n precargados y se ha graficado un diagrama de dispersi√≥n inicial para mostrar los datos por mes. Tal vez podamos obtener un poco m√°s de detalle sobre la naturaleza de los datos limpi√°ndolos m√°s.

## Una l√≠nea de regresi√≥n lineal

Como aprendiste en la Lecci√≥n 1, el objetivo de un ejercicio de regresi√≥n lineal es poder trazar una l√≠nea para:

- **Mostrar relaciones entre variables**. Mostrar la relaci√≥n entre las variables.
- **Hacer predicciones**. Hacer predicciones precisas sobre d√≥nde caer√≠a un nuevo punto de datos en relaci√≥n con esa l√≠nea.

Es t√≠pico de la **Regresi√≥n de M√≠nimos Cuadrados** trazar este tipo de l√≠nea. El t√©rmino 'm√≠nimos cuadrados' significa que todos los puntos de datos que rodean la l√≠nea de regresi√≥n se elevan al cuadrado y luego se suman. Idealmente, esa suma final es lo m√°s peque√±a posible, porque queremos un n√∫mero bajo de errores, o `m√≠nimos cuadrados`.

Hacemos esto porque queremos modelar una l√≠nea que tenga la menor distancia acumulada de todos nuestros puntos de datos. Tambi√©n elevamos los t√©rminos al cuadrado antes de sumarlos porque nos interesa su magnitud m√°s que su direcci√≥n.

> **üßÆ Mu√©strame las matem√°ticas** 
> 
> Esta l√≠nea, llamada la _l√≠nea de mejor ajuste_, puede expresarse mediante [una ecuaci√≥n](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` es la 'variable explicativa'. `Y` es la 'variable dependiente'. La pendiente de la l√≠nea es `b` y `a` es la intersecci√≥n con el eje Y, que se refiere al valor de `Y` cuando `X = 0`. 
>
>![calcular la pendiente](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.es.png)
>
> Primero, calcula la pendiente `b`. Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)
>
> En otras palabras, y refiri√©ndonos a la pregunta original de los datos de calabazas: "predecir el precio de una calabaza por bushel seg√∫n el mes", `X` se referir√≠a al precio y `Y` se referir√≠a al mes de venta. 
>
>![completar la ecuaci√≥n](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.es.png)
>
> Calcula el valor de Y. Si est√°s pagando alrededor de $4, ¬°debe ser abril! Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)
>
> Las matem√°ticas que calculan la l√≠nea deben demostrar la pendiente de la l√≠nea, que tambi√©n depende de la intersecci√≥n, o d√≥nde se sit√∫a `Y` cuando `X = 0`.
>
> Puedes observar el m√©todo de c√°lculo de estos valores en el sitio web [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). Tambi√©n visita [este calculador de m√≠nimos cuadrados](https://www.mathsisfun.com/data/least-squares-calculator.html) para ver c√≥mo los valores de los n√∫meros impactan la l√≠nea.

## Correlaci√≥n

Otro t√©rmino que debes entender es el **Coeficiente de Correlaci√≥n** entre las variables X y Y dadas. Usando un diagrama de dispersi√≥n, puedes visualizar r√°pidamente este coeficiente. Un gr√°fico con puntos de datos dispersos en una l√≠nea ordenada tiene alta correlaci√≥n, pero un gr√°fico con puntos de datos dispersos por todas partes entre X y Y tiene baja correlaci√≥n.

Un buen modelo de regresi√≥n lineal ser√° aquel que tenga un Coeficiente de Correlaci√≥n alto (m√°s cercano a 1 que a 0) utilizando el m√©todo de M√≠nimos Cuadrados con una l√≠nea de regresi√≥n.

‚úÖ Ejecuta el notebook que acompa√±a esta lecci√≥n y observa el diagrama de dispersi√≥n de Mes a Precio. Seg√∫n tu interpretaci√≥n visual del diagrama de dispersi√≥n, ¬øparece que los datos que asocian Mes con Precio para las ventas de calabazas tienen alta o baja correlaci√≥n? ¬øCambia eso si usas una medida m√°s detallada en lugar de `Mes`, por ejemplo, *d√≠a del a√±o* (es decir, n√∫mero de d√≠as desde el inicio del a√±o)?

En el c√≥digo a continuaci√≥n, asumiremos que hemos limpiado los datos y obtenido un marco de datos llamado `new_pumpkins`, similar al siguiente:

ID | Mes | D√≠aDelA√±o | Variedad | Ciudad | Paquete | Precio Bajo | Precio Alto | Precio
---|-----|-----------|----------|--------|---------|-------------|-------------|-------
70 | 9 | 267 | TIPO PARA PIE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | TIPO PARA PIE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | TIPO PARA PIE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | TIPO PARA PIE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | TIPO PARA PIE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> El c√≥digo para limpiar los datos est√° disponible en [`notebook.ipynb`](notebook.ipynb). Hemos realizado los mismos pasos de limpieza que en la lecci√≥n anterior y hemos calculado la columna `D√≠aDelA√±o` usando la siguiente expresi√≥n: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Ahora que tienes una comprensi√≥n de las matem√°ticas detr√°s de la regresi√≥n lineal, vamos a crear un modelo de regresi√≥n para ver si podemos predecir qu√© paquete de calabazas tendr√° los mejores precios. Alguien que compre calabazas para un huerto de calabazas festivo podr√≠a querer esta informaci√≥n para optimizar sus compras de paquetes de calabazas para el huerto.

## Buscando correlaci√≥n

[![ML para principiantes - Buscando correlaci√≥n: La clave de la regresi√≥n lineal](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para principiantes - Buscando correlaci√≥n: La clave de la regresi√≥n lineal")

> üé• Haz clic en la imagen de arriba para ver un breve video sobre correlaci√≥n.

De la lecci√≥n anterior probablemente hayas visto que el precio promedio para diferentes meses se ve as√≠:

<img alt="Precio promedio por mes" src="../2-Data/images/barchart.png" width="50%"/>

Esto sugiere que deber√≠a haber alguna correlaci√≥n, y podemos intentar entrenar un modelo de regresi√≥n lineal para predecir la relaci√≥n entre `Mes` y `Precio`, o entre `D√≠aDelA√±o` y `Precio`. Aqu√≠ est√° el diagrama de dispersi√≥n que muestra esta √∫ltima relaci√≥n:

<img alt="Diagrama de dispersi√≥n de Precio vs. D√≠a del A√±o" src="images/scatter-dayofyear.png" width="50%" /> 

Veamos si hay una correlaci√≥n usando la funci√≥n `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que la correlaci√≥n es bastante peque√±a, -0.15 por `Mes` y -0.17 por el `D√≠aDelMes`, pero podr√≠a haber otra relaci√≥n importante. Parece que hay diferentes grupos de precios que corresponden a diferentes variedades de calabazas. Para confirmar esta hip√≥tesis, tracemos cada categor√≠a de calabaza usando un color diferente. Al pasar un par√°metro `ax` a la funci√≥n de trazado `scatter`, podemos graficar todos los puntos en el mismo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Diagrama de dispersi√≥n de Precio vs. D√≠a del A√±o" src="images/scatter-dayofyear-color.png" width="50%" /> 

Nuestra investigaci√≥n sugiere que la variedad tiene m√°s efecto en el precio general que la fecha de venta real. Podemos ver esto con un gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de precio vs variedad" src="images/price-by-variety.png" width="50%" /> 

Centr√©monos por el momento solo en una variedad de calabaza, el 'tipo para pie', y veamos qu√© efecto tiene la fecha en el precio:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Diagrama de dispersi√≥n de Precio vs. D√≠a del A√±o" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Si ahora calculamos la correlaci√≥n entre `Precio` y `D√≠aDelA√±o` usando la funci√≥n `corr`, obtendremos algo como `-0.27`, lo que significa que entrenar un modelo predictivo tiene sentido.

> Antes de entrenar un modelo de regresi√≥n lineal, es importante asegurarse de que nuestros datos est√©n limpios. La regresi√≥n lineal no funciona bien con valores faltantes, por lo que tiene sentido eliminar todas las celdas vac√≠as:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Otra opci√≥n ser√≠a llenar esos valores vac√≠os con valores promedio de la columna correspondiente.

## Regresi√≥n Lineal Simple

[![ML para principiantes - Regresi√≥n Lineal y Polin√≥mica usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para principiantes - Regresi√≥n Lineal y Polin√≥mica usando Scikit-learn")

> üé• Haz clic en la imagen de arriba para ver un breve video sobre regresi√≥n lineal y polin√≥mica.

Para entrenar nuestro modelo de Regresi√≥n Lineal, utilizaremos la biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Comenzamos separando los valores de entrada (caracter√≠sticas) y la salida esperada (etiqueta) en arreglos numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Nota que tuvimos que realizar `reshape` en los datos de entrada para que el paquete de Regresi√≥n Lineal los entienda correctamente. La Regresi√≥n Lineal espera un arreglo 2D como entrada, donde cada fila del arreglo corresponde a un vector de caracter√≠sticas de entrada. En nuestro caso, dado que solo tenemos una entrada, necesitamos un arreglo con forma N√ó1, donde N es el tama√±o del conjunto de datos.

Luego, necesitamos dividir los datos en conjuntos de entrenamiento y prueba, para poder validar nuestro modelo despu√©s del entrenamiento:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, entrenar el modelo de Regresi√≥n Lineal real toma solo dos l√≠neas de c√≥digo. Definimos el objeto `LinearRegression` y lo ajustamos a nuestros datos usando el m√©todo `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

El objeto `LinearRegression` despu√©s de ajustarse contiene todos los coeficientes de la regresi√≥n, que pueden accederse usando la propiedad `.coef_`. En nuestro caso, solo hay un coeficiente, que deber√≠a estar alrededor de `-0.017`. Esto significa que los precios parecen bajar un poco con el tiempo, pero no demasiado, alrededor de 2 centavos por d√≠a. Tambi√©n podemos acceder al punto de intersecci√≥n de la regresi√≥n con el eje Y usando `lin_reg.intercept_`, que estar√° alrededor de `21` en nuestro caso, indicando el precio al inicio del a√±o.

Para ver qu√© tan preciso es nuestro modelo, podemos predecir precios en un conjunto de datos de prueba y luego medir qu√© tan cerca est√°n nuestras predicciones de los valores esperados. Esto puede hacerse usando la m√©trica de error cuadr√°tico medio (MSE), que es el promedio de todas las diferencias al cuadrado entre el valor esperado y el predicho.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Nuestro error parece estar en torno a 2 puntos, lo que equivale a ~17%. No es muy bueno. Otro indicador de la calidad del modelo es el **coeficiente de determinaci√≥n**, que se puede obtener de la siguiente manera:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Si el valor es 0, significa que el modelo no toma en cuenta los datos de entrada y act√∫a como el *peor predictor lineal*, que simplemente es el valor promedio del resultado. El valor de 1 significa que podemos predecir perfectamente todos los resultados esperados. En nuestro caso, el coeficiente est√° alrededor de 0.06, lo cual es bastante bajo.

Tambi√©n podemos graficar los datos de prueba junto con la l√≠nea de regresi√≥n para ver mejor c√≥mo funciona la regresi√≥n en nuestro caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="Regresi√≥n lineal" src="images/linear-results.png" width="50%" />

## Regresi√≥n Polin√≥mica  

Otro tipo de Regresi√≥n Lineal es la Regresi√≥n Polin√≥mica. Aunque a veces existe una relaci√≥n lineal entre las variables - cuanto mayor es el volumen de la calabaza, mayor es el precio - en ocasiones estas relaciones no pueden representarse como un plano o una l√≠nea recta.  

‚úÖ Aqu√≠ hay [algunos ejemplos](https://online.stat.psu.edu/stat501/lesson/9/9.8) de datos que podr√≠an usar Regresi√≥n Polin√≥mica  

Observa nuevamente la relaci√≥n entre Fecha y Precio. ¬øEste diagrama de dispersi√≥n parece que deber√≠a analizarse necesariamente con una l√≠nea recta? ¬øNo pueden fluctuar los precios? En este caso, puedes intentar con regresi√≥n polin√≥mica.  

‚úÖ Los polinomios son expresiones matem√°ticas que pueden consistir en una o m√°s variables y coeficientes  

La regresi√≥n polin√≥mica crea una l√≠nea curva para ajustar mejor los datos no lineales. En nuestro caso, si incluimos una variable `DayOfYear` al cuadrado en los datos de entrada, deber√≠amos poder ajustar nuestros datos con una curva parab√≥lica, que tendr√° un m√≠nimo en cierto punto del a√±o.  

Scikit-learn incluye una √∫til [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) para combinar diferentes pasos de procesamiento de datos. Un **pipeline** es una cadena de **estimadores**. En nuestro caso, crearemos un pipeline que primero agrega caracter√≠sticas polin√≥micas a nuestro modelo y luego entrena la regresi√≥n:  

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

Usar `PolynomialFeatures(2)` significa que incluiremos todos los polinomios de segundo grado de los datos de entrada. En nuestro caso, simplemente significar√° `DayOfYear`<sup>2</sup>, pero dado dos variables de entrada X e Y, esto agregar√° X<sup>2</sup>, XY y Y<sup>2</sup>. Tambi√©n podemos usar polinomios de mayor grado si lo deseamos.  

Los pipelines pueden usarse de la misma manera que el objeto original `LinearRegression`, es decir, podemos usar `fit` en el pipeline y luego usar `predict` para obtener los resultados de predicci√≥n. Aqu√≠ est√° el gr√°fico que muestra los datos de prueba y la curva de aproximaci√≥n:  

<img alt="Regresi√≥n polin√≥mica" src="images/poly-results.png" width="50%" />  

Usando Regresi√≥n Polin√≥mica, podemos obtener un MSE ligeramente m√°s bajo y un coeficiente de determinaci√≥n m√°s alto, pero no significativamente. ¬°Necesitamos tomar en cuenta otras caracter√≠sticas!  

> Puedes observar que los precios m√≠nimos de las calabazas se registran en alg√∫n momento cerca de Halloween. ¬øC√≥mo puedes explicar esto?  

üéÉ ¬°Felicidades! Acabas de crear un modelo que puede ayudar a predecir el precio de las calabazas para pastel. Probablemente puedas repetir el mismo procedimiento para todos los tipos de calabazas, pero eso ser√≠a tedioso. ¬°Ahora aprendamos c√≥mo tomar en cuenta la variedad de calabazas en nuestro modelo!  

## Caracter√≠sticas Categ√≥ricas  

En un mundo ideal, queremos poder predecir precios para diferentes variedades de calabazas usando el mismo modelo. Sin embargo, la columna `Variety` es algo diferente de columnas como `Month`, porque contiene valores no num√©ricos. Estas columnas se llaman **categ√≥ricas**.  

[![ML para principiantes - Predicciones con caracter√≠sticas categ√≥ricas usando Regresi√≥n Lineal](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML para principiantes - Predicciones con caracter√≠sticas categ√≥ricas usando Regresi√≥n Lineal")  

> üé• Haz clic en la imagen de arriba para ver un breve video sobre el uso de caracter√≠sticas categ√≥ricas.  

Aqu√≠ puedes ver c√≥mo el precio promedio depende de la variedad:  

<img alt="Precio promedio por variedad" src="images/price-by-variety.png" width="50%" />  

Para tomar en cuenta la variedad, primero necesitamos convertirla a forma num√©rica, o **codificarla**. Hay varias formas de hacerlo:  

* La **codificaci√≥n num√©rica simple** construir√° una tabla de diferentes variedades y luego reemplazar√° el nombre de la variedad por un √≠ndice en esa tabla. Esta no es la mejor idea para la regresi√≥n lineal, porque la regresi√≥n lineal toma el valor num√©rico real del √≠ndice y lo agrega al resultado, multiplic√°ndolo por alg√∫n coeficiente. En nuestro caso, la relaci√≥n entre el n√∫mero de √≠ndice y el precio claramente no es lineal, incluso si nos aseguramos de que los √≠ndices est√©n ordenados de alguna manera espec√≠fica.  
* La **codificaci√≥n one-hot** reemplazar√° la columna `Variety` por 4 columnas diferentes, una para cada variedad. Cada columna contendr√° `1` si la fila correspondiente es de una variedad dada, y `0` en caso contrario. Esto significa que habr√° cuatro coeficientes en la regresi√≥n lineal, uno para cada variedad de calabaza, responsables del "precio inicial" (o m√°s bien "precio adicional") para esa variedad en particular.  

El siguiente c√≥digo muestra c√≥mo podemos codificar una variedad usando one-hot encoding:  

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

Para entrenar la regresi√≥n lineal usando la variedad codificada como one-hot en los datos de entrada, solo necesitamos inicializar correctamente los datos `X` y `y`:  

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

El resto del c√≥digo es el mismo que usamos anteriormente para entrenar la Regresi√≥n Lineal. Si lo pruebas, ver√°s que el error cuadr√°tico medio es aproximadamente el mismo, pero obtenemos un coeficiente de determinaci√≥n mucho m√°s alto (~77%). Para obtener predicciones a√∫n m√°s precisas, podemos tomar en cuenta m√°s caracter√≠sticas categ√≥ricas, as√≠ como caracter√≠sticas num√©ricas, como `Month` o `DayOfYear`. Para obtener un gran conjunto de caracter√≠sticas, podemos usar `join`:  

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

Aqu√≠ tambi√©n tomamos en cuenta `City` y el tipo de `Package`, lo que nos da un MSE de 2.84 (10%) y un coeficiente de determinaci√≥n de 0.94.  

## Junt√°ndolo todo  

Para crear el mejor modelo, podemos usar datos combinados (categ√≥ricos codificados como one-hot + num√©ricos) del ejemplo anterior junto con Regresi√≥n Polin√≥mica. Aqu√≠ est√° el c√≥digo completo para tu conveniencia:  

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Esto deber√≠a darnos el mejor coeficiente de determinaci√≥n de casi 97% y un MSE=2.23 (~8% de error de predicci√≥n).  

| Modelo | MSE | Determinaci√≥n |  
|-------|-----|---------------|  
| `DayOfYear` Lineal | 2.77 (17.2%) | 0.07 |  
| `DayOfYear` Polin√≥mico | 2.73 (17.0%) | 0.08 |  
| `Variety` Lineal | 5.24 (19.7%) | 0.77 |  
| Todas las caracter√≠sticas Lineal | 2.84 (10.5%) | 0.94 |  
| Todas las caracter√≠sticas Polin√≥mico | 2.23 (8.25%) | 0.97 |  

üèÜ ¬°Bien hecho! Creaste cuatro modelos de Regresi√≥n en una sola lecci√≥n y mejoraste la calidad del modelo al 97%. En la secci√≥n final sobre Regresi√≥n, aprender√°s sobre Regresi√≥n Log√≠stica para determinar categor√≠as.  

---  
## üöÄDesaf√≠o  

Prueba varias variables diferentes en este notebook para ver c√≥mo la correlaci√≥n corresponde a la precisi√≥n del modelo.  

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)  

## Revisi√≥n y Autoestudio  

En esta lecci√≥n aprendimos sobre Regresi√≥n Lineal. Hay otros tipos importantes de Regresi√≥n. Lee sobre las t√©cnicas Stepwise, Ridge, Lasso y Elasticnet. Un buen curso para aprender m√°s es el [curso de Aprendizaje Estad√≠stico de Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).  

## Tarea  

[Construye un Modelo](assignment.md)  

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.