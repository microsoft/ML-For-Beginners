# Construyendo soluciones de aprendizaje autom√°tico con IA responsable

![Resumen de IA responsable en Aprendizaje Autom√°tico en un sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.es.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introducci√≥n

En este curr√≠culo, comenzar√°s a descubrir c√≥mo el aprendizaje autom√°tico puede y est√° impactando nuestras vidas cotidianas. Incluso ahora, los sistemas y modelos est√°n involucrados en tareas de toma de decisiones diarias, como diagn√≥sticos de salud, aprobaciones de pr√©stamos o detecci√≥n de fraudes. Por lo tanto, es importante que estos modelos funcionen bien para proporcionar resultados confiables. Al igual que cualquier aplicaci√≥n de software, los sistemas de IA pueden no cumplir con las expectativas o tener un resultado indeseable. Es por eso que es esencial poder entender y explicar el comportamiento de un modelo de IA.

Imagina lo que puede suceder cuando los datos que utilizas para construir estos modelos carecen de ciertos datos demogr√°ficos, como raza, g√©nero, visi√≥n pol√≠tica, religi√≥n, o representan desproporcionadamente dichos datos demogr√°ficos. ¬øQu√© pasa cuando la salida del modelo se interpreta para favorecer a algunos demogr√°ficos? ¬øCu√°l es la consecuencia para la aplicaci√≥n? Adem√°s, ¬øqu√© sucede cuando el modelo tiene un resultado adverso y es perjudicial para las personas? ¬øQui√©n es responsable del comportamiento de los sistemas de IA? Estas son algunas preguntas que exploraremos en este curr√≠culo.

En esta lecci√≥n, t√∫:

- Aumentar√°s tu conciencia sobre la importancia de la equidad en el aprendizaje autom√°tico y los da√±os relacionados con la equidad.
- Te familiarizar√°s con la pr√°ctica de explorar valores at√≠picos y escenarios inusuales para garantizar la fiabilidad y seguridad.
- Comprender√°s la necesidad de empoderar a todos dise√±ando sistemas inclusivos.
- Explorar√°s la importancia de proteger la privacidad y seguridad de los datos y las personas.
- Ver√°s la importancia de tener un enfoque de caja de cristal para explicar el comportamiento de los modelos de IA.
- Ser√°s consciente de c√≥mo la responsabilidad es esencial para construir confianza en los sistemas de IA.

## Prerrequisito

Como prerrequisito, toma la ruta de aprendizaje "Principios de IA Responsable" y mira el video a continuaci√≥n sobre el tema:

Aprende m√°s sobre IA Responsable siguiendo esta [Ruta de Aprendizaje](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Enfoque de Microsoft hacia la IA Responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Enfoque de Microsoft hacia la IA Responsable")

> üé• Haz clic en la imagen de arriba para ver un video: Enfoque de Microsoft hacia la IA Responsable

## Equidad

Los sistemas de IA deben tratar a todos de manera justa y evitar afectar a grupos similares de personas de diferentes maneras. Por ejemplo, cuando los sistemas de IA proporcionan orientaci√≥n sobre tratamiento m√©dico, solicitudes de pr√©stamos o empleo, deben hacer las mismas recomendaciones a todos con s√≠ntomas similares, circunstancias financieras o cualificaciones profesionales. Cada uno de nosotros, como seres humanos, lleva consigo sesgos heredados que afectan nuestras decisiones y acciones. Estos sesgos pueden ser evidentes en los datos que utilizamos para entrenar los sistemas de IA. Dicha manipulaci√≥n puede ocurrir a veces de manera involuntaria. A menudo es dif√≠cil saber conscientemente cu√°ndo est√°s introduciendo sesgo en los datos.

**"Injusticia"** abarca impactos negativos, o "da√±os", para un grupo de personas, como aquellos definidos en t√©rminos de raza, g√©nero, edad o estado de discapacidad. Los principales da√±os relacionados con la equidad se pueden clasificar como:

- **Asignaci√≥n**, si por ejemplo se favorece a un g√©nero o etnia sobre otro.
- **Calidad del servicio**. Si entrenas los datos para un escenario espec√≠fico pero la realidad es mucho m√°s compleja, lleva a un servicio de bajo rendimiento. Por ejemplo, un dispensador de jab√≥n de manos que no parece ser capaz de detectar personas con piel oscura. [Referencia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigraci√≥n**. Criticar y etiquetar injustamente algo o alguien. Por ejemplo, una tecnolog√≠a de etiquetado de im√°genes etiquet√≥ infamemente im√°genes de personas de piel oscura como gorilas.
- **Sobre o subrepresentaci√≥n**. La idea es que un cierto grupo no se vea en una cierta profesi√≥n, y cualquier servicio o funci√≥n que siga promoviendo eso est√° contribuyendo al da√±o.
- **Estereotipos**. Asociar un grupo dado con atributos preasignados. Por ejemplo, un sistema de traducci√≥n de idiomas entre ingl√©s y turco puede tener inexactitudes debido a palabras con asociaciones estereotipadas de g√©nero.

![traducci√≥n al turco](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.es.png)
> traducci√≥n al turco

![traducci√≥n de vuelta al ingl√©s](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.es.png)
> traducci√≥n de vuelta al ingl√©s

Al dise√±ar y probar sistemas de IA, debemos asegurarnos de que la IA sea justa y no est√© programada para tomar decisiones sesgadas o discriminatorias, que los seres humanos tambi√©n tienen prohibido tomar. Garantizar la equidad en la IA y el aprendizaje autom√°tico sigue siendo un desaf√≠o sociot√©cnico complejo.

### Fiabilidad y seguridad

Para generar confianza, los sistemas de IA deben ser fiables, seguros y consistentes bajo condiciones normales e inesperadas. Es importante saber c√≥mo se comportar√°n los sistemas de IA en una variedad de situaciones, especialmente cuando son casos at√≠picos. Al construir soluciones de IA, se debe poner un enfoque sustancial en c√≥mo manejar una amplia variedad de circunstancias que las soluciones de IA encontrar√≠an. Por ejemplo, un coche aut√≥nomo debe poner la seguridad de las personas como una prioridad principal. Como resultado, la IA que impulsa el coche necesita considerar todos los posibles escenarios que el coche podr√≠a encontrar, como noche, tormentas el√©ctricas o ventiscas, ni√±os corriendo por la calle, mascotas, construcciones en la carretera, etc. Qu√© tan bien un sistema de IA puede manejar una amplia gama de condiciones de manera fiable y segura refleja el nivel de anticipaci√≥n que el cient√≠fico de datos o el desarrollador de IA consider√≥ durante el dise√±o o prueba del sistema.

> [üé• Haz clic aqu√≠ para un video:](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusi√≥n

Los sistemas de IA deben ser dise√±ados para involucrar y empoderar a todos. Al dise√±ar e implementar sistemas de IA, los cient√≠ficos de datos y desarrolladores de IA identifican y abordan posibles barreras en el sistema que podr√≠an excluir involuntariamente a las personas. Por ejemplo, hay 1 mil millones de personas con discapacidades en todo el mundo. Con el avance de la IA, pueden acceder a una amplia gama de informaci√≥n y oportunidades m√°s f√°cilmente en sus vidas diarias. Al abordar las barreras, se crean oportunidades para innovar y desarrollar productos de IA con mejores experiencias que beneficien a todos.

> [üé• Haz clic aqu√≠ para un video: inclusi√≥n en IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Seguridad y privacidad

Los sistemas de IA deben ser seguros y respetar la privacidad de las personas. Las personas conf√≠an menos en los sistemas que ponen en riesgo su privacidad, informaci√≥n o vidas. Al entrenar modelos de aprendizaje autom√°tico, dependemos de los datos para producir los mejores resultados. Al hacerlo, se debe considerar el origen de los datos y su integridad. Por ejemplo, ¬øfueron los datos enviados por el usuario o estaban disponibles p√∫blicamente? Luego, al trabajar con los datos, es crucial desarrollar sistemas de IA que puedan proteger la informaci√≥n confidencial y resistir ataques. A medida que la IA se vuelve m√°s prevalente, proteger la privacidad y asegurar informaci√≥n personal y empresarial importante se vuelve m√°s cr√≠tico y complejo. Los problemas de privacidad y seguridad de datos requieren una atenci√≥n especialmente cercana para la IA porque el acceso a los datos es esencial para que los sistemas de IA hagan predicciones y decisiones precisas e informadas sobre las personas.

> [üé• Haz clic aqu√≠ para un video: seguridad en IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como industria, hemos logrado avances significativos en privacidad y seguridad, impulsados significativamente por regulaciones como el GDPR (Reglamento General de Protecci√≥n de Datos).
- Sin embargo, con los sistemas de IA debemos reconocer la tensi√≥n entre la necesidad de m√°s datos personales para hacer los sistemas m√°s personales y efectivos y la privacidad.
- Al igual que con el nacimiento de las computadoras conectadas a Internet, tambi√©n estamos viendo un gran aumento en el n√∫mero de problemas de seguridad relacionados con la IA.
- Al mismo tiempo, hemos visto que la IA se usa para mejorar la seguridad. Como ejemplo, la mayor√≠a de los esc√°neres antivirus modernos est√°n impulsados por heur√≠sticas de IA hoy en d√≠a.
- Necesitamos asegurarnos de que nuestros procesos de Ciencia de Datos se mezclen armoniosamente con las √∫ltimas pr√°cticas de privacidad y seguridad.

### Transparencia

Los sistemas de IA deben ser comprensibles. Una parte crucial de la transparencia es explicar el comportamiento de los sistemas de IA y sus componentes. Mejorar la comprensi√≥n de los sistemas de IA requiere que las partes interesadas comprendan c√≥mo y por qu√© funcionan para que puedan identificar posibles problemas de rendimiento, preocupaciones de seguridad y privacidad, sesgos, pr√°cticas excluyentes o resultados no deseados. Tambi√©n creemos que aquellos que usan sistemas de IA deben ser honestos y abiertos sobre cu√°ndo, por qu√© y c√≥mo eligen implementarlos, as√≠ como las limitaciones de los sistemas que usan. Por ejemplo, si un banco usa un sistema de IA para apoyar sus decisiones de pr√©stamos al consumidor, es importante examinar los resultados y entender qu√© datos influyen en las recomendaciones del sistema. Los gobiernos est√°n comenzando a regular la IA en diversas industrias, por lo que los cient√≠ficos de datos y las organizaciones deben explicar si un sistema de IA cumple con los requisitos regulatorios, especialmente cuando hay un resultado no deseado.

> [üé• Haz clic aqu√≠ para un video: transparencia en IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Debido a que los sistemas de IA son tan complejos, es dif√≠cil entender c√≥mo funcionan e interpretar los resultados.
- Esta falta de comprensi√≥n afecta la forma en que se gestionan, operacionalizan y documentan estos sistemas.
- Esta falta de comprensi√≥n afecta m√°s importantemente las decisiones tomadas utilizando los resultados que estos sistemas producen.

### Responsabilidad

Las personas que dise√±an y despliegan sistemas de IA deben ser responsables de c√≥mo operan sus sistemas. La necesidad de responsabilidad es particularmente crucial con tecnolog√≠as de uso sensible como el reconocimiento facial. Recientemente, ha habido una creciente demanda de tecnolog√≠a de reconocimiento facial, especialmente por parte de organizaciones de aplicaci√≥n de la ley que ven el potencial de la tecnolog√≠a en usos como encontrar ni√±os desaparecidos. Sin embargo, estas tecnolog√≠as podr√≠an ser potencialmente utilizadas por un gobierno para poner en riesgo las libertades fundamentales de sus ciudadanos al, por ejemplo, permitir la vigilancia continua de individuos espec√≠ficos. Por lo tanto, los cient√≠ficos de datos y las organizaciones deben ser responsables de c√≥mo su sistema de IA impacta a individuos o a la sociedad.

[![Investigador l√≠der en IA advierte sobre la vigilancia masiva a trav√©s del reconocimiento facial](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.es.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Enfoque de Microsoft hacia la IA Responsable")

> üé• Haz clic en la imagen de arriba para ver un video: Advertencias sobre la vigilancia masiva a trav√©s del reconocimiento facial

En √∫ltima instancia, una de las preguntas m√°s grandes para nuestra generaci√≥n, como la primera generaci√≥n que est√° llevando la IA a la sociedad, es c√≥mo asegurarse de que las computadoras sigan siendo responsables ante las personas y c√≥mo asegurarse de que las personas que dise√±an computadoras sean responsables ante todos los dem√°s.

## Evaluaci√≥n de impacto

Antes de entrenar un modelo de aprendizaje autom√°tico, es importante realizar una evaluaci√≥n de impacto para comprender el prop√≥sito del sistema de IA; cu√°l es el uso previsto; d√≥nde se desplegar√°; y qui√©n interactuar√° con el sistema. Estos son √∫tiles para el/los revisor(es) o probadores que eval√∫an el sistema para saber qu√© factores considerar al identificar posibles riesgos y consecuencias esperadas.

Las siguientes son √°reas de enfoque al realizar una evaluaci√≥n de impacto:

* **Impacto adverso en individuos**. Ser consciente de cualquier restricci√≥n o requisito, uso no soportado o cualquier limitaci√≥n conocida que obstaculice el rendimiento del sistema es vital para garantizar que el sistema no se use de manera que pueda causar da√±o a los individuos.
* **Requisitos de datos**. Comprender c√≥mo y d√≥nde el sistema usar√° los datos permite a los revisores explorar cualquier requisito de datos que debas tener en cuenta (por ejemplo, regulaciones de datos GDPR o HIPPA). Adem√°s, examina si la fuente o cantidad de datos es sustancial para el entrenamiento.
* **Resumen del impacto**. Re√∫ne una lista de posibles da√±os que podr√≠an surgir del uso del sistema. A lo largo del ciclo de vida del ML, revisa si los problemas identificados est√°n mitigados o abordados.
* **Objetivos aplicables** para cada uno de los seis principios fundamentales. Eval√∫a si los objetivos de cada uno de los principios se cumplen y si hay alguna brecha.

## Depuraci√≥n con IA responsable

Al igual que depurar una aplicaci√≥n de software, depurar un sistema de IA es un proceso necesario de identificaci√≥n y resoluci√≥n de problemas en el sistema. Hay muchos factores que afectar√≠an a un modelo que no se desempe√±a como se espera o de manera responsable. La mayor√≠a de las m√©tricas de rendimiento de modelos tradicionales son agregados cuantitativos del rendimiento de un modelo, que no son suficientes para analizar c√≥mo un modelo viola los principios de IA responsable. Adem√°s, un modelo de aprendizaje autom√°tico es una caja negra que hace dif√≠cil entender qu√© impulsa su resultado o proporcionar una explicaci√≥n cuando comete un error. M√°s adelante en este curso, aprenderemos c√≥mo usar el panel de IA Responsable para ayudar a depurar sistemas de IA. El panel proporciona una herramienta hol√≠stica para que los cient√≠ficos de datos y desarrolladores de IA realicen:

* **An√°lisis de errores**. Para identificar la distribuci√≥n de errores del modelo que puede afectar la equidad o fiabilidad del sistema.
* **Visi√≥n general del modelo**. Para descubrir d√≥nde hay disparidades en el rendimiento del modelo a trav√©s de cohortes de datos.
* **An√°lisis de datos**. Para entender la distribuci√≥n de datos e identificar cualquier posible sesgo en los datos que podr√≠a llevar a problemas de equidad, inclusi√≥n y fiabilidad.
* **Interpretabilidad del modelo**. Para entender qu√© afecta o influye en las predicciones del modelo. Esto ayuda a explicar el comportamiento del modelo, lo cual es importante para la transparencia y la responsabilidad.

## üöÄ Desaf√≠o

Para prevenir da√±os desde el principio, debemos:

- tener una diversidad de antecedentes y perspectivas entre las personas que trabajan en los sistemas
- invertir en conjuntos de datos que reflejen la diversidad de nuestra sociedad
- desarrollar mejores m√©todos a lo largo del ciclo de vida del aprendizaje autom√°tico para detectar y corregir la IA responsable cuando ocurra

Piensa en escenarios de la vida real donde la falta de confiabilidad de un modelo es evidente en la construcci√≥n y uso del modelo. ¬øQu√© m√°s deber√≠amos considerar?

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revisi√≥n y autoestudio

En esta lecci√≥n, has aprendido algunos conceptos b√°sicos sobre la equidad y la injusticia en el aprendizaje autom√°tico.

Mira este taller para profundizar en los temas:

- En busca de IA responsable: Llevando los principios a la pr√°ctica por Besmira Nushi, Mehrnoosh Sameki y Amit Sharma

[![RAI Toolbox: Un marco de c√≥digo abierto para construir IA responsable](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Un marco de c√≥digo abierto para construir IA responsable")

> üé• Haz clic en la imagen de arriba para ver un video: RAI Toolbox: Un marco de c√≥digo abierto para construir IA responsable por Besmira Nushi, Mehrnoosh Sameki y Amit Sharma

Tambi√©n lee:

- Centro de recursos de RAI de Microsoft: [Recursos de IA Responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de investigaci√≥n FATE de Microsoft: [FATE: Equidad, Responsabilidad, Transparencia y √âtica en IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Repositorio de GitHub de Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Lee sobre las herramientas de Azure Machine Learning para garantizar la equidad:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Tarea

[Explora RAI Toolbox](assignment.md)

**Descargo de responsabilidad**:
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en inteligencia artificial. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n profesional humana. No somos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.