<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8f819813b2ca08ec7b9f60a2c9336045",
  "translation_date": "2025-09-03T23:28:50+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "es"
}
-->
# Construyendo soluciones de aprendizaje autom√°tico con IA responsable

![Resumen de IA responsable en aprendizaje autom√°tico en un sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.es.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introducci√≥n

En este curr√≠culo, comenzar√°s a descubrir c√≥mo el aprendizaje autom√°tico puede y est√° impactando nuestras vidas cotidianas. Incluso ahora, los sistemas y modelos est√°n involucrados en tareas de toma de decisiones diarias, como diagn√≥sticos m√©dicos, aprobaciones de pr√©stamos o detecci√≥n de fraudes. Por lo tanto, es importante que estos modelos funcionen bien para proporcionar resultados confiables. Al igual que cualquier aplicaci√≥n de software, los sistemas de IA pueden no cumplir con las expectativas o tener un resultado no deseado. Es por eso que es esencial poder entender y explicar el comportamiento de un modelo de IA.

Imagina lo que puede suceder cuando los datos que utilizas para construir estos modelos carecen de ciertos datos demogr√°ficos, como raza, g√©nero, visi√≥n pol√≠tica, religi√≥n, o representan desproporcionadamente dichos datos demogr√°ficos. ¬øQu√© pasa cuando la salida del modelo se interpreta para favorecer a alg√∫n grupo demogr√°fico? ¬øCu√°l es la consecuencia para la aplicaci√≥n? Adem√°s, ¬øqu√© sucede cuando el modelo tiene un resultado adverso y es perjudicial para las personas? ¬øQui√©n es responsable del comportamiento de los sistemas de IA? Estas son algunas preguntas que exploraremos en este curr√≠culo.

En esta lecci√≥n, aprender√°s a:

- Reconocer la importancia de la equidad en el aprendizaje autom√°tico y los da√±os relacionados con la falta de equidad.
- Familiarizarte con la pr√°ctica de explorar valores at√≠picos y escenarios inusuales para garantizar confiabilidad y seguridad.
- Comprender la necesidad de empoderar a todos mediante el dise√±o de sistemas inclusivos.
- Explorar lo vital que es proteger la privacidad y seguridad de los datos y las personas.
- Ver la importancia de tener un enfoque de caja de cristal para explicar el comportamiento de los modelos de IA.
- Ser consciente de c√≥mo la responsabilidad es esencial para generar confianza en los sistemas de IA.

## Prerrequisito

Como prerrequisito, toma el "Camino de Aprendizaje de Principios de IA Responsable" y mira el video a continuaci√≥n sobre el tema:

Aprende m√°s sobre IA Responsable siguiendo este [Camino de Aprendizaje](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Enfoque de Microsoft hacia la IA Responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Enfoque de Microsoft hacia la IA Responsable")

> üé• Haz clic en la imagen de arriba para ver un video: Enfoque de Microsoft hacia la IA Responsable

## Equidad

Los sistemas de IA deben tratar a todos de manera justa y evitar afectar a grupos similares de personas de diferentes maneras. Por ejemplo, cuando los sistemas de IA proporcionan orientaci√≥n sobre tratamientos m√©dicos, solicitudes de pr√©stamos o empleo, deben hacer las mismas recomendaciones a todos con s√≠ntomas similares, circunstancias financieras o calificaciones profesionales. Cada uno de nosotros, como humanos, lleva consigo sesgos heredados que afectan nuestras decisiones y acciones. Estos sesgos pueden ser evidentes en los datos que usamos para entrenar sistemas de IA. A veces, esta manipulaci√≥n ocurre de manera no intencional. A menudo es dif√≠cil saber conscientemente cu√°ndo est√°s introduciendo sesgos en los datos.

**‚ÄúInjusticia‚Äù** abarca impactos negativos, o ‚Äúda√±os‚Äù, para un grupo de personas, como aquellos definidos en t√©rminos de raza, g√©nero, edad o estado de discapacidad. Los principales da√±os relacionados con la equidad pueden clasificarse como:

- **Asignaci√≥n**, si un g√©nero o etnia, por ejemplo, es favorecido sobre otro.
- **Calidad del servicio**. Si entrenas los datos para un escenario espec√≠fico pero la realidad es mucho m√°s compleja, esto lleva a un servicio de bajo rendimiento. Por ejemplo, un dispensador de jab√≥n que no parece ser capaz de detectar personas con piel oscura. [Referencia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigraci√≥n**. Criticar y etiquetar injustamente algo o alguien. Por ejemplo, una tecnolog√≠a de etiquetado de im√°genes etiquet√≥ err√≥neamente im√°genes de personas de piel oscura como gorilas.
- **Sobre- o sub-representaci√≥n**. La idea de que un cierto grupo no se ve en una determinada profesi√≥n, y cualquier servicio o funci√≥n que siga promoviendo eso est√° contribuyendo al da√±o.
- **Estereotipos**. Asociar un grupo dado con atributos preasignados. Por ejemplo, un sistema de traducci√≥n entre ingl√©s y turco puede tener inexactitudes debido a palabras con asociaciones estereotipadas de g√©nero.

![traducci√≥n al turco](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.es.png)
> traducci√≥n al turco

![traducci√≥n de regreso al ingl√©s](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.es.png)
> traducci√≥n de regreso al ingl√©s

Al dise√±ar y probar sistemas de IA, debemos asegurarnos de que la IA sea justa y no est√© programada para tomar decisiones sesgadas o discriminatorias, las cuales tambi√©n est√°n prohibidas para los seres humanos. Garantizar la equidad en la IA y el aprendizaje autom√°tico sigue siendo un desaf√≠o sociot√©cnico complejo.

### Confiabilidad y seguridad

Para generar confianza, los sistemas de IA deben ser confiables, seguros y consistentes bajo condiciones normales e inesperadas. Es importante saber c√≥mo se comportar√°n los sistemas de IA en una variedad de situaciones, especialmente cuando son valores at√≠picos. Al construir soluciones de IA, se necesita un enfoque sustancial en c√≥mo manejar una amplia variedad de circunstancias que las soluciones de IA podr√≠an encontrar. Por ejemplo, un autom√≥vil aut√≥nomo debe priorizar la seguridad de las personas. Como resultado, la IA que impulsa el autom√≥vil necesita considerar todos los posibles escenarios que el autom√≥vil podr√≠a enfrentar, como la noche, tormentas el√©ctricas o ventiscas, ni√±os corriendo por la calle, mascotas, construcciones en la carretera, etc. Qu√© tan bien un sistema de IA puede manejar una amplia gama de condiciones de manera confiable y segura refleja el nivel de anticipaci√≥n que el cient√≠fico de datos o desarrollador de IA consider√≥ durante el dise√±o o prueba del sistema.

> [üé• Haz clic aqu√≠ para ver un video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusi√≥n

Los sistemas de IA deben dise√±arse para involucrar y empoderar a todos. Al dise√±ar e implementar sistemas de IA, los cient√≠ficos de datos y desarrolladores de IA identifican y abordan posibles barreras en el sistema que podr√≠an excluir a las personas de manera no intencional. Por ejemplo, hay 1,000 millones de personas con discapacidades en todo el mundo. Con el avance de la IA, pueden acceder a una amplia gama de informaci√≥n y oportunidades m√°s f√°cilmente en su vida diaria. Al abordar las barreras, se crean oportunidades para innovar y desarrollar productos de IA con mejores experiencias que beneficien a todos.

> [üé• Haz clic aqu√≠ para ver un video: inclusi√≥n en IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Seguridad y privacidad

Los sistemas de IA deben ser seguros y respetar la privacidad de las personas. Las personas tienen menos confianza en sistemas que ponen en riesgo su privacidad, informaci√≥n o vidas. Al entrenar modelos de aprendizaje autom√°tico, dependemos de los datos para producir los mejores resultados. Al hacerlo, se debe considerar el origen de los datos y su integridad. Por ejemplo, ¬ølos datos fueron enviados por el usuario o estaban disponibles p√∫blicamente? Luego, al trabajar con los datos, es crucial desarrollar sistemas de IA que puedan proteger informaci√≥n confidencial y resistir ataques. A medida que la IA se vuelve m√°s prevalente, proteger la privacidad y asegurar informaci√≥n personal y empresarial importante se est√° volviendo m√°s cr√≠tico y complejo. Los problemas de privacidad y seguridad de datos requieren especial atenci√≥n en la IA porque el acceso a los datos es esencial para que los sistemas de IA hagan predicciones y decisiones precisas e informadas sobre las personas.

> [üé• Haz clic aqu√≠ para ver un video: seguridad en IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como industria, hemos logrado avances significativos en privacidad y seguridad, impulsados significativamente por regulaciones como el GDPR (Reglamento General de Protecci√≥n de Datos).
- Sin embargo, con los sistemas de IA debemos reconocer la tensi√≥n entre la necesidad de m√°s datos personales para hacer los sistemas m√°s efectivos y la privacidad.
- Al igual que con el nacimiento de las computadoras conectadas a internet, tambi√©n estamos viendo un gran aumento en el n√∫mero de problemas de seguridad relacionados con la IA.
- Al mismo tiempo, hemos visto que la IA se utiliza para mejorar la seguridad. Por ejemplo, la mayor√≠a de los esc√°neres antivirus modernos est√°n impulsados por heur√≠sticas de IA.
- Necesitamos asegurarnos de que nuestros procesos de ciencia de datos se integren armoniosamente con las √∫ltimas pr√°cticas de privacidad y seguridad.

### Transparencia

Los sistemas de IA deben ser comprensibles. Una parte crucial de la transparencia es explicar el comportamiento de los sistemas de IA y sus componentes. Mejorar la comprensi√≥n de los sistemas de IA requiere que las partes interesadas comprendan c√≥mo y por qu√© funcionan para que puedan identificar posibles problemas de rendimiento, preocupaciones de seguridad y privacidad, sesgos, pr√°cticas excluyentes o resultados no deseados. Tambi√©n creemos que quienes usan sistemas de IA deben ser honestos y transparentes sobre cu√°ndo, por qu√© y c√≥mo eligen implementarlos, as√≠ como las limitaciones de los sistemas que utilizan. Por ejemplo, si un banco utiliza un sistema de IA para apoyar sus decisiones de pr√©stamos al consumidor, es importante examinar los resultados y entender qu√© datos influyen en las recomendaciones del sistema. Los gobiernos est√°n comenzando a regular la IA en diversas industrias, por lo que los cient√≠ficos de datos y las organizaciones deben explicar si un sistema de IA cumple con los requisitos regulatorios, especialmente cuando hay un resultado no deseado.

> [üé• Haz clic aqu√≠ para ver un video: transparencia en IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Debido a que los sistemas de IA son tan complejos, es dif√≠cil entender c√≥mo funcionan e interpretar los resultados.
- Esta falta de comprensi√≥n afecta la forma en que se gestionan, operacionalizan y documentan estos sistemas.
- M√°s importante a√∫n, esta falta de comprensi√≥n afecta las decisiones tomadas utilizando los resultados que producen estos sistemas.

### Responsabilidad

Las personas que dise√±an y despliegan sistemas de IA deben ser responsables de c√≥mo operan sus sistemas. La necesidad de responsabilidad es particularmente crucial con tecnolog√≠as de uso sensible como el reconocimiento facial. Recientemente, ha habido una creciente demanda de tecnolog√≠a de reconocimiento facial, especialmente por parte de organizaciones de aplicaci√≥n de la ley que ven el potencial de la tecnolog√≠a en usos como encontrar ni√±os desaparecidos. Sin embargo, estas tecnolog√≠as podr√≠an ser utilizadas por un gobierno para poner en riesgo las libertades fundamentales de sus ciudadanos al, por ejemplo, habilitar la vigilancia continua de individuos espec√≠ficos. Por lo tanto, los cient√≠ficos de datos y las organizaciones deben ser responsables de c√≥mo su sistema de IA impacta a las personas o la sociedad.

[![Investigador l√≠der en IA advierte sobre vigilancia masiva a trav√©s del reconocimiento facial](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.es.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Enfoque de Microsoft hacia la IA Responsable")

> üé• Haz clic en la imagen de arriba para ver un video: Advertencias sobre vigilancia masiva a trav√©s del reconocimiento facial

En √∫ltima instancia, una de las mayores preguntas para nuestra generaci√≥n, como la primera generaci√≥n que est√° llevando la IA a la sociedad, es c√≥mo garantizar que las computadoras sigan siendo responsables ante las personas y c√≥mo garantizar que las personas que dise√±an computadoras sean responsables ante todos los dem√°s.

## Evaluaci√≥n de impacto

Antes de entrenar un modelo de aprendizaje autom√°tico, es importante realizar una evaluaci√≥n de impacto para entender el prop√≥sito del sistema de IA; cu√°l es su uso previsto; d√≥nde se desplegar√°; y qui√©n interactuar√° con el sistema. Estas evaluaciones son √∫tiles para los revisores o evaluadores del sistema para saber qu√© factores considerar al identificar riesgos potenciales y consecuencias esperadas.

Las siguientes son √°reas de enfoque al realizar una evaluaci√≥n de impacto:

* **Impacto adverso en individuos**. Ser consciente de cualquier restricci√≥n o requisito, uso no compatible o cualquier limitaci√≥n conocida que obstaculice el rendimiento del sistema es vital para garantizar que el sistema no se utilice de manera que pueda causar da√±o a las personas.
* **Requisitos de datos**. Comprender c√≥mo y d√≥nde el sistema utilizar√° datos permite a los revisores explorar cualquier requisito de datos que debas tener en cuenta (por ejemplo, regulaciones de datos como GDPR o HIPAA). Adem√°s, examina si la fuente o cantidad de datos es suficiente para el entrenamiento.
* **Resumen del impacto**. Re√∫ne una lista de posibles da√±os que podr√≠an surgir del uso del sistema. A lo largo del ciclo de vida del aprendizaje autom√°tico, revisa si los problemas identificados se han mitigado o abordado.
* **Metas aplicables** para cada uno de los seis principios fundamentales. Eval√∫a si se cumplen las metas de cada principio y si hay alguna brecha.

## Depuraci√≥n con IA responsable

Al igual que depurar una aplicaci√≥n de software, depurar un sistema de IA es un proceso necesario para identificar y resolver problemas en el sistema. Hay muchos factores que pueden afectar que un modelo no funcione como se espera o de manera responsable. La mayor√≠a de las m√©tricas tradicionales de rendimiento de modelos son agregados cuantitativos del rendimiento de un modelo, lo cual no es suficiente para analizar c√≥mo un modelo viola los principios de IA responsable. Adem√°s, un modelo de aprendizaje autom√°tico es una caja negra que dificulta entender qu√© impulsa su resultado o proporcionar explicaciones cuando comete un error. M√°s adelante en este curso, aprenderemos c√≥mo usar el panel de IA Responsable para ayudar a depurar sistemas de IA. El panel proporciona una herramienta integral para que los cient√≠ficos de datos y desarrolladores de IA realicen:

* **An√°lisis de errores**. Para identificar la distribuci√≥n de errores del modelo que puede afectar la equidad o confiabilidad del sistema.
* **Visi√≥n general del modelo**. Para descubrir d√≥nde hay disparidades en el rendimiento del modelo entre cohortes de datos.
* **An√°lisis de datos**. Para entender la distribuci√≥n de datos e identificar cualquier posible sesgo en los datos que podr√≠a llevar a problemas de equidad, inclusi√≥n y confiabilidad.
* **Interpretabilidad del modelo**. Para entender qu√© afecta o influye en las predicciones del modelo. Esto ayuda a explicar el comportamiento del modelo, lo cual es importante para la transparencia y la responsabilidad.

## üöÄ Desaf√≠o

Para prevenir da√±os desde el principio, debemos:

- tener diversidad de antecedentes y perspectivas entre las personas que trabajan en los sistemas
- invertir en conjuntos de datos que reflejen la diversidad de nuestra sociedad
- desarrollar mejores m√©todos a lo largo del ciclo de vida del aprendizaje autom√°tico para detectar y corregir problemas de IA responsable cuando ocurran

Piensa en escenarios de la vida real donde la falta de confianza en un modelo sea evidente en su construcci√≥n y uso. ¬øQu√© m√°s deber√≠amos considerar?

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revisi√≥n y autoestudio
En esta lecci√≥n, has aprendido algunos conceptos b√°sicos sobre la equidad y la falta de equidad en el aprendizaje autom√°tico.  

Mira este taller para profundizar en los temas: 

- En busca de una IA responsable: Llevando los principios a la pr√°ctica por Besmira Nushi, Mehrnoosh Sameki y Amit Sharma

[![Responsible AI Toolbox: Un marco de c√≥digo abierto para construir IA responsable](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Un marco de c√≥digo abierto para construir IA responsable")


> üé• Haz clic en la imagen de arriba para ver el video: RAI Toolbox: Un marco de c√≥digo abierto para construir IA responsable por Besmira Nushi, Mehrnoosh Sameki y Amit Sharma

Adem√°s, lee: 

- Centro de recursos de IA responsable de Microsoft: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Grupo de investigaci√≥n FATE de Microsoft: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox: 

- [Repositorio de GitHub de Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Lee sobre las herramientas de Azure Machine Learning para garantizar la equidad:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## Tarea

[Explora RAI Toolbox](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.