<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6534e145d52a3890590d27be75386e5d",
  "translation_date": "2025-09-04T00:35:26+00:00",
  "source_file": "6-NLP/2-Tasks/README.md",
  "language_code": "es"
}
-->
# Tareas y t√©cnicas comunes de procesamiento de lenguaje natural

Para la mayor√≠a de las tareas de *procesamiento de lenguaje natural*, el texto que se va a procesar debe descomponerse, examinarse y los resultados almacenarse o cruzarse con reglas y conjuntos de datos. Estas tareas permiten al programador derivar el _significado_, la _intenci√≥n_ o solo la _frecuencia_ de t√©rminos y palabras en un texto.

## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/33/)

Descubramos t√©cnicas comunes utilizadas en el procesamiento de texto. Combinadas con aprendizaje autom√°tico, estas t√©cnicas te ayudan a analizar grandes cantidades de texto de manera eficiente. Sin embargo, antes de aplicar ML a estas tareas, entendamos los problemas que enfrenta un especialista en NLP.

## Tareas comunes en NLP

Existen diferentes formas de analizar un texto en el que est√°s trabajando. Hay tareas que puedes realizar y, a trav√©s de ellas, puedes comprender el texto y sacar conclusiones. Por lo general, llevas a cabo estas tareas en una secuencia.

### Tokenizaci√≥n

Probablemente lo primero que la mayor√≠a de los algoritmos de NLP tienen que hacer es dividir el texto en tokens o palabras. Aunque esto suena simple, tener en cuenta la puntuaci√≥n y los delimitadores de palabras y oraciones en diferentes idiomas puede complicarlo. Es posible que tengas que usar varios m√©todos para determinar las demarcaciones.

![tokenizaci√≥n](../../../../translated_images/tokenization.1641a160c66cd2d93d4524e8114e93158a9ce0eba3ecf117bae318e8a6ad3487.es.png)
> Tokenizando una oraci√≥n de **Orgullo y Prejuicio**. Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)

### Embeddings

[Word embeddings](https://wikipedia.org/wiki/Word_embedding) son una forma de convertir tus datos de texto en valores num√©ricos. Los embeddings se realizan de manera que las palabras con un significado similar o palabras que se usan juntas se agrupen.

![word embeddings](../../../../translated_images/embedding.2cf8953c4b3101d188c2f61a5de5b6f53caaa5ad4ed99236d42bc3b6bd6a1fe2.es.png)
> "Tengo el mayor respeto por tus nervios, son mis viejos amigos." - Word embeddings para una oraci√≥n en **Orgullo y Prejuicio**. Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)

‚úÖ Prueba [esta herramienta interesante](https://projector.tensorflow.org/) para experimentar con word embeddings. Al hacer clic en una palabra, se muestran grupos de palabras similares: 'juguete' se agrupa con 'disney', 'lego', 'playstation' y 'consola'.

### Parsing y etiquetado de partes del discurso

Cada palabra que ha sido tokenizada puede etiquetarse como una parte del discurso: un sustantivo, verbo o adjetivo. La oraci√≥n `el r√°pido zorro rojo salt√≥ sobre el perro marr√≥n perezoso` podr√≠a etiquetarse como POS con zorro = sustantivo, salt√≥ = verbo.

![parsing](../../../../translated_images/parse.d0c5bbe1106eae8fe7d60a183cd1736c8b6cec907f38000366535f84f3036101.es.png)

> Parseando una oraci√≥n de **Orgullo y Prejuicio**. Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)

El parsing consiste en reconocer qu√© palabras est√°n relacionadas entre s√≠ en una oraci√≥n; por ejemplo, `el r√°pido zorro rojo salt√≥` es una secuencia de adjetivo-sustantivo-verbo que est√° separada de la secuencia `perro marr√≥n perezoso`.

### Frecuencia de palabras y frases

Un procedimiento √∫til al analizar un gran cuerpo de texto es construir un diccionario de cada palabra o frase de inter√©s y cu√°ntas veces aparece. La frase `el r√°pido zorro rojo salt√≥ sobre el perro marr√≥n perezoso` tiene una frecuencia de palabras de 2 para "el".

Veamos un texto de ejemplo donde contamos la frecuencia de palabras. El poema Los Ganadores de Rudyard Kipling contiene el siguiente verso:

```output
What the moral? Who rides may read.
When the night is thick and the tracks are blind
A friend at a pinch is a friend, indeed,
But a fool to wait for the laggard behind.
Down to Gehenna or up to the Throne,
He travels the fastest who travels alone.
```

Como las frecuencias de frases pueden ser sensibles o insensibles a may√∫sculas seg√∫n se requiera, la frase `un amigo` tiene una frecuencia de 2, `el` tiene una frecuencia de 6 y `viajes` tiene una frecuencia de 2.

### N-grams

Un texto puede dividirse en secuencias de palabras de una longitud establecida: una sola palabra (unigramas), dos palabras (bigramas), tres palabras (trigramas) o cualquier n√∫mero de palabras (n-grams).

Por ejemplo, `el r√°pido zorro rojo salt√≥ sobre el perro marr√≥n perezoso` con un puntaje de n-gram de 2 produce los siguientes n-grams:

1. el r√°pido  
2. r√°pido zorro  
3. zorro rojo  
4. rojo salt√≥  
5. salt√≥ sobre  
6. sobre el  
7. el perro  
8. perro marr√≥n  
9. marr√≥n perezoso  

Podr√≠a ser m√°s f√°cil visualizarlo como una ventana deslizante sobre la oraci√≥n. Aqu√≠ est√° para n-grams de 3 palabras, el n-gram est√° en negrita en cada oraci√≥n:

1.   <u>**el r√°pido zorro**</u> rojo salt√≥ sobre el perro marr√≥n perezoso  
2.   el **<u>r√°pido zorro rojo</u>** salt√≥ sobre el perro marr√≥n perezoso  
3.   el r√°pido **<u>zorro rojo salt√≥</u>** sobre el perro marr√≥n perezoso  
4.   el r√°pido zorro **<u>rojo salt√≥ sobre</u>** el perro marr√≥n perezoso  
5.   el r√°pido zorro rojo **<u>salt√≥ sobre el</u>** perro marr√≥n perezoso  
6.   el r√°pido zorro rojo salt√≥ **<u>sobre el perro</u>** marr√≥n perezoso  
7.   el r√°pido zorro rojo salt√≥ sobre <u>**el perro marr√≥n**</u> perezoso  
8.   el r√°pido zorro rojo salt√≥ sobre el **<u>perro marr√≥n perezoso</u>**

![ventana deslizante de n-grams](../../../../6-NLP/2-Tasks/images/n-grams.gif)

> Valor de n-gram de 3: Infograf√≠a por [Jen Looper](https://twitter.com/jenlooper)

### Extracci√≥n de frases nominales

En la mayor√≠a de las oraciones, hay un sustantivo que es el sujeto u objeto de la oraci√≥n. En ingl√©s, a menudo se identifica porque tiene 'a', 'an' o 'the' antes de √©l. Identificar el sujeto u objeto de una oraci√≥n mediante la 'extracci√≥n de la frase nominal' es una tarea com√∫n en NLP al intentar comprender el significado de una oraci√≥n.

‚úÖ En la oraci√≥n "No puedo fijar la hora, ni el lugar, ni la mirada ni las palabras, que sentaron las bases. Hace demasiado tiempo. Estaba en medio antes de darme cuenta de que hab√≠a comenzado.", ¬øpuedes identificar las frases nominales?

En la oraci√≥n `el r√°pido zorro rojo salt√≥ sobre el perro marr√≥n perezoso` hay 2 frases nominales: **r√°pido zorro rojo** y **perro marr√≥n perezoso**.

### An√°lisis de sentimiento

Una oraci√≥n o texto puede analizarse para determinar su sentimiento, o cu√°n *positivo* o *negativo* es. El sentimiento se mide en *polaridad* y *objetividad/subjetividad*. La polaridad se mide de -1.0 a 1.0 (negativo a positivo) y de 0.0 a 1.0 (m√°s objetivo a m√°s subjetivo).

‚úÖ M√°s adelante aprender√°s que hay diferentes formas de determinar el sentimiento utilizando aprendizaje autom√°tico, pero una forma es tener una lista de palabras y frases que han sido categorizadas como positivas o negativas por un experto humano y aplicar ese modelo al texto para calcular un puntaje de polaridad. ¬øPuedes ver c√≥mo esto funcionar√≠a en algunas circunstancias y menos en otras?

### Inflexi√≥n

La inflexi√≥n te permite tomar una palabra y obtener el singular o plural de la misma.

### Lematizaci√≥n

Un *lema* es la ra√≠z o palabra principal de un conjunto de palabras; por ejemplo, *vol√≥*, *vuela*, *volando* tienen como lema el verbo *volar*.

Tambi√©n hay bases de datos √∫tiles disponibles para el investigador de NLP, en particular:

### WordNet

[WordNet](https://wordnet.princeton.edu/) es una base de datos de palabras, sin√≥nimos, ant√≥nimos y muchos otros detalles para cada palabra en muchos idiomas diferentes. Es incre√≠blemente √∫til al intentar construir traducciones, correctores ortogr√°ficos o herramientas de lenguaje de cualquier tipo.

## Bibliotecas de NLP

Afortunadamente, no tienes que construir todas estas t√©cnicas t√∫ mismo, ya que hay excelentes bibliotecas de Python disponibles que hacen que sea mucho m√°s accesible para desarrolladores que no est√°n especializados en procesamiento de lenguaje natural o aprendizaje autom√°tico. Las pr√≥ximas lecciones incluyen m√°s ejemplos de estas, pero aqu√≠ aprender√°s algunos ejemplos √∫tiles para ayudarte con la pr√≥xima tarea.

### Ejercicio - usando la biblioteca `TextBlob`

Usemos una biblioteca llamada TextBlob, ya que contiene APIs √∫tiles para abordar este tipo de tareas. TextBlob "se basa en los hombros gigantes de [NLTK](https://nltk.org) y [pattern](https://github.com/clips/pattern), y funciona bien con ambos." Tiene una cantidad considerable de ML integrado en su API.

> Nota: Una √∫til [Gu√≠a de inicio r√°pido](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) est√° disponible para TextBlob y se recomienda para desarrolladores experimentados en Python.

Al intentar identificar *frases nominales*, TextBlob ofrece varias opciones de extractores para encontrarlas.

1. Echa un vistazo a `ConllExtractor`.

    ```python
    from textblob import TextBlob
    from textblob.np_extractors import ConllExtractor
    # import and create a Conll extractor to use later 
    extractor = ConllExtractor()
    
    # later when you need a noun phrase extractor:
    user_input = input("> ")
    user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified
    np = user_input_blob.noun_phrases                                    
    ```

    > ¬øQu√© est√° pasando aqu√≠? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) es "Un extractor de frases nominales que utiliza chunk parsing entrenado con el corpus de entrenamiento ConLL-2000." ConLL-2000 se refiere a la Conferencia de Aprendizaje Computacional de Lenguaje Natural de 2000. Cada a√±o, la conferencia organizaba un taller para abordar un problema dif√≠cil de NLP, y en 2000 fue el chunking de frases nominales. Se entren√≥ un modelo en el Wall Street Journal, con "las secciones 15-18 como datos de entrenamiento (211727 tokens) y la secci√≥n 20 como datos de prueba (47377 tokens)". Puedes ver los procedimientos utilizados [aqu√≠](https://www.clips.uantwerpen.be/conll2000/chunking/) y los [resultados](https://ifarm.nl/erikt/research/np-chunking.html).

### Desaf√≠o - mejorando tu bot con NLP

En la lecci√≥n anterior construiste un bot de preguntas y respuestas muy simple. Ahora, har√°s que Marvin sea un poco m√°s emp√°tico analizando tu entrada para determinar el sentimiento y mostrando una respuesta que coincida con el sentimiento. Tambi√©n necesitar√°s identificar una `frase nominal` y preguntar sobre ella.

Tus pasos al construir un bot conversacional mejorado:

1. Imprime instrucciones aconsejando al usuario c√≥mo interactuar con el bot.  
2. Inicia un bucle:  
   1. Acepta la entrada del usuario.  
   2. Si el usuario ha pedido salir, entonces sal.  
   3. Procesa la entrada del usuario y determina una respuesta de sentimiento adecuada.  
   4. Si se detecta una frase nominal en el sentimiento, plural√≠zala y pide m√°s informaci√≥n sobre ese tema.  
   5. Imprime la respuesta.  
3. Vuelve al paso 2.  

Aqu√≠ est√° el fragmento de c√≥digo para determinar el sentimiento usando TextBlob. Nota que solo hay cuatro *gradientes* de respuesta de sentimiento (puedes tener m√°s si lo deseas):

```python
if user_input_blob.polarity <= -0.5:
  response = "Oh dear, that sounds bad. "
elif user_input_blob.polarity <= 0:
  response = "Hmm, that's not great. "
elif user_input_blob.polarity <= 0.5:
  response = "Well, that sounds positive. "
elif user_input_blob.polarity <= 1:
  response = "Wow, that sounds great. "
```

Aqu√≠ hay un ejemplo de salida para guiarte (la entrada del usuario est√° en las l√≠neas que comienzan con >):

```output
Hello, I am Marvin, the friendly robot.
You can end this conversation at any time by typing 'bye'
After typing each answer, press 'enter'
How are you today?
> I am ok
Well, that sounds positive. Can you tell me more?
> I went for a walk and saw a lovely cat
Well, that sounds positive. Can you tell me more about lovely cats?
> cats are the best. But I also have a cool dog
Wow, that sounds great. Can you tell me more about cool dogs?
> I have an old hounddog but he is sick
Hmm, that's not great. Can you tell me more about old hounddogs?
> bye
It was nice talking to you, goodbye!
```

Una posible soluci√≥n a la tarea est√° [aqu√≠](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/2-Tasks/solution/bot.py)

‚úÖ Verificaci√≥n de conocimiento

1. ¬øCrees que las respuestas emp√°ticas podr√≠an 'enga√±ar' a alguien para que piense que el bot realmente los entiende?  
2. ¬øHace que el bot sea m√°s 'cre√≠ble' identificar la frase nominal?  
3. ¬øPor qu√© ser√≠a √∫til extraer una 'frase nominal' de una oraci√≥n?  

---

Implementa el bot en la verificaci√≥n de conocimiento anterior y pru√©balo con un amigo. ¬øPuede enga√±arlos? ¬øPuedes hacer que tu bot sea m√°s 'cre√≠ble'?

## üöÄDesaf√≠o

Toma una tarea de la verificaci√≥n de conocimiento anterior e intenta implementarla. Prueba el bot con un amigo. ¬øPuede enga√±arlos? ¬øPuedes hacer que tu bot sea m√°s 'cre√≠ble'?

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/34/)

## Revisi√≥n y autoestudio

En las pr√≥ximas lecciones aprender√°s m√°s sobre an√°lisis de sentimiento. Investiga esta t√©cnica interesante en art√≠culos como estos en [KDNuggets](https://www.kdnuggets.com/tag/nlp)

## Tarea 

[Haz que un bot responda](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisi√≥n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.