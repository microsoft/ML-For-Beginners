# Posdata: Depuraci√≥n de Modelos en Aprendizaje Autom√°tico usando Componentes del Tablero de IA Responsable

## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introducci√≥n

El aprendizaje autom√°tico impacta nuestras vidas cotidianas. La IA est√° encontrando su camino en algunos de los sistemas m√°s importantes que nos afectan tanto a nivel individual como a nuestra sociedad, desde la atenci√≥n m√©dica, las finanzas, la educaci√≥n y el empleo. Por ejemplo, los sistemas y modelos est√°n involucrados en tareas diarias de toma de decisiones, como diagn√≥sticos m√©dicos o la detecci√≥n de fraudes. En consecuencia, los avances en IA junto con la adopci√≥n acelerada est√°n siendo recibidos con expectativas sociales en evoluci√≥n y una creciente regulaci√≥n en respuesta. Constantemente vemos √°reas donde los sistemas de IA siguen sin cumplir las expectativas; exponen nuevos desaf√≠os; y los gobiernos est√°n comenzando a regular las soluciones de IA. Por lo tanto, es importante que estos modelos sean analizados para proporcionar resultados justos, fiables, inclusivos, transparentes y responsables para todos.

En este curr√≠culo, veremos herramientas pr√°cticas que se pueden usar para evaluar si un modelo tiene problemas de IA responsable. Las t√©cnicas tradicionales de depuraci√≥n de aprendizaje autom√°tico tienden a basarse en c√°lculos cuantitativos como la precisi√≥n agregada o la p√©rdida de error promedio. Imagina lo que puede suceder cuando los datos que est√°s utilizando para construir estos modelos carecen de ciertos datos demogr√°ficos, como raza, g√©nero, visi√≥n pol√≠tica, religi√≥n, o representan desproporcionadamente estos datos demogr√°ficos. ¬øQu√© pasa cuando la salida del modelo se interpreta para favorecer a algunos demogr√°ficos? Esto puede introducir una representaci√≥n excesiva o insuficiente de estos grupos de caracter√≠sticas sensibles, resultando en problemas de equidad, inclusi√≥n o fiabilidad del modelo. Otro factor es que los modelos de aprendizaje autom√°tico se consideran cajas negras, lo que dificulta entender y explicar qu√© impulsa la predicci√≥n de un modelo. Todos estos son desaf√≠os que enfrentan los cient√≠ficos de datos y desarrolladores de IA cuando no tienen herramientas adecuadas para depurar y evaluar la equidad o confiabilidad de un modelo.

En esta lecci√≥n, aprender√°s sobre la depuraci√≥n de tus modelos usando:

- **An√°lisis de Errores**: identificar d√≥nde en la distribuci√≥n de tus datos el modelo tiene altas tasas de error.
- **Visi√≥n General del Modelo**: realizar un an√°lisis comparativo entre diferentes cohortes de datos para descubrir disparidades en las m√©tricas de rendimiento de tu modelo.
- **An√°lisis de Datos**: investigar d√≥nde podr√≠a haber una representaci√≥n excesiva o insuficiente de tus datos que puedan sesgar tu modelo para favorecer a un demogr√°fico de datos sobre otro.
- **Importancia de Caracter√≠sticas**: entender qu√© caracter√≠sticas est√°n impulsando las predicciones de tu modelo a nivel global o local.

## Requisito previo

Como requisito previo, revisa [Herramientas de IA Responsable para desarrolladores](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif sobre Herramientas de IA Responsable](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## An√°lisis de Errores

Las m√©tricas tradicionales de rendimiento del modelo utilizadas para medir la precisi√≥n son principalmente c√°lculos basados en predicciones correctas vs incorrectas. Por ejemplo, determinar que un modelo es preciso el 89% del tiempo con una p√©rdida de error de 0.001 puede considerarse un buen rendimiento. Los errores a menudo no se distribuyen uniformemente en tu conjunto de datos subyacente. Puedes obtener una puntuaci√≥n de precisi√≥n del modelo del 89%, pero descubrir que hay diferentes regiones de tus datos en las que el modelo falla el 42% del tiempo. La consecuencia de estos patrones de falla con ciertos grupos de datos puede llevar a problemas de equidad o fiabilidad. Es esencial entender las √°reas donde el modelo est√° funcionando bien o no. Las regiones de datos donde hay un alto n√∫mero de inexactitudes en tu modelo pueden resultar ser un demogr√°fico de datos importante.

![Analizar y depurar errores del modelo](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.es.png)

El componente de An√°lisis de Errores en el tablero de RAI ilustra c√≥mo la falla del modelo se distribuye a trav√©s de varios cohortes con una visualizaci√≥n en √°rbol. Esto es √∫til para identificar caracter√≠sticas o √°reas donde hay una alta tasa de error en tu conjunto de datos. Al ver de d√≥nde provienen la mayor√≠a de las inexactitudes del modelo, puedes comenzar a investigar la causa ra√≠z. Tambi√©n puedes crear cohortes de datos para realizar an√°lisis. Estos cohortes de datos ayudan en el proceso de depuraci√≥n para determinar por qu√© el rendimiento del modelo es bueno en un cohorte, pero err√≥neo en otro.

![An√°lisis de Errores](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.es.png)

Los indicadores visuales en el mapa de √°rbol ayudan a localizar las √°reas problem√°ticas m√°s r√°pidamente. Por ejemplo, cuanto m√°s oscuro es el tono de color rojo en un nodo del √°rbol, mayor es la tasa de error.

El mapa de calor es otra funcionalidad de visualizaci√≥n que los usuarios pueden usar para investigar la tasa de error usando una o dos caracter√≠sticas para encontrar un contribuyente a los errores del modelo en todo el conjunto de datos o cohortes.

![Mapa de calor de An√°lisis de Errores](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.es.png)

Usa el an√°lisis de errores cuando necesites:

* Obtener una comprensi√≥n profunda de c√≥mo se distribuyen las fallas del modelo en un conjunto de datos y a trav√©s de varias dimensiones de entrada y caracter√≠sticas.
* Desglosar las m√©tricas de rendimiento agregadas para descubrir autom√°ticamente cohortes err√≥neos que informen tus pasos de mitigaci√≥n espec√≠ficos.

## Visi√≥n General del Modelo

Evaluar el rendimiento de un modelo de aprendizaje autom√°tico requiere obtener una comprensi√≥n hol√≠stica de su comportamiento. Esto se puede lograr revisando m√°s de una m√©trica, como tasa de error, precisi√≥n, recall, precisi√≥n, o MAE (Error Absoluto Medio) para encontrar disparidades entre las m√©tricas de rendimiento. Una m√©trica de rendimiento puede verse excelente, pero las inexactitudes pueden exponerse en otra m√©trica. Adem√°s, comparar las m√©tricas por disparidades en todo el conjunto de datos o cohortes ayuda a arrojar luz sobre d√≥nde el modelo est√° funcionando bien o no. Esto es especialmente importante al ver el rendimiento del modelo entre caracter√≠sticas sensibles vs insensibles (por ejemplo, raza del paciente, g√©nero o edad) para descubrir posibles injusticias que el modelo pueda tener. Por ejemplo, descubrir que el modelo es m√°s err√≥neo en un cohorte que tiene caracter√≠sticas sensibles puede revelar posibles injusticias que el modelo pueda tener.

El componente de Visi√≥n General del Modelo del tablero de RAI ayuda no solo a analizar las m√©tricas de rendimiento de la representaci√≥n de datos en un cohorte, sino que da a los usuarios la capacidad de comparar el comportamiento del modelo entre diferentes cohortes.

![Cohortes de conjunto de datos - visi√≥n general del modelo en el tablero de RAI](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.es.png)

La funcionalidad de an√°lisis basada en caracter√≠sticas del componente permite a los usuarios reducir subgrupos de datos dentro de una caracter√≠stica particular para identificar anomal√≠as a nivel granular. Por ejemplo, el tablero tiene inteligencia incorporada para generar autom√°ticamente cohortes para una caracter√≠stica seleccionada por el usuario (por ejemplo, *"time_in_hospital < 3"* o *"time_in_hospital >= 7"*). Esto permite a un usuario aislar una caracter√≠stica particular de un grupo de datos m√°s grande para ver si es un influenciador clave de los resultados err√≥neos del modelo.

![Cohortes de caracter√≠sticas - visi√≥n general del modelo en el tablero de RAI](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.es.png)

El componente de Visi√≥n General del Modelo soporta dos clases de m√©tricas de disparidad:

**Disparidad en el rendimiento del modelo**: Estos conjuntos de m√©tricas calculan la disparidad (diferencia) en los valores de la m√©trica de rendimiento seleccionada entre subgrupos de datos. Aqu√≠ hay algunos ejemplos:

* Disparidad en la tasa de precisi√≥n
* Disparidad en la tasa de error
* Disparidad en la precisi√≥n
* Disparidad en el recall
* Disparidad en el error absoluto medio (MAE)

**Disparidad en la tasa de selecci√≥n**: Esta m√©trica contiene la diferencia en la tasa de selecci√≥n (predicci√≥n favorable) entre subgrupos. Un ejemplo de esto es la disparidad en las tasas de aprobaci√≥n de pr√©stamos. La tasa de selecci√≥n significa la fracci√≥n de puntos de datos en cada clase clasificados como 1 (en clasificaci√≥n binaria) o la distribuci√≥n de valores de predicci√≥n (en regresi√≥n).

## An√°lisis de Datos

> "Si torturas los datos lo suficiente, confesar√°n cualquier cosa" - Ronald Coase

Esta declaraci√≥n suena extrema, pero es cierto que los datos pueden ser manipulados para apoyar cualquier conclusi√≥n. Tal manipulaci√≥n a veces puede ocurrir de manera no intencional. Como humanos, todos tenemos sesgos, y a menudo es dif√≠cil saber conscientemente cu√°ndo est√°s introduciendo sesgo en los datos. Garantizar la equidad en la IA y el aprendizaje autom√°tico sigue siendo un desaf√≠o complejo.

Los datos son un gran punto ciego para las m√©tricas tradicionales de rendimiento del modelo. Puedes tener altas puntuaciones de precisi√≥n, pero esto no siempre refleja el sesgo subyacente en tus datos. Por ejemplo, si un conjunto de datos de empleados tiene el 27% de mujeres en puestos ejecutivos en una empresa y el 73% de hombres en el mismo nivel, un modelo de publicidad de empleo de IA entrenado en estos datos puede dirigirse principalmente a una audiencia masculina para puestos de trabajo de alto nivel. Tener este desequilibrio en los datos sesg√≥ la predicci√≥n del modelo para favorecer a un g√©nero. Esto revela un problema de equidad donde hay un sesgo de g√©nero en el modelo de IA.

El componente de An√°lisis de Datos en el tablero de RAI ayuda a identificar √°reas donde hay una sobre- y sub-representaci√≥n en el conjunto de datos. Ayuda a los usuarios a diagnosticar la causa ra√≠z de errores y problemas de equidad introducidos por desequilibrios de datos o falta de representaci√≥n de un grupo de datos particular. Esto da a los usuarios la capacidad de visualizar conjuntos de datos basados en resultados predichos y reales, grupos de errores y caracter√≠sticas espec√≠ficas. A veces, descubrir un grupo de datos subrepresentado tambi√©n puede revelar que el modelo no est√° aprendiendo bien, de ah√≠ las altas inexactitudes. Tener un modelo con sesgo de datos no es solo un problema de equidad, sino que muestra que el modelo no es inclusivo ni confiable.

![Componente de An√°lisis de Datos en el Tablero de RAI](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.es.png)

Usa el an√°lisis de datos cuando necesites:

* Explorar las estad√≠sticas de tu conjunto de datos seleccionando diferentes filtros para dividir tus datos en diferentes dimensiones (tambi√©n conocidas como cohortes).
* Entender la distribuci√≥n de tu conjunto de datos a trav√©s de diferentes cohortes y grupos de caracter√≠sticas.
* Determinar si tus hallazgos relacionados con la equidad, el an√°lisis de errores y la causalidad (derivados de otros componentes del tablero) son el resultado de la distribuci√≥n de tu conjunto de datos.
* Decidir en qu√© √°reas recolectar m√°s datos para mitigar errores que provienen de problemas de representaci√≥n, ruido de etiquetas, ruido de caracter√≠sticas, sesgo de etiquetas y factores similares.

## Interpretabilidad del Modelo

Los modelos de aprendizaje autom√°tico tienden a ser cajas negras. Entender qu√© caracter√≠sticas clave de los datos impulsan la predicci√≥n de un modelo puede ser un desaf√≠o. Es importante proporcionar transparencia sobre por qu√© un modelo hace una cierta predicci√≥n. Por ejemplo, si un sistema de IA predice que un paciente diab√©tico est√° en riesgo de ser readmitido en un hospital en menos de 30 d√≠as, deber√≠a poder proporcionar datos de apoyo que llevaron a su predicci√≥n. Tener indicadores de datos de apoyo aporta transparencia para ayudar a los cl√≠nicos u hospitales a tomar decisiones bien informadas. Adem√°s, poder explicar por qu√© un modelo hizo una predicci√≥n para un paciente individual permite la responsabilidad con las regulaciones de salud. Cuando est√°s utilizando modelos de aprendizaje autom√°tico de maneras que afectan la vida de las personas, es crucial entender y explicar qu√© influye en el comportamiento de un modelo. La explicabilidad e interpretabilidad del modelo ayuda a responder preguntas en escenarios como:

* Depuraci√≥n del modelo: ¬øPor qu√© mi modelo cometi√≥ este error? ¬øC√≥mo puedo mejorar mi modelo?
* Colaboraci√≥n humano-IA: ¬øC√≥mo puedo entender y confiar en las decisiones del modelo?
* Cumplimiento regulatorio: ¬øCumple mi modelo con los requisitos legales?

El componente de Importancia de Caracter√≠sticas del tablero de RAI te ayuda a depurar y obtener una comprensi√≥n integral de c√≥mo un modelo hace predicciones. Tambi√©n es una herramienta √∫til para profesionales de aprendizaje autom√°tico y tomadores de decisiones para explicar y mostrar evidencia de caracter√≠sticas que influyen en el comportamiento de un modelo para el cumplimiento regulatorio. A continuaci√≥n, los usuarios pueden explorar tanto explicaciones globales como locales para validar qu√© caracter√≠sticas impulsan la predicci√≥n de un modelo. Las explicaciones globales enumeran las principales caracter√≠sticas que afectaron la predicci√≥n general de un modelo. Las explicaciones locales muestran qu√© caracter√≠sticas llevaron a la predicci√≥n de un modelo para un caso individual. La capacidad de evaluar explicaciones locales tambi√©n es √∫til para depurar o auditar un caso espec√≠fico para comprender mejor y interpretar por qu√© un modelo hizo una predicci√≥n precisa o inexacta.

![Componente de Importancia de Caracter√≠sticas del tablero de RAI](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.es.png)

* Explicaciones globales: Por ejemplo, ¬øqu√© caracter√≠sticas afectan el comportamiento general de un modelo de readmisi√≥n hospitalaria para diabetes?
* Explicaciones locales: Por ejemplo, ¬øpor qu√© se predijo que un paciente diab√©tico mayor de 60 a√±os con hospitalizaciones previas ser√≠a readmitido o no readmitido dentro de los 30 d√≠as en un hospital?

En el proceso de depuraci√≥n de examinar el rendimiento de un modelo a trav√©s de diferentes cohortes, la Importancia de Caracter√≠sticas muestra qu√© nivel de impacto tiene una caracter√≠stica a trav√©s de los cohortes. Ayuda a revelar anomal√≠as al comparar el nivel de influencia que tiene la caracter√≠stica en impulsar las predicciones err√≥neas de un modelo. El componente de Importancia de Caracter√≠sticas puede mostrar qu√© valores en una caracter√≠stica influyeron positiva o negativamente en el resultado del modelo. Por ejemplo, si un modelo hizo una predicci√≥n inexacta, el componente te da la capacidad de profundizar y se√±alar qu√© caracter√≠sticas o valores de caracter√≠sticas impulsaron la predicci√≥n. Este nivel de detalle ayuda no solo en la depuraci√≥n, sino que proporciona transparencia y responsabilidad en situaciones de auditor√≠a. Finalmente, el componente puede ayudarte a identificar problemas de equidad. Para ilustrar, si una caracter√≠stica sensible como la etnia o el g√©nero es altamente influyente en impulsar la predicci√≥n de un modelo, esto podr√≠a ser un indicio de sesgo de raza o g√©nero en el modelo.

![Importancia de caracter√≠sticas](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.es.png)

Usa la interpretabilidad cuando necesites:

* Determinar cu√°n confiables son las predicciones de tu sistema de IA al entender qu√© caracter√≠sticas son m√°s importantes para las predicciones.
* Abordar la depuraci√≥n de tu modelo entendi√©ndolo primero e identificando si el modelo est√° utilizando caracter√≠sticas saludables o meramente correlaciones falsas.
* Descubrir posibles fuentes de injusticia al entender si el modelo est√° basando predicciones en caracter√≠sticas sensibles o en caracter√≠sticas que est√°n altamente correlacionadas con ellas.
* Construir confianza en las decisiones de tu modelo generando explicaciones locales para ilustrar sus resultados.
* Completar una auditor√≠a regulatoria de un sistema de IA para validar modelos y monitorear el impacto de las decisiones del modelo en los humanos.

## Conclusi√≥n

Todos los componentes del tablero de RAI son herramientas pr√°cticas para ayudarte a construir modelos de aprendizaje autom√°tico que sean menos da√±inos y m√°s confiables para la sociedad. Mejora la prevenci√≥n de amenazas a los derechos humanos; discriminar o excluir a ciertos grupos de oportunidades de vida; y el riesgo de da√±o f√≠sico o psicol√≥gico. Tambi√©n ayuda a construir confianza en las decisiones de tu modelo generando explicaciones locales para ilustrar sus resultados. Algunos de los posibles da√±os se pueden clasificar como:

- **Asignaci√≥n**, si un g√©nero o etnia, por ejemplo, es favorecido sobre otro.
- **Calidad del servicio**. Si entrenas los datos para un escenario espec√≠fico pero la realidad es mucho m√°s compleja, lleva a un servicio de bajo rendimiento.
- **Estereotipado**. Asociar a un grupo dado con atributos preasignados.
- **Denigraci√≥n**. Criticar injustamente y etiquetar algo o a alguien.
- **Sobre- o sub- representaci√≥n**. La idea es que un cierto grupo no se ve en una cierta profesi√≥n, y cualquier servicio o funci√≥n que siga promoviendo eso est√° contribuyendo al da√±o.

### Tablero de RAI de Azure

[El tablero de RAI de Azure](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) est√° construido sobre herramientas de c√≥digo abierto desarrolladas por las principales instituciones acad√©micas y organizaciones, incluidas Microsoft, que son instrumentales para que los cient√≠ficos de datos y desarrolladores de IA comprendan mejor el comportamiento del modelo, descubran y mitiguen problemas indeseables de los modelos de IA.

- Aprende a usar los diferentes componentes consultando la [documentaci√≥n del tablero de RAI.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Consulta algunos [cuadernos de muestra del tablero de RAI](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) para depurar m√°s escenarios de IA responsable en Azure Machine Learning.

---
## üöÄ Desaf√≠o

Para evitar que se introduzcan sesgos estad√≠sticos o de datos en primer lugar, debemos:

- tener una diversidad de antecedentes y perspectivas entre las personas que trabajan en los sistemas
- invertir en conjuntos de datos que reflejen la diversidad de nuestra sociedad
- desarrollar mejores m√©todos para detectar y corregir el sesgo cuando ocurra

Piensa en escenarios de la vida real donde la injusticia es evidente en la construcci√≥n y uso de modelos. ¬øQu√© m√°s deber√≠amos considerar?

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revisi√≥n y Autoestudio

En esta lecci√≥n, has aprendido algunas de las herramientas pr√°cticas para incorporar IA responsable en el aprendizaje autom√°tico.

Mira este taller para profundizar en los temas:

- Tablero de IA Responsable: Un centro integral para operacionalizar la RAI en la pr√°ctica por Besmira Nushi y Mehrnoosh Sameki

[![Tablero de IA Responsable: Un centro integral para operacionalizar la RAI en la pr√°ctica](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Tablero de IA Responsable: Un centro integral para operacionalizar la RAI en la pr√°ctica")

> üé• Haz clic en la imagen de arriba para ver un video: Tablero de IA Responsable: Un centro integral para operacionalizar la RAI en la pr√°ctica por Bes

        **Descargo de responsabilidad**:
        Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en IA. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda la traducci√≥n profesional humana. No nos hacemos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.