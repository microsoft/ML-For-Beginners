<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-09-03T23:22:40+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "es"
}
-->
# Posdata: Depuraci√≥n de modelos de aprendizaje autom√°tico utilizando componentes del panel de IA Responsable

## [Cuestionario previo a la clase](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introducci√≥n

El aprendizaje autom√°tico impacta nuestra vida cotidiana. La IA est√° encontrando su lugar en algunos de los sistemas m√°s importantes que nos afectan como individuos y como sociedad, desde la atenci√≥n m√©dica, las finanzas, la educaci√≥n y el empleo. Por ejemplo, los sistemas y modelos est√°n involucrados en tareas de toma de decisiones diarias, como diagn√≥sticos m√©dicos o detecci√≥n de fraudes. En consecuencia, los avances en IA junto con su adopci√≥n acelerada est√°n siendo recibidos con expectativas sociales en evoluci√≥n y una creciente regulaci√≥n en respuesta. Constantemente vemos √°reas donde los sistemas de IA no cumplen con las expectativas; exponen nuevos desaf√≠os; y los gobiernos est√°n comenzando a regular las soluciones de IA. Por lo tanto, es importante que estos modelos sean analizados para proporcionar resultados justos, confiables, inclusivos, transparentes y responsables para todos.

En este curr√≠culo, exploraremos herramientas pr√°cticas que pueden ser utilizadas para evaluar si un modelo tiene problemas relacionados con IA Responsable. Las t√©cnicas tradicionales de depuraci√≥n de aprendizaje autom√°tico tienden a basarse en c√°lculos cuantitativos como la precisi√≥n agregada o la p√©rdida promedio de error. Imagina lo que puede suceder cuando los datos que est√°s utilizando para construir estos modelos carecen de ciertos grupos demogr√°ficos, como raza, g√©nero, visi√≥n pol√≠tica, religi√≥n, o representan desproporcionadamente dichos grupos demogr√°ficos. ¬øQu√© sucede cuando la salida del modelo se interpreta como favorable para algunos grupos demogr√°ficos? Esto puede introducir una representaci√≥n excesiva o insuficiente de estos grupos sensibles, resultando en problemas de equidad, inclusi√≥n o confiabilidad del modelo. Otro factor es que los modelos de aprendizaje autom√°tico son considerados cajas negras, lo que dificulta entender y explicar qu√© impulsa las predicciones de un modelo. Todos estos son desaf√≠os que enfrentan los cient√≠ficos de datos y desarrolladores de IA cuando no tienen herramientas adecuadas para depurar y evaluar la equidad o confiabilidad de un modelo.

En esta lecci√≥n, aprender√°s a depurar tus modelos utilizando:

- **An√°lisis de errores**: identificar d√≥nde en la distribuci√≥n de tus datos el modelo tiene altas tasas de error.
- **Visi√≥n general del modelo**: realizar an√°lisis comparativos entre diferentes cohortes de datos para descubrir disparidades en las m√©tricas de rendimiento de tu modelo.
- **An√°lisis de datos**: investigar d√≥nde podr√≠a haber una representaci√≥n excesiva o insuficiente de tus datos que pueda sesgar tu modelo para favorecer un grupo demogr√°fico sobre otro.
- **Importancia de las caracter√≠sticas**: entender qu√© caracter√≠sticas est√°n impulsando las predicciones de tu modelo a nivel global o local.

## Prerrequisito

Como prerrequisito, revisa [Herramientas de IA Responsable para desarrolladores](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif sobre herramientas de IA Responsable](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## An√°lisis de errores

Las m√©tricas tradicionales de rendimiento de modelos utilizadas para medir la precisi√≥n son principalmente c√°lculos basados en predicciones correctas frente a incorrectas. Por ejemplo, determinar que un modelo es preciso el 89% del tiempo con una p√©rdida de error de 0.001 puede considerarse un buen rendimiento. Los errores a menudo no se distribuyen uniformemente en tu conjunto de datos subyacente. Puedes obtener una puntuaci√≥n de precisi√≥n del modelo del 89%, pero descubrir que hay diferentes regiones de tus datos en las que el modelo falla el 42% del tiempo. Las consecuencias de estos patrones de falla con ciertos grupos de datos pueden llevar a problemas de equidad o confiabilidad. Es esencial entender las √°reas donde el modelo est√° funcionando bien o no. Las regiones de datos donde hay un alto n√∫mero de inexactitudes en tu modelo pueden resultar ser un grupo demogr√°fico importante.

![Analizar y depurar errores del modelo](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.es.png)

El componente de An√°lisis de Errores en el panel de IA Responsable ilustra c√≥mo se distribuyen las fallas del modelo entre varios cohortes con una visualizaci√≥n en forma de √°rbol. Esto es √∫til para identificar caracter√≠sticas o √°reas donde hay una alta tasa de error en tu conjunto de datos. Al ver de d√≥nde provienen la mayor√≠a de las inexactitudes del modelo, puedes comenzar a investigar la causa ra√≠z. Tambi√©n puedes crear cohortes de datos para realizar an√°lisis. Estos cohortes de datos ayudan en el proceso de depuraci√≥n para determinar por qu√© el rendimiento del modelo es bueno en un cohorte, pero err√≥neo en otro.

![An√°lisis de errores](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.es.png)

Los indicadores visuales en el mapa del √°rbol ayudan a localizar las √°reas problem√°ticas m√°s r√°pidamente. Por ejemplo, cuanto m√°s oscuro sea el tono de rojo en un nodo del √°rbol, mayor ser√° la tasa de error.

El mapa de calor es otra funcionalidad de visualizaci√≥n que los usuarios pueden utilizar para investigar la tasa de error utilizando una o dos caracter√≠sticas para encontrar un contribuyente a los errores del modelo en todo el conjunto de datos o cohortes.

![Mapa de calor de an√°lisis de errores](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.es.png)

Utiliza el an√°lisis de errores cuando necesites:

* Obtener una comprensi√≥n profunda de c√≥mo se distribuyen las fallas del modelo en un conjunto de datos y en varias dimensiones de entrada y caracter√≠sticas.
* Desglosar las m√©tricas de rendimiento agregadas para descubrir autom√°ticamente cohortes err√≥neos que informen tus pasos de mitigaci√≥n espec√≠ficos.

## Visi√≥n general del modelo

Evaluar el rendimiento de un modelo de aprendizaje autom√°tico requiere obtener una comprensi√≥n hol√≠stica de su comportamiento. Esto se puede lograr revisando m√°s de una m√©trica como tasa de error, precisi√≥n, recall, precisi√≥n o MAE (Error Absoluto Medio) para encontrar disparidades entre las m√©tricas de rendimiento. Una m√©trica de rendimiento puede parecer excelente, pero las inexactitudes pueden exponerse en otra m√©trica. Adem√°s, comparar las m√©tricas para encontrar disparidades en todo el conjunto de datos o cohortes ayuda a arrojar luz sobre d√≥nde el modelo est√° funcionando bien o no. Esto es especialmente importante para observar el rendimiento del modelo entre caracter√≠sticas sensibles e insensibles (por ejemplo, raza, g√©nero o edad del paciente) para descubrir posibles problemas de equidad que pueda tener el modelo. Por ejemplo, descubrir que el modelo es m√°s err√≥neo en un cohorte que tiene caracter√≠sticas sensibles puede revelar posibles problemas de equidad.

El componente de Visi√≥n General del Modelo en el panel de IA Responsable ayuda no solo a analizar las m√©tricas de rendimiento de la representaci√≥n de datos en un cohorte, sino que tambi√©n brinda a los usuarios la capacidad de comparar el comportamiento del modelo entre diferentes cohortes.

![Cohortes de conjuntos de datos - visi√≥n general del modelo en el panel de IA Responsable](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.es.png)

La funcionalidad de an√°lisis basada en caracter√≠sticas del componente permite a los usuarios reducir subgrupos de datos dentro de una caracter√≠stica particular para identificar anomal√≠as a nivel granular. Por ejemplo, el panel tiene inteligencia incorporada para generar autom√°ticamente cohortes para una caracter√≠stica seleccionada por el usuario (por ejemplo, *"time_in_hospital < 3"* o *"time_in_hospital >= 7"*). Esto permite a un usuario aislar una caracter√≠stica particular de un grupo de datos m√°s grande para ver si es un factor clave en los resultados err√≥neos del modelo.

![Cohortes de caracter√≠sticas - visi√≥n general del modelo en el panel de IA Responsable](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.es.png)

El componente de Visi√≥n General del Modelo admite dos clases de m√©tricas de disparidad:

**Disparidad en el rendimiento del modelo**: Este conjunto de m√©tricas calcula la disparidad (diferencia) en los valores de la m√©trica de rendimiento seleccionada entre subgrupos de datos. Aqu√≠ hay algunos ejemplos:

* Disparidad en la tasa de precisi√≥n
* Disparidad en la tasa de error
* Disparidad en la precisi√≥n
* Disparidad en el recall
* Disparidad en el error absoluto medio (MAE)

**Disparidad en la tasa de selecci√≥n**: Esta m√©trica contiene la diferencia en la tasa de selecci√≥n (predicci√≥n favorable) entre subgrupos. Un ejemplo de esto es la disparidad en las tasas de aprobaci√≥n de pr√©stamos. La tasa de selecci√≥n significa la fracci√≥n de puntos de datos en cada clase clasificados como 1 (en clasificaci√≥n binaria) o la distribuci√≥n de valores de predicci√≥n (en regresi√≥n).

## An√°lisis de datos

> "Si torturas los datos lo suficiente, confesar√°n cualquier cosa" - Ronald Coase

Esta afirmaci√≥n suena extrema, pero es cierto que los datos pueden ser manipulados para respaldar cualquier conclusi√≥n. Tal manipulaci√≥n a veces puede ocurrir de manera no intencional. Como humanos, todos tenemos sesgos, y a menudo es dif√≠cil saber conscientemente cu√°ndo est√°s introduciendo sesgos en los datos. Garantizar la equidad en la IA y el aprendizaje autom√°tico sigue siendo un desaf√≠o complejo.

Los datos son un gran punto ciego para las m√©tricas tradicionales de rendimiento de modelos. Puedes tener puntuaciones de precisi√≥n altas, pero esto no siempre refleja el sesgo subyacente en los datos que podr√≠a estar en tu conjunto de datos. Por ejemplo, si un conjunto de datos de empleados tiene un 27% de mujeres en puestos ejecutivos en una empresa y un 73% de hombres en el mismo nivel, un modelo de IA de publicidad de empleo entrenado con estos datos puede dirigirse principalmente a una audiencia masculina para puestos de nivel superior. Tener este desequilibrio en los datos sesg√≥ la predicci√≥n del modelo para favorecer un g√©nero. Esto revela un problema de equidad donde hay un sesgo de g√©nero en el modelo de IA.

El componente de An√°lisis de Datos en el panel de IA Responsable ayuda a identificar √°reas donde hay una representaci√≥n excesiva o insuficiente en el conjunto de datos. Ayuda a los usuarios a diagnosticar la causa ra√≠z de errores y problemas de equidad introducidos por desequilibrios de datos o falta de representaci√≥n de un grupo de datos particular. Esto brinda a los usuarios la capacidad de visualizar conjuntos de datos basados en resultados predichos y reales, grupos de errores y caracter√≠sticas espec√≠ficas. A veces, descubrir un grupo de datos subrepresentado tambi√©n puede revelar que el modelo no est√° aprendiendo bien, de ah√≠ las altas inexactitudes. Tener un modelo con sesgo de datos no solo es un problema de equidad, sino que muestra que el modelo no es inclusivo ni confiable.

![Componente de an√°lisis de datos en el panel de IA Responsable](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.es.png)

Utiliza el an√°lisis de datos cuando necesites:

* Explorar las estad√≠sticas de tu conjunto de datos seleccionando diferentes filtros para dividir tus datos en diferentes dimensiones (tambi√©n conocidas como cohortes).
* Comprender la distribuci√≥n de tu conjunto de datos entre diferentes cohortes y grupos de caracter√≠sticas.
* Determinar si tus hallazgos relacionados con equidad, an√°lisis de errores y causalidad (derivados de otros componentes del panel) son resultado de la distribuci√≥n de tu conjunto de datos.
* Decidir en qu√© √°reas recolectar m√°s datos para mitigar errores que provienen de problemas de representaci√≥n, ruido en las etiquetas, ruido en las caracter√≠sticas, sesgo en las etiquetas y factores similares.

## Interpretabilidad del modelo

Los modelos de aprendizaje autom√°tico tienden a ser cajas negras. Entender qu√© caracter√≠sticas clave de los datos impulsan la predicci√≥n de un modelo puede ser un desaf√≠o. Es importante proporcionar transparencia sobre por qu√© un modelo hace una cierta predicci√≥n. Por ejemplo, si un sistema de IA predice que un paciente diab√©tico est√° en riesgo de ser readmitido en un hospital en menos de 30 d√≠as, deber√≠a poder proporcionar datos de apoyo que llevaron a su predicci√≥n. Tener indicadores de datos de apoyo aporta transparencia para ayudar a los m√©dicos o hospitales a tomar decisiones bien informadas. Adem√°s, poder explicar por qu√© un modelo hizo una predicci√≥n para un paciente individual permite responsabilidad con las regulaciones de salud. Cuando utilizas modelos de aprendizaje autom√°tico de maneras que afectan la vida de las personas, es crucial entender y explicar qu√© influye en el comportamiento de un modelo. La explicabilidad e interpretabilidad del modelo ayuda a responder preguntas en escenarios como:

* Depuraci√≥n del modelo: ¬øPor qu√© mi modelo cometi√≥ este error? ¬øC√≥mo puedo mejorar mi modelo?
* Colaboraci√≥n humano-IA: ¬øC√≥mo puedo entender y confiar en las decisiones del modelo?
* Cumplimiento regulatorio: ¬øCumple mi modelo con los requisitos legales?

El componente de Importancia de las Caracter√≠sticas del panel de IA Responsable te ayuda a depurar y obtener una comprensi√≥n integral de c√≥mo un modelo hace predicciones. Tambi√©n es una herramienta √∫til para profesionales de aprendizaje autom√°tico y tomadores de decisiones para explicar y mostrar evidencia de las caracter√≠sticas que influyen en el comportamiento de un modelo para el cumplimiento regulatorio. Los usuarios pueden explorar tanto explicaciones globales como locales para validar qu√© caracter√≠sticas impulsan la predicci√≥n de un modelo. Las explicaciones globales enumeran las principales caracter√≠sticas que afectaron la predicci√≥n general de un modelo. Las explicaciones locales muestran qu√© caracter√≠sticas llevaron a la predicci√≥n de un modelo para un caso individual. La capacidad de evaluar explicaciones locales tambi√©n es √∫til para depurar o auditar un caso espec√≠fico para comprender mejor y explicar por qu√© un modelo hizo una predicci√≥n precisa o inexacta.

![Componente de importancia de caracter√≠sticas del panel de IA Responsable](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.es.png)

* Explicaciones globales: Por ejemplo, ¬øqu√© caracter√≠sticas afectan el comportamiento general de un modelo de readmisi√≥n hospitalaria para pacientes diab√©ticos?
* Explicaciones locales: Por ejemplo, ¬øpor qu√© se predijo que un paciente diab√©tico mayor de 60 a√±os con hospitalizaciones previas ser√≠a readmitido o no readmitido dentro de los 30 d√≠as en un hospital?

En el proceso de depuraci√≥n al examinar el rendimiento de un modelo entre diferentes cohortes, la Importancia de las Caracter√≠sticas muestra qu√© nivel de impacto tiene una caracter√≠stica entre los cohortes. Ayuda a revelar anomal√≠as al comparar el nivel de influencia que tiene la caracter√≠stica en impulsar las predicciones err√≥neas de un modelo. El componente de Importancia de las Caracter√≠sticas puede mostrar qu√© valores en una caracter√≠stica influyeron positiva o negativamente en el resultado del modelo. Por ejemplo, si un modelo hizo una predicci√≥n inexacta, el componente te da la capacidad de profundizar y se√±alar qu√© caracter√≠sticas o valores de caracter√≠sticas impulsaron la predicci√≥n. Este nivel de detalle no solo ayuda en la depuraci√≥n, sino que proporciona transparencia y responsabilidad en situaciones de auditor√≠a. Finalmente, el componente puede ayudarte a identificar problemas de equidad. Para ilustrar, si una caracter√≠stica sensible como etnicidad o g√©nero tiene una alta influencia en impulsar la predicci√≥n de un modelo, esto podr√≠a ser un indicio de sesgo racial o de g√©nero en el modelo.

![Importancia de caracter√≠sticas](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.es.png)

Utiliza la interpretabilidad cuando necesites:

* Determinar cu√°n confiables son las predicciones de tu sistema de IA entendiendo qu√© caracter√≠sticas son m√°s importantes para las predicciones.
* Abordar la depuraci√≥n de tu modelo entendi√©ndolo primero e identificando si el modelo est√° utilizando caracter√≠sticas saludables o simplemente correlaciones falsas.
* Descubrir posibles fuentes de inequidad entendiendo si el modelo est√° basando sus predicciones en caracter√≠sticas sensibles o en caracter√≠sticas altamente correlacionadas con ellas.
* Generar confianza del usuario en las decisiones de tu modelo generando explicaciones locales para ilustrar sus resultados.
* Completar una auditor√≠a regulatoria de un sistema de IA para validar modelos y monitorear el impacto de las decisiones del modelo en las personas.

## Conclusi√≥n

Todos los componentes del panel de IA Responsable son herramientas pr√°cticas para ayudarte a construir modelos de aprendizaje autom√°tico que sean menos da√±inos y m√°s confiables para la sociedad. Mejoran la prevenci√≥n de amenazas a los derechos humanos; la discriminaci√≥n o exclusi√≥n de ciertos grupos de oportunidades de vida; y el riesgo de da√±o f√≠sico o psicol√≥gico. Tambi√©n ayudan a generar confianza en las decisiones de tu modelo al generar explicaciones locales para ilustrar sus resultados. Algunos de los posibles da√±os pueden clasificarse como:

- **Asignaci√≥n**, si un g√©nero o etnicidad, por ejemplo, es favorecido sobre otro.
- **Calidad del servicio**. Si entrenas los datos para un escenario espec√≠fico pero la realidad es mucho m√°s compleja, esto lleva a un servicio de bajo rendimiento.
- **Estereotipos**. Asociar un grupo dado con atributos preasignados.
- **Denigraci√≥n**. Criticar injustamente y etiquetar algo o alguien.
- **Representaci√≥n excesiva o insuficiente**. La idea es que un cierto grupo no se vea representado en una determinada profesi√≥n, y cualquier servicio o funci√≥n que siga promoviendo eso est√° contribuyendo al da√±o.

### Azure RAI dashboard

[Azure RAI dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) est√° construido sobre herramientas de c√≥digo abierto desarrolladas por instituciones acad√©micas y organizaciones l√≠deres, incluyendo Microsoft. Estas herramientas son fundamentales para que los cient√≠ficos de datos y desarrolladores de IA comprendan mejor el comportamiento de los modelos, descubran y mitiguen problemas indeseables en los modelos de IA.

- Aprende c√≥mo usar los diferentes componentes consultando la [documentaci√≥n del RAI dashboard.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Consulta algunos [notebooks de ejemplo del RAI dashboard](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) para depurar escenarios de IA m√°s responsables en Azure Machine Learning.

---
## üöÄ Desaf√≠o

Para evitar que se introduzcan sesgos estad√≠sticos o de datos desde el principio, deber√≠amos:

- contar con una diversidad de antecedentes y perspectivas entre las personas que trabajan en los sistemas
- invertir en conjuntos de datos que reflejen la diversidad de nuestra sociedad
- desarrollar mejores m√©todos para detectar y corregir sesgos cuando ocurran

Piensa en escenarios de la vida real donde la injusticia sea evidente en la construcci√≥n y uso de modelos. ¬øQu√© m√°s deber√≠amos considerar?

## [Cuestionario posterior a la clase](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revisi√≥n y Autoestudio

En esta lecci√≥n, has aprendido algunas herramientas pr√°cticas para incorporar IA responsable en el aprendizaje autom√°tico.

Mira este taller para profundizar en los temas:

- Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica por Besmira Nushi y Mehrnoosh Sameki

[![Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica")


> üé• Haz clic en la imagen de arriba para ver el video: Responsible AI Dashboard: Una soluci√≥n integral para operacionalizar la IA responsable en la pr√°ctica por Besmira Nushi y Mehrnoosh Sameki

Consulta los siguientes materiales para aprender m√°s sobre IA responsable y c√≥mo construir modelos m√°s confiables:

- Herramientas del RAI dashboard de Microsoft para depurar modelos de aprendizaje autom√°tico: [Recursos de herramientas de IA responsable](https://aka.ms/rai-dashboard)

- Explora el kit de herramientas de IA responsable: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Centro de recursos de IA responsable de Microsoft: [Recursos de IA Responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de investigaci√≥n FATE de Microsoft: [FATE: Equidad, Responsabilidad, Transparencia y √âtica en IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Tarea

[Explora el RAI dashboard](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.