<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "730225ea274c9174fe688b21d421539d",
  "translation_date": "2025-09-04T22:17:37+00:00",
  "source_file": "5-Clustering/1-Visualize/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n a la agrupaci√≥n

La agrupaci√≥n es un tipo de [aprendizaje no supervisado](https://wikipedia.org/wiki/Unsupervised_learning) que asume que un conjunto de datos no est√° etiquetado o que sus entradas no est√°n asociadas con salidas predefinidas. Utiliza varios algoritmos para clasificar datos no etiquetados y proporcionar agrupaciones seg√∫n los patrones que detecta en los datos.

[![No One Like You de PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM "No One Like You de PSquare")

> üé• Haz clic en la imagen de arriba para ver un video. Mientras estudias aprendizaje autom√°tico con agrupaci√≥n, disfruta de algunos temas de Dance Hall nigeriano: esta es una canci√≥n muy popular de 2014 de PSquare.

## [Cuestionario previo a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

### Introducci√≥n

[La agrupaci√≥n](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) es muy √∫til para la exploraci√≥n de datos. Veamos si puede ayudar a descubrir tendencias y patrones en la forma en que las audiencias nigerianas consumen m√∫sica.

‚úÖ T√≥mate un minuto para pensar en los usos de la agrupaci√≥n. En la vida real, la agrupaci√≥n ocurre cada vez que tienes un mont√≥n de ropa y necesitas clasificar la ropa de los miembros de tu familia üß¶üëïüëñü©≤. En ciencia de datos, la agrupaci√≥n ocurre al intentar analizar las preferencias de un usuario o determinar las caracter√≠sticas de cualquier conjunto de datos no etiquetado. La agrupaci√≥n, de alguna manera, ayuda a dar sentido al caos, como un caj√≥n de calcetines.

[![Introducci√≥n al aprendizaje autom√°tico](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds "Introducci√≥n a la agrupaci√≥n")

> üé• Haz clic en la imagen de arriba para ver un video: John Guttag del MIT introduce la agrupaci√≥n.

En un entorno profesional, la agrupaci√≥n puede usarse para determinar cosas como la segmentaci√≥n de mercado, identificando qu√© grupos de edad compran qu√© productos, por ejemplo. Otro uso ser√≠a la detecci√≥n de anomal√≠as, tal vez para identificar fraudes en un conjunto de datos de transacciones con tarjetas de cr√©dito. O podr√≠as usar la agrupaci√≥n para identificar tumores en un lote de escaneos m√©dicos.

‚úÖ Piensa un minuto en c√≥mo podr√≠as haber encontrado la agrupaci√≥n 'en la vida real', en un entorno bancario, de comercio electr√≥nico o empresarial.

> üéì Curiosamente, el an√°lisis de agrupaci√≥n se origin√≥ en los campos de la Antropolog√≠a y la Psicolog√≠a en la d√©cada de 1930. ¬øPuedes imaginar c√≥mo podr√≠a haberse utilizado?

Alternativamente, podr√≠as usarlo para agrupar resultados de b√∫squeda, como enlaces de compras, im√°genes o rese√±as, por ejemplo. La agrupaci√≥n es √∫til cuando tienes un conjunto de datos grande que deseas reducir y sobre el cual deseas realizar un an√°lisis m√°s detallado, por lo que la t√©cnica puede usarse para aprender sobre los datos antes de construir otros modelos.

‚úÖ Una vez que tus datos est√°n organizados en grupos, les asignas un Id de grupo, y esta t√©cnica puede ser √∫til para preservar la privacidad de un conjunto de datos; en lugar de referirte a un punto de datos por informaci√≥n identificable, puedes referirte a √©l por su Id de grupo. ¬øPuedes pensar en otras razones por las que preferir√≠as referirte a un Id de grupo en lugar de otros elementos del grupo para identificarlo?

Profundiza tu comprensi√≥n de las t√©cnicas de agrupaci√≥n en este [m√≥dulo de aprendizaje](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott).

## Introducci√≥n a la agrupaci√≥n

[Scikit-learn ofrece una amplia variedad](https://scikit-learn.org/stable/modules/clustering.html) de m√©todos para realizar agrupaci√≥n. El tipo que elijas depender√° de tu caso de uso. Seg√∫n la documentaci√≥n, cada m√©todo tiene varios beneficios. Aqu√≠ hay una tabla simplificada de los m√©todos compatibles con Scikit-learn y sus casos de uso apropiados:

| Nombre del m√©todo            | Caso de uso                                                            |
| :--------------------------- | :--------------------------------------------------------------------- |
| K-Means                      | prop√≥sito general, inductivo                                           |
| Affinity propagation         | muchos, grupos desiguales, inductivo                                  |
| Mean-shift                   | muchos, grupos desiguales, inductivo                                  |
| Spectral clustering          | pocos, grupos iguales, transductivo                                   |
| Ward hierarchical clustering | muchos, grupos restringidos, transductivo                             |
| Agglomerative clustering     | muchos, restringidos, distancias no euclidianas, transductivo         |
| DBSCAN                       | geometr√≠a no plana, grupos desiguales, transductivo                   |
| OPTICS                       | geometr√≠a no plana, grupos desiguales con densidad variable, transductivo |
| Gaussian mixtures            | geometr√≠a plana, inductivo                                            |
| BIRCH                        | conjunto de datos grande con valores at√≠picos, inductivo              |

> üéì C√≥mo creamos grupos tiene mucho que ver con c√≥mo agrupamos los puntos de datos. Desglos√©moslo:
>
> üéì ['Transductivo' vs. 'inductivo'](https://wikipedia.org/wiki/Transduction_(machine_learning))
> 
> La inferencia transductiva se deriva de casos de entrenamiento observados que se asignan a casos de prueba espec√≠ficos. La inferencia inductiva se deriva de casos de entrenamiento que se asignan a reglas generales que luego se aplican a casos de prueba.
> 
> Un ejemplo: Imagina que tienes un conjunto de datos que est√° solo parcialmente etiquetado. Algunas cosas son 'discos', otras 'CDs', y otras est√°n en blanco. Tu trabajo es proporcionar etiquetas para los elementos en blanco. Si eliges un enfoque inductivo, entrenar√≠as un modelo buscando 'discos' y 'CDs', y aplicar√≠as esas etiquetas a tus datos no etiquetados. Este enfoque tendr√° problemas para clasificar cosas que en realidad son 'cassettes'. Un enfoque transductivo, por otro lado, maneja estos datos desconocidos de manera m√°s efectiva al agrupar elementos similares y luego aplicar una etiqueta a un grupo. En este caso, los grupos podr√≠an reflejar 'cosas musicales redondas' y 'cosas musicales cuadradas'.
> 
> üéì ['Geometr√≠a no plana' vs. 'plana'](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)
> 
> Derivado de la terminolog√≠a matem√°tica, la geometr√≠a no plana vs. plana se refiere a la medida de distancias entre puntos mediante m√©todos geom√©tricos 'planos' ([Euclidianos](https://wikipedia.org/wiki/Euclidean_geometry)) o 'no planos' (no Euclidianos).
>
>'Plana' en este contexto se refiere a la geometr√≠a Euclidiana (partes de la cual se ense√±an como geometr√≠a 'plana'), y no plana se refiere a la geometr√≠a no Euclidiana. ¬øQu√© tiene que ver la geometr√≠a con el aprendizaje autom√°tico? Bueno, como dos campos que est√°n arraigados en las matem√°ticas, debe haber una forma com√∫n de medir distancias entre puntos en grupos, y eso puede hacerse de manera 'plana' o 'no plana', dependiendo de la naturaleza de los datos. [Las distancias Euclidianas](https://wikipedia.org/wiki/Euclidean_distance) se miden como la longitud de un segmento de l√≠nea entre dos puntos. [Las distancias no Euclidianas](https://wikipedia.org/wiki/Non-Euclidean_geometry) se miden a lo largo de una curva. Si tus datos, visualizados, parecen no existir en un plano, podr√≠as necesitar usar un algoritmo especializado para manejarlos.
>
![Infograf√≠a de geometr√≠a plana vs. no plana](../../../../5-Clustering/1-Visualize/images/flat-nonflat.png)
> Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)
> 
> üéì ['Distancias'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)
> 
> Los grupos se definen por su matriz de distancias, es decir, las distancias entre puntos. Esta distancia puede medirse de varias maneras. Los grupos Euclidianos se definen por el promedio de los valores de los puntos y contienen un 'centroide' o punto central. Las distancias se miden as√≠ por la distancia a ese centroide. Las distancias no Euclidianas se refieren a 'clustroides', el punto m√°s cercano a otros puntos. Los clustroides, a su vez, pueden definirse de varias maneras.
> 
> üéì ['Restringido'](https://wikipedia.org/wiki/Constrained_clustering)
> 
> [La agrupaci√≥n restringida](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduce el aprendizaje 'semi-supervisado' en este m√©todo no supervisado. Las relaciones entre puntos se marcan como 'no puede vincular' o 'debe vincular', por lo que se imponen algunas reglas al conjunto de datos.
>
>Un ejemplo: Si un algoritmo se deja libre en un lote de datos no etiquetados o semi-etiquetados, los grupos que produce pueden ser de baja calidad. En el ejemplo anterior, los grupos podr√≠an agrupar 'cosas musicales redondas', 'cosas musicales cuadradas', 'cosas triangulares' y 'galletas'. Si se le dan algunas restricciones o reglas a seguir ("el art√≠culo debe estar hecho de pl√°stico", "el art√≠culo necesita poder producir m√∫sica"), esto puede ayudar a 'restringir' el algoritmo para tomar mejores decisiones.
> 
> üéì 'Densidad'
> 
> Los datos que son 'ruidosos' se consideran 'densos'. Las distancias entre puntos en cada uno de sus grupos pueden resultar, al examinarlas, m√°s o menos densas, o 'congestionadas', y por lo tanto estos datos necesitan analizarse con el m√©todo de agrupaci√≥n apropiado. [Este art√≠culo](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) demuestra la diferencia entre usar agrupaci√≥n K-Means vs. algoritmos HDBSCAN para explorar un conjunto de datos ruidoso con densidad de grupo desigual.

## Algoritmos de agrupaci√≥n

Existen m√°s de 100 algoritmos de agrupaci√≥n, y su uso depende de la naturaleza de los datos en cuesti√≥n. Hablemos de algunos de los principales:

- **Agrupaci√≥n jer√°rquica**. Si un objeto se clasifica por su proximidad a un objeto cercano, en lugar de a uno m√°s lejano, los grupos se forman en funci√≥n de la distancia de sus miembros hacia y desde otros objetos. La agrupaci√≥n aglomerativa de Scikit-learn es jer√°rquica.

   ![Infograf√≠a de agrupaci√≥n jer√°rquica](../../../../5-Clustering/1-Visualize/images/hierarchical.png)
   > Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Agrupaci√≥n por centroides**. Este algoritmo popular requiere elegir 'k', o el n√∫mero de grupos a formar, despu√©s de lo cual el algoritmo determina el punto central de un grupo y re√∫ne datos alrededor de ese punto. [La agrupaci√≥n K-means](https://wikipedia.org/wiki/K-means_clustering) es una versi√≥n popular de la agrupaci√≥n por centroides. El centro se determina por la media m√°s cercana, de ah√≠ el nombre. La distancia cuadrada desde el grupo se minimiza.

   ![Infograf√≠a de agrupaci√≥n por centroides](../../../../5-Clustering/1-Visualize/images/centroid.png)
   > Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Agrupaci√≥n basada en distribuci√≥n**. Basada en modelos estad√≠sticos, la agrupaci√≥n basada en distribuci√≥n se centra en determinar la probabilidad de que un punto de datos pertenezca a un grupo y asignarlo en consecuencia. Los m√©todos de mezcla gaussiana pertenecen a este tipo.

- **Agrupaci√≥n basada en densidad**. Los puntos de datos se asignan a grupos seg√∫n su densidad, o su agrupaci√≥n entre s√≠. Los puntos de datos alejados del grupo se consideran valores at√≠picos o ruido. DBSCAN, Mean-shift y OPTICS pertenecen a este tipo de agrupaci√≥n.

- **Agrupaci√≥n basada en cuadr√≠cula**. Para conjuntos de datos multidimensionales, se crea una cuadr√≠cula y los datos se dividen entre las celdas de la cuadr√≠cula, creando as√≠ grupos.

## Ejercicio - agrupa tus datos

La t√©cnica de agrupaci√≥n se beneficia enormemente de una visualizaci√≥n adecuada, as√≠ que comencemos visualizando nuestros datos musicales. Este ejercicio nos ayudar√° a decidir cu√°l de los m√©todos de agrupaci√≥n deber√≠amos usar de manera m√°s efectiva para la naturaleza de estos datos.

1. Abre el archivo [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) en esta carpeta.

1. Importa el paquete `Seaborn` para una buena visualizaci√≥n de datos.

    ```python
    !pip install seaborn
    ```

1. Agrega los datos de canciones desde [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Carga un dataframe con algunos datos sobre las canciones. Prep√°rate para explorar estos datos importando las bibliotecas y mostrando los datos:

    ```python
    import matplotlib.pyplot as plt
    import pandas as pd
    
    df = pd.read_csv("../data/nigerian-songs.csv")
    df.head()
    ```

    Revisa las primeras l√≠neas de datos:

    |     | name                     | album                        | artist              | artist_top_genre | release_date | length | popularity | danceability | acousticness | energy | instrumentalness | liveness | loudness | speechiness | tempo   | time_signature |
    | --- | ------------------------ | ---------------------------- | ------------------- | ---------------- | ------------ | ------ | ---------- | ------------ | ------------ | ------ | ---------------- | -------- | -------- | ----------- | ------- | -------------- |
    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | alternative r&b  | 2019         | 144000 | 48         | 0.666        | 0.851        | 0.42   | 0.534            | 0.11     | -6.699   | 0.0829      | 133.015 | 5              |
    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop          | 2020         | 89488  | 30         | 0.71         | 0.0822       | 0.683  | 0.000169         | 0.101    | -5.64    | 0.36        | 129.993 | 3              |
| 2   | LITT!                    | LITT!                        | AYL√ò                | indie r&b        | 2018         | 207758 | 40         | 0.836        | 0.272        | 0.564  | 0.000537         | 0.11     | -7.127   | 0.0424      | 130.005 | 4              |
| 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop     | 2019         | 175135 | 14         | 0.894        | 0.798        | 0.611  | 0.000187         | 0.0964   | -4.961   | 0.113       | 111.087 | 4              |
| 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop          | 2018         | 152049 | 25         | 0.702        | 0.116        | 0.833  | 0.91             | 0.348    | -6.044   | 0.0447      | 105.115 | 4              |

1. Obt√©n informaci√≥n sobre el dataframe llamando a `info()`:

    ```python
    df.info()
    ```

   El resultado se ve as√≠:

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 530 entries, 0 to 529
    Data columns (total 16 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   name              530 non-null    object 
     1   album             530 non-null    object 
     2   artist            530 non-null    object 
     3   artist_top_genre  530 non-null    object 
     4   release_date      530 non-null    int64  
     5   length            530 non-null    int64  
     6   popularity        530 non-null    int64  
     7   danceability      530 non-null    float64
     8   acousticness      530 non-null    float64
     9   energy            530 non-null    float64
     10  instrumentalness  530 non-null    float64
     11  liveness          530 non-null    float64
     12  loudness          530 non-null    float64
     13  speechiness       530 non-null    float64
     14  tempo             530 non-null    float64
     15  time_signature    530 non-null    int64  
    dtypes: float64(8), int64(4), object(4)
    memory usage: 66.4+ KB
    ```

1. Verifica nuevamente si hay valores nulos llamando a `isnull()` y asegur√°ndote de que la suma sea 0:

    ```python
    df.isnull().sum()
    ```

    Todo se ve bien:

    ```output
    name                0
    album               0
    artist              0
    artist_top_genre    0
    release_date        0
    length              0
    popularity          0
    danceability        0
    acousticness        0
    energy              0
    instrumentalness    0
    liveness            0
    loudness            0
    speechiness         0
    tempo               0
    time_signature      0
    dtype: int64
    ```

1. Describe los datos:

    ```python
    df.describe()
    ```

    |       | release_date | length      | popularity | danceability | acousticness | energy   | instrumentalness | liveness | loudness  | speechiness | tempo      | time_signature |
    | ----- | ------------ | ----------- | ---------- | ------------ | ------------ | -------- | ---------------- | -------- | --------- | ----------- | ---------- | -------------- |
    | count | 530          | 530         | 530        | 530          | 530          | 530      | 530              | 530      | 530       | 530         | 530        | 530            |
    | mean  | 2015.390566  | 222298.1698 | 17.507547  | 0.741619     | 0.265412     | 0.760623 | 0.016305         | 0.147308 | -4.953011 | 0.130748    | 116.487864 | 3.986792       |
    | std   | 3.131688     | 39696.82226 | 18.992212  | 0.117522     | 0.208342     | 0.148533 | 0.090321         | 0.123588 | 2.464186  | 0.092939    | 23.518601  | 0.333701       |
    | min   | 1998         | 89488       | 0          | 0.255        | 0.000665     | 0.111    | 0                | 0.0283   | -19.362   | 0.0278      | 61.695     | 3              |
    | 25%   | 2014         | 199305      | 0          | 0.681        | 0.089525     | 0.669    | 0                | 0.07565  | -6.29875  | 0.0591      | 102.96125  | 4              |
    | 50%   | 2016         | 218509      | 13         | 0.761        | 0.2205       | 0.7845   | 0.000004         | 0.1035   | -4.5585   | 0.09795     | 112.7145   | 4              |
    | 75%   | 2017         | 242098.5    | 31         | 0.8295       | 0.403        | 0.87575  | 0.000234         | 0.164    | -3.331    | 0.177       | 125.03925  | 4              |
    | max   | 2020         | 511738      | 73         | 0.966        | 0.954        | 0.995    | 0.91             | 0.811    | 0.582     | 0.514       | 206.007    | 5              |

> ü§î Si estamos trabajando con clustering, un m√©todo no supervisado que no requiere datos etiquetados, ¬øpor qu√© estamos mostrando estos datos con etiquetas? En la fase de exploraci√≥n de datos son √∫tiles, pero no son necesarios para que los algoritmos de clustering funcionen. Podr√≠as eliminar los encabezados de las columnas y referirte a los datos por n√∫mero de columna.

Observa los valores generales de los datos. Nota que la popularidad puede ser '0', lo que muestra canciones que no tienen ranking. Eliminemos esos valores pronto.

1. Usa un gr√°fico de barras para encontrar los g√©neros m√°s populares:

    ```python
    import seaborn as sns
    
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top[:5].index,y=top[:5].values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

    ![most popular](../../../../5-Clustering/1-Visualize/images/popular.png)

‚úÖ Si deseas ver m√°s valores principales, cambia el top `[:5]` a un valor mayor o elim√≠nalo para ver todo.

Nota que cuando el g√©nero principal se describe como 'Missing', significa que Spotify no lo clasific√≥, as√≠ que elimin√©moslo.

1. Elimina los datos faltantes filtr√°ndolos:

    ```python
    df = df[df['artist_top_genre'] != 'Missing']
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top.index,y=top.values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

    Ahora verifica nuevamente los g√©neros:

    ![most popular](../../../../5-Clustering/1-Visualize/images/all-genres.png)

1. Los tres g√©neros principales dominan este conjunto de datos. Concentr√©monos en `afro dancehall`, `afropop` y `nigerian pop`, y adem√°s filtremos el conjunto de datos para eliminar cualquier valor de popularidad igual a 0 (lo que significa que no fue clasificado con una popularidad en el conjunto de datos y puede considerarse ruido para nuestros prop√≥sitos):

    ```python
    df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]
    df = df[(df['popularity'] > 0)]
    top = df['artist_top_genre'].value_counts()
    plt.figure(figsize=(10,7))
    sns.barplot(x=top.index,y=top.values)
    plt.xticks(rotation=45)
    plt.title('Top genres',color = 'blue')
    ```

1. Haz una prueba r√°pida para ver si los datos tienen alguna correlaci√≥n particularmente fuerte:

    ```python
    corrmat = df.corr(numeric_only=True)
    f, ax = plt.subplots(figsize=(12, 9))
    sns.heatmap(corrmat, vmax=.8, square=True)
    ```

    ![correlations](../../../../5-Clustering/1-Visualize/images/correlation.png)

    La √∫nica correlaci√≥n fuerte es entre `energy` y `loudness`, lo cual no es muy sorprendente, dado que la m√∫sica fuerte suele ser bastante energ√©tica. Por lo dem√°s, las correlaciones son relativamente d√©biles. Ser√° interesante ver qu√© puede hacer un algoritmo de clustering con estos datos.

    > üéì ¬°Nota que la correlaci√≥n no implica causalidad! Tenemos prueba de correlaci√≥n pero no prueba de causalidad. Un [sitio web divertido](https://tylervigen.com/spurious-correlations) tiene algunos gr√°ficos que enfatizan este punto.

¬øHay alguna convergencia en este conjunto de datos en torno a la popularidad percibida de una canci√≥n y su capacidad de baile? Un FacetGrid muestra que hay c√≠rculos conc√©ntricos que se alinean, independientemente del g√©nero. ¬øPodr√≠a ser que los gustos nigerianos convergen en un cierto nivel de capacidad de baile para este g√©nero?

‚úÖ Prueba diferentes puntos de datos (energy, loudness, speechiness) y m√°s o diferentes g√©neros musicales. ¬øQu√© puedes descubrir? Mira la tabla `df.describe()` para ver la distribuci√≥n general de los puntos de datos.

### Ejercicio - distribuci√≥n de datos

¬øSon estos tres g√©neros significativamente diferentes en la percepci√≥n de su capacidad de baile, basada en su popularidad?

1. Examina la distribuci√≥n de datos de nuestros tres g√©neros principales para popularidad y capacidad de baile a lo largo de un eje x y y dado.

    ```python
    sns.set_theme(style="ticks")
    
    g = sns.jointplot(
        data=df,
        x="popularity", y="danceability", hue="artist_top_genre",
        kind="kde",
    )
    ```

    Puedes descubrir c√≠rculos conc√©ntricos alrededor de un punto general de convergencia, mostrando la distribuci√≥n de puntos.

    > üéì Nota que este ejemplo utiliza un gr√°fico KDE (Kernel Density Estimate) que representa los datos usando una curva de densidad de probabilidad continua. Esto nos permite interpretar los datos cuando trabajamos con m√∫ltiples distribuciones.

    En general, los tres g√©neros se alinean de manera suelta en t√©rminos de su popularidad y capacidad de baile. Determinar clusters en estos datos alineados de manera suelta ser√° un desaf√≠o:

    ![distribution](../../../../5-Clustering/1-Visualize/images/distribution.png)

1. Crea un gr√°fico de dispersi√≥n:

    ```python
    sns.FacetGrid(df, hue="artist_top_genre", height=5) \
       .map(plt.scatter, "popularity", "danceability") \
       .add_legend()
    ```

    Un gr√°fico de dispersi√≥n de los mismos ejes muestra un patr√≥n similar de convergencia.

    ![Facetgrid](../../../../5-Clustering/1-Visualize/images/facetgrid.png)

En general, para clustering, puedes usar gr√°ficos de dispersi√≥n para mostrar clusters de datos, por lo que dominar este tipo de visualizaci√≥n es muy √∫til. En la pr√≥xima lecci√≥n, tomaremos estos datos filtrados y usaremos clustering k-means para descubrir grupos en estos datos que parecen superponerse de maneras interesantes.

---

## üöÄDesaf√≠o

En preparaci√≥n para la pr√≥xima lecci√≥n, haz un gr√°fico sobre los diversos algoritmos de clustering que podr√≠as descubrir y usar en un entorno de producci√≥n. ¬øQu√© tipo de problemas est√° tratando de abordar el clustering?

## [Cuestionario posterior a la lecci√≥n](https://ff-quizzes.netlify.app/en/ml/)

## Revisi√≥n y autoestudio

Antes de aplicar algoritmos de clustering, como hemos aprendido, es una buena idea entender la naturaleza de tu conjunto de datos. Lee m√°s sobre este tema [aqu√≠](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)

[Este art√≠culo √∫til](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) te gu√≠a a trav√©s de las diferentes formas en que varios algoritmos de clustering se comportan, dadas diferentes formas de datos.

## Tarea

[Investiga otras visualizaciones para clustering](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.