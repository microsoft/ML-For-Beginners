<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T11:35:35+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "sl"
}
-->
# LogistiÄna regresija za napovedovanje kategorij

![Infografika: LogistiÄna vs. linearna regresija](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ml/)

> ### [Ta lekcija je na voljo tudi v R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Uvod

V tej zadnji lekciji o regresiji, eni izmed osnovnih _klasiÄnih_ tehnik strojnega uÄenja, si bomo ogledali logistiÄno regresijo. To tehniko bi uporabili za odkrivanje vzorcev za napovedovanje binarnih kategorij. Ali je ta sladkarija Äokolada ali ne? Ali je ta bolezen nalezljiva ali ne? Ali bo ta stranka izbrala ta izdelek ali ne?

V tej lekciji boste spoznali:

- Novo knjiÅ¾nico za vizualizacijo podatkov
- Tehnike logistiÄne regresije

âœ… Poglobite svoje razumevanje dela s to vrsto regresije v tem [uÄnem modulu](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Predpogoji

Ker smo Å¾e delali s podatki o buÄah, smo dovolj seznanjeni, da opazimo, da obstaja ena binarna kategorija, s katero lahko delamo: `Barva`.

Zgradimo model logistiÄne regresije, da napovemo, _kakÅ¡ne barve bo doloÄena buÄa_ (oranÅ¾na ğŸƒ ali bela ğŸ‘»), glede na nekatere spremenljivke.

> Zakaj govorimo o binarni klasifikaciji v lekciji o regresiji? IzkljuÄno zaradi jezikovne priroÄnosti, saj je logistiÄna regresija [pravzaprav metoda klasifikacije](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), Äeprav temelji na linearni osnovi. O drugih naÄinih klasifikacije podatkov se boste nauÄili v naslednji skupini lekcij.

## DoloÄite vpraÅ¡anje

Za naÅ¡e namene bomo to izrazili kot binarno: 'Bela' ali 'Ne bela'. V naÅ¡em naboru podatkov obstaja tudi kategorija 'Ärtasta', vendar je primerov te kategorije malo, zato je ne bomo uporabili. Ta kategorija tako ali tako izgine, ko odstranimo manjkajoÄe vrednosti iz nabora podatkov.

> ğŸƒ Zabavno dejstvo: bele buÄe vÄasih imenujemo 'duhove buÄe'. Niso zelo enostavne za rezljanje, zato niso tako priljubljene kot oranÅ¾ne, vendar so videti kul! Tako bi lahko naÅ¡e vpraÅ¡anje preoblikovali tudi kot: 'Duh' ali 'Ne duh'. ğŸ‘»

## O logistiÄni regresiji

LogistiÄna regresija se od linearne regresije, ki ste jo spoznali prej, razlikuje v nekaj pomembnih vidikih.

[![Strojno uÄenje za zaÄetnike - Razumevanje logistiÄne regresije za klasifikacijo](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "Strojno uÄenje za zaÄetnike - Razumevanje logistiÄne regresije za klasifikacijo")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled logistiÄne regresije.

### Binarna klasifikacija

LogistiÄna regresija ne ponuja enakih funkcij kot linearna regresija. Prva ponuja napoved binarne kategorije ("bela ali ne bela"), medtem ko je druga sposobna napovedovati neprekinjene vrednosti, na primer glede na izvor buÄe in Äas Å¾etve, _koliko se bo njena cena zviÅ¡ala_.

![Model klasifikacije buÄ](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika avtorja [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Druge klasifikacije

Obstajajo tudi druge vrste logistiÄne regresije, vkljuÄno z multinomno in ordinalno:

- **Multinomna**, ki vkljuÄuje veÄ kot eno kategorijo - "OranÅ¾na, Bela in ÄŒrtasta".
- **Ordinalna**, ki vkljuÄuje urejene kategorije, uporabna, Äe Å¾elimo logiÄno urediti naÅ¡e rezultate, kot so buÄe, ki so razvrÅ¡Äene po konÄnem Å¡tevilu velikosti (mini, sm, med, lg, xl, xxl).

![Multinomna vs. ordinalna regresija](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Spremenljivke NI NUJNO, da so korelirane

Se spomnite, kako je linearna regresija bolje delovala z bolj koreliranimi spremenljivkami? LogistiÄna regresija je nasprotje - spremenljivke ni nujno, da se ujemajo. To ustreza tem podatkom, ki imajo nekoliko Å¡ibke korelacije.

### Potrebujete veliko Äistih podatkov

LogistiÄna regresija bo dala natanÄnejÅ¡e rezultate, Äe uporabite veÄ podatkov; naÅ¡ majhen nabor podatkov ni optimalen za to nalogo, zato to upoÅ¡tevajte.

[![Strojno uÄenje za zaÄetnike - Analiza in priprava podatkov za logistiÄno regresijo](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "Strojno uÄenje za zaÄetnike - Analiza in priprava podatkov za logistiÄno regresijo")

âœ… Razmislite o vrstah podatkov, ki bi bile primerne za logistiÄno regresijo.

## Naloga - uredite podatke

Najprej nekoliko oÄistite podatke, odstranite manjkajoÄe vrednosti in izberite le nekatere stolpce:

1. Dodajte naslednjo kodo:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Vedno lahko pokukate v svoj novi podatkovni okvir:

    ```python
    pumpkins.info
    ```

### Vizualizacija - kategorni graf

Do zdaj ste znova naloÅ¾ili [zaÄetni zvezek](../../../../2-Regression/4-Logistic/notebook.ipynb) s podatki o buÄah in jih oÄistili, da ste ohranili nabor podatkov, ki vsebuje nekaj spremenljivk, vkljuÄno z `Barvo`. Vizualizirajmo podatkovni okvir v zvezku z uporabo druge knjiÅ¾nice: [Seaborn](https://seaborn.pydata.org/index.html), ki temelji na Matplotlibu, ki smo ga uporabljali prej.

Seaborn ponuja nekaj zanimivih naÄinov za vizualizacijo vaÅ¡ih podatkov. Na primer, lahko primerjate porazdelitve podatkov za vsako `Sorto` in `Barvo` v kategorni graf.

1. Ustvarite tak graf z uporabo funkcije `catplot`, uporabite naÅ¡e podatke o buÄah `pumpkins` in doloÄite barvno preslikavo za vsako kategorijo buÄ (oranÅ¾na ali bela):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![MreÅ¾a vizualiziranih podatkov](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Z opazovanjem podatkov lahko vidite, kako se podatki o barvi nanaÅ¡ajo na sorto.

    âœ… Glede na ta kategorni graf, kakÅ¡ne zanimive raziskave si lahko zamislite?

### Predobdelava podatkov: kodiranje znaÄilnosti in oznak
NaÅ¡ nabor podatkov o buÄah vsebuje nizovne vrednosti za vse svoje stolpce. Delo s kategorijskimi podatki je za ljudi intuitivno, za stroje pa ne. Algoritmi strojnega uÄenja dobro delujejo s Å¡tevilkami. Zato je kodiranje zelo pomemben korak v fazi predobdelave podatkov, saj nam omogoÄa pretvorbo kategorijskih podatkov v Å¡tevilÄne podatke, ne da bi pri tem izgubili kakrÅ¡ne koli informacije. Dobro kodiranje vodi k gradnji dobrega modela.

Za kodiranje znaÄilnosti obstajata dve glavni vrsti kodirnikov:

1. Ordinalni kodirnik: dobro se prilega ordinalnim spremenljivkam, ki so kategorijske spremenljivke, kjer njihovi podatki sledijo logiÄnemu vrstnemu redu, kot je stolpec `Item Size` v naÅ¡em naboru podatkov. Ustvari preslikavo, tako da je vsaka kategorija predstavljena s Å¡tevilko, ki je vrstni red kategorije v stolpcu.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorni kodirnik: dobro se prilega nominalnim spremenljivkam, ki so kategorijske spremenljivke, kjer njihovi podatki ne sledijo logiÄnemu vrstnemu redu, kot so vse znaÄilnosti, razen `Item Size` v naÅ¡em naboru podatkov. To je kodiranje z eno vroÄo vrednostjo, kar pomeni, da je vsaka kategorija predstavljena z binarnim stolpcem: kodirana spremenljivka je enaka 1, Äe buÄa pripada tej sorti, in 0 sicer.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Nato se `ColumnTransformer` uporabi za zdruÅ¾itev veÄ kodirnikov v en korak in njihovo uporabo na ustreznih stolpcih.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
Po drugi strani pa za kodiranje oznake uporabimo razred `LabelEncoder` knjiÅ¾nice scikit-learn, ki je pripomoÄek za normalizacijo oznak, tako da vsebujejo le vrednosti med 0 in n_classes-1 (tukaj 0 in 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
Ko smo kodirali znaÄilnosti in oznako, jih lahko zdruÅ¾imo v nov podatkovni okvir `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
âœ… KakÅ¡ne so prednosti uporabe ordinalnega kodirnika za stolpec `Item Size`?

### Analizirajte odnose med spremenljivkami

Zdaj, ko smo predhodno obdelali naÅ¡e podatke, lahko analiziramo odnose med znaÄilnostmi in oznako, da dobimo idejo o tem, kako dobro bo model lahko napovedal oznako glede na znaÄilnosti.
NajboljÅ¡i naÄin za izvedbo te vrste analize je risanje podatkov. Ponovno bomo uporabili funkcijo `catplot` knjiÅ¾nice Seaborn, da vizualiziramo odnose med `Item Size`, `Variety` in `Color` v kategorni graf. Za boljÅ¡e risanje podatkov bomo uporabili kodiran stolpec `Item Size` in nekodiran stolpec `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![Kategorni graf vizualiziranih podatkov](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Uporabite graf 'swarm'

Ker je `Color` binarna kategorija (Bela ali Ne bela), potrebuje '[specializiran pristop](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) za vizualizacijo'. Obstajajo drugi naÄini za vizualizacijo odnosa te kategorije z drugimi spremenljivkami.

Spremenljivke lahko vizualizirate vzporedno z grafi knjiÅ¾nice Seaborn.

1. Poskusite graf 'swarm', da prikaÅ¾ete porazdelitev vrednosti:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Swarm graf vizualiziranih podatkov](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Opozorilo**: zgornja koda lahko ustvari opozorilo, saj Seaborn ne uspe predstaviti takÅ¡ne koliÄine podatkovnih toÄk v grafu swarm. MoÅ¾na reÅ¡itev je zmanjÅ¡anje velikosti oznaÄevalca z uporabo parametra 'size'. Vendar bodite pozorni, da to vpliva na berljivost grafa.

> **ğŸ§® PokaÅ¾i mi matematiko**
>
> LogistiÄna regresija temelji na konceptu 'maksimalne verjetnosti' z uporabo [sigmoidnih funkcij](https://wikipedia.org/wiki/Sigmoid_function). 'Sigmoidna funkcija' na grafu izgleda kot oblika Ärke 'S'. Vzame vrednost in jo preslika nekam med 0 in 1. Njena krivulja se imenuje tudi 'logistiÄna krivulja'. Njena formula izgleda takole:
>
> ![logistiÄna funkcija](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> kjer se sredinska toÄka sigmoidne funkcije nahaja na x-ovi toÄki 0, L je najveÄja vrednost krivulje, k pa je strmina krivulje. ÄŒe je rezultat funkcije veÄji od 0,5, bo oznaka dodeljena razredu '1' binarne izbire. ÄŒe ne, bo razvrÅ¡Äena kot '0'.

## Zgradite svoj model

Gradnja modela za iskanje teh binarnih klasifikacij je presenetljivo enostavna v Scikit-learn.

[![Strojno uÄenje za zaÄetnike - LogistiÄna regresija za klasifikacijo podatkov](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "Strojno uÄenje za zaÄetnike - LogistiÄna regresija za klasifikacijo podatkov")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled gradnje modela linearne regresije.

1. Izberite spremenljivke, ki jih Å¾elite uporabiti v svojem klasifikacijskem modelu, in razdelite uÄne ter testne nize z uporabo `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Zdaj lahko trenirate svoj model z uporabo `fit()` z uÄnimi podatki in natisnete njegov rezultat:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Oglejte si oceno svojega modela. Ni slabo, glede na to, da imate le pribliÅ¾no 1000 vrstic podatkov:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## BoljÅ¡e razumevanje prek matrike zmede

Medtem ko lahko dobite poroÄilo o oceni modela [pogoji](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) z natisom zgornjih elementov, boste morda laÅ¾je razumeli svoj model z uporabo [matrike zmede](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), ki nam pomaga razumeti, kako model deluje.

> ğŸ“ '[Matrika zmede](https://wikipedia.org/wiki/Confusion_matrix)' (ali 'matrika napak') je tabela, ki izraÅ¾a resniÄne in napaÄne pozitivne ter negativne rezultate vaÅ¡ega modela, s Äimer ocenjuje natanÄnost napovedi.

1. Za uporabo matrike zmede pokliÄite `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Oglejte si matriko zmede svojega modela:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

V Scikit-learn matrike zmede vrstice (os 0) predstavljajo dejanske oznake, stolpci (os 1) pa napovedane oznake.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Kaj se dogaja tukaj? Recimo, da je naÅ¡ model pozvan, da klasificira buÄe med dvema binarnima kategorijama, kategorijo 'bela' in kategorijo 'ne bela'.

- ÄŒe vaÅ¡ model napove buÄo kot ne belo in v resnici pripada kategoriji 'ne bela', to imenujemo resniÄni negativni rezultat, prikazan z zgornjo levo Å¡tevilko.
- ÄŒe vaÅ¡ model napove buÄo kot belo in v resnici pripada kategoriji 'ne bela', to imenujemo napaÄni negativni rezultat, prikazan z spodnjo levo Å¡tevilko.
- ÄŒe vaÅ¡ model napove buÄo kot ne belo in v resnici pripada kategoriji 'bela', to imenujemo napaÄni pozitivni rezultat, prikazan z zgornjo desno Å¡tevilko.
- ÄŒe vaÅ¡ model napove buÄo kot belo in v resnici pripada kategoriji 'bela', to imenujemo resniÄni pozitivni rezultat, prikazan z spodnjo desno Å¡tevilko.

Kot ste morda uganili, je zaÅ¾eleno imeti veÄje Å¡tevilo resniÄnih pozitivnih in resniÄnih negativnih rezultatov ter manjÅ¡e Å¡tevilo napaÄnih pozitivnih in napaÄnih negativnih rezultatov, kar pomeni, da model deluje bolje.
Kako se matrika zmede povezuje s natanÄnostjo in priklicem? Spomnite se, da je poroÄilo o klasifikaciji, ki je bilo natisnjeno zgoraj, pokazalo natanÄnost (0,85) in priklic (0,67).

NatanÄnost = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

Priklic = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

âœ… V: Glede na matriko zmede, kako se je model odrezal? O: Ni slabo; obstaja kar nekaj pravih negativnih primerov, vendar tudi nekaj laÅ¾nih negativnih primerov.

Ponovno si poglejmo izraze, ki smo jih videli prej, s pomoÄjo matrike zmede in njenega preslikavanja TP/TN ter FP/FN:

ğŸ“ NatanÄnost: TP/(TP + FP) DeleÅ¾ relevantnih primerov med pridobljenimi primeri (npr. katere oznake so bile dobro oznaÄene).

ğŸ“ Priklic: TP/(TP + FN) DeleÅ¾ relevantnih primerov, ki so bili pridobljeni, ne glede na to, ali so bili dobro oznaÄeni ali ne.

ğŸ“ f1-ocena: (2 * natanÄnost * priklic)/(natanÄnost + priklic) Tehtano povpreÄje natanÄnosti in priklica, pri Äemer je najboljÅ¡a ocena 1, najslabÅ¡a pa 0.

ğŸ“ Podpora: Å tevilo pojavitev vsake pridobljene oznake.

ğŸ“ ToÄnost: (TP + TN)/(TP + TN + FP + FN) DeleÅ¾ oznak, ki so bile pravilno napovedane za vzorec.

ğŸ“ Makro povpreÄje: IzraÄun neuteÅ¾enega povpreÄja metrik za vsako oznako, ne upoÅ¡tevajoÄ neravnovesje oznak.

ğŸ“ Tehtano povpreÄje: IzraÄun povpreÄja metrik za vsako oznako, pri Äemer se upoÅ¡teva neravnovesje oznak z njihovo teÅ¾o glede na podporo (Å¡tevilo pravih primerov za vsako oznako).

âœ… Ali lahko ugotovite, katero metriko bi morali spremljati, Äe Å¾elite, da vaÅ¡ model zmanjÅ¡a Å¡tevilo laÅ¾nih negativnih primerov?

## Vizualizacija ROC krivulje tega modela

[![ML za zaÄetnike - Analiza zmogljivosti logistiÄne regresije z ROC krivuljami](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML za zaÄetnike - Analiza zmogljivosti logistiÄne regresije z ROC krivuljami")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled ROC krivulj.

Naredimo Å¡e eno vizualizacijo, da si ogledamo tako imenovano 'ROC' krivuljo:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

S pomoÄjo Matplotliba nariÅ¡ite [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) ali ROC krivuljo modela. ROC krivulje se pogosto uporabljajo za pregled rezultatov klasifikatorja glede na njegove prave in laÅ¾ne pozitivne primere. "ROC krivulje obiÄajno prikazujejo stopnjo pravih pozitivnih primerov na Y osi in stopnjo laÅ¾nih pozitivnih primerov na X osi." Zato sta strmina krivulje in prostor med sredinsko Ärto ter krivuljo pomembna: Å¾elite krivuljo, ki hitro gre navzgor in Äez Ärto. V naÅ¡em primeru so na zaÄetku prisotni laÅ¾ni pozitivni primeri, nato pa se Ärta pravilno dvigne in gre Äez.

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Na koncu uporabite Scikit-learnov [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) za izraÄun dejanskega 'PodroÄja pod krivuljo' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Rezultat je `0.9749908725812341`. Ker se AUC giblje med 0 in 1, si Å¾elite visok rezultat, saj bo model, ki je 100 % pravilen v svojih napovedih, imel AUC 1; v tem primeru je model _kar dober_.

V prihodnjih lekcijah o klasifikacijah se boste nauÄili, kako iterirati za izboljÅ¡anje rezultatov modela. Za zdaj pa Äestitke! ZakljuÄili ste te lekcije o regresiji!

---
## ğŸš€Izziv

LogistiÄna regresija skriva Å¡e veliko veÄ! NajboljÅ¡i naÄin za uÄenje pa je eksperimentiranje. PoiÅ¡Äite podatkovni niz, ki je primeren za tovrstno analizo, in z njim zgradite model. Kaj ste se nauÄili? namig: poskusite [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) za zanimive podatkovne nize.

## [Kvizi po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Pregled in samostojno uÄenje

Preberite prve strani [tega dokumenta s Stanforda](https://web.stanford.edu/~jurafsky/slp3/5.pdf) o nekaterih praktiÄnih uporabah logistiÄne regresije. Razmislite o nalogah, ki so bolj primerne za eno ali drugo vrsto regresijskih nalog, ki smo jih preuÄevali do zdaj. Kaj bi delovalo najbolje?

## Naloga 

[Ponovno preizkusite to regresijo](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki bi izhajale iz uporabe tega prevoda.