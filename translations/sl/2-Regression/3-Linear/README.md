<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T11:28:40+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "sl"
}
-->
# Ustvarjanje regresijskega modela s Scikit-learn: Å¡tiri naÄini regresije

![Infografika linearne in polinomske regresije](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika avtorja [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Predavanje kviz](https://ff-quizzes.netlify.app/en/ml/)

> ### [To lekcijo lahko najdete tudi v jeziku R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Uvod

Do sedaj ste raziskali, kaj je regresija, z vzorÄnimi podatki iz nabora podatkov o cenah buÄ, ki ga bomo uporabljali skozi celotno lekcijo. Prav tako ste podatke vizualizirali z uporabo knjiÅ¾nice Matplotlib.

Zdaj ste pripravljeni, da se poglobite v regresijo za strojno uÄenje. Medtem ko vizualizacija omogoÄa razumevanje podatkov, je prava moÄ strojnega uÄenja v _treningu modelov_. Modele treniramo na zgodovinskih podatkih, da samodejno zajamejo odvisnosti podatkov, kar omogoÄa napovedovanje rezultatov za nove podatke, ki jih model Å¡e ni videl.

V tej lekciji boste izvedeli veÄ o dveh vrstah regresije: _osnovni linearni regresiji_ in _polinomski regresiji_, skupaj z nekaj matematike, ki stoji za temi tehnikami. Ti modeli nam bodo omogoÄili napovedovanje cen buÄ glede na razliÄne vhodne podatke.

[![ML za zaÄetnike - Razumevanje linearne regresije](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML za zaÄetnike - Razumevanje linearne regresije")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled linearne regresije.

> Skozi celoten uÄni naÄrt predpostavljamo minimalno matematiÄno predznanje in si prizadevamo, da bi bilo gradivo dostopno Å¡tudentom iz drugih podroÄij. Zato bodite pozorni na opombe, ğŸ§® poudarke, diagrame in druga uÄna orodja, ki pomagajo pri razumevanju.

### Predpogoji

Do zdaj bi morali biti Å¾e seznanjeni s strukturo podatkov o buÄah, ki jih preuÄujemo. Najdete jih prednaloÅ¾ene in predhodno oÄiÅ¡Äene v datoteki _notebook.ipynb_ te lekcije. V datoteki so cene buÄ prikazane na buÅ¡el v novem podatkovnem okviru. PrepriÄajte se, da lahko zaÅ¾enete te beleÅ¾nice v jedrih Visual Studio Code.

### Priprava

Kot opomnik, te podatke nalagate, da bi si zastavili vpraÅ¡anja, kot so:

- Kdaj je najboljÅ¡i Äas za nakup buÄ?
- KakÅ¡no ceno lahko priÄakujem za zabojÄek mini buÄ?
- Ali naj jih kupim v poloviÄnih buÅ¡elskih koÅ¡arah ali v Å¡katlah velikosti 1 1/9 buÅ¡la?
Poglobimo se v te podatke.

V prejÅ¡nji lekciji ste ustvarili podatkovni okvir Pandas in ga napolnili z delom izvirnega nabora podatkov, standardizirali cene na buÅ¡el. S tem pa ste lahko zbrali le pribliÅ¾no 400 podatkovnih toÄk in le za jesenske mesece.

Oglejte si podatke, ki so prednaloÅ¾eni v beleÅ¾nici, ki spremlja to lekcijo. Podatki so prednaloÅ¾eni, zaÄetni raztrosni diagram pa je narisan, da prikaÅ¾e podatke po mesecih. Morda lahko z dodatnim ÄiÅ¡Äenjem pridobimo veÄ podrobnosti o naravi podatkov.

## Linearna regresijska premica

Kot ste se nauÄili v 1. lekciji, je cilj linearne regresije narisati premico, ki:

- **PrikaÅ¾e odnose med spremenljivkami**. PrikaÅ¾e razmerje med spremenljivkami.
- **OmogoÄa napovedi**. OmogoÄa natanÄne napovedi, kje bi nova podatkovna toÄka padla v razmerju do te premice.

Za risanje te vrste premice je znaÄilna metoda **najmanjÅ¡ih kvadratov**. Izraz 'najmanjÅ¡i kvadrati' pomeni, da so vse podatkovne toÄke okoli regresijske premice kvadrirane in nato seÅ¡tejejo. Idealno je, da je ta konÄna vsota Äim manjÅ¡a, saj Å¾elimo nizko Å¡tevilo napak ali `najmanjÅ¡e kvadrate`.

To storimo, ker Å¾elimo modelirati premico, ki ima najmanjÅ¡o kumulativno razdaljo od vseh naÅ¡ih podatkovnih toÄk. ÄŒlene kvadriramo pred seÅ¡tevanjem, saj nas zanima njihova velikost, ne pa smer.

> **ğŸ§® PokaÅ¾i mi matematiko**
>
> Ta premica, imenovana _premica najboljÅ¡e prileganja_, je izraÅ¾ena z [enaÄbo](https://en.wikipedia.org/wiki/Simple_linear_regression):
>
> ```
> Y = a + bX
> ```
>
> `X` je 'pojasnjevalna spremenljivka'. `Y` je 'odvisna spremenljivka'. Naklon premice je `b`, `a` pa je preseÄiÅ¡Äe z osjo y, kar se nanaÅ¡a na vrednost `Y`, ko `X = 0`.
>
>![izraÄun naklona](../../../../2-Regression/3-Linear/images/slope.png)
>
> Najprej izraÄunajte naklon `b`. Infografika avtorja [Jen Looper](https://twitter.com/jenlooper)
>
> Z drugimi besedami, glede na naÅ¡e izvirno vpraÅ¡anje o podatkih o buÄah: "napovedati ceno buÄe na buÅ¡el glede na mesec", bi `X` predstavljal ceno, `Y` pa mesec prodaje.
>
>![dopolnitev enaÄbe](../../../../2-Regression/3-Linear/images/calculation.png)
>
> IzraÄunajte vrednost Y. ÄŒe plaÄujete pribliÅ¾no 4 $, mora biti april! Infografika avtorja [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika, ki izraÄuna premico, mora prikazati njen naklon, ki je odvisen tudi od preseÄiÅ¡Äa, torej od tega, kje se `Y` nahaja, ko `X = 0`.
>
> Metodo izraÄuna teh vrednosti si lahko ogledate na spletnem mestu [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). ObiÅ¡Äite tudi [ta kalkulator najmanjÅ¡ih kvadratov](https://www.mathsisfun.com/data/least-squares-calculator.html), da vidite, kako vrednosti Å¡tevilk vplivajo na premico.

## Korelacija

Å e en izraz, ki ga je treba razumeti, je **koeficient korelacije** med danima spremenljivkama X in Y. Z raztrosnim diagramom lahko hitro vizualizirate ta koeficient. Diagram s podatkovnimi toÄkami, razporejenimi v ravni Ärti, ima visoko korelacijo, medtem ko ima diagram s podatkovnimi toÄkami, razprÅ¡enimi povsod med X in Y, nizko korelacijo.

Dober model linearne regresije bo tisti, ki ima visok (bliÅ¾je 1 kot 0) koeficient korelacije z uporabo metode najmanjÅ¡ih kvadratov in regresijske premice.

âœ… ZaÅ¾enite beleÅ¾nico, ki spremlja to lekcijo, in si oglejte raztrosni diagram meseca in cene. Ali se vam zdi, da imajo podatki, ki povezujejo mesec in ceno prodaje buÄ, visoko ali nizko korelacijo glede na vaÅ¡o vizualno interpretacijo raztrosnega diagrama? Ali se to spremeni, Äe uporabite bolj natanÄno merilo namesto `Mesec`, npr. *dan v letu* (tj. Å¡tevilo dni od zaÄetka leta)?

V spodnji kodi bomo predpostavili, da smo oÄistili podatke in pridobili podatkovni okvir z imenom `new_pumpkins`, podoben naslednjemu:

ID | Mesec | DanVLeto | Vrsta | Mesto | Paket | NajniÅ¾ja cena | NajviÅ¡ja cena | Cena
---|-------|----------|-------|-------|-------|---------------|---------------|------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 Å¡katle | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 Å¡katle | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 Å¡katle | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 Å¡katle | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 Å¡katle | 15.0 | 15.0 | 13.636364

> Koda za ÄiÅ¡Äenje podatkov je na voljo v datoteki [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Izvedli smo enake korake ÄiÅ¡Äenja kot v prejÅ¡nji lekciji in izraÄunali stolpec `DanVLeto` z naslednjim izrazom:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Zdaj, ko razumete matematiko za linearno regresijo, ustvarimo regresijski model, da preverimo, ali lahko napovemo, kateri paket buÄ bo imel najboljÅ¡e cene. Nekdo, ki kupuje buÄe za prazniÄni buÄni nasad, bi morda Å¾elel te informacije, da bi optimiziral svoje nakupe paketov buÄ za nasad.

## Iskanje korelacije

[![ML za zaÄetnike - Iskanje korelacije: KljuÄ do linearne regresije](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML za zaÄetnike - Iskanje korelacije: KljuÄ do linearne regresije")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled korelacije.

Iz prejÅ¡nje lekcije ste verjetno videli, da povpreÄna cena za razliÄne mesece izgleda takole:

<img alt="PovpreÄna cena po mesecih" src="../2-Data/images/barchart.png" width="50%"/>

To nakazuje, da bi morala obstajati neka korelacija, in lahko poskusimo trenirati linearni regresijski model za napovedovanje razmerja med `Mesec` in `Cena` ali med `DanVLeto` in `Cena`. Tukaj je raztrosni diagram, ki prikazuje slednje razmerje:

<img alt="Raztrosni diagram cene glede na dan v letu" src="images/scatter-dayofyear.png" width="50%" /> 

Preverimo, ali obstaja korelacija, z uporabo funkcije `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Zdi se, da je korelacija precej majhna, -0,15 za `Mesec` in -0,17 za `DanVLeto`, vendar bi lahko obstajalo drugo pomembno razmerje. Zdi se, da obstajajo razliÄni grozdi cen, ki ustrezajo razliÄnim vrstam buÄ. Da potrdimo to hipotezo, nariÅ¡imo vsako kategorijo buÄ z drugo barvo. S posredovanjem parametra `ax` funkciji za risanje raztrosa lahko nariÅ¡emo vse toÄke na isti grafikon:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Raztrosni diagram cene glede na dan v letu" src="images/scatter-dayofyear-color.png" width="50%" /> 

NaÅ¡a raziskava nakazuje, da ima vrsta buÄe veÄji vpliv na skupno ceno kot dejanski datum prodaje. To lahko vidimo z barvnim grafom:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Barvni graf cene glede na vrsto" src="images/price-by-variety.png" width="50%" /> 

OsredotoÄimo se za trenutek samo na eno vrsto buÄ, 'pie type', in preverimo, kakÅ¡en vpliv ima datum na ceno:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Raztrosni diagram cene glede na dan v letu" src="images/pie-pumpkins-scatter.png" width="50%" /> 

ÄŒe zdaj izraÄunamo korelacijo med `Cena` in `DanVLeto` z uporabo funkcije `corr`, bomo dobili nekaj okoli `-0,27` - kar pomeni, da ima smisel trenirati napovedni model.

> Pred treningom linearnega regresijskega modela je pomembno zagotoviti, da so naÅ¡i podatki Äisti. Linearna regresija ne deluje dobro z manjkajoÄimi vrednostmi, zato je smiselno odstraniti vse prazne celice:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Drugi pristop bi bil zapolniti te prazne vrednosti s povpreÄnimi vrednostmi iz ustreznega stolpca.

## Enostavna linearna regresija

[![ML za zaÄetnike - Linearna in polinomska regresija z uporabo Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML za zaÄetnike - Linearna in polinomska regresija z uporabo Scikit-learn")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled linearne in polinomske regresije.

Za treniranje naÅ¡ega linearnega regresijskega modela bomo uporabili knjiÅ¾nico **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZaÄnemo z loÄevanjem vhodnih vrednosti (znaÄilnosti) in priÄakovanega izhoda (oznake) v loÄena numpy polja:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> UpoÅ¡tevajte, da smo morali izvesti `reshape` na vhodnih podatkih, da jih Linear Regression paket pravilno razume. Linearna regresija priÄakuje 2D polje kot vhod, kjer vsaka vrstica polja ustreza vektorju vhodnih znaÄilnosti. V naÅ¡em primeru, ker imamo samo en vhod, potrebujemo polje oblike NÃ—1, kjer je N velikost nabora podatkov.

Nato moramo podatke razdeliti na uÄni in testni nabor, da lahko po treningu preverimo naÅ¡ model:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Na koncu trening dejanskega linearnega regresijskega modela zahteva le dve vrstici kode. Definiramo objekt `LinearRegression` in ga prilagodimo naÅ¡im podatkom z uporabo metode `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po prilagoditvi vsebuje vse koeficiente regresije, do katerih lahko dostopamo z lastnostjo `.coef_`. V naÅ¡em primeru je le en koeficient, ki bi moral biti okoli `-0,017`. To pomeni, da se cene zdi, da rahlo padajo s Äasom, vendar ne preveÄ, pribliÅ¾no 2 centa na dan. Prav tako lahko dostopamo do preseÄiÅ¡Äa regresije z osjo Y z uporabo `lin_reg.intercept_` - v naÅ¡em primeru bo to okoli `21`, kar kaÅ¾e na ceno na zaÄetku leta.

Da vidimo, kako natanÄen je naÅ¡ model, lahko napovemo cene na testnem naboru podatkov in nato izmerimo, kako blizu so naÅ¡e napovedi priÄakovanim vrednostim. To lahko storimo z uporabo metrike srednje kvadratne napake (MSE), ki je povpreÄje vseh kvadratnih razlik med priÄakovano in napovedano vrednostjo.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
NaÅ¡a napaka se zdi okoli 2 toÄk, kar je pribliÅ¾no 17 %. Ni ravno dobro. Drug kazalnik kakovosti modela je **koeficient determinacije**, ki ga lahko pridobimo na naslednji naÄin:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
ÄŒe je vrednost 0, to pomeni, da model ne upoÅ¡teva vhodnih podatkov in deluje kot *najslabÅ¡i linearni napovedovalec*, kar je preprosto povpreÄna vrednost rezultata. Vrednost 1 pomeni, da lahko popolnoma napovemo vse priÄakovane izhode. V naÅ¡em primeru je koeficient okoli 0,06, kar je precej nizko.

Prav tako lahko nariÅ¡emo testne podatke skupaj z regresijsko premico, da bolje vidimo, kako regresija deluje v naÅ¡em primeru:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linearna regresija" src="images/linear-results.png" width="50%" />

## Polinomska regresija

Druga vrsta linearne regresije je polinomska regresija. ÄŒeprav vÄasih obstaja linearna povezava med spremenljivkami â€“ veÄja kot je buÄa po prostornini, viÅ¡ja je cena â€“ vÄasih teh povezav ni mogoÄe prikazati kot ravnino ali premico.

âœ… Tukaj so [nekateri dodatni primeri](https://online.stat.psu.edu/stat501/lesson/9/9.8) podatkov, ki bi lahko uporabili polinomsko regresijo.

Poglejte Å¡e enkrat razmerje med datumom in ceno. Ali se zdi, da bi moral biti ta raztros nujno analiziran s premico? Ali cene ne morejo nihati? V tem primeru lahko poskusite polinomsko regresijo.

âœ… Polinomi so matematiÄni izrazi, ki lahko vsebujejo eno ali veÄ spremenljivk in koeficientov.

Polinomska regresija ustvari ukrivljeno premico, ki bolje ustreza nelinearnim podatkom. V naÅ¡em primeru, Äe v vhodne podatke vkljuÄimo kvadratno spremenljivko `DayOfYear`, bi morali biti sposobni prilagoditi naÅ¡e podatke s paraboliÄno krivuljo, ki bo imela minimum na doloÄenem mestu v letu.

Scikit-learn vkljuÄuje uporabno [API za pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), ki zdruÅ¾uje razliÄne korake obdelave podatkov. **Pipeline** je veriga **ocenjevalcev**. V naÅ¡em primeru bomo ustvarili pipeline, ki najprej doda polinomske znaÄilnosti naÅ¡emu modelu, nato pa izvede regresijo:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Uporaba `PolynomialFeatures(2)` pomeni, da bomo vkljuÄili vse polinome druge stopnje iz vhodnih podatkov. V naÅ¡em primeru to pomeni le `DayOfYear`<sup>2</sup>, vendar ob dveh vhodnih spremenljivkah X in Y to doda X<sup>2</sup>, XY in Y<sup>2</sup>. Uporabimo lahko tudi polinome viÅ¡jih stopenj, Äe Å¾elimo.

Pipeline lahko uporabljamo na enak naÄin kot originalni objekt `LinearRegression`, tj. lahko uporabimo `fit` za prilagoditev modela in nato `predict` za pridobitev rezultatov napovedi. Tukaj je graf, ki prikazuje testne podatke in aproksimacijsko krivuljo:

<img alt="Polinomska regresija" src="images/poly-results.png" width="50%" />

S polinomsko regresijo lahko doseÅ¾emo nekoliko niÅ¾ji MSE in viÅ¡ji koeficient determinacije, vendar ne bistveno. UpoÅ¡tevati moramo tudi druge znaÄilnosti!

> Opazite, da so najniÅ¾je cene buÄ opazovane nekje okoli noÄi Äarovnic. Kako to razloÅ¾ite?

ğŸƒ ÄŒestitke, pravkar ste ustvarili model, ki lahko pomaga napovedati ceno buÄ za pite. Verjetno lahko ponovite isti postopek za vse vrste buÄ, vendar bi bilo to zamudno. Zdaj se nauÄimo, kako upoÅ¡tevati razliÄne vrste buÄ v naÅ¡em modelu!

## Kategorijske znaÄilnosti

V idealnem svetu bi Å¾eleli napovedati cene za razliÄne vrste buÄ z istim modelom. Vendar je stolpec `Variety` nekoliko drugaÄen od stolpcev, kot je `Month`, saj vsebuje nenumeriÄne vrednosti. TakÅ¡ni stolpci se imenujejo **kategorijski**.

[![ML za zaÄetnike - Napovedovanje kategorijskih znaÄilnosti z linearno regresijo](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML za zaÄetnike - Napovedovanje kategorijskih znaÄilnosti z linearno regresijo")

> ğŸ¥ Kliknite zgornjo sliko za kratek video pregled uporabe kategorijskih znaÄilnosti.

Tukaj lahko vidite, kako povpreÄna cena odvisna od vrste buÄe:

<img alt="PovpreÄna cena po vrsti" src="images/price-by-variety.png" width="50%" />

Da upoÅ¡tevamo vrsto buÄe, jo moramo najprej pretvoriti v numeriÄno obliko, ali jo **kodirati**. Obstaja veÄ naÄinov, kako to storiti:

* Preprosto **numeriÄno kodiranje** bo ustvarilo tabelo razliÄnih vrst in nato zamenjalo ime vrste z indeksom v tej tabeli. To ni najboljÅ¡a ideja za linearno regresijo, saj linearna regresija upoÅ¡teva dejansko numeriÄno vrednost indeksa in jo doda rezultatu, pomnoÅ¾eno z nekim koeficientom. V naÅ¡em primeru je razmerje med Å¡tevilko indeksa in ceno oÄitno nelinearno, tudi Äe poskrbimo, da so indeksi urejeni na doloÄen naÄin.
* **One-hot kodiranje** bo zamenjalo stolpec `Variety` s 4 razliÄnimi stolpci, enim za vsako vrsto. Vsak stolpec bo vseboval `1`, Äe je ustrezna vrstica doloÄene vrste, in `0` sicer. To pomeni, da bo v linearni regresiji Å¡tiri koeficiente, po enega za vsako vrsto buÄe, ki bodo odgovorni za "zaÄetno ceno" (ali bolje "dodatno ceno") za to doloÄeno vrsto.

Spodnja koda prikazuje, kako lahko izvedemo one-hot kodiranje vrste:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Za treniranje linearne regresije z uporabo one-hot kodirane vrste kot vhodnih podatkov moramo le pravilno inicializirati podatke `X` in `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Preostala koda je enaka kot tista, ki smo jo uporabili zgoraj za treniranje linearne regresije. ÄŒe jo preizkusite, boste videli, da je povpreÄna kvadratna napaka pribliÅ¾no enaka, vendar dobimo veliko viÅ¡ji koeficient determinacije (~77 %). Za Å¡e bolj natanÄne napovedi lahko upoÅ¡tevamo veÄ kategorijskih znaÄilnosti, pa tudi numeriÄne znaÄilnosti, kot sta `Month` ali `DayOfYear`. Za pridobitev ene velike matrike znaÄilnosti lahko uporabimo `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Tukaj upoÅ¡tevamo tudi `City` in vrsto `Package`, kar nam daje MSE 2,84 (10 %) in determinacijo 0,94!

## Vse skupaj

Za najboljÅ¡i model lahko uporabimo kombinirane (one-hot kodirane kategorijske + numeriÄne) podatke iz zgornjega primera skupaj s polinomsko regresijo. Tukaj je celotna koda za vaÅ¡o uporabo:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

To bi nam moralo dati najboljÅ¡i koeficient determinacije skoraj 97 % in MSE=2,23 (~8 % napake pri napovedi).

| Model | MSE | Determinacija |
|-------|-----|---------------|
| `DayOfYear` Linear | 2,77 (17,2 %) | 0,07 |
| `DayOfYear` Polinomski | 2,73 (17,0 %) | 0,08 |
| `Variety` Linear | 5,24 (19,7 %) | 0,77 |
| Vse znaÄilnosti Linear | 2,84 (10,5 %) | 0,94 |
| Vse znaÄilnosti Polinomski | 2,23 (8,25 %) | 0,97 |

ğŸ† OdliÄno! Ustvarili ste Å¡tiri regresijske modele v eni lekciji in izboljÅ¡ali kakovost modela na 97 %. V zadnjem delu o regresiji se boste nauÄili o logistiÄni regresiji za doloÄanje kategorij.

---
## ğŸš€Izziv

Preizkusite veÄ razliÄnih spremenljivk v tej beleÅ¾nici, da vidite, kako korelacija ustreza natanÄnosti modela.

## [Kvizi po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Pregled in samostojno uÄenje

V tej lekciji smo se nauÄili o linearni regresiji. Obstajajo tudi druge pomembne vrste regresije. Preberite o tehnikah Stepwise, Ridge, Lasso in Elasticnet. Dober teÄaj za nadaljnje uÄenje je [Stanfordov teÄaj statistiÄnega uÄenja](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Naloga 

[Ustvarite model](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.