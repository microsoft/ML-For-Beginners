<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T12:31:06+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "sl"
}
-->
# Postscript: Odpravljanje napak modelov strojnega uÄenja z uporabo komponent nadzorne ploÅ¡Äe za odgovorno umetno inteligenco

## [Predavanje kviz](https://ff-quizzes.netlify.app/en/ml/)

## Uvod

Strojno uÄenje vpliva na naÅ¡a vsakodnevna Å¾ivljenja. Umetna inteligenca (UI) se vkljuÄuje v nekatere najpomembnejÅ¡e sisteme, ki vplivajo na nas kot posameznike in na naÅ¡o druÅ¾bo, od zdravstva, financ, izobraÅ¾evanja do zaposlovanja. Na primer, sistemi in modeli sodelujejo pri vsakodnevnih odloÄitvah, kot so zdravstvene diagnoze ali odkrivanje goljufij. PoslediÄno so napredki v UI skupaj s pospeÅ¡eno uporabo sooÄeni z razvijajoÄimi se druÅ¾benimi priÄakovanji in naraÅ¡ÄajoÄo regulacijo. Nenehno opaÅ¾amo podroÄja, kjer sistemi UI ne izpolnjujejo priÄakovanj, razkrivajo nove izzive, in vlade zaÄenjajo regulirati reÅ¡itve UI. Zato je pomembno, da so ti modeli analizirani, da zagotovijo praviÄne, zanesljive, vkljuÄujoÄe, pregledne in odgovorne rezultate za vse.

V tem uÄnem naÄrtu bomo raziskali praktiÄna orodja, ki jih lahko uporabimo za oceno, ali ima model teÅ¾ave z odgovorno UI. Tradicionalne tehnike odpravljanja napak pri strojnem uÄenju so obiÄajno osnovane na kvantitativnih izraÄunih, kot so zdruÅ¾ena natanÄnost ali povpreÄna izguba napake. Predstavljajte si, kaj se lahko zgodi, ko podatki, ki jih uporabljate za gradnjo teh modelov, ne vkljuÄujejo doloÄenih demografskih skupin, kot so rasa, spol, politiÄno prepriÄanje, religija, ali pa te demografske skupine nesorazmerno predstavljajo. Kaj pa, Äe je izhod modela interpretiran tako, da daje prednost doloÄeni demografski skupini? To lahko povzroÄi prekomerno ali nezadostno zastopanost teh obÄutljivih skupin, kar vodi do vpraÅ¡anj praviÄnosti, vkljuÄenosti ali zanesljivosti modela. Drug dejavnik je, da so modeli strojnega uÄenja pogosto obravnavani kot "Ärne Å¡katle", kar oteÅ¾uje razumevanje in razlago, kaj vpliva na napovedi modela. Vse to so izzivi, s katerimi se sooÄajo podatkovni znanstveniki in razvijalci UI, kadar nimajo ustreznih orodij za odpravljanje napak ali ocenjevanje praviÄnosti in zanesljivosti modela.

V tej lekciji se boste nauÄili odpravljati napake v svojih modelih z uporabo:

- **Analize napak**: prepoznajte, kje v porazdelitvi podatkov ima model visoke stopnje napak.
- **Pregleda modela**: izvedite primerjalno analizo med razliÄnimi kohortami podatkov, da odkrijete razlike v metrikah uspeÅ¡nosti modela.
- **Analize podatkov**: raziÅ¡Äite, kje bi lahko priÅ¡lo do prekomerne ali nezadostne zastopanosti podatkov, kar lahko povzroÄi, da model daje prednost eni demografski skupini pred drugo.
- **Pomembnosti znaÄilnosti**: razumite, katere znaÄilnosti vplivajo na napovedi modela na globalni ali lokalni ravni.

## Predpogoj

Kot predpogoj si oglejte [Orodja za odgovorno UI za razvijalce](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif o orodjih za odgovorno UI](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Analiza napak

Tradicionalne metrike uspeÅ¡nosti modelov, ki se uporabljajo za merjenje natanÄnosti, so veÄinoma izraÄuni, osnovani na pravilnih in nepravilnih napovedih. Na primer, doloÄitev, da je model natanÄen 89 % Äasa z izgubo napake 0,001, se lahko Å¡teje za dobro uspeÅ¡nost. Napake pa pogosto niso enakomerno porazdeljene v osnovnem naboru podatkov. Lahko doseÅ¾ete 89 % natanÄnost modela, vendar ugotovite, da obstajajo razliÄni deli vaÅ¡ih podatkov, kjer model odpove 42 % Äasa. Posledice teh vzorcev napak pri doloÄenih skupinah podatkov lahko vodijo do vpraÅ¡anj praviÄnosti ali zanesljivosti. KljuÄno je razumeti podroÄja, kjer model deluje dobro ali ne. PodroÄja podatkov, kjer je veliko netoÄnosti modela, se lahko izkaÅ¾ejo za pomembne demografske skupine podatkov.

![Analizirajte in odpravljajte napake modela](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

Komponenta za analizo napak na nadzorni ploÅ¡Äi RAI prikazuje, kako so napake modela porazdeljene med razliÄnimi kohortami s pomoÄjo vizualizacije drevesa. To je uporabno za prepoznavanje znaÄilnosti ali podroÄij, kjer je stopnja napak v vaÅ¡em naboru podatkov visoka. Z opazovanjem, od kod prihaja veÄina netoÄnosti modela, lahko zaÄnete raziskovati osnovni vzrok. Prav tako lahko ustvarite kohorte podatkov za izvedbo analize. Te kohorte podatkov pomagajo pri odpravljanju napak, da ugotovite, zakaj je uspeÅ¡nost modela dobra v eni kohorti, a napaÄna v drugi.

![Analiza napak](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

Vizualni kazalniki na zemljevidu drevesa pomagajo hitreje locirati problematiÄna podroÄja. Na primer, temnejÅ¡i odtenek rdeÄe barve na vozliÅ¡Äu drevesa pomeni viÅ¡jo stopnjo napak.

Toplotni zemljevid je Å¡e ena funkcionalnost vizualizacije, ki jo uporabniki lahko uporabijo za raziskovanje stopnje napak z uporabo ene ali dveh znaÄilnosti, da najdejo prispevek k napakam modela v celotnem naboru podatkov ali kohortah.

![Toplotni zemljevid analize napak](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Uporabite analizo napak, kadar potrebujete:

* Globoko razumevanje, kako so napake modela porazdeljene po naboru podatkov in po veÄ vhodnih in znaÄilnostnih dimenzijah.
* RazÄlenitev zdruÅ¾enih metrik uspeÅ¡nosti za samodejno odkrivanje napaÄnih kohort, da obvestite svoje ciljno usmerjene korake za odpravljanje teÅ¾av.

## Pregled modela

Ocenjevanje uspeÅ¡nosti modela strojnega uÄenja zahteva celostno razumevanje njegovega vedenja. To je mogoÄe doseÄi z analizo veÄ kot ene metrike, kot so stopnja napak, natanÄnost, priklic, natanÄnost ali MAE (povpreÄna absolutna napaka), da bi odkrili razlike med metrikami uspeÅ¡nosti. Ena metrika uspeÅ¡nosti se lahko zdi odliÄna, vendar lahko netoÄnosti razkrije druga metrika. Poleg tega primerjava metrik za razlike v celotnem naboru podatkov ali kohortah pomaga osvetliti, kje model deluje dobro ali ne. To je Å¡e posebej pomembno pri opazovanju uspeÅ¡nosti modela med obÄutljivimi in neobÄutljivimi znaÄilnostmi (npr. rasa, spol ali starost pacienta), da bi odkrili morebitno nepraviÄnost modela. Na primer, odkritje, da je model bolj napaÄen v kohorti z obÄutljivimi znaÄilnostmi, lahko razkrije morebitno nepraviÄnost modela.

Komponenta Pregled modela na nadzorni ploÅ¡Äi RAI pomaga ne le pri analizi metrik uspeÅ¡nosti predstavitve podatkov v kohorti, temveÄ uporabnikom omogoÄa tudi primerjavo vedenja modela med razliÄnimi kohortami.

![Kohorte nabora podatkov - pregled modela na nadzorni ploÅ¡Äi RAI](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

Funkcionalnost analize, osnovane na znaÄilnostih, omogoÄa uporabnikom, da zoÅ¾ijo podskupine podatkov znotraj doloÄene znaÄilnosti, da prepoznajo anomalije na bolj podrobni ravni. Na primer, nadzorna ploÅ¡Äa ima vgrajeno inteligenco za samodejno ustvarjanje kohort za uporabniÅ¡ko izbrano znaÄilnost (npr. *"Äas_v_bolniÅ¡nici < 3"* ali *"Äas_v_bolniÅ¡nici >= 7"*). To omogoÄa uporabniku, da izolira doloÄeno znaÄilnost iz veÄje skupine podatkov, da preveri, ali je kljuÄni dejavnik napaÄnih rezultatov modela.

![Kohorte znaÄilnosti - pregled modela na nadzorni ploÅ¡Äi RAI](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

Komponenta Pregled modela podpira dva razreda metrik razlik:

**Razlike v uspeÅ¡nosti modela**: Ti nabori metrik izraÄunajo razlike v vrednostih izbrane metrike uspeÅ¡nosti med podskupinami podatkov. Tukaj je nekaj primerov:

* Razlike v stopnji natanÄnosti
* Razlike v stopnji napak
* Razlike v natanÄnosti
* Razlike v priklicu
* Razlike v povpreÄni absolutni napaki (MAE)

**Razlike v stopnji izbire**: Ta metrika vsebuje razliko v stopnji izbire (ugodna napoved) med podskupinami. Primer tega je razlika v stopnjah odobritve posojil. Stopnja izbire pomeni deleÅ¾ podatkovnih toÄk v vsakem razredu, ki so razvrÅ¡Äene kot 1 (pri binarni klasifikaciji) ali porazdelitev vrednosti napovedi (pri regresiji).

## Analiza podatkov

> "ÄŒe dovolj dolgo muÄite podatke, bodo priznali karkoli." - Ronald Coase

Ta izjava se sliÅ¡i skrajna, vendar drÅ¾i, da je mogoÄe podatke manipulirati, da podpirajo katerikoli zakljuÄek. TakÅ¡na manipulacija se lahko vÄasih zgodi nenamerno. Kot ljudje imamo vsi pristranskosti in pogosto je teÅ¾ko zavestno vedeti, kdaj vnaÅ¡amo pristranskost v podatke. Zagotavljanje praviÄnosti v UI in strojnem uÄenju ostaja kompleksen izziv.

Podatki so velika slepa pega za tradicionalne metrike uspeÅ¡nosti modelov. Lahko imate visoke ocene natanÄnosti, vendar to ne odraÅ¾a vedno osnovne pristranskosti podatkov, ki bi lahko bila prisotna v vaÅ¡em naboru podatkov. Na primer, Äe ima nabor podatkov o zaposlenih 27 % Å¾ensk na izvrÅ¡nih poloÅ¾ajih v podjetju in 73 % moÅ¡kih na isti ravni, lahko model za oglaÅ¡evanje delovnih mest, usposobljen na teh podatkih, cilja predvsem na moÅ¡ko obÄinstvo za viÅ¡je poloÅ¾aje. Ta neuravnoteÅ¾enost v podatkih je izkrivila napoved modela, da daje prednost enemu spolu. To razkriva vpraÅ¡anje praviÄnosti, kjer je v modelu UI prisotna spolna pristranskost.

Komponenta Analiza podatkov na nadzorni ploÅ¡Äi RAI pomaga prepoznati podroÄja, kjer je v naboru podatkov prisotna prekomerna ali nezadostna zastopanost. Uporabnikom pomaga diagnosticirati osnovni vzrok napak in vpraÅ¡anj praviÄnosti, ki so posledica neuravnoteÅ¾enosti podatkov ali pomanjkanja zastopanosti doloÄene skupine podatkov. To uporabnikom omogoÄa vizualizacijo naborov podatkov na podlagi napovedanih in dejanskih rezultatov, skupin napak in specifiÄnih znaÄilnosti. VÄasih lahko odkritje premalo zastopane skupine podatkov razkrije tudi, da se model ne uÄi dobro, kar vodi do visokih netoÄnosti. Model, ki ima pristranskost v podatkih, ni le vpraÅ¡anje praviÄnosti, temveÄ kaÅ¾e, da model ni vkljuÄujoÄ ali zanesljiv.

![Komponenta Analiza podatkov na nadzorni ploÅ¡Äi RAI](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Uporabite analizo podatkov, kadar potrebujete:

* Raziskovanje statistike vaÅ¡ega nabora podatkov z izbiro razliÄnih filtrov za razdelitev podatkov na razliÄne dimenzije (znane tudi kot kohorte).
* Razumevanje porazdelitve vaÅ¡ega nabora podatkov med razliÄnimi kohortami in skupinami znaÄilnosti.
* DoloÄitev, ali so vaÅ¡a odkritja, povezana s praviÄnostjo, analizo napak in vzroÄnostjo (pridobljena iz drugih komponent nadzorne ploÅ¡Äe), posledica porazdelitve vaÅ¡ega nabora podatkov.
* OdloÄitev, na katerih podroÄjih zbrati veÄ podatkov za zmanjÅ¡anje napak, ki izhajajo iz teÅ¾av z zastopanostjo, Å¡uma oznak, Å¡uma znaÄilnosti, pristranskosti oznak in podobnih dejavnikov.

## RazloÅ¾ljivost modela

Modeli strojnega uÄenja so pogosto obravnavani kot "Ärne Å¡katle". Razumevanje, katere kljuÄne znaÄilnosti podatkov vplivajo na napovedi modela, je lahko zahtevno. Pomembno je zagotoviti preglednost, zakaj model poda doloÄeno napoved. Na primer, Äe sistem UI napove, da je diabetiÄni pacient v nevarnosti ponovne hospitalizacije v manj kot 30 dneh, bi moral biti sposoben zagotoviti podporne podatke, ki so privedli do te napovedi. Imati podporne kazalnike podatkov prinaÅ¡a preglednost, ki pomaga zdravnikom ali bolniÅ¡nicam sprejemati dobro informirane odloÄitve. Poleg tega omogoÄanje razlage, zakaj je model podal doloÄeno napoved za posameznega pacienta, omogoÄa odgovornost v skladu z zdravstvenimi predpisi. Ko uporabljate modele strojnega uÄenja na naÄine, ki vplivajo na Å¾ivljenja ljudi, je kljuÄno razumeti in razloÅ¾iti, kaj vpliva na vedenje modela. RazloÅ¾ljivost in interpretacija modela pomagata odgovoriti na vpraÅ¡anja v scenarijih, kot so:

* Odpravljanje napak modela: Zakaj je moj model naredil to napako? Kako lahko izboljÅ¡am svoj model?
* Sodelovanje Älovek-UI: Kako lahko razumem in zaupam odloÄitvam modela?
* Skladnost s predpisi: Ali moj model izpolnjuje zakonske zahteve?

Komponenta Pomembnost znaÄilnosti na nadzorni ploÅ¡Äi RAI vam pomaga odpravljati napake in pridobiti celovito razumevanje, kako model podaja napovedi. Prav tako je uporabno orodje za strokovnjake za strojno uÄenje in odloÄevalce, da razloÅ¾ijo in pokaÅ¾ejo dokaze o znaÄilnostih, ki vplivajo na vedenje modela, za skladnost s predpisi. Nato lahko uporabniki raziskujejo tako globalne kot lokalne razlage, da preverijo, katere znaÄilnosti vplivajo na napovedi modela. Globalne razlage navajajo glavne znaÄilnosti, ki so vplivale na sploÅ¡no napoved modela. Lokalne razlage prikazujejo, katere znaÄilnosti so privedle do napovedi modela za posamezen primer. Sposobnost ocenjevanja lokalnih razlag je prav tako koristna pri odpravljanju napak ali reviziji doloÄenega primera, da bi bolje razumeli in interpretirali, zakaj je model podal pravilno ali napaÄno napoved.

![Komponenta Pomembnost znaÄilnosti na nadzorni ploÅ¡Äi RAI](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Globalne razlage: Na primer, katere znaÄilnosti vplivajo na sploÅ¡no vedenje modela za ponovno hospitalizacijo diabetikov?
* Lokalne razlage: Na primer, zakaj je bil diabetiÄni pacient, starejÅ¡i od 60 let, z veÄ prejÅ¡njimi hospitalizacijami napovedan, da bo ponovno hospitaliziran ali ne v 30 dneh?

V procesu odpravljanja napak pri pregledu uspeÅ¡nosti modela med razliÄnimi kohortami Pomembnost znaÄilnosti prikazuje, kakÅ¡en vpliv ima znaÄilnost na kohorte. Pomaga razkriti anomalije pri primerjavi ravni vpliva znaÄilnosti na napaÄne napovedi modela. Komponenta Pomembnost znaÄilnosti lahko pokaÅ¾e, katere vrednosti v znaÄilnosti so pozitivno ali negativno vplivale na izid modela. Na primer, Äe je model podal napaÄno napoved, komponenta omogoÄa podrobno analizo in prepoznavanje, katere znaÄilnosti ali vrednosti znaÄilnosti so vplivale na napoved. Ta raven podrobnosti pomaga ne le pri odpravljanju napak, temveÄ zagotavlja preglednost in odgovornost v revizijskih situacijah. Na koncu lahko komponenta pomaga prepoznati vpraÅ¡anja praviÄnosti. Na primer, Äe obÄutljiva znaÄilnost, kot je etniÄna pripadnost ali spol, moÄno vpliva na napoved modela, bi to lahko bil znak rasne ali spolne pristranskosti v modelu.

![Pomembnost znaÄilnosti](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Uporabite razloÅ¾ljivost, kadar potrebujete:

* DoloÄiti, kako zanesljive so napovedi vaÅ¡ega sistema UI, z razumevanjem, katere znaÄilnosti so najpomembnejÅ¡e za napovedi.
* Pristopiti k odpravljanju napak modela tako, da ga najprej razumete in ugotovite, ali model uporablja ustrezne znaÄilnosti ali zgolj napaÄne korelacije.
* Razkriti morebitne vire nepraviÄnosti z razumevanjem, ali model temelji na obÄutljivih znaÄilnostih ali na znaÄil
- **Prekomerna ali nezadostna zastopanost**. Ideja je, da doloÄena skupina ni vidna v doloÄenem poklicu, in vsaka storitev ali funkcija, ki to Å¡e naprej spodbuja, prispeva k Å¡kodi.

### Azure RAI nadzorna ploÅ¡Äa

[Azure RAI nadzorna ploÅ¡Äa](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) temelji na odprtokodnih orodjih, ki so jih razvile vodilne akademske institucije in organizacije, vkljuÄno z Microsoftom. Ta orodja so kljuÄna za podatkovne znanstvenike in razvijalce umetne inteligence, da bolje razumejo vedenje modelov, odkrijejo in odpravijo neÅ¾elene teÅ¾ave v modelih umetne inteligence.

- NauÄite se uporabljati razliÄne komponente tako, da si ogledate dokumentacijo nadzorne ploÅ¡Äe RAI [docs.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)

- Oglejte si nekaj primerov zvezkov nadzorne ploÅ¡Äe RAI [sample notebooks](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) za odpravljanje teÅ¾av pri bolj odgovornem scenariju umetne inteligence v Azure Machine Learning.

---
## ğŸš€ Izziv

Da bi prepreÄili uvajanje statistiÄnih ali podatkovnih pristranskosti, moramo:

- zagotoviti raznolikost ozadij in perspektiv med ljudmi, ki delajo na sistemih
- vlagati v podatkovne nabore, ki odraÅ¾ajo raznolikost naÅ¡e druÅ¾be
- razviti boljÅ¡e metode za zaznavanje in odpravljanje pristranskosti, ko se pojavi

Razmislite o resniÄnih scenarijih, kjer je nepoÅ¡tenost oÄitna pri gradnji in uporabi modelov. Kaj bi Å¡e morali upoÅ¡tevati?

## [Kvizi po predavanju](https://ff-quizzes.netlify.app/en/ml/)
## Pregled in samostojno uÄenje

V tej lekciji ste spoznali nekaj praktiÄnih orodij za vkljuÄevanje odgovorne umetne inteligence v strojno uÄenje.

Oglejte si ta delavnico za poglobitev v teme:

- Nadzorna ploÅ¡Äa odgovorne umetne inteligence: Celovita reÅ¡itev za operacionalizacijo RAI v praksi, avtorja Besmira Nushi in Mehrnoosh Sameki

[![Nadzorna ploÅ¡Äa odgovorne umetne inteligence: Celovita reÅ¡itev za operacionalizacijo RAI v praksi](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Nadzorna ploÅ¡Äa odgovorne umetne inteligence: Celovita reÅ¡itev za operacionalizacijo RAI v praksi")

> ğŸ¥ Kliknite zgornjo sliko za video: Nadzorna ploÅ¡Äa odgovorne umetne inteligence: Celovita reÅ¡itev za operacionalizacijo RAI v praksi, avtorja Besmira Nushi in Mehrnoosh Sameki

Oglejte si naslednje materiale, da se nauÄite veÄ o odgovorni umetni inteligenci in kako graditi bolj zaupanja vredne modele:

- Microsoftova orodja nadzorne ploÅ¡Äe RAI za odpravljanje teÅ¾av pri modelih strojnega uÄenja: [Viri orodij odgovorne umetne inteligence](https://aka.ms/rai-dashboard)

- RaziÅ¡Äite orodja odgovorne umetne inteligence: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsoftov center virov za RAI: [Viri odgovorne umetne inteligence â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsoftova raziskovalna skupina FATE: [FATE: PraviÄnost, odgovornost, transparentnost in etika v umetni inteligenci - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Naloga

[RaziÅ¡Äite nadzorno ploÅ¡Äo RAI](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.