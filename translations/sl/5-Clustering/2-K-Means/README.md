<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T12:19:11+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "sl"
}
-->
# K-Means grozdenje

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

V tej lekciji se boste nauÄili, kako ustvariti grozde z uporabo Scikit-learn in nigerijskega glasbenega nabora podatkov, ki ste ga uvozili prej. Pokrili bomo osnove K-Means za grozdenje. UpoÅ¡tevajte, da obstaja veliko naÄinov za delo z grozdi, kot ste se nauÄili v prejÅ¡nji lekciji, in metoda, ki jo uporabite, je odvisna od vaÅ¡ih podatkov. Poskusili bomo K-Means, saj je to najpogostejÅ¡a tehnika grozdenja. ZaÄnimo!

Pojmi, o katerih se boste uÄili:

- Silhuetno ocenjevanje
- Metoda komolca
- Inercija
- Varianca

## Uvod

[K-Means grozdenje](https://wikipedia.org/wiki/K-means_clustering) je metoda, ki izvira iz podroÄja obdelave signalov. Uporablja se za razdelitev in razvrÅ¡Äanje skupin podatkov v 'k' grozde z uporabo serije opazovanj. Vsako opazovanje deluje tako, da razvrsti doloÄeno podatkovno toÄko najbliÅ¾je njenemu 'povpreÄju' ali srediÅ¡Äni toÄki grozda.

Grozde je mogoÄe vizualizirati kot [Voronoijeve diagrame](https://wikipedia.org/wiki/Voronoi_diagram), ki vkljuÄujejo toÄko (ali 'seme') in njeno ustrezno regijo.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografika avtorice [Jen Looper](https://twitter.com/jenlooper)

Postopek K-Means grozdenja [poteka v treh korakih](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritem izbere k-Å¡tevilo srediÅ¡Änih toÄk z vzorÄenjem iz nabora podatkov. Nato se zanka nadaljuje:
    1. Vsak vzorec dodeli najbliÅ¾jemu centroidu.
    2. Ustvari nove centroide z izraÄunom povpreÄne vrednosti vseh vzorcev, dodeljenih prejÅ¡njim centroidom.
    3. Nato izraÄuna razliko med novimi in starimi centroidi ter ponavlja, dokler se centroidi ne stabilizirajo.

Ena od pomanjkljivosti uporabe K-Means je, da morate doloÄiti 'k', torej Å¡tevilo centroidov. Na sreÄo metoda 'komolca' pomaga oceniti dobro zaÄetno vrednost za 'k'. To boste poskusili Äez trenutek.

## Predpogoj

Delali boste v datoteki [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb), ki vkljuÄuje uvoz podatkov in predhodno ÄiÅ¡Äenje, ki ste ga opravili v prejÅ¡nji lekciji.

## Naloga - priprava

ZaÄnite tako, da ponovno pregledate podatke o pesmih.

1. Ustvarite boxplot za vsak stolpec z uporabo funkcije `boxplot()`:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Ti podatki so nekoliko hrupni: z opazovanjem vsakega stolpca kot boxplota lahko vidite odstopajoÄe vrednosti.

    ![outliers](../../../../5-Clustering/2-K-Means/images/boxplots.png)

Lahko bi pregledali nabor podatkov in odstranili te odstopajoÄe vrednosti, vendar bi to podatke precej zmanjÅ¡alo.

1. Za zdaj izberite, katere stolpce boste uporabili za nalogo grozdenja. Izberite tiste s podobnimi razponi in kodirajte stolpec `artist_top_genre` kot numeriÄne podatke:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Zdaj morate izbrati, koliko grozdov boste ciljali. Veste, da obstajajo 3 glasbeni Å¾anri, ki smo jih izluÅ¡Äili iz nabora podatkov, zato poskusimo s 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Vidite natisnjen niz s predvidenimi grozdi (0, 1 ali 2) za vsako vrstico podatkovnega okvira.

1. Uporabite ta niz za izraÄun 'silhuetne ocene':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhuetna ocena

IÅ¡Äite silhuetno oceno bliÅ¾je 1. Ta ocena se giblje od -1 do 1, in Äe je ocena 1, je grozd gost in dobro loÄen od drugih grozdov. Vrednost blizu 0 predstavlja prekrivajoÄe se grozde z vzorci, ki so zelo blizu odloÄitveni meji sosednjih grozdov. [(Vir)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

NaÅ¡a ocena je **0,53**, torej nekje na sredini. To kaÅ¾e, da naÅ¡i podatki niso posebej primerni za to vrsto grozdenja, vendar nadaljujmo.

### Naloga - izdelava modela

1. Uvozite `KMeans` in zaÄnite postopek grozdenja.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Tukaj je nekaj delov, ki jih je vredno pojasniti.

    > ğŸ“ range: To so iteracije postopka grozdenja.

    > ğŸ“ random_state: "DoloÄa generacijo nakljuÄnih Å¡tevil za inicializacijo centroidov." [Vir](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > ğŸ“ WCSS: "vsota kvadratov znotraj grozda" meri povpreÄno kvadratno razdaljo vseh toÄk znotraj grozda do centroida grozda. [Vir](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > ğŸ“ Inercija: Algoritmi K-Means poskuÅ¡ajo izbrati centroide, da minimizirajo 'inercijo', "merilo, kako notranje koherentni so grozdi." [Vir](https://scikit-learn.org/stable/modules/clustering.html). Vrednost se doda spremenljivki wcss pri vsaki iteraciji.

    > ğŸ“ k-means++: V [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) lahko uporabite optimizacijo 'k-means++', ki "inicializira centroide tako, da so (na sploÅ¡no) oddaljeni drug od drugega, kar vodi do verjetno boljÅ¡ih rezultatov kot nakljuÄna inicializacija."

### Metoda komolca

Prej ste domnevali, da bi morali izbrati 3 grozde, ker ste ciljali 3 glasbene Å¾anre. Je to res?

1. Uporabite metodo 'komolca', da se prepriÄate.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Uporabite spremenljivko `wcss`, ki ste jo ustvarili v prejÅ¡njem koraku, da ustvarite graf, ki prikazuje, kje je 'pregib' v komolcu, kar oznaÄuje optimalno Å¡tevilo grozdov. Morda je **res** 3!

    ![elbow method](../../../../5-Clustering/2-K-Means/images/elbow.png)

## Naloga - prikaz grozdov

1. Poskusite postopek znova, tokrat nastavite tri grozde in prikaÅ¾ite grozde kot razprÅ¡en diagram:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Preverite natanÄnost modela:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    NatanÄnost tega modela ni zelo dobra, oblika grozdov pa vam daje namig, zakaj.

    ![clusters](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Ti podatki so preveÄ neuravnoteÅ¾eni, premalo korelirani in med vrednostmi stolpcev je preveÄ variance, da bi jih dobro razvrstili v grozde. Pravzaprav so grozdi, ki se oblikujejo, verjetno moÄno vplivani ali pristranski zaradi treh kategorij Å¾anrov, ki smo jih opredelili zgoraj. To je bil uÄni proces!

    V dokumentaciji Scikit-learn lahko vidite, da ima model, kot je ta, z grozdi, ki niso dobro razmejeni, teÅ¾avo z 'varianco':

    ![problem models](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografika iz Scikit-learn

## Varianca

Varianca je definirana kot "povpreÄje kvadratnih razlik od povpreÄja" [(Vir)](https://www.mathsisfun.com/data/standard-deviation.html). V kontekstu tega problema grozdenja se nanaÅ¡a na podatke, pri katerih se Å¡tevilke naÅ¡ega nabora podatkov preveÄ oddaljujejo od povpreÄja.

âœ… To je odliÄen trenutek, da razmislite o vseh naÄinih, kako bi lahko odpravili to teÅ¾avo. Bi Å¡e malo prilagodili podatke? Uporabili druge stolpce? Uporabili drugaÄen algoritem? Namig: Poskusite [normalizirati podatke](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) in preizkusiti druge stolpce.

> Poskusite ta '[kalkulator variance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)', da bolje razumete koncept.

---

## ğŸš€Izziv

PreÅ¾ivite nekaj Äasa s tem zvezkom in prilagodite parametre. Ali lahko izboljÅ¡ate natanÄnost modela z dodatnim ÄiÅ¡Äenjem podatkov (na primer odstranjevanjem odstopajoÄih vrednosti)? Lahko uporabite uteÅ¾i, da doloÄene vzorce podatkov bolj poudarite. Kaj Å¡e lahko storite, da ustvarite boljÅ¡e grozde?

Namig: Poskusite normalizirati podatke. V zvezku je komentirana koda, ki dodaja standardno skaliranje, da stolpci podatkov bolj spominjajo drug na drugega glede na razpon. Ugotovili boste, da se silhuetna ocena zniÅ¾a, vendar se 'pregib' v grafu komolca zgladi. To je zato, ker neobdelani podatki omogoÄajo, da podatki z manj varianco nosijo veÄjo teÅ¾o. Preberite veÄ o tej teÅ¾avi [tukaj](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Pregled in samostojno uÄenje

Oglejte si simulator K-Means [kot je ta](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). S tem orodjem lahko vizualizirate vzorÄne podatkovne toÄke in doloÄite njihove centroide. Lahko urejate nakljuÄnost podatkov, Å¡tevilo grozdov in Å¡tevilo centroidov. Ali vam to pomaga pridobiti predstavo o tem, kako je mogoÄe podatke razvrstiti?

Prav tako si oglejte [ta priroÄnik o K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) s Stanforda.

## Naloga

[Preizkusite razliÄne metode grozdenja](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne prevzemamo odgovornosti za morebitna napaÄna razumevanja ali napaÄne interpretacije, ki bi nastale zaradi uporabe tega prevoda.