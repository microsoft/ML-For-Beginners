<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T12:38:13+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "sl"
}
-->
# Gradnja reÅ¡itev strojnega uÄenja z odgovorno umetno inteligenco

![Povzetek odgovorne umetne inteligence v strojnem uÄenju v sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote avtorja [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ml/)

## Uvod

V tem uÄnem naÄrtu boste zaÄeli odkrivati, kako strojno uÄenje vpliva na naÅ¡e vsakdanje Å¾ivljenje. Å½e zdaj so sistemi in modeli vkljuÄeni v vsakodnevne odloÄitve, kot so zdravstvene diagnoze, odobritve posojil ali odkrivanje goljufij. Zato je pomembno, da ti modeli delujejo zanesljivo in zagotavljajo rezultate, ki jim lahko zaupamo. Tako kot pri vsaki programski aplikaciji tudi sistemi umetne inteligence vÄasih ne izpolnijo priÄakovanj ali privedejo do neÅ¾elenih rezultatov. Zato je kljuÄno razumeti in pojasniti vedenje modela umetne inteligence.

Predstavljajte si, kaj se lahko zgodi, Äe podatki, ki jih uporabljate za gradnjo teh modelov, ne vkljuÄujejo doloÄenih demografskih skupin, kot so rasa, spol, politiÄno prepriÄanje, religija, ali pa so te skupine nesorazmerno zastopane. Kaj pa, Äe je izhod modela interpretiran tako, da favorizira doloÄeno demografsko skupino? KakÅ¡ne so posledice za aplikacijo? Poleg tega, kaj se zgodi, Äe model povzroÄi Å¡kodljive rezultate? Kdo je odgovoren za vedenje sistema umetne inteligence? To so nekatera vpraÅ¡anja, ki jih bomo raziskali v tem uÄnem naÄrtu.

V tej lekciji boste:

- PoveÄali zavedanje o pomembnosti praviÄnosti v strojnem uÄenju in Å¡kodah, povezanih s praviÄnostjo.
- Spoznali prakso raziskovanja odstopanj in nenavadnih scenarijev za zagotavljanje zanesljivosti in varnosti.
- Pridobili razumevanje o potrebi po vkljuÄevanju vseh z oblikovanjem inkluzivnih sistemov.
- Raziskali, kako pomembno je varovati zasebnost in varnost podatkov ter ljudi.
- Spoznali pomen pristopa "steklene Å¡katle" za pojasnjevanje vedenja modelov umetne inteligence.
- Postali pozorni na to, kako je odgovornost kljuÄna za gradnjo zaupanja v sisteme umetne inteligence.

## Predpogoj

Kot predpogoj si oglejte uÄni naÄrt "NaÄela odgovorne umetne inteligence" in si oglejte spodnji video na to temo:

VeÄ o odgovorni umetni inteligenci lahko izveste s sledenjem temu [uÄnemu naÄrtu](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott).

[![Microsoftov pristop k odgovorni umetni inteligenci](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsoftov pristop k odgovorni umetni inteligenci")

> ğŸ¥ Kliknite zgornjo sliko za video: Microsoftov pristop k odgovorni umetni inteligenci

## PraviÄnost

Sistemi umetne inteligence bi morali obravnavati vse enako in se izogibati razliÄnemu vplivu na podobne skupine ljudi. Na primer, ko sistemi umetne inteligence svetujejo pri medicinskem zdravljenju, proÅ¡njah za posojila ali zaposlitvi, bi morali podati enaka priporoÄila vsem z enakimi simptomi, finanÄnimi okoliÅ¡Äinami ali poklicnimi kvalifikacijami. Vsak od nas kot Älovek nosi podedovane pristranskosti, ki vplivajo na naÅ¡e odloÄitve in dejanja. Te pristranskosti so lahko vidne v podatkih, ki jih uporabljamo za treniranje sistemov umetne inteligence. TakÅ¡na manipulacija se vÄasih zgodi nenamerno. Pogosto je teÅ¾ko zavestno vedeti, kdaj vnaÅ¡ate pristranskost v podatke.

**"NepraviÄnost"** zajema negativne vplive ali "Å¡kode" za skupino ljudi, kot so tiste, opredeljene glede na raso, spol, starost ali status invalidnosti. Glavne Å¡kode, povezane s praviÄnostjo, lahko razvrstimo kot:

- **Dodeljevanje**, Äe je na primer spol ali etniÄna pripadnost favorizirana pred drugo.
- **Kakovost storitve**. ÄŒe trenirate podatke za en specifiÄen scenarij, vendar je resniÄnost veliko bolj kompleksna, to vodi do slabo delujoÄe storitve. Na primer, podajalnik mila, ki ne zazna ljudi s temno koÅ¾o. [Referenca](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **OmalovaÅ¾evanje**. NepraviÄno kritiziranje in oznaÄevanje neÄesa ali nekoga. Na primer, tehnologija za oznaÄevanje slik je zloglasno napaÄno oznaÄila slike temnopolte ljudi kot gorile.
- **Prekomerna ali nezadostna zastopanost**. Ideja, da doloÄena skupina ni vidna v doloÄenem poklicu, in vsaka storitev ali funkcija, ki to Å¡e naprej promovira, prispeva k Å¡kodi.
- **Stereotipiziranje**. Povezovanje doloÄene skupine s predhodno doloÄenimi lastnostmi. Na primer, sistem za prevajanje med angleÅ¡Äino in turÅ¡Äino lahko vsebuje netoÄnosti zaradi besed s stereotipnimi povezavami s spolom.

![prevod v turÅ¡Äino](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> prevod v turÅ¡Äino

![prevod nazaj v angleÅ¡Äino](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> prevod nazaj v angleÅ¡Äino

Pri oblikovanju in testiranju sistemov umetne inteligence moramo zagotoviti, da je umetna inteligenca praviÄna in ni programirana za sprejemanje pristranskih ali diskriminatornih odloÄitev, ki jih ljudje prav tako ne smejo sprejemati. Zagotavljanje praviÄnosti v umetni inteligenci in strojnem uÄenju ostaja kompleksen sociotehniÄni izziv.

### Zanesljivost in varnost

Za gradnjo zaupanja morajo biti sistemi umetne inteligence zanesljivi, varni in dosledni v obiÄajnih in nepriÄakovanih pogojih. Pomembno je vedeti, kako se bodo sistemi umetne inteligence obnaÅ¡ali v razliÄnih situacijah, Å¡e posebej, ko gre za odstopanja. Pri gradnji reÅ¡itev umetne inteligence je treba nameniti veliko pozornosti temu, kako obravnavati Å¡irok spekter okoliÅ¡Äin, s katerimi se lahko sreÄajo reÅ¡itve umetne inteligence. Na primer, avtonomni avtomobil mora postaviti varnost ljudi kot najviÅ¡jo prioriteto. PoslediÄno mora umetna inteligenca, ki poganja avtomobil, upoÅ¡tevati vse moÅ¾ne scenarije, s katerimi se lahko avtomobil sreÄa, kot so noÄ, nevihte ali sneÅ¾ni meteÅ¾i, otroci, ki teÄejo Äez cesto, hiÅ¡ni ljubljenÄki, gradbena dela itd. Kako dobro sistem umetne inteligence obravnava Å¡irok spekter pogojev zanesljivo in varno, odraÅ¾a raven predvidevanja, ki jo je podatkovni znanstvenik ali razvijalec umetne inteligence upoÅ¡teval med oblikovanjem ali testiranjem sistema.

> [ğŸ¥ Kliknite tukaj za video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inkluzivnost

Sistemi umetne inteligence bi morali biti zasnovani tako, da vkljuÄujejo in opolnomoÄijo vse. Pri oblikovanju in implementaciji sistemov umetne inteligence podatkovni znanstveniki in razvijalci umetne inteligence identificirajo in obravnavajo morebitne ovire v sistemu, ki bi lahko nenamerno izkljuÄile ljudi. Na primer, po svetu je 1 milijarda ljudi z invalidnostmi. Z napredkom umetne inteligence lahko dostopajo do Å¡irokega spektra informacij in priloÅ¾nosti bolj enostavno v svojem vsakdanjem Å¾ivljenju. Z odpravljanjem ovir se ustvarjajo priloÅ¾nosti za inovacije in razvoj izdelkov umetne inteligence z boljÅ¡imi izkuÅ¡njami, ki koristijo vsem.

> [ğŸ¥ Kliknite tukaj za video: inkluzivnost v umetni inteligenci](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Varnost in zasebnost

Sistemi umetne inteligence bi morali biti varni in spoÅ¡tovati zasebnost ljudi. Ljudje manj zaupajo sistemom, ki ogroÅ¾ajo njihovo zasebnost, informacije ali Å¾ivljenja. Pri treniranju modelov strojnega uÄenja se zanaÅ¡amo na podatke za doseganje najboljÅ¡ih rezultatov. Pri tem je treba upoÅ¡tevati izvor podatkov in njihovo integriteto. Na primer, ali so podatki uporabniÅ¡ko posredovani ali javno dostopni? Poleg tega je med delom s podatki kljuÄno razviti sisteme umetne inteligence, ki lahko zaÅ¡Äitijo zaupne informacije in se uprejo napadom. Ker umetna inteligenca postaja vse bolj razÅ¡irjena, postaja zaÅ¡Äita zasebnosti in varnost pomembnih osebnih ter poslovnih informacij vse bolj kritiÄna in kompleksna. Zasebnost in varnost podatkov zahtevata posebno pozornost pri umetni inteligenci, saj je dostop do podatkov bistven za to, da sistemi umetne inteligence sprejemajo natanÄne in informirane napovedi ter odloÄitve o ljudeh.

> [ğŸ¥ Kliknite tukaj za video: varnost v umetni inteligenci](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Kot industrija smo dosegli pomemben napredek na podroÄju zasebnosti in varnosti, ki ga moÄno spodbujajo regulacije, kot je GDPR (SploÅ¡na uredba o varstvu podatkov).
- Kljub temu moramo pri sistemih umetne inteligence priznati napetost med potrebo po veÄ osebnih podatkih za izboljÅ¡anje uÄinkovitosti sistemov in zasebnostjo.
- Tako kot ob rojstvu povezanih raÄunalnikov z internetom, opaÅ¾amo tudi velik porast Å¡tevila varnostnih teÅ¾av, povezanih z umetno inteligenco.
- Hkrati pa opaÅ¾amo, da se umetna inteligenca uporablja za izboljÅ¡anje varnosti. Na primer, veÄina sodobnih protivirusnih skenerjev temelji na AI heuristiki.
- Zagotoviti moramo, da se naÅ¡i procesi podatkovne znanosti harmoniÄno prepletajo z najnovejÅ¡imi praksami zasebnosti in varnosti.

### Transparentnost

Sistemi umetne inteligence bi morali biti razumljivi. KljuÄni del transparentnosti je pojasnjevanje vedenja sistemov umetne inteligence in njihovih komponent. IzboljÅ¡anje razumevanja sistemov umetne inteligence zahteva, da deleÅ¾niki razumejo, kako in zakaj delujejo, da lahko identificirajo morebitne teÅ¾ave z zmogljivostjo, varnostjo in zasebnostjo, pristranskosti, izkljuÄujoÄe prakse ali nenamerne rezultate. Prav tako verjamemo, da bi morali biti tisti, ki uporabljajo sisteme umetne inteligence, iskreni in odkriti glede tega, kdaj, zakaj in kako se odloÄijo za njihovo uporabo. Prav tako morajo pojasniti omejitve sistemov, ki jih uporabljajo. Na primer, Äe banka uporablja sistem umetne inteligence za podporo pri odloÄanju o potroÅ¡niÅ¡kih posojilih, je pomembno preuÄiti rezultate in razumeti, kateri podatki vplivajo na priporoÄila sistema. Vlade zaÄenjajo regulirati umetno inteligenco v razliÄnih industrijah, zato morajo podatkovni znanstveniki in organizacije pojasniti, ali sistem umetne inteligence izpolnjuje regulativne zahteve, Å¡e posebej, ko pride do neÅ¾elenega rezultata.

> [ğŸ¥ Kliknite tukaj za video: transparentnost v umetni inteligenci](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Ker so sistemi umetne inteligence tako kompleksni, je teÅ¾ko razumeti, kako delujejo in interpretirati rezultate.
- To pomanjkanje razumevanja vpliva na naÄin upravljanja, operacionalizacije in dokumentiranja teh sistemov.
- Å e pomembneje pa to pomanjkanje razumevanja vpliva na odloÄitve, sprejete na podlagi rezultatov, ki jih ti sistemi proizvajajo.

### Odgovornost

Ljudje, ki oblikujejo in uvajajo sisteme umetne inteligence, morajo biti odgovorni za delovanje svojih sistemov. Potreba po odgovornosti je Å¡e posebej kljuÄna pri obÄutljivih tehnologijah, kot je prepoznavanje obrazov. V zadnjem Äasu se poveÄuje povpraÅ¡evanje po tehnologiji prepoznavanja obrazov, zlasti s strani organov pregona, ki vidijo potencial tehnologije pri uporabi, kot je iskanje pogreÅ¡anih otrok. Vendar pa bi te tehnologije lahko potencialno uporabila vlada za ogroÅ¾anje temeljnih svoboÅ¡Äin svojih drÅ¾avljanov, na primer z omogoÄanjem neprekinjenega nadzora doloÄenih posameznikov. Zato morajo podatkovni znanstveniki in organizacije prevzeti odgovornost za to, kako njihov sistem umetne inteligence vpliva na posameznike ali druÅ¾bo.

[![Vodilni raziskovalec umetne inteligence opozarja na mnoÅ¾iÄni nadzor prek prepoznavanja obrazov](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsoftov pristop k odgovorni umetni inteligenci")

> ğŸ¥ Kliknite zgornjo sliko za video: Opozorila o mnoÅ¾iÄnem nadzoru prek prepoznavanja obrazov

Na koncu je eno najveÄjih vpraÅ¡anj naÅ¡e generacije, kot prve generacije, ki prinaÅ¡a umetno inteligenco v druÅ¾bo, kako zagotoviti, da bodo raÄunalniki ostali odgovorni ljudem in kako zagotoviti, da bodo ljudje, ki oblikujejo raÄunalnike, ostali odgovorni vsem drugim.

## Ocena vpliva

Pred treniranjem modela strojnega uÄenja je pomembno izvesti oceno vpliva, da razumemo namen sistema umetne inteligence; kakÅ¡na je predvidena uporaba; kje bo uveden; in kdo bo interagiral s sistemom. To je koristno za pregledovalce ali preizkuÅ¡evalce, ki ocenjujejo sistem, da vedo, katere dejavnike je treba upoÅ¡tevati pri prepoznavanju morebitnih tveganj in priÄakovanih posledic.

Naslednja podroÄja so v ospredju pri izvajanju ocene vpliva:

* **Negativen vpliv na posameznike**. Zavedanje o kakrÅ¡nih koli omejitvah ali zahtevah, nepodprti uporabi ali znanih omejitvah, ki ovirajo delovanje sistema, je kljuÄno za zagotovitev, da sistem ni uporabljen na naÄin, ki bi lahko Å¡kodoval posameznikom.
* **Zahteve glede podatkov**. Razumevanje, kako in kje bo sistem uporabljal podatke, omogoÄa pregledovalcem, da raziÅ¡Äejo morebitne zahteve glede podatkov, na katere morate biti pozorni (npr. GDPR ali HIPPA regulacije podatkov). Poleg tega preuÄite, ali je vir ali koliÄina podatkov zadostna za treniranje.
* **Povzetek vpliva**. Zberite seznam morebitnih Å¡kod, ki bi lahko nastale zaradi uporabe sistema. Skozi Å¾ivljenjski cikel strojnega uÄenja preverite, ali so identificirane teÅ¾ave ublaÅ¾ene ali obravnavane.
* **Ustrezni cilji** za vsako od Å¡estih temeljnih naÄel. Ocenite, ali so cilji iz vsakega naÄela doseÅ¾eni in ali obstajajo kakrÅ¡ne koli vrzeli.

## Odpravljanje napak z odgovorno umetno inteligenco

Podobno kot odpravljanje napak v programski aplikaciji je odpravljanje napak v sistemu umetne inteligence nujen proces prepoznavanja in reÅ¡evanja teÅ¾av v sistemu. Obstaja veliko dejavnikov, ki lahko vplivajo na to, da model ne deluje, kot je priÄakovano ali odgovorno. VeÄina tradicionalnih metrik zmogljivosti modela so kvantitativni agregati zmogljivosti modela, ki niso dovolj za analizo, kako model krÅ¡i naÄela odgovorne umetne inteligence. Poleg tega je model strojnega uÄenja Ärna Å¡katla, kar oteÅ¾uje razumevanje, kaj vodi do njegovih rezultatov ali zagotavljanje pojasnil, ko naredi napako. Kasneje v tem teÄaju se bomo nauÄili, kako uporabljati nadzorno ploÅ¡Äo odgovorne umetne inteligence za pomoÄ pri odpravljanju napak v sistemih umetne inteligence. Nadzorna ploÅ¡Äa zagotavlja celovit pripomoÄek za podatkovne znanstvenike in razvijalce umetne inteligence za izvajanje:

* **Analiza napak**. Za prepoznavanje porazdelitve napak modela, ki lahko vplivajo na praviÄnost ali zanesljivost sistema.
* **Pregled modela**. Za odkrivanje, kje obstajajo razlike v zmogljivosti modela med podatkovnimi skupinami.
* **Analiza podatkov**. Za razumevanje porazdelitve podatkov in prepoznavanje morebitne pristranskosti v podatkih, ki bi lahko povzroÄila teÅ¾ave s pravi
Oglejte si ta delavnico za poglobitev v teme:

- Na poti do odgovorne umetne inteligence: Prenos naÄel v prakso, avtorji Besmira Nushi, Mehrnoosh Sameki in Amit Sharma

[![Responsible AI Toolbox: Odprtokodni okvir za gradnjo odgovorne umetne inteligence](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Odprtokodni okvir za gradnjo odgovorne umetne inteligence")

> ğŸ¥ Kliknite zgornjo sliko za video: RAI Toolbox: Odprtokodni okvir za gradnjo odgovorne umetne inteligence, avtorji Besmira Nushi, Mehrnoosh Sameki in Amit Sharma

Preberite tudi:

- Microsoftov center virov za RAI: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsoftova raziskovalna skupina FATE: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [GitHub repozitorij Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Preberite o orodjih Azure Machine Learning za zagotavljanje praviÄnosti:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Naloga

[RaziÅ¡Äite RAI Toolbox](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.