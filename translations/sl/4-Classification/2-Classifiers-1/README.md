<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T13:05:40+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "sl"
}
-->
# RazvrÅ¡Äevalniki kuhinj 1

V tej lekciji boste uporabili podatkovni niz, ki ste ga shranili v prejÅ¡nji lekciji, poln uravnoteÅ¾enih in Äistih podatkov o kuhinjah.

Ta podatkovni niz boste uporabili z razliÄnimi razvrÅ¡Äevalniki, da _napoveste doloÄeno nacionalno kuhinjo na podlagi skupine sestavin_. Med tem boste spoznali veÄ o tem, kako lahko algoritme uporabimo za naloge razvrÅ¡Äanja.

## [Predlekcijski kviz](https://ff-quizzes.netlify.app/en/ml/)
# Priprava

ÄŒe ste zakljuÄili [Lekcijo 1](../1-Introduction/README.md), preverite, ali datoteka _cleaned_cuisines.csv_ obstaja v korenskem imeniku `/data` za te Å¡tiri lekcije.

## Vaja - napoved nacionalne kuhinje

1. V mapi _notebook.ipynb_ te lekcije uvozite to datoteko skupaj s knjiÅ¾nico Pandas:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Podatki izgledajo takole:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Zdaj uvozite Å¡e nekaj knjiÅ¾nic:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Razdelite koordinate X in y v dva podatkovna okvira za uÄenje. `cuisine` lahko uporabite kot podatkovni okvir z oznakami:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Videti bo takole:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. Odstranite stolpca `Unnamed: 0` in `cuisine` z uporabo funkcije `drop()`. Preostale podatke shranite kot znaÄilnosti za uÄenje:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    VaÅ¡e znaÄilnosti izgledajo takole:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Zdaj ste pripravljeni na uÄenje modela!

## Izbira razvrÅ¡Äevalnika

Zdaj, ko so vaÅ¡i podatki Äisti in pripravljeni za uÄenje, morate izbrati algoritem za nalogo.

Scikit-learn razvrÅ¡Äa razvrÅ¡Äanje pod Nadzorovano uÄenje, v tej kategoriji pa najdete veliko naÄinov za razvrÅ¡Äanje. [Raznolikost](https://scikit-learn.org/stable/supervised_learning.html) je na prvi pogled precej osupljiva. Naslednje metode vkljuÄujejo tehnike razvrÅ¡Äanja:

- Linearni modeli
- Podporni vektorski stroji
- StohastiÄni gradientni spust
- NajbliÅ¾ji sosedje
- Gaussovi procesi
- OdloÄitvena drevesa
- Metode ansambla (glasovalni razvrÅ¡Äevalnik)
- VeÄrazredni in veÄizhodni algoritmi (veÄrazredna in veÄoznaÄna razvrstitev, veÄrazredna-veÄizhodna razvrstitev)

> Za razvrÅ¡Äanje podatkov lahko uporabite tudi [nevronske mreÅ¾e](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), vendar to presega obseg te lekcije.

### Kateri razvrÅ¡Äevalnik izbrati?

Torej, kateri razvrÅ¡Äevalnik izbrati? Pogosto je smiselno preizkusiti veÄ razvrÅ¡Äevalnikov in iskati najboljÅ¡i rezultat. Scikit-learn ponuja [primerjavo](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) na ustvarjenem podatkovnem nizu, kjer primerja KNeighbors, SVC na dva naÄina, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB in QuadraticDiscriminationAnalysis, pri Äemer so rezultati vizualizirani:

![primerjava razvrÅ¡Äevalnikov](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Grafi, ustvarjeni v dokumentaciji Scikit-learn

> AutoML to teÅ¾avo elegantno reÅ¡i tako, da te primerjave izvaja v oblaku, kar vam omogoÄa izbiro najboljÅ¡ega algoritma za vaÅ¡e podatke. Preizkusite ga [tukaj](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### BoljÅ¡i pristop

BoljÅ¡i naÄin kot nakljuÄno ugibanje je, da sledite idejam na tem prenosljivem [ML plonk listu](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). Tukaj ugotovimo, da imamo za naÅ¡o veÄrazredno teÅ¾avo nekaj moÅ¾nosti:

![plonk list za veÄrazredne teÅ¾ave](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> Del Microsoftovega plonk lista algoritmov, ki podrobno opisuje moÅ¾nosti za veÄrazredno razvrÅ¡Äanje

âœ… Prenesite ta plonk list, natisnite ga in obesite na steno!

### RazmiÅ¡ljanje

Poglejmo, ali lahko z razmiÅ¡ljanjem izberemo razliÄne pristope glede na omejitve, ki jih imamo:

- **Nevronske mreÅ¾e so preteÅ¾ke**. Glede na naÅ¡ Äist, a minimalen podatkovni niz in dejstvo, da izvajamo uÄenje lokalno prek beleÅ¾k, so nevronske mreÅ¾e preteÅ¾ke za to nalogo.
- **Ni razvrÅ¡Äevalnika za dva razreda**. Ne uporabljamo razvrÅ¡Äevalnika za dva razreda, zato izkljuÄimo one-vs-all.
- **OdloÄitveno drevo ali logistiÄna regresija bi lahko delovala**. OdloÄitveno drevo bi lahko delovalo, prav tako logistiÄna regresija za veÄrazredne podatke.
- **VeÄrazredna izboljÅ¡ana odloÄitvena drevesa reÅ¡ujejo drugaÄen problem**. VeÄrazredna izboljÅ¡ana odloÄitvena drevesa so najbolj primerna za neparametriÄne naloge, npr. naloge za ustvarjanje razvrstitev, zato za nas niso uporabna.

### Uporaba Scikit-learn 

Za analizo podatkov bomo uporabili Scikit-learn. Vendar pa obstaja veliko naÄinov za uporabo logistiÄne regresije v Scikit-learn. Oglejte si [parametre za nastavitev](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

V bistvu sta dva pomembna parametra - `multi_class` in `solver` - ki ju moramo doloÄiti, ko Scikit-learn prosimo za izvedbo logistiÄne regresije. Vrednost `multi_class` doloÄa doloÄeno vedenje. Vrednost `solver` doloÄa, kateri algoritem uporabiti. Vsi reÅ¡evalci ne morejo biti zdruÅ¾eni z vsemi vrednostmi `multi_class`.

Po dokumentaciji, v primeru veÄrazredne naloge, algoritem za uÄenje:

- **Uporablja shemo one-vs-rest (OvR)**, Äe je moÅ¾nost `multi_class` nastavljena na `ovr`
- **Uporablja izgubo navzkriÅ¾ne entropije**, Äe je moÅ¾nost `multi_class` nastavljena na `multinomial`. (Trenutno moÅ¾nost `multinomial` podpirajo samo reÅ¡evalci â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™ in â€˜newton-cgâ€™.)

> ğŸ“ 'Shema' tukaj je lahko 'ovr' (one-vs-rest) ali 'multinomial'. Ker je logistiÄna regresija zasnovana za podporo binarni razvrstitvi, ji te sheme omogoÄajo boljÅ¡e obravnavanje veÄrazrednih nalog razvrÅ¡Äanja. [vir](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> ğŸ“ 'Solver' je definiran kot "algoritem za uporabo pri optimizacijskem problemu". [vir](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn ponuja to tabelo za razlago, kako reÅ¡evalci obravnavajo razliÄne izzive, ki jih predstavljajo razliÄne vrste podatkovnih struktur:

![reÅ¡evalci](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## Vaja - razdelitev podatkov

OsredotoÄimo se na logistiÄno regresijo za naÅ¡ prvi poskus uÄenja, saj ste se o njej nedavno uÄili v prejÅ¡nji lekciji.
Razdelite svoje podatke v skupine za uÄenje in testiranje z uporabo funkcije `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## Vaja - uporaba logistiÄne regresije

Ker uporabljate veÄrazredno nalogo, morate izbrati, katero _shemo_ uporabiti in kateri _reÅ¡evalec_ nastaviti. Uporabite LogisticRegression z veÄrazredno nastavitvijo in reÅ¡evalcem **liblinear** za uÄenje.

1. Ustvarite logistiÄno regresijo z `multi_class` nastavljeno na `ovr` in reÅ¡evalcem nastavljenim na `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    âœ… Preizkusite drugega reÅ¡evalca, kot je `lbfgs`, ki je pogosto nastavljen kot privzet.
> UpoÅ¡tevajte, uporabite funkcijo Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) za sploÅ¡Äitev vaÅ¡ih podatkov, kadar je to potrebno.
NatanÄnost je dobra pri veÄ kot **80%**!

1. Ta model lahko preizkusite z eno vrstico podatkov (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Rezultat se izpiÅ¡e:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   âœ… Poskusite z drugo Å¡tevilko vrstice in preverite rezultate.

1. ÄŒe Å¾elite podrobneje raziskati, lahko preverite natanÄnost te napovedi:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Rezultat se izpiÅ¡e - indijska kuhinja je najboljÅ¡a ugotovitev z dobro verjetnostjo:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    âœ… Ali lahko razloÅ¾ite, zakaj je model precej prepriÄan, da gre za indijsko kuhinjo?

1. Pridobite veÄ podrobnosti z izpisom poroÄila o klasifikaciji, kot ste to storili pri lekcijah o regresiji:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | precision | recall | f1-score | support |
    | ------------ | --------- | ------ | -------- | ------- |
    | chinese      | 0.73      | 0.71   | 0.72     | 229     |
    | indian       | 0.91      | 0.93   | 0.92     | 254     |
    | japanese     | 0.70      | 0.75   | 0.72     | 220     |
    | korean       | 0.86      | 0.76   | 0.81     | 242     |
    | thai         | 0.79      | 0.85   | 0.82     | 254     |
    | accuracy     | 0.80      | 1199   |          |         |
    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |
    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |

## ğŸš€Izziv

V tej lekciji ste uporabili oÄiÅ¡Äene podatke za izdelavo modela strojnega uÄenja, ki lahko napove nacionalno kuhinjo na podlagi serije sestavin. Vzemite si Äas in preberite Å¡tevilne moÅ¾nosti, ki jih Scikit-learn ponuja za klasifikacijo podatkov. Podrobneje raziÅ¡Äite koncept 'solverja', da boste razumeli, kaj se dogaja v ozadju.

## [Kvizi po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Pregled & Samostojno uÄenje

Podrobneje raziÅ¡Äite matematiko za logistiÄno regresijo v [tej lekciji](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)
## Naloga 

[PreuÄite solverje](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.