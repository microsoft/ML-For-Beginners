<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T13:32:10+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "sl"
}
-->
# Uvod v u캜enje z okrepitvami

U캜enje z okrepitvami, RL, velja za enega osnovnih paradigm strojnega u캜enja, poleg nadzorovanega in nenadzorovanega u캜enja. RL se osredoto캜a na sprejemanje odlo캜itev: sprejemanje pravih odlo캜itev ali vsaj u캜enje iz njih.

Predstavljajte si simulirano okolje, kot je borza. Kaj se zgodi, 캜e uvedete dolo캜eno regulacijo? Ali ima pozitiven ali negativen u캜inek? 캛e se zgodi nekaj negativnega, morate to _negativno okrepitev_ uporabiti, se iz nje nau캜iti in spremeniti smer. 캛e je rezultat pozitiven, morate graditi na tej _pozitivni okrepitvi_.

![Peter in volk](../../../8-Reinforcement/images/peter.png)

> Peter in njegovi prijatelji morajo pobegniti la캜nemu volku! Slika: [Jen Looper](https://twitter.com/jenlooper)

## Regionalna tema: Peter in volk (Rusija)

[Peter in volk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) je glasbena pravljica, ki jo je napisal ruski skladatelj [Sergej Prokofjev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Gre za zgodbo o mladem pionirju Petru, ki pogumno zapusti svojo hi코o in se odpravi na gozdno jaso, da bi ujel volka. V tem poglavju bomo trenirali algoritme strojnega u캜enja, ki bodo Petru pomagali:

- **Raziskovati** okolico in zgraditi optimalen navigacijski zemljevid
- **Nau캜iti se** uporabljati rolko in ohranjati ravnote쬵e, da se bo lahko hitreje premikal.

[![Peter in volk](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 游꿘 Kliknite zgornjo sliko, da poslu코ate Peter in volk, skladbo Prokofjeva

## U캜enje z okrepitvami

V prej코njih poglavjih ste videli dva primera problemov strojnega u캜enja:

- **Nadzorovano u캜enje**, kjer imamo podatkovne nabore, ki predlagajo vzor캜ne re코itve za problem, ki ga 쬰limo re코iti. [Klasifikacija](../4-Classification/README.md) in [regresija](../2-Regression/README.md) sta nalogi nadzorovanega u캜enja.
- **Nenadzorovano u캜enje**, pri katerem nimamo ozna캜enih podatkov za u캜enje. Glavni primer nenadzorovanega u캜enja je [Gru캜enje](../5-Clustering/README.md).

V tem poglavju vam bomo predstavili nov tip problema u캜enja, ki ne zahteva ozna캜enih podatkov za u캜enje. Obstaja ve캜 vrst tak코nih problemov:

- **[Polnadzorovano u캜enje](https://wikipedia.org/wiki/Semi-supervised_learning)**, kjer imamo veliko neozna캜enih podatkov, ki jih lahko uporabimo za predhodno treniranje modela.
- **[U캜enje z okrepitvami](https://wikipedia.org/wiki/Reinforcement_learning)**, pri katerem agent u캜i, kako se obna코ati, z izvajanjem eksperimentov v simuliranem okolju.

### Primer - ra캜unalni코ka igra

Recimo, da 쬰lite nau캜iti ra캜unalnik igrati igro, kot sta 코ah ali [Super Mario](https://wikipedia.org/wiki/Super_Mario). Da bi ra캜unalnik igral igro, mora napovedati, katero potezo naj izvede v vsakem stanju igre. 캛eprav se to morda zdi kot problem klasifikacije, ni - ker nimamo podatkovnega nabora s stanji in ustreznimi akcijami. 캛eprav imamo morda nekaj podatkov, kot so obstoje캜e 코ahovske partije ali posnetki igralcev, ki igrajo Super Mario, je verjetno, da ti podatki ne bodo zadostno pokrili velikega 코tevila mo쬹ih stanj.

Namesto iskanja obstoje캜ih podatkov o igri se **u캜enje z okrepitvami** (RL) opira na idejo, da *ra캜unalnik ve캜krat igra igro* in opazuje rezultate. Tako za uporabo u캜enja z okrepitvami potrebujemo dve stvari:

- **Okolje** in **simulator**, ki nam omogo캜ata, da igro ve캜krat igramo. Ta simulator bi dolo캜al vsa pravila igre ter mo쬹a stanja in akcije.

- **Funkcijo nagrajevanja**, ki nam pove, kako dobro smo se odrezali med posamezno potezo ali igro.

Glavna razlika med drugimi vrstami strojnega u캜enja in RL je, da pri RL obi캜ajno ne vemo, ali zmagamo ali izgubimo, dokler ne kon캜amo igre. Tako ne moremo re캜i, ali je dolo캜ena poteza sama po sebi dobra ali ne - nagrado prejmemo 코ele na koncu igre. Na코 cilj je oblikovati algoritme, ki nam omogo캜ajo treniranje modela v negotovih razmerah. Spoznali bomo en RL-algoritem, imenovan **Q-u캜enje**.

## Lekcije

1. [Uvod v u캜enje z okrepitvami in Q-u캜enje](1-QLearning/README.md)
2. [Uporaba simulacijskega okolja Gym](2-Gym/README.md)

## Zasluge

"Uvod v u캜enje z okrepitvami" je bilo napisano z 鮫봺잺 avtorja [Dmitry Soshnikov](http://soshnikov.com)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). 캛eprav si prizadevamo za natan캜nost, vas prosimo, da upo코tevate, da lahko avtomatizirani prevodi vsebujejo napake ali neto캜nosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za klju캜ne informacije priporo캜amo profesionalni 캜love코ki prevod. Ne prevzemamo odgovornosti za morebitna napa캜na razumevanja ali napa캜ne interpretacije, ki bi nastale zaradi uporabe tega prevoda.