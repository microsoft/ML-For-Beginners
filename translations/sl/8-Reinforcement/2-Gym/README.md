<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T13:46:38+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "sl"
}
-->
# CartPole Drsanje

Problem, ki smo ga reÅ¡evali v prejÅ¡nji lekciji, se morda zdi kot igraÄa, ki ni uporabna v resniÄnih scenarijih. To ni res, saj veliko resniÄnih problemov deli podobne znaÄilnosti â€“ vkljuÄno z igranjem Å¡aha ali Go. Ti problemi so podobni, ker imamo tudi tukaj ploÅ¡Äo z doloÄenimi pravili in **diskretno stanje**.

## [Predhodni kviz](https://ff-quizzes.netlify.app/en/ml/)

## Uvod

V tej lekciji bomo uporabili enaka naÄela Q-uÄenja za problem s **kontinuiranim stanjem**, tj. stanjem, ki ga doloÄajo ena ali veÄ realnih Å¡tevil. Ukvarjali se bomo z naslednjim problemom:

> **Problem**: ÄŒe Å¾eli Peter pobegniti volku, mora biti sposoben hitreje premikati. Videli bomo, kako se Peter lahko nauÄi drsati, zlasti ohranjati ravnoteÅ¾je, z uporabo Q-uÄenja.

![Veliki pobeg!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> Peter in njegovi prijatelji postanejo ustvarjalni, da pobegnejo volku! Slika: [Jen Looper](https://twitter.com/jenlooper)

Uporabili bomo poenostavljeno razliÄico ohranjanja ravnoteÅ¾ja, znano kot problem **CartPole**. V svetu CartPole imamo horizontalni drsnik, ki se lahko premika levo ali desno, cilj pa je uravnoteÅ¾iti vertikalno palico na vrhu drsnika.

## Predpogoji

V tej lekciji bomo uporabili knjiÅ¾nico **OpenAI Gym** za simulacijo razliÄnih **okolij**. Kodo te lekcije lahko zaÅ¾enete lokalno (npr. iz Visual Studio Code), v tem primeru se bo simulacija odprla v novem oknu. Pri izvajanju kode na spletu boste morda morali narediti nekaj prilagoditev, kot je opisano [tukaj](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

V prejÅ¡nji lekciji so bila pravila igre in stanje podana z razredom `Board`, ki smo ga sami definirali. Tukaj bomo uporabili posebno **simulacijsko okolje**, ki bo simuliralo fiziko palice za uravnoteÅ¾enje. Eno najbolj priljubljenih simulacijskih okolij za treniranje algoritmov za krepitev uÄenja se imenuje [Gym](https://gym.openai.com/), ki ga vzdrÅ¾uje [OpenAI](https://openai.com/). Z uporabo tega Gym-a lahko ustvarimo razliÄna **okolja**, od simulacije CartPole do Atari iger.

> **Opomba**: Druge okolja, ki jih ponuja OpenAI Gym, si lahko ogledate [tukaj](https://gym.openai.com/envs/#classic_control).

Najprej namestimo Gym in uvozimo potrebne knjiÅ¾nice (koda blok 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Naloga - inicializacija okolja CartPole

Za delo s problemom uravnoteÅ¾enja CartPole moramo inicializirati ustrezno okolje. Vsako okolje je povezano z:

- **Prostorom opazovanja**, ki doloÄa strukturo informacij, ki jih prejmemo iz okolja. Pri problemu CartPole prejmemo poloÅ¾aj palice, hitrost in nekatere druge vrednosti.

- **Prostorom akcij**, ki doloÄa moÅ¾ne akcije. V naÅ¡em primeru je prostor akcij diskreten in obsega dve akciji - **levo** in **desno**. (koda blok 2)

1. Za inicializacijo vnesite naslednjo kodo:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Da vidimo, kako okolje deluje, zaÅ¾enimo kratko simulacijo za 100 korakov. Na vsakem koraku podamo eno od akcij, ki jih je treba izvesti â€“ v tej simulaciji nakljuÄno izberemo akcijo iz `action_space`.

1. ZaÅ¾enite spodnjo kodo in preverite rezultat.

    âœ… Ne pozabite, da je priporoÄljivo to kodo zagnati na lokalni Python namestitvi! (koda blok 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Videti bi morali nekaj podobnega tej sliki:

    ![ne-uravnoteÅ¾en CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Med simulacijo moramo pridobiti opazovanja, da se odloÄimo, kako ukrepati. Pravzaprav funkcija koraka vrne trenutna opazovanja, funkcijo nagrade in zastavico "done", ki oznaÄuje, ali ima smisel nadaljevati simulacijo ali ne: (koda blok 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    V zvezku boste videli nekaj podobnega temu:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Vektor opazovanja, ki se vrne na vsakem koraku simulacije, vsebuje naslednje vrednosti:
    - PoloÅ¾aj voziÄka
    - Hitrost voziÄka
    - Kot palice
    - Hitrost vrtenja palice

1. Pridobite minimalno in maksimalno vrednost teh Å¡tevil: (koda blok 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Opazili boste tudi, da je vrednost nagrade na vsakem koraku simulacije vedno 1. To je zato, ker je naÅ¡ cilj preÅ¾iveti Äim dlje, tj. ohraniti palico v razumno vertikalnem poloÅ¾aju Äim dlje.

    âœ… Pravzaprav se simulacija CartPole Å¡teje za reÅ¡eno, Äe nam uspe doseÄi povpreÄno nagrado 195 v 100 zaporednih poskusih.

## Diskretizacija stanja

Pri Q-uÄenju moramo zgraditi Q-tabelo, ki doloÄa, kaj storiti v vsakem stanju. Da bi to lahko storili, mora biti stanje **diskretno**, natanÄneje, vsebovati mora konÄno Å¡tevilo diskretnih vrednosti. Zato moramo nekako **diskretizirati** naÅ¡a opazovanja in jih preslikati v konÄno mnoÅ¾ico stanj.

Obstaja nekaj naÄinov, kako to storiti:

- **Razdelitev na razrede**. ÄŒe poznamo interval doloÄene vrednosti, lahko ta interval razdelimo na Å¡tevilo **razredov** in nato vrednost zamenjamo s Å¡tevilko razreda, kateremu pripada. To lahko storimo z metodo numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). V tem primeru bomo natanÄno poznali velikost stanja, saj bo odvisna od Å¡tevila razredov, ki jih izberemo za digitalizacijo.

âœ… Uporabimo lahko linearno interpolacijo, da vrednosti prenesemo na nek konÄni interval (recimo od -20 do 20), nato pa Å¡tevilke pretvorimo v cela Å¡tevila z zaokroÅ¾evanjem. To nam daje nekoliko manj nadzora nad velikostjo stanja, zlasti Äe ne poznamo natanÄnih razponov vhodnih vrednosti. Na primer, v naÅ¡em primeru 2 od 4 vrednosti nimata zgornjih/spodnjih mej svojih vrednosti, kar lahko povzroÄi neskonÄno Å¡tevilo stanj.

V naÅ¡em primeru bomo uporabili drugi pristop. Kot boste morda opazili kasneje, kljub neomejenim zgornjim/spodnjim mejam te vrednosti redko zavzamejo vrednosti zunaj doloÄenih konÄnih intervalov, zato bodo ta stanja z ekstremnimi vrednostmi zelo redka.

1. Tukaj je funkcija, ki bo vzela opazovanje iz naÅ¡ega modela in ustvarila nabor 4 celih Å¡tevil: (koda blok 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. RaziÅ¡Äimo Å¡e drugo metodo diskretizacije z uporabo razredov: (koda blok 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Zdaj zaÅ¾enimo kratko simulacijo in opazujmo te diskretne vrednosti okolja. Preizkusite `discretize` in `discretize_bins` ter preverite, ali obstaja razlika.

    âœ… `discretize_bins` vrne Å¡tevilko razreda, ki je osnovana na 0. Tako za vrednosti vhodne spremenljivke okoli 0 vrne Å¡tevilko iz sredine intervala (10). Pri `discretize` nas ni skrbelo za razpon izhodnih vrednosti, kar omogoÄa, da so negativne, zato vrednosti stanja niso premaknjene, in 0 ustreza 0. (koda blok 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Odkomentirajte vrstico, ki se zaÄne z `env.render`, Äe Å¾elite videti, kako se okolje izvaja. Sicer pa lahko izvedete v ozadju, kar je hitreje. To "nevidno" izvajanje bomo uporabili med procesom Q-uÄenja.

## Struktura Q-tabele

V prejÅ¡nji lekciji je bilo stanje preprosta dvojica Å¡tevil od 0 do 8, zato je bilo priroÄno predstaviti Q-tabelo z numpy tensorjem oblike 8x8x2. ÄŒe uporabimo diskretizacijo z razredi, je velikost naÅ¡ega vektorskega stanja prav tako znana, zato lahko uporabimo enak pristop in predstavimo stanje z matriko oblike 20x20x10x10x2 (tu je 2 dimenzija prostora akcij, prve dimenzije pa ustrezajo Å¡tevilu razredov, ki smo jih izbrali za vsako od parametrov v prostoru opazovanja).

Vendar pa vÄasih natanÄne dimenzije prostora opazovanja niso znane. V primeru funkcije `discretize` morda nikoli ne bomo prepriÄani, da naÅ¡e stanje ostane znotraj doloÄenih omejitev, ker nekatere izvirne vrednosti niso omejene. Zato bomo uporabili nekoliko drugaÄen pristop in predstavili Q-tabelo z uporabo slovarja.

1. Uporabite par *(state,action)* kot kljuÄ slovarja, vrednost pa bo ustrezala vrednosti vnosa Q-tabele. (koda blok 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Tukaj definiramo tudi funkcijo `qvalues()`, ki vrne seznam vrednosti Q-tabele za dano stanje, ki ustreza vsem moÅ¾nim akcijam. ÄŒe vnos ni prisoten v Q-tabeli, bomo privzeto vrnili 0.

## ZaÄnimo Q-uÄenje

Zdaj smo pripravljeni nauÄiti Petra, kako ohranjati ravnoteÅ¾je!

1. Najprej nastavimo nekaj hiperparametrov: (koda blok 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Tukaj je `alpha` **stopnja uÄenja**, ki doloÄa, v kolikÅ¡ni meri naj prilagodimo trenutne vrednosti Q-tabele na vsakem koraku. V prejÅ¡nji lekciji smo zaÄeli z 1, nato pa zmanjÅ¡ali `alpha` na niÅ¾je vrednosti med treningom. V tem primeru bomo ohranili konstantno vrednost za preprostost, kasneje pa lahko eksperimentirate z nastavitvijo vrednosti `alpha`.

    `gamma` je **faktor popusta**, ki kaÅ¾e, v kolikÅ¡ni meri naj dajemo prednost prihodnji nagradi pred trenutno nagrado.

    `epsilon` je **faktor raziskovanja/izkoriÅ¡Äanja**, ki doloÄa, ali naj raje raziskujemo ali izkoriÅ¡Äamo. V naÅ¡em algoritmu bomo v `epsilon` odstotkih primerov izbrali naslednjo akcijo glede na vrednosti Q-tabele, v preostalih primerih pa bomo izvedli nakljuÄno akcijo. To nam bo omogoÄilo raziskovanje podroÄij iskalnega prostora, ki jih Å¡e nismo videli.

    âœ… Kar zadeva uravnoteÅ¾enje â€“ izbira nakljuÄne akcije (raziskovanje) bi delovala kot nakljuÄni udarec v napaÄno smer, palica pa bi se morala nauÄiti, kako obnoviti ravnoteÅ¾je iz teh "napak".

### IzboljÅ¡ajmo algoritem

Algoritem iz prejÅ¡nje lekcije lahko izboljÅ¡amo na dva naÄina:

- **IzraÄunaj povpreÄno kumulativno nagrado** skozi veÄ simulacij. Napredek bomo tiskali vsakih 5000 iteracij, povpreÄno kumulativno nagrado pa bomo izraÄunali za to obdobje. ÄŒe doseÅ¾emo veÄ kot 195 toÄk, lahko problem Å¡tejemo za reÅ¡en, in to z boljÅ¡o kakovostjo, kot je zahtevano.

- **IzraÄunaj najveÄji povpreÄni kumulativni rezultat**, `Qmax`, in shranili bomo Q-tabelo, ki ustreza temu rezultatu. Ko zaÅ¾enete trening, boste opazili, da se povpreÄni kumulativni rezultat vÄasih zaÄne zmanjÅ¡evati, zato Å¾elimo obdrÅ¾ati vrednosti Q-tabele, ki ustrezajo najboljÅ¡emu modelu, opaÅ¾enemu med treningom.

1. Zberite vse kumulativne nagrade pri vsaki simulaciji v vektorju `rewards` za nadaljnje risanje. (koda blok 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Kaj lahko opazite iz teh rezultatov:

- **Blizu naÅ¡ega cilja**. Zelo smo blizu doseganju cilja 195 kumulativnih nagrad v veÄ kot 100 zaporednih simulacijah, ali pa smo ga morda Å¾e dosegli! Tudi Äe dobimo manjÅ¡e Å¡tevilke, tega ne vemo, ker povpreÄimo skozi 5000 poskusov, medtem ko je formalno merilo le 100 poskusov.

- **Nagrada zaÄne upadati**. VÄasih se nagrada zaÄne zmanjÅ¡evati, kar pomeni, da lahko "uniÄimo" Å¾e nauÄene vrednosti v Q-tabeli z novimi, ki poslabÅ¡ajo situacijo.

To opazovanje je bolj jasno vidno, Äe nariÅ¡emo napredek treninga.

## Risanje napredka treninga

Med treningom smo zbirali vrednosti kumulativne nagrade pri vsaki iteraciji v vektorju `rewards`. Tukaj je, kako to izgleda, ko ga nariÅ¡emo glede na Å¡tevilko iteracije:

```python
plt.plot(rewards)
```

![surov napredek](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Iz tega grafa ni mogoÄe razbrati niÄesar, saj se zaradi narave stohastiÄnega procesa treninga dolÅ¾ina treninga moÄno razlikuje. Da bi ta graf imel veÄ smisla, lahko izraÄunamo **tekoÄe povpreÄje** skozi serijo poskusov, recimo 100. To lahko priroÄno storimo z `np.convolve`: (koda blok 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![napredek treninga](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Spreminjanje hiperparametrov

Da bi uÄenje postalo bolj stabilno, je smiselno med treningom prilagoditi nekatere naÅ¡e hiperparametre. Zlasti:

- **Za stopnjo uÄenja**, `alpha`, lahko zaÄnemo z vrednostmi blizu 1, nato pa postopoma zmanjÅ¡ujemo parameter. SÄasoma bomo dobili dobre verjetnostne vrednosti v Q-tabeli, zato jih moramo le rahlo prilagoditi in ne popolnoma prepisati z novimi vrednostmi.

- **PoveÄaj epsilon**. Morda Å¾elimo poÄasi poveÄati `epsilon`, da bi manj raziskovali in bolj izkoriÅ¡Äali. Verjetno ima smisel zaÄeti z niÅ¾jo vrednostjo `epsilon` in jo postopoma poveÄati skoraj do 1.
> **Naloga 1**: Preizkusite razliÄne vrednosti hiperparametrov in preverite, ali lahko doseÅ¾ete viÅ¡jo kumulativno nagrado. Ali doseÅ¾ete veÄ kot 195?
> **Naloga 2**: Da formalno reÅ¡ite problem, morate doseÄi povpreÄno nagrado 195 v 100 zaporednih poskusih. Merite to med treningom in se prepriÄajte, da ste problem formalno reÅ¡ili!

## Ogled rezultata v praksi

Zanimivo bi bilo dejansko videti, kako se trenirani model obnaÅ¡a. ZaÅ¾enimo simulacijo in sledimo isti strategiji izbire akcij kot med treningom, vzorÄimo glede na porazdelitev verjetnosti v Q-tabeli: (koda blok 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Videti bi morali nekaj takega:

![a balancing cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€Izziv

> **Naloga 3**: Tukaj smo uporabljali konÄno kopijo Q-tabele, ki morda ni najboljÅ¡a. Ne pozabite, da smo najboljÅ¡o Q-tabelo shranili v spremenljivko `Qbest`! Poskusite isti primer z najboljÅ¡o Q-tabelo tako, da kopirate `Qbest` v `Q` in preverite, ali opazite razliko.

> **Naloga 4**: Tukaj nismo izbirali najboljÅ¡e akcije na vsakem koraku, ampak smo vzorÄili glede na ustrezno porazdelitev verjetnosti. Ali bi bilo bolj smiselno vedno izbrati najboljÅ¡o akcijo z najviÅ¡jo vrednostjo v Q-tabeli? To lahko storite z uporabo funkcije `np.argmax`, da ugotovite Å¡tevilko akcije, ki ustreza najviÅ¡ji vrednosti v Q-tabeli. Implementirajte to strategijo in preverite, ali izboljÅ¡a ravnoteÅ¾je.

## [Kvizi po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Naloga
[Trenirajte Mountain Car](assignment.md)

## ZakljuÄek

Sedaj smo se nauÄili, kako trenirati agente, da doseÅ¾ejo dobre rezultate zgolj z zagotavljanjem funkcije nagrajevanja, ki definira Å¾eleno stanje igre, in z omogoÄanjem inteligentnega raziskovanja iskalnega prostora. UspeÅ¡no smo uporabili algoritem Q-Learning v primerih diskretnih in kontinuiranih okolij, vendar z diskretnimi akcijami.

Pomembno je preuÄiti tudi situacije, kjer je stanje akcij kontinuirano in kjer je opazovalni prostor veliko bolj kompleksen, kot na primer slika zaslona igre Atari. Pri teh problemih pogosto potrebujemo moÄnejÅ¡e tehnike strojnega uÄenja, kot so nevronske mreÅ¾e, da doseÅ¾emo dobre rezultate. Te bolj napredne teme so predmet naÅ¡ega prihajajoÄega naprednega teÄaja umetne inteligence.

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za strojno prevajanje [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem izvirnem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo strokovno ÄloveÅ¡ko prevajanje. Ne prevzemamo odgovornosti za morebitna nesporazumevanja ali napaÄne razlage, ki izhajajo iz uporabe tega prevoda.