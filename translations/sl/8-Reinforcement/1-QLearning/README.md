<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T13:37:15+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "sl"
}
-->
# Uvod v u캜enje z okrepitvijo in Q-u캜enje

![Povzetek u캜enja z okrepitvijo v strojnem u캜enju v obliki sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote avtorja [Tomomi Imura](https://www.twitter.com/girlie_mac)

U캜enje z okrepitvijo vklju캜uje tri pomembne koncepte: agenta, dolo캜ena stanja in niz dejanj za vsako stanje. Z izvajanjem dejanja v dolo캜enem stanju agent prejme nagrado. Predstavljajte si ra캜unalni코ko igro Super Mario. Vi ste Mario, nahajate se na ravni igre, stojite ob robu prepada. Nad vami je kovanec. Vi kot Mario, na dolo캜eni ravni igre, na dolo캜enem polo쬬ju ... to je va코e stanje. 캛e se premaknete korak v desno (dejanje), boste padli 캜ez rob in prejeli nizko 코tevil캜no oceno. 캛e pa pritisnete gumb za skok, boste dosegli to캜ko in ostali 쬴vi. To je pozitiven izid, ki bi vam moral prinesti pozitivno 코tevil캜no oceno.

Z uporabo u캜enja z okrepitvijo in simulatorja (igre) se lahko nau캜ite igrati igro tako, da maksimizirate nagrado, kar pomeni, da ostanete 쬴vi in dose쬰te 캜im ve캜 to캜k.

[![Uvod v u캜enje z okrepitvijo](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 游꿘 Kliknite zgornjo sliko, da poslu코ate Dmitryja, kako razpravlja o u캜enju z okrepitvijo

## [Pred-predavanje kviz](https://ff-quizzes.netlify.app/en/ml/)

## Predpogoji in nastavitev

V tej lekciji bomo eksperimentirali s kodo v Pythonu. Kodo iz Jupyter Notebooka iz te lekcije bi morali biti sposobni zagnati na svojem ra캜unalniku ali v oblaku.

Odprite [notebook lekcije](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) in sledite lekciji za gradnjo.

> **Opomba:** 캛e odpirate to kodo iz oblaka, morate pridobiti tudi datoteko [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), ki se uporablja v kodi notebooka. Dodajte jo v isto mapo kot notebook.

## Uvod

V tej lekciji bomo raziskali svet **[Peter in volk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, ki ga je navdihnila glasbena pravljica ruskega skladatelja [Sergeja Prokofjeva](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Uporabili bomo **u캜enje z okrepitvijo**, da bomo Petru omogo캜ili raziskovanje njegovega okolja, zbiranje okusnih jabolk in izogibanje volku.

**U캜enje z okrepitvijo** (RL) je tehnika u캜enja, ki nam omogo캜a, da se nau캜imo optimalnega vedenja **agenta** v dolo캜enem **okolju** z izvajanjem 코tevilnih eksperimentov. Agent v tem okolju mora imeti dolo캜en **cilj**, ki ga dolo캜a **funkcija nagrade**.

## Okolje

Za enostavnost si predstavljajmo Petrov svet kot kvadratno plo코캜o velikosti `코irina` x `vi코ina`, kot je ta:

![Petrovo okolje](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Vsaka celica na tej plo코캜i je lahko:

* **zemlja**, po kateri lahko Peter in druga bitja hodijo.
* **voda**, po kateri o캜itno ne morete hoditi.
* **drevo** ali **trava**, kjer se lahko spo캜ijete.
* **jabolko**, ki predstavlja nekaj, kar bi Peter z veseljem na코el, da se nahrani.
* **volk**, ki je nevaren in se mu je treba izogniti.

Obstaja lo캜en Python modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), ki vsebuje kodo za delo s tem okoljem. Ker ta koda ni pomembna za razumevanje na코ih konceptov, bomo modul uvozili in ga uporabili za ustvarjanje vzor캜ne plo코캜e (koda blok 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Ta koda bi morala natisniti sliko okolja, podobno zgornji.

## Dejanja in politika

V na코em primeru bi bil Petrov cilj najti jabolko, medtem ko se izogiba volku in drugim oviram. Da bi to dosegel, se lahko preprosto sprehaja, dokler ne najde jabolka.

Zato lahko na katerem koli polo쬬ju izbere eno od naslednjih dejanj: gor, dol, levo in desno.

Ta dejanja bomo definirali kot slovar in jih preslikali v pare ustreznih sprememb koordinat. Na primer, premik v desno (`R`) bi ustrezal paru `(1,0)`. (koda blok 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

캛e povzamemo, strategija in cilj tega scenarija sta naslednja:

- **Strategija** na코ega agenta (Petra) je definirana z tako imenovano **politiko**. Politika je funkcija, ki vrne dejanje v katerem koli danem stanju. V na코em primeru je stanje problema predstavljeno s plo코캜o, vklju캜no s trenutnim polo쬬jem igralca.

- **Cilj** u캜enja z okrepitvijo je s캜asoma nau캜iti dobro politiko, ki nam bo omogo캜ila u캜inkovito re코evanje problema. Vendar pa kot osnovo upo코tevajmo najpreprostej코o politiko, imenovano **naklju캜na hoja**.

## Naklju캜na hoja

Najprej re코imo na코 problem z implementacijo strategije naklju캜ne hoje. Pri naklju캜ni hoji bomo naklju캜no izbrali naslednje dejanje iz dovoljenih dejanj, dokler ne dose쬰mo jabolka (koda blok 3).

1. Implementirajte naklju캜no hojo s spodnjo kodo:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Klic funkcije `walk` bi moral vrniti dol쬴no ustrezne poti, ki se lahko razlikuje od enega zagona do drugega.

1. Izvedite eksperiment hoje ve캜krat (recimo 100-krat) in natisnite rezultate statistike (koda blok 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Opazite, da je povpre캜na dol쬴na poti okoli 30-40 korakov, kar je precej, glede na to, da je povpre캜na razdalja do najbli쬵ega jabolka okoli 5-6 korakov.

    Prav tako lahko vidite, kako izgleda Petrov premik med naklju캜no hojo:

    ![Petrova naklju캜na hoja](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funkcija nagrade

Da bi bila na코a politika bolj inteligentna, moramo razumeti, katera dejanja so "bolj코a" od drugih. Za to moramo definirati na코 cilj.

Cilj lahko definiramo v smislu **funkcije nagrade**, ki bo vrnila neko vrednost ocene za vsako stanje. Vi코ja kot je 코tevilka, bolj코a je nagrada. (koda blok 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Zanimivo pri funkcijah nagrade je, da v ve캜ini primerov *prejmemo bistveno nagrado 코ele na koncu igre*. To pomeni, da mora na코 algoritem nekako zapomniti "dobre" korake, ki vodijo do pozitivne nagrade na koncu, in pove캜ati njihov pomen. Podobno je treba odvra캜ati vse poteze, ki vodijo do slabih rezultatov.

## Q-u캜enje

Algoritem, ki ga bomo tukaj obravnavali, se imenuje **Q-u캜enje**. V tem algoritmu je politika definirana s funkcijo (ali podatkovno strukturo), imenovano **Q-tabela**. Ta bele쬴 "dobroto" vsakega dejanja v dolo캜enem stanju.

Imenuje se Q-tabela, ker jo je pogosto priro캜no predstavljati kot tabelo ali ve캜dimenzionalno matriko. Ker ima na코a plo코캜a dimenzije `코irina` x `vi코ina`, lahko Q-tabelo predstavimo z numpy matriko oblike `코irina` x `vi코ina` x `len(actions)`: (koda blok 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Opazite, da vse vrednosti Q-tabele inicializiramo z enako vrednostjo, v na코em primeru - 0,25. To ustreza politiki "naklju캜ne hoje", ker so vse poteze v vsakem stanju enako dobre. Q-tabelo lahko posredujemo funkciji `plot`, da vizualiziramo tabelo na plo코캜i: `m.plot(Q)`.

![Petrovo okolje](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

V sredi코캜u vsake celice je "pu코캜ica", ki ozna캜uje prednostno smer gibanja. Ker so vse smeri enake, je prikazana pika.

Zdaj moramo zagnati simulacijo, raziskati na코e okolje in se nau캜iti bolj코e porazdelitve vrednosti Q-tabele, kar nam bo omogo캜ilo veliko hitrej코e iskanje poti do jabolka.

## Bistvo Q-u캜enja: Bellmanova ena캜ba

Ko se za캜nemo premikati, bo vsako dejanje imelo ustrezno nagrado, tj. teoreti캜no lahko izberemo naslednje dejanje na podlagi najvi코je takoj코nje nagrade. Vendar pa v ve캜ini stanj poteza ne bo dosegla na코ega cilja dose캜i jabolko, zato ne moremo takoj odlo캜iti, katera smer je bolj코a.

> Ne pozabite, da ni pomemben trenutni rezultat, temve캜 kon캜ni rezultat, ki ga bomo dosegli na koncu simulacije.

Da bi upo코tevali to odlo쬰no nagrado, moramo uporabiti na캜ela **[dinami캜nega programiranja](https://en.wikipedia.org/wiki/Dynamic_programming)**, ki nam omogo캜ajo, da o problemu razmi코ljamo rekurzivno.

Recimo, da smo zdaj v stanju *s* in se 쬰limo premakniti v naslednje stanje *s'*. S tem bomo prejeli takoj코njo nagrado *r(s,a)*, dolo캜eno s funkcijo nagrade, plus neko prihodnjo nagrado. 캛e predpostavimo, da na코a Q-tabela pravilno odra쬬 "privla캜nost" vsakega dejanja, bomo v stanju *s'* izbrali dejanje *a*, ki ustreza najve캜ji vrednosti *Q(s',a')*. Tako bo najbolj코a mo쬹a prihodnja nagrada, ki jo lahko dobimo v stanju *s*, definirana kot `max`

## Preverjanje politike

Ker Q-tabela prikazuje "privla캜nost" vsakega dejanja v vsakem stanju, jo je precej enostavno uporabiti za dolo캜itev u캜inkovite navigacije v na코em svetu. V najpreprostej코em primeru lahko izberemo dejanje, ki ustreza najvi코ji vrednosti v Q-tabeli: (koda 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> 캛e zgornjo kodo poskusite ve캜krat, boste morda opazili, da se v캜asih "zatakne" in morate pritisniti gumb STOP v bele쬹ici, da jo prekinete. To se zgodi, ker lahko pride do situacij, ko si dve stanji "ka쬰ta" druga na drugo glede na optimalno Q-vrednost, zaradi 캜esar agent neskon캜no prehaja med tema dvema stanjema.

## 游Izziv

> **Naloga 1:** Spremenite funkcijo `walk`, da omejite najve캜jo dol쬴no poti na dolo캜eno 코tevilo korakov (na primer 100) in opazujte, kako zgornja koda ob캜asno vrne to vrednost.

> **Naloga 2:** Spremenite funkcijo `walk`, da se ne vra캜a na mesta, kjer je 쬰 bil. To bo prepre캜ilo, da bi se `walk` zanko ponavljal, vendar se agent 코e vedno lahko "ujame" na lokaciji, iz katere ne more pobegniti.

## Navigacija

Bolj코a navigacijska politika bi bila tista, ki smo jo uporabili med u캜enjem, in ki zdru쬿je izkori코캜anje in raziskovanje. Pri tej politiki bomo vsako dejanje izbrali z dolo캜eno verjetnostjo, sorazmerno z vrednostmi v Q-tabeli. Ta strategija lahko 코e vedno povzro캜i, da se agent vrne na 쬰 raziskano mesto, vendar, kot lahko vidite iz spodnje kode, vodi do zelo kratke povpre캜ne poti do 쬰lene lokacije (ne pozabite, da `print_statistics` simulacijo za쬰ne 100-krat): (koda 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Po zagonu te kode bi morali dobiti precej kraj코o povpre캜no dol쬴no poti kot prej, v razponu od 3 do 6.

## Preu캜evanje u캜nega procesa

Kot smo omenili, je u캜ni proces ravnovesje med raziskovanjem in izkori코캜anjem pridobljenega znanja o strukturi problemskega prostora. Videli smo, da so se rezultati u캜enja (sposobnost pomagati agentu najti kratko pot do cilja) izbolj코ali, vendar je zanimivo tudi opazovati, kako se povpre캜na dol쬴na poti spreminja med u캜nim procesom:

Ugotovitve lahko povzamemo takole:

- **Povpre캜na dol쬴na poti se pove캜a.** Na za캜etku opazimo, da se povpre캜na dol쬴na poti pove캜uje. To je verjetno zato, ker ko ne vemo ni캜esar o okolju, se zlahka ujamemo v slaba stanja, kot so voda ali volk. Ko se nau캜imo ve캜 in za캜nemo uporabljati to znanje, lahko okolje raziskujemo dlje, vendar 코e vedno ne vemo dobro, kje so jabolka.

- **Dol쬴na poti se zmanj코a, ko se nau캜imo ve캜.** Ko se nau캜imo dovolj, postane agentu la쬵e dose캜i cilj, dol쬴na poti pa se za캜ne zmanj코evati. Vendar smo 코e vedno odprti za raziskovanje, zato pogosto odstopimo od najbolj코e poti in raziskujemo nove mo쬹osti, kar podalj코a pot.

- **Dol쬴na se nenadoma pove캜a.** Na grafu opazimo tudi, da se dol쬴na na neki to캜ki nenadoma pove캜a. To ka쬰 na stohasti캜no naravo procesa in na to, da lahko na neki to캜ki "pokvarimo" koeficiente v Q-tabeli, tako da jih prepi코emo z novimi vrednostmi. To bi morali idealno zmanj코ati z zni쬰vanjem u캜ne stopnje (na primer, proti koncu u캜enja Q-tabeli dodajamo le majhne vrednosti).

Na splo코no je pomembno vedeti, da uspeh in kakovost u캜nega procesa mo캜no odvisna od parametrov, kot so u캜na stopnja, zmanj코evanje u캜ne stopnje in faktor diskontiranja. Ti se pogosto imenujejo **hiperparametri**, da jih lo캜imo od **parametrov**, ki jih optimiziramo med u캜enjem (na primer koeficienti Q-tabele). Proces iskanja najbolj코ih vrednosti hiperparametrov se imenuje **optimizacija hiperparametrov** in si zaslu쬴 lo캜eno obravnavo.

## [Kvizi po predavanju](https://ff-quizzes.netlify.app/en/ml/)

## Naloga 
[Bolj realisti캜en svet](assignment.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). 캛eprav si prizadevamo za natan캜nost, vas prosimo, da upo코tevate, da lahko avtomatizirani prevodi vsebujejo napake ali neto캜nosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za klju캜ne informacije priporo캜amo profesionalni 캜love코ki prevod. Ne prevzemamo odgovornosti za morebitne nesporazume ali napa캜ne razlage, ki bi nastale zaradi uporabe tega prevoda.