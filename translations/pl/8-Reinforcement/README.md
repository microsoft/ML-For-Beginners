<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-03T18:26:17+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do uczenia ze wzmocnieniem

Uczenie ze wzmocnieniem (RL) jest postrzegane jako jeden z podstawowych paradygmat贸w uczenia maszynowego, obok uczenia nadzorowanego i nienadzorowanego. RL dotyczy podejmowania decyzji: dostarczania waciwych decyzji lub przynajmniej uczenia si na ich podstawie.

Wyobra藕 sobie, 偶e masz symulowane rodowisko, takie jak rynek akcji. Co si stanie, jeli wprowadzisz okrelone regulacje? Czy bdzie to miao pozytywny czy negatywny efekt? Jeli wydarzy si co negatywnego, musisz przyj t _negatywn informacj zwrotn_, nauczy si z niej i zmieni kierunek dziaania. Jeli wynik jest pozytywny, musisz budowa na tej _pozytywnej informacji zwrotnej_.

![Piotru i wilk](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.pl.png)

> Piotru i jego przyjaciele musz uciec przed godnym wilkiem! Obraz autorstwa [Jen Looper](https://twitter.com/jenlooper)

## Temat regionalny: Piotru i wilk (Rosja)

[Piotru i wilk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) to muzyczna bajka napisana przez rosyjskiego kompozytora [Siergieja Prokofiewa](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Opowiada histori modego pioniera Piotrusia, kt贸ry odwa偶nie wychodzi z domu na polan w lesie, aby ciga wilka. W tej sekcji bdziemy trenowa algorytmy uczenia maszynowego, kt贸re pomog Piotrusiowi:

- **Eksplorowa** otaczajcy teren i stworzy optymaln map nawigacyjn.
- **Nauczy si** korzysta z deskorolki i utrzymywa r贸wnowag, aby porusza si szybciej.

[![Piotru i wilk](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

>  Kliknij obrazek powy偶ej, aby posucha "Piotru i wilk" autorstwa Prokofiewa

## Uczenie ze wzmocnieniem

W poprzednich sekcjach widziae dwa przykady problem贸w uczenia maszynowego:

- **Nadzorowane**, gdzie mamy zbiory danych sugerujce przykadowe rozwizania problemu, kt贸ry chcemy rozwiza. [Klasyfikacja](../4-Classification/README.md) i [regresja](../2-Regression/README.md) to zadania uczenia nadzorowanego.
- **Nienadzorowane**, w kt贸rym nie mamy oznaczonych danych treningowych. G贸wnym przykadem uczenia nienadzorowanego jest [Grupowanie](../5-Clustering/README.md).

W tej sekcji wprowadzimy nowy typ problemu uczenia, kt贸ry nie wymaga oznaczonych danych treningowych. Istnieje kilka rodzaj贸w takich problem贸w:

- **[Uczenie p贸nadzorowane](https://wikipedia.org/wiki/Semi-supervised_learning)**, gdzie mamy du偶o nieoznaczonych danych, kt贸re mo偶na wykorzysta do wstpnego trenowania modelu.
- **[Uczenie ze wzmocnieniem](https://wikipedia.org/wiki/Reinforcement_learning)**, w kt贸rym agent uczy si, jak si zachowywa, wykonujc eksperymenty w symulowanym rodowisku.

### Przykad - gra komputerowa

Za贸偶my, 偶e chcesz nauczy komputer gra w gr, na przykad w szachy lub [Super Mario](https://wikipedia.org/wiki/Super_Mario). Aby komputer m贸g gra w gr, musimy nauczy go przewidywa, jaki ruch wykona w ka偶dym stanie gry. Cho mo偶e si to wydawa problemem klasyfikacji, tak nie jest - poniewa偶 nie mamy zbioru danych ze stanami i odpowiadajcymi im akcjami. Chocia偶 mo偶emy mie dane, takie jak istniejce partie szachowe lub nagrania graczy grajcych w Super Mario, prawdopodobnie te dane nie bd wystarczajco obejmowa du偶ej liczby mo偶liwych stan贸w.

Zamiast szuka istniejcych danych o grze, **Uczenie ze wzmocnieniem** (RL) opiera si na idei *sprawienia, by komputer gra* wiele razy i obserwowa wynik. Aby zastosowa uczenie ze wzmocnieniem, potrzebujemy dw贸ch rzeczy:

- **rodowiska** i **symulatora**, kt贸re pozwol nam gra w gr wiele razy. Ten symulator definiuje wszystkie zasady gry, a tak偶e mo偶liwe stany i akcje.

- **Funkcji nagrody**, kt贸ra powie nam, jak dobrze radzilimy sobie podczas ka偶dego ruchu lub gry.

G贸wna r贸偶nica midzy innymi typami uczenia maszynowego a RL polega na tym, 偶e w RL zazwyczaj nie wiemy, czy wygrywamy, czy przegrywamy, dop贸ki nie zakoczymy gry. Dlatego nie mo偶emy powiedzie, czy dany ruch sam w sobie jest dobry czy nie - otrzymujemy nagrod dopiero na kocu gry. Naszym celem jest zaprojektowanie algorytm贸w, kt贸re pozwol nam trenowa model w warunkach niepewnoci. Poznamy jeden algorytm RL zwany **Q-learning**.

## Lekcje

1. [Wprowadzenie do uczenia ze wzmocnieniem i Q-Learning](1-QLearning/README.md)
2. [Korzystanie z symulowanego rodowiska gym](2-Gym/README.md)

## Podzikowania

"Wprowadzenie do uczenia ze wzmocnieniem" zostao napisane z ワ przez [Dmitry Soshnikov](http://soshnikov.com)

---

**Zastrze偶enie**:  
Ten dokument zosta przetumaczony za pomoc usugi tumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia偶 dokadamy wszelkich stara, aby tumaczenie byo precyzyjne, prosimy pamita, 偶e automatyczne tumaczenia mog zawiera bdy lub niecisoci. Oryginalny dokument w jego rodzimym jzyku powinien by uznawany za 藕r贸do autorytatywne. W przypadku informacji o kluczowym znaczeniu zaleca si skorzystanie z profesjonalnego tumaczenia przez czowieka. Nie ponosimy odpowiedzialnoci za jakiekolwiek nieporozumienia lub bdne interpretacje wynikajce z u偶ycia tego tumaczenia.