<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T08:28:22+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "pl"
}
-->
# CartPole Skating

Problem, ktÃ³ry rozwiÄ…zywaliÅ›my w poprzedniej lekcji, moÅ¼e wydawaÄ‡ siÄ™ zabawkowy i maÅ‚o przydatny w rzeczywistych scenariuszach. Jednak tak nie jest, poniewaÅ¼ wiele problemÃ³w w prawdziwym Å›wiecie rÃ³wnieÅ¼ ma podobny charakter â€“ na przykÅ‚ad gra w szachy czy Go. SÄ… one podobne, poniewaÅ¼ rÃ³wnieÅ¼ mamy planszÄ™ z okreÅ›lonymi zasadami i **dyskretny stan**.

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ml/)

## Wprowadzenie

W tej lekcji zastosujemy te same zasady Q-Learningu do problemu z **ciÄ…gÅ‚ym stanem**, czyli stanem opisanym przez jednÄ… lub wiÄ™cej liczb rzeczywistych. Zajmiemy siÄ™ nastÄ™pujÄ…cym problemem:

> **Problem**: JeÅ›li Piotr chce uciec przed wilkiem, musi nauczyÄ‡ siÄ™ poruszaÄ‡ szybciej. Zobaczymy, jak Piotr moÅ¼e nauczyÄ‡ siÄ™ jeÅºdziÄ‡ na Å‚yÅ¼wach, a w szczegÃ³lnoÅ›ci utrzymywaÄ‡ rÃ³wnowagÄ™, korzystajÄ…c z Q-Learningu.

![Wielka ucieczka!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> Piotr i jego przyjaciele wykazujÄ… siÄ™ kreatywnoÅ›ciÄ…, aby uciec przed wilkiem! Obraz autorstwa [Jen Looper](https://twitter.com/jenlooper)

UÅ¼yjemy uproszczonej wersji problemu utrzymywania rÃ³wnowagi, znanej jako problem **CartPole**. W Å›wiecie CartPole mamy poziomy suwak, ktÃ³ry moÅ¼e poruszaÄ‡ siÄ™ w lewo lub w prawo, a celem jest utrzymanie pionowego sÅ‚upka na gÃ³rze suwaka.

## Wymagania wstÄ™pne

W tej lekcji bÄ™dziemy korzystaÄ‡ z biblioteki **OpenAI Gym**, aby symulowaÄ‡ rÃ³Å¼ne **Å›rodowiska**. MoÅ¼esz uruchomiÄ‡ kod z tej lekcji lokalnie (np. w Visual Studio Code), w takim przypadku symulacja otworzy siÄ™ w nowym oknie. JeÅ›li uruchamiasz kod online, moÅ¼e byÄ‡ konieczne wprowadzenie pewnych zmian w kodzie, jak opisano [tutaj](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

W poprzedniej lekcji zasady gry i stan byÅ‚y okreÅ›lone przez klasÄ™ `Board`, ktÃ³rÄ… sami zdefiniowaliÅ›my. Tutaj uÅ¼yjemy specjalnego **Å›rodowiska symulacyjnego**, ktÃ³re zasymuluje fizykÄ™ stojÄ…cÄ… za balansujÄ…cym sÅ‚upkiem. Jednym z najpopularniejszych Å›rodowisk symulacyjnych do trenowania algorytmÃ³w uczenia ze wzmocnieniem jest [Gym](https://gym.openai.com/), utrzymywany przez [OpenAI](https://openai.com/). DziÄ™ki Gym moÅ¼emy tworzyÄ‡ rÃ³Å¼ne **Å›rodowiska**, od symulacji CartPole po gry Atari.

> **Uwaga**: Inne Å›rodowiska dostÄ™pne w OpenAI Gym moÅ¼esz zobaczyÄ‡ [tutaj](https://gym.openai.com/envs/#classic_control).

Najpierw zainstalujmy Gym i zaimportujmy wymagane biblioteki (blok kodu 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Ä†wiczenie â€“ inicjalizacja Å›rodowiska CartPole

Aby pracowaÄ‡ z problemem balansowania CartPole, musimy zainicjalizowaÄ‡ odpowiednie Å›rodowisko. KaÅ¼de Å›rodowisko jest zwiÄ…zane z:

- **PrzestrzeniÄ… obserwacji**, ktÃ³ra definiuje strukturÄ™ informacji, jakie otrzymujemy ze Å›rodowiska. W przypadku problemu CartPole otrzymujemy pozycjÄ™ sÅ‚upka, prÄ™dkoÅ›Ä‡ i inne wartoÅ›ci.

- **PrzestrzeniÄ… akcji**, ktÃ³ra definiuje moÅ¼liwe dziaÅ‚ania. W naszym przypadku przestrzeÅ„ akcji jest dyskretna i skÅ‚ada siÄ™ z dwÃ³ch dziaÅ‚aÅ„ â€“ **lewo** i **prawo**. (blok kodu 2)

1. Aby zainicjalizowaÄ‡ Å›rodowisko, wpisz nastÄ™pujÄ…cy kod:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Aby zobaczyÄ‡, jak dziaÅ‚a Å›rodowisko, uruchommy krÃ³tkÄ… symulacjÄ™ na 100 krokÃ³w. Na kaÅ¼dym kroku podajemy jednÄ… z akcji do wykonania â€“ w tej symulacji losowo wybieramy akcjÄ™ z `action_space`.

1. Uruchom poniÅ¼szy kod i zobacz, co siÄ™ stanie.

    âœ… PamiÄ™taj, Å¼e preferowane jest uruchamianie tego kodu na lokalnej instalacji Pythona! (blok kodu 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    PowinieneÅ› zobaczyÄ‡ coÅ› podobnego do tego obrazu:

    ![CartPole bez rÃ³wnowagi](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Podczas symulacji musimy uzyskaÄ‡ obserwacje, aby zdecydowaÄ‡, jak dziaÅ‚aÄ‡. W rzeczywistoÅ›ci funkcja `step` zwraca bieÅ¼Ä…ce obserwacje, funkcjÄ™ nagrody oraz flagÄ™ `done`, ktÃ³ra wskazuje, czy symulacja powinna byÄ‡ kontynuowana, czy nie: (blok kodu 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    W notatniku powinieneÅ› zobaczyÄ‡ coÅ› takiego:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Wektor obserwacji zwracany na kaÅ¼dym kroku symulacji zawiera nastÄ™pujÄ…ce wartoÅ›ci:
    - Pozycja wÃ³zka
    - PrÄ™dkoÅ›Ä‡ wÃ³zka
    - KÄ…t sÅ‚upka
    - PrÄ™dkoÅ›Ä‡ obrotowa sÅ‚upka

1. Uzyskaj minimalne i maksymalne wartoÅ›ci tych liczb: (blok kodu 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    MoÅ¼esz rÃ³wnieÅ¼ zauwaÅ¼yÄ‡, Å¼e wartoÅ›Ä‡ nagrody na kaÅ¼dym kroku symulacji wynosi zawsze 1. Dzieje siÄ™ tak, poniewaÅ¼ naszym celem jest przetrwanie jak najdÅ‚uÅ¼ej, tj. utrzymanie sÅ‚upka w moÅ¼liwie pionowej pozycji przez najdÅ‚uÅ¼szy czas.

    âœ… W rzeczywistoÅ›ci symulacja CartPole jest uznawana za rozwiÄ…zanÄ…, jeÅ›li uda nam siÄ™ uzyskaÄ‡ Å›redniÄ… nagrodÄ™ 195 w 100 kolejnych prÃ³bach.

## Dyskretyzacja stanu

W Q-Learningu musimy zbudowaÄ‡ Q-Table, ktÃ³ra okreÅ›la, co robiÄ‡ w kaÅ¼dym stanie. Aby to zrobiÄ‡, stan musi byÄ‡ **dyskretny**, a dokÅ‚adniej, powinien zawieraÄ‡ skoÅ„czonÄ… liczbÄ™ wartoÅ›ci dyskretnych. Dlatego musimy w jakiÅ› sposÃ³b **zdyskretyzowaÄ‡** nasze obserwacje, mapujÄ…c je na skoÅ„czony zbiÃ³r stanÃ³w.

Istnieje kilka sposobÃ³w, aby to zrobiÄ‡:

- **PodziaÅ‚ na przedziaÅ‚y**. JeÅ›li znamy zakres danej wartoÅ›ci, moÅ¼emy podzieliÄ‡ ten zakres na liczbÄ™ **przedziaÅ‚Ã³w**, a nastÄ™pnie zastÄ…piÄ‡ wartoÅ›Ä‡ numerem przedziaÅ‚u, do ktÃ³rego naleÅ¼y. MoÅ¼na to zrobiÄ‡ za pomocÄ… metody numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). W tym przypadku dokÅ‚adnie znamy rozmiar stanu, poniewaÅ¼ zaleÅ¼y on od liczby przedziaÅ‚Ã³w, ktÃ³re wybierzemy do digitalizacji.

âœ… MoÅ¼emy uÅ¼yÄ‡ interpolacji liniowej, aby sprowadziÄ‡ wartoÅ›ci do pewnego skoÅ„czonego zakresu (np. od -20 do 20), a nastÄ™pnie przekonwertowaÄ‡ liczby na liczby caÅ‚kowite przez zaokrÄ…glenie. Daje nam to nieco mniej kontroli nad rozmiarem stanu, szczegÃ³lnie jeÅ›li nie znamy dokÅ‚adnych zakresÃ³w wartoÅ›ci wejÅ›ciowych. Na przykÅ‚ad w naszym przypadku 2 z 4 wartoÅ›ci nie majÄ… gÃ³rnych/dolnych ograniczeÅ„, co moÅ¼e skutkowaÄ‡ nieskoÅ„czonÄ… liczbÄ… stanÃ³w.

W naszym przykÅ‚adzie wybierzemy drugie podejÅ›cie. Jak zauwaÅ¼ysz pÃ³Åºniej, mimo nieokreÅ›lonych gÃ³rnych/dolnych ograniczeÅ„, te wartoÅ›ci rzadko przyjmujÄ… wartoÅ›ci poza pewnymi skoÅ„czonymi zakresami, wiÄ™c stany z ekstremalnymi wartoÅ›ciami bÄ™dÄ… bardzo rzadkie.

1. Oto funkcja, ktÃ³ra pobiera obserwacjÄ™ z naszego modelu i zwraca krotkÄ™ 4 wartoÅ›ci caÅ‚kowitych: (blok kodu 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Przyjrzyjmy siÄ™ rÃ³wnieÅ¼ innej metodzie dyskretyzacji za pomocÄ… przedziaÅ‚Ã³w: (blok kodu 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Uruchommy teraz krÃ³tkÄ… symulacjÄ™ i zaobserwujmy te zdyskretyzowane wartoÅ›ci Å›rodowiska. MoÅ¼esz wyprÃ³bowaÄ‡ zarÃ³wno `discretize`, jak i `discretize_bins`, aby zobaczyÄ‡, czy istnieje rÃ³Å¼nica.

    âœ… `discretize_bins` zwraca numer przedziaÅ‚u, ktÃ³ry zaczyna siÄ™ od 0. Dlatego dla wartoÅ›ci zmiennej wejÅ›ciowej w okolicach 0 zwraca numer ze Å›rodka zakresu (10). W `discretize` nie przejmowaliÅ›my siÄ™ zakresem wartoÅ›ci wyjÅ›ciowych, pozwalajÄ…c im byÄ‡ ujemnymi, wiÄ™c wartoÅ›ci stanu nie sÄ… przesuniÄ™te, a 0 odpowiada 0. (blok kodu 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Odkomentuj liniÄ™ zaczynajÄ…cÄ… siÄ™ od `env.render`, jeÅ›li chcesz zobaczyÄ‡, jak Å›rodowisko dziaÅ‚a. W przeciwnym razie moÅ¼esz uruchomiÄ‡ je w tle, co jest szybsze. TÄ™ "niewidzialnÄ…" egzekucjÄ™ wykorzystamy podczas procesu Q-Learningu.

## Struktura Q-Table

W poprzedniej lekcji stan byÅ‚ prostÄ… parÄ… liczb od 0 do 8, wiÄ™c wygodnie byÅ‚o reprezentowaÄ‡ Q-Table jako tensor numpy o ksztaÅ‚cie 8x8x2. JeÅ›li uÅ¼ywamy dyskretyzacji za pomocÄ… przedziaÅ‚Ã³w, rozmiar naszego wektora stanu jest rÃ³wnieÅ¼ znany, wiÄ™c moÅ¼emy uÅ¼yÄ‡ tego samego podejÅ›cia i reprezentowaÄ‡ stan jako tablicÄ™ o ksztaÅ‚cie 20x20x10x10x2 (gdzie 2 to wymiar przestrzeni akcji, a pierwsze wymiary odpowiadajÄ… liczbie przedziaÅ‚Ã³w, ktÃ³re wybraliÅ›my dla kaÅ¼dej z wartoÅ›ci w przestrzeni obserwacji).

Jednak czasami dokÅ‚adne wymiary przestrzeni obserwacji nie sÄ… znane. W przypadku funkcji `discretize` nigdy nie moÅ¼emy byÄ‡ pewni, Å¼e nasz stan pozostaje w okreÅ›lonych granicach, poniewaÅ¼ niektÃ³re z oryginalnych wartoÅ›ci nie sÄ… ograniczone. Dlatego uÅ¼yjemy nieco innego podejÅ›cia i przedstawimy Q-Table jako sÅ‚ownik.

1. UÅ¼yj pary *(stan, akcja)* jako klucza sÅ‚ownika, a wartoÅ›Ä‡ bÄ™dzie odpowiadaÄ‡ wartoÅ›ci wpisu w Q-Table. (blok kodu 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Tutaj definiujemy rÃ³wnieÅ¼ funkcjÄ™ `qvalues()`, ktÃ³ra zwraca listÄ™ wartoÅ›ci Q-Table dla danego stanu, odpowiadajÄ…cÄ… wszystkim moÅ¼liwym akcjom. JeÅ›li wpis nie jest obecny w Q-Table, zwrÃ³cimy 0 jako wartoÅ›Ä‡ domyÅ›lnÄ….

## Zaczynamy Q-Learning

Teraz jesteÅ›my gotowi, aby nauczyÄ‡ Piotra balansowania!

1. Najpierw ustawmy kilka hiperparametrÃ³w: (blok kodu 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Tutaj `alpha` to **wspÃ³Å‚czynnik uczenia siÄ™**, ktÃ³ry okreÅ›la, w jakim stopniu powinniÅ›my dostosowywaÄ‡ bieÅ¼Ä…ce wartoÅ›ci Q-Table na kaÅ¼dym kroku. W poprzedniej lekcji zaczynaliÅ›my od 1, a nastÄ™pnie zmniejszaliÅ›my `alpha` do niÅ¼szych wartoÅ›ci podczas treningu. W tym przykÅ‚adzie utrzymamy go na staÅ‚ym poziomie dla uproszczenia, ale moÅ¼esz eksperymentowaÄ‡ z dostosowywaniem wartoÅ›ci `alpha` pÃ³Åºniej.

    `gamma` to **wspÃ³Å‚czynnik dyskontowy**, ktÃ³ry pokazuje, w jakim stopniu powinniÅ›my priorytetyzowaÄ‡ przyszÅ‚Ä… nagrodÄ™ nad bieÅ¼Ä…cÄ….

    `epsilon` to **wspÃ³Å‚czynnik eksploracji/eksploatacji**, ktÃ³ry okreÅ›la, czy powinniÅ›my preferowaÄ‡ eksploracjÄ™ czy eksploatacjÄ™. W naszym algorytmie w `epsilon` procentach przypadkÃ³w wybierzemy nastÄ™pnÄ… akcjÄ™ zgodnie z wartoÅ›ciami Q-Table, a w pozostaÅ‚ych przypadkach wykonamy losowÄ… akcjÄ™. Pozwoli nam to eksplorowaÄ‡ obszary przestrzeni poszukiwaÅ„, ktÃ³rych wczeÅ›niej nie widzieliÅ›my.

    âœ… W kontekÅ›cie balansowania â€“ wybÃ³r losowej akcji (eksploracja) dziaÅ‚aÅ‚by jak przypadkowe "pchniÄ™cie" w zÅ‚Ä… stronÄ™, a sÅ‚upek musiaÅ‚by nauczyÄ‡ siÄ™, jak odzyskaÄ‡ rÃ³wnowagÄ™ po tych "bÅ‚Ä™dach".

### Ulepszanie algorytmu

MoÅ¼emy rÃ³wnieÅ¼ wprowadziÄ‡ dwa ulepszenia do naszego algorytmu z poprzedniej lekcji:

- **Obliczanie Å›redniej skumulowanej nagrody** w serii symulacji. BÄ™dziemy drukowaÄ‡ postÄ™p co 5000 iteracji i uÅ›redniaÄ‡ naszÄ… skumulowanÄ… nagrodÄ™ w tym okresie. Oznacza to, Å¼e jeÅ›li uzyskamy wiÄ™cej niÅ¼ 195 punktÃ³w â€“ moÅ¼emy uznaÄ‡ problem za rozwiÄ…zany, i to z jeszcze wyÅ¼szÄ… jakoÅ›ciÄ… niÅ¼ wymagana.

- **Obliczanie maksymalnego Å›redniego wyniku skumulowanego**, `Qmax`, i przechowywanie Q-Table odpowiadajÄ…cej temu wynikowi. Podczas treningu zauwaÅ¼ysz, Å¼e czasami Å›redni wynik skumulowany zaczyna spadaÄ‡, a my chcemy zachowaÄ‡ wartoÅ›ci Q-Table odpowiadajÄ…ce najlepszemu modelowi zaobserwowanemu podczas treningu.

1. Zbierz wszystkie skumulowane nagrody z kaÅ¼dej symulacji w wektorze `rewards` do dalszego wykreÅ›lania. (blok kodu 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Co moÅ¼esz zauwaÅ¼yÄ‡ z tych wynikÃ³w:

- **Blisko naszego celu**. JesteÅ›my bardzo blisko osiÄ…gniÄ™cia celu uzyskania 195 skumulowanych nagrÃ³d w ponad 100 kolejnych uruchomieniach symulacji, lub moÅ¼emy juÅ¼ go osiÄ…gnÄ™liÅ›my! Nawet jeÅ›li uzyskamy mniejsze liczby, nadal nie wiemy, poniewaÅ¼ uÅ›redniamy wyniki z 5000 uruchomieÅ„, a formalne kryterium wymaga tylko 100 uruchomieÅ„.

- **Nagroda zaczyna spadaÄ‡**. Czasami nagroda zaczyna spadaÄ‡, co oznacza, Å¼e moÅ¼emy "zniszczyÄ‡" juÅ¼ wyuczone wartoÅ›ci w Q-Table, zastÄ™pujÄ…c je tymi, ktÃ³re pogarszajÄ… sytuacjÄ™.

To zjawisko jest bardziej widoczne, jeÅ›li wykreÅ›limy postÄ™p treningu.

## Wykres postÄ™pu treningu

Podczas treningu zbieraliÅ›my wartoÅ›Ä‡ skumulowanej nagrody na kaÅ¼dej iteracji w wektorze `rewards`. Oto jak wyglÄ…da, gdy wykreÅ›limy jÄ… wzglÄ™dem numeru iteracji:

```python
plt.plot(rewards)
```

![surowy postÄ™p](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Z tego wykresu trudno coÅ› wywnioskowaÄ‡, poniewaÅ¼ ze wzglÄ™du na charakter stochastycznego procesu treningowego dÅ‚ugoÅ›Ä‡ sesji treningowych znacznie siÄ™ rÃ³Å¼ni. Aby lepiej zrozumieÄ‡ ten wykres, moÅ¼emy obliczyÄ‡ **Å›redniÄ… kroczÄ…cÄ…** dla serii eksperymentÃ³w, na przykÅ‚ad 100. MoÅ¼na to wygodnie zrobiÄ‡ za pomocÄ… `np.convolve`: (blok kodu 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![postÄ™p treningu](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Zmienianie hiperparametrÃ³w

Aby uczyniÄ‡ proces uczenia bardziej stabilnym, warto dostosowaÄ‡ niektÃ³re z naszych hiperparametrÃ³w podczas treningu. W szczegÃ³lnoÅ›ci:

- **Dla wspÃ³Å‚czynnika uczenia siÄ™**, `alpha`, moÅ¼emy zaczÄ…Ä‡ od wartoÅ›ci bliskich 1, a nastÄ™pnie stopniowo zmniejszaÄ‡ ten parametr. Z czasem bÄ™dziemy uzyskiwaÄ‡ dobre wartoÅ›ci prawdopodobieÅ„stwa w Q-Table, wiÄ™c powinniÅ›my je dostosowywaÄ‡ delikatnie, a nie caÅ‚kowicie nadpisywaÄ‡ nowymi wartoÅ›ciami.

- **ZwiÄ™kszanie epsilon**. MoÅ¼emy chcieÄ‡ stopniowo zwiÄ™kszaÄ‡ `epsilon`, aby mniej eksplorowaÄ‡, a bardziej eksploatowaÄ‡. Prawdopodobnie warto zaczÄ…Ä‡ od niÅ¼szej wartoÅ›ci `epsilon` i stopniowo zwiÄ™kszaÄ‡ jÄ… do prawie 1.
> **Zadanie 1**: Pobaw siÄ™ wartoÅ›ciami hiperparametrÃ³w i sprawdÅº, czy moÅ¼esz osiÄ…gnÄ…Ä‡ wyÅ¼szÄ… skumulowanÄ… nagrodÄ™. Czy udaje Ci siÄ™ przekroczyÄ‡ 195?
> **Zadanie 2**: Aby formalnie rozwiÄ…zaÄ‡ problem, musisz osiÄ…gnÄ…Ä‡ Å›redniÄ… nagrodÄ™ na poziomie 195 w 100 kolejnych prÃ³bach. Mierz to podczas treningu i upewnij siÄ™, Å¼e formalnie rozwiÄ…zaÅ‚eÅ› problem!

## Zobaczenie wynikÃ³w w praktyce

Ciekawie byÅ‚oby zobaczyÄ‡, jak zachowuje siÄ™ wytrenowany model. Uruchommy symulacjÄ™ i zastosujmy tÄ™ samÄ… strategiÄ™ wyboru akcji, co podczas treningu, prÃ³bkujÄ…c zgodnie z rozkÅ‚adem prawdopodobieÅ„stwa w Q-Table: (blok kodu 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

PowinieneÅ› zobaczyÄ‡ coÅ› takiego:

![a balancing cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€Wyzwanie

> **Zadanie 3**: Tutaj korzystaliÅ›my z ostatecznej wersji Q-Table, ktÃ³ra moÅ¼e nie byÄ‡ najlepsza. PamiÄ™taj, Å¼e zapisaliÅ›my najlepiej dziaÅ‚ajÄ…cÄ… Q-Table w zmiennej `Qbest`! WyprÃ³buj ten sam przykÅ‚ad, uÅ¼ywajÄ…c najlepiej dziaÅ‚ajÄ…cej Q-Table, kopiujÄ…c `Qbest` do `Q`, i sprawdÅº, czy zauwaÅ¼ysz rÃ³Å¼nicÄ™.

> **Zadanie 4**: W tym przypadku nie wybieraliÅ›my najlepszej akcji na kaÅ¼dym kroku, lecz prÃ³bkowaliÅ›my zgodnie z odpowiadajÄ…cym rozkÅ‚adem prawdopodobieÅ„stwa. Czy miaÅ‚oby wiÄ™cej sensu zawsze wybieraÄ‡ najlepszÄ… akcjÄ™, z najwyÅ¼szÄ… wartoÅ›ciÄ… w Q-Table? MoÅ¼na to zrobiÄ‡, uÅ¼ywajÄ…c funkcji `np.argmax`, aby znaleÅºÄ‡ numer akcji odpowiadajÄ…cy najwyÅ¼szej wartoÅ›ci w Q-Table. Zaimplementuj tÄ™ strategiÄ™ i sprawdÅº, czy poprawia to balansowanie.

## [Quiz po wykÅ‚adzie](https://ff-quizzes.netlify.app/en/ml/)

## Zadanie
[Wytrenuj Mountain Car](assignment.md)

## Podsumowanie

NauczyliÅ›my siÄ™, jak trenowaÄ‡ agentÃ³w, aby osiÄ…gali dobre wyniki, dostarczajÄ…c im jedynie funkcjÄ™ nagrody, ktÃ³ra definiuje poÅ¼Ä…dany stan gry, oraz dajÄ…c im moÅ¼liwoÅ›Ä‡ inteligentnego eksplorowania przestrzeni poszukiwaÅ„. Z powodzeniem zastosowaliÅ›my algorytm Q-Learning w przypadkach Å›rodowisk dyskretnych i ciÄ…gÅ‚ych, ale z dyskretnymi akcjami.

WaÅ¼ne jest rÃ³wnieÅ¼ badanie sytuacji, w ktÃ³rych przestrzeÅ„ akcji jest rÃ³wnieÅ¼ ciÄ…gÅ‚a, a przestrzeÅ„ obserwacji jest znacznie bardziej zÅ‚oÅ¼ona, na przykÅ‚ad obraz z ekranu gry Atari. W takich problemach czÄ™sto musimy korzystaÄ‡ z bardziej zaawansowanych technik uczenia maszynowego, takich jak sieci neuronowe, aby osiÄ…gnÄ…Ä‡ dobre wyniki. Te bardziej zaawansowane tematy bÄ™dÄ… przedmiotem naszego kolejnego, bardziej zaawansowanego kursu AI.

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ dokÅ‚adamy wszelkich staraÅ„, aby tÅ‚umaczenie byÅ‚o precyzyjne, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji o kluczowym znaczeniu zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.