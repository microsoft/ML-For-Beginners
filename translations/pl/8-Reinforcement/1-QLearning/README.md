<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T08:27:35+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "pl"
}
-->
# Wprowadzenie do uczenia ze wzmocnieniem i Q-Learningu

![Podsumowanie uczenia ze wzmocnieniem w uczeniu maszynowym w formie sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote autorstwa [Tomomi Imura](https://www.twitter.com/girlie_mac)

Uczenie ze wzmocnieniem opiera si na trzech kluczowych pojciach: agencie, stanach oraz zestawie akcji dla ka偶dego stanu. Wykonujc akcj w okrelonym stanie, agent otrzymuje nagrod. Wyobra藕 sobie gr komputerow Super Mario. Jeste Mario, znajdujesz si na poziomie gry, stojc obok krawdzi klifu. Nad tob jest moneta. Ty, jako Mario, w poziomie gry, w okrelonej pozycji... to tw贸j stan. Przesunicie si o krok w prawo (akcja) spowoduje, 偶e spadniesz z klifu, co da ci nisk warto punktow. Jednak nacinicie przycisku skoku pozwoli ci zdoby punkt i pozosta przy 偶yciu. To pozytywny wynik, kt贸ry powinien nagrodzi ci dodatni wartoci punktow.

Korzystajc z uczenia ze wzmocnieniem i symulatora (gry), mo偶esz nauczy si gra w gr, aby maksymalizowa nagrod, czyli pozostawa przy 偶yciu i zdobywa jak najwicej punkt贸w.

[![Wprowadzenie do uczenia ze wzmocnieniem](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

>  Kliknij obrazek powy偶ej, aby posucha Dmitry'ego omawiajcego uczenie ze wzmocnieniem

## [Quiz przed wykadem](https://ff-quizzes.netlify.app/en/ml/)

## Wymagania wstpne i konfiguracja

W tej lekcji bdziemy eksperymentowa z kodem w Pythonie. Powiniene by w stanie uruchomi kod z Jupyter Notebook z tej lekcji, zar贸wno na swoim komputerze, jak i w chmurze.

Mo偶esz otworzy [notebook lekcji](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) i przej przez t lekcj, aby j zbudowa.

> **Uwaga:** Jeli otwierasz ten kod z chmury, musisz r贸wnie偶 pobra plik [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kt贸ry jest u偶ywany w kodzie notebooka. Dodaj go do tego samego katalogu co notebook.

## Wprowadzenie

W tej lekcji zagbimy si w wiat **[Piotrusia i Wilka](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirowany muzyczn bajk rosyjskiego kompozytora, [Siergieja Prokofiewa](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Wykorzystamy **uczenie ze wzmocnieniem**, aby pozwoli Piotrusiowi eksplorowa swoje otoczenie, zbiera smaczne jabka i unika spotkania z wilkiem.

**Uczenie ze wzmocnieniem** (RL) to technika uczenia, kt贸ra pozwala nam nauczy si optymalnego zachowania **agenta** w okrelonym **rodowisku** poprzez przeprowadzanie wielu eksperyment贸w. Agent w tym rodowisku powinien mie jaki **cel**, zdefiniowany przez **funkcj nagrody**.

## rodowisko

Dla uproszczenia, wyobra藕my sobie wiat Piotrusia jako kwadratow plansz o rozmiarze `szeroko` x `wysoko`, jak poni偶ej:

![rodowisko Piotrusia](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Ka偶da kom贸rka na tej planszy mo偶e by:

* **ziemi**, po kt贸rej Piotru i inne stworzenia mog chodzi.
* **wod**, po kt贸rej oczywicie nie mo偶na chodzi.
* **drzewem** lub **traw**, miejscem, gdzie mo偶na odpocz.
* **jabkiem**, kt贸re Piotru chtnie znajdzie, aby si nakarmi.
* **wilkiem**, kt贸ry jest niebezpieczny i nale偶y go unika.

Istnieje osobny modu Pythona, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kt贸ry zawiera kod do pracy z tym rodowiskiem. Poniewa偶 ten kod nie jest istotny dla zrozumienia naszych koncepcji, zaimportujemy modu i u偶yjemy go do stworzenia przykadowej planszy (blok kodu 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Ten kod powinien wydrukowa obraz rodowiska podobny do powy偶szego.

## Akcje i polityka

W naszym przykadzie celem Piotrusia bdzie znalezienie jabka, unikajc wilka i innych przeszk贸d. Aby to zrobi, mo偶e zasadniczo chodzi po planszy, a偶 znajdzie jabko.

Dlatego w dowolnej pozycji mo偶e wybra jedn z nastpujcych akcji: g贸ra, d贸, lewo i prawo.

Zdefiniujemy te akcje jako sownik i przypiszemy je do par odpowiadajcych zmian wsp贸rzdnych. Na przykad, ruch w prawo (`R`) odpowiada parze `(1,0)`. (blok kodu 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Podsumowujc, strategia i cel tego scenariusza s nastpujce:

- **Strategia** naszego agenta (Piotrusia) jest zdefiniowana przez tzw. **polityk**. Polityka to funkcja, kt贸ra zwraca akcj w dowolnym stanie. W naszym przypadku stan problemu jest reprezentowany przez plansz, w tym aktualn pozycj gracza.

- **Cel** uczenia ze wzmocnieniem to ostatecznie nauczenie si dobrej polityki, kt贸ra pozwoli nam efektywnie rozwiza problem. Jednak jako punkt odniesienia rozwa偶my najprostsz polityk zwan **losowym spacerem**.

## Losowy spacer

Najpierw rozwi偶my nasz problem, implementujc strategi losowego spaceru. W przypadku losowego spaceru bdziemy losowo wybiera nastpn akcj z dozwolonych akcji, a偶 dotrzemy do jabka (blok kodu 3).

1. Zaimplementuj losowy spacer za pomoc poni偶szego kodu:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Wywoanie `walk` powinno zwr贸ci dugo odpowiadajcej cie偶ki, kt贸ra mo偶e si r贸偶ni w zale偶noci od uruchomienia.

1. Uruchom eksperyment spaceru kilka razy (np. 100) i wydrukuj wynikowe statystyki (blok kodu 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Zauwa偶, 偶e rednia dugo cie偶ki wynosi okoo 30-40 krok贸w, co jest do du偶o, biorc pod uwag fakt, 偶e rednia odlego do najbli偶szego jabka wynosi okoo 5-6 krok贸w.

    Mo偶esz r贸wnie偶 zobaczy, jak wyglda ruch Piotrusia podczas losowego spaceru:

    ![Losowy spacer Piotrusia](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funkcja nagrody

Aby nasza polityka bya bardziej inteligentna, musimy zrozumie, kt贸re ruchy s "lepsze" od innych. Aby to zrobi, musimy zdefiniowa nasz cel.

Cel mo偶na zdefiniowa w kategoriach **funkcji nagrody**, kt贸ra zwr贸ci pewn warto punktow dla ka偶dego stanu. Im wy偶sza liczba, tym lepsza funkcja nagrody. (blok kodu 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Interesujc rzecz dotyczc funkcji nagrody jest to, 偶e w wikszoci przypadk贸w *otrzymujemy znaczc nagrod dopiero na kocu gry*. Oznacza to, 偶e nasz algorytm powinien jako zapamita "dobre" kroki, kt贸re prowadz do pozytywnej nagrody na kocu, i zwikszy ich znaczenie. Podobnie, wszystkie ruchy prowadzce do zych wynik贸w powinny by zniechcane.

## Q-Learning

Algorytm, kt贸ry om贸wimy tutaj, nazywa si **Q-Learning**. W tym algorytmie polityka jest definiowana przez funkcj (lub struktur danych) zwan **Q-Tablic**. Rejestruje ona "dobro" ka偶dej z akcji w danym stanie.

Nazywa si j Q-Tablic, poniewa偶 czsto wygodnie jest j reprezentowa jako tablic lub wielowymiarow macierz. Poniewa偶 nasza plansza ma wymiary `szeroko` x `wysoko`, mo偶emy reprezentowa Q-Tablic za pomoc tablicy numpy o ksztacie `szeroko` x `wysoko` x `len(actions)`: (blok kodu 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Zauwa偶, 偶e inicjalizujemy wszystkie wartoci Q-Tablicy r贸wn wartoci, w naszym przypadku - 0.25. Odpowiada to polityce "losowego spaceru", poniewa偶 wszystkie ruchy w ka偶dym stanie s r贸wnie dobre. Mo偶emy przekaza Q-Tablic do funkcji `plot`, aby zwizualizowa tablic na planszy: `m.plot(Q)`.

![rodowisko Piotrusia](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

W centrum ka偶dej kom贸rki znajduje si "strzaka", kt贸ra wskazuje preferowany kierunek ruchu. Poniewa偶 wszystkie kierunki s r贸wne, wywietlany jest punkt.

Teraz musimy uruchomi symulacj, zbada nasze rodowisko i nauczy si lepszego rozkadu wartoci Q-Tablicy, kt贸ry pozwoli nam znacznie szybciej znale藕 drog do jabka.

## Istota Q-Learningu: R贸wnanie Bellmana

Gdy zaczniemy si porusza, ka偶da akcja bdzie miaa odpowiadajc jej nagrod, tj. teoretycznie mo偶emy wybra nastpn akcj na podstawie najwy偶szej natychmiastowej nagrody. Jednak w wikszoci stan贸w ruch nie osignie naszego celu, jakim jest dotarcie do jabka, i dlatego nie mo偶emy od razu zdecydowa, kt贸ry kierunek jest lepszy.

> Pamitaj, 偶e nie liczy si natychmiastowy wynik, ale raczej ostateczny wynik, kt贸ry uzyskamy na kocu symulacji.

Aby uwzgldni t op贸藕nion nagrod, musimy skorzysta z zasad **[programowania dynamicznego](https://en.wikipedia.org/wiki/Dynamic_programming)**, kt贸re pozwalaj nam myle o naszym problemie w spos贸b rekurencyjny.

Za贸偶my, 偶e teraz znajdujemy si w stanie *s*, i chcemy przej do nastpnego stanu *s'*. Wykonujc to, otrzymamy natychmiastow nagrod *r(s,a)*, zdefiniowan przez funkcj nagrody, plus jak przysz nagrod. Jeli zao偶ymy, 偶e nasza Q-Tablica poprawnie odzwierciedla "atrakcyjno" ka偶dej akcji, to w stanie *s'* wybierzemy akcj *a*, kt贸ra odpowiada maksymalnej wartoci *Q(s',a')*. Tak wic najlepsza mo偶liwa przysza nagroda, jak mo偶emy uzyska w stanie *s*, bdzie zdefiniowana jako `max`

## Sprawdzanie polityki

Poniewa偶 Q-Table zawiera "atrakcyjno" ka偶dej akcji w ka偶dym stanie, atwo jest wykorzysta j do zdefiniowania efektywnej nawigacji w naszym wiecie. W najprostszym przypadku mo偶emy wybra akcj odpowiadajc najwy偶szej wartoci w Q-Table: (blok kodu 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Jeli uruchomisz powy偶szy kod kilka razy, mo偶esz zauwa偶y, 偶e czasami "zawiesza si" i musisz nacisn przycisk STOP w notebooku, aby go przerwa. Dzieje si tak, poniewa偶 mog wystpi sytuacje, w kt贸rych dwa stany "wskazuj" na siebie nawzajem pod wzgldem optymalnej wartoci Q-Value, w wyniku czego agent porusza si midzy tymi stanami w nieskoczono.

## Wyzwanie

> **Zadanie 1:** Zmodyfikuj funkcj `walk`, aby ograniczy maksymaln dugo cie偶ki do okrelonej liczby krok贸w (np. 100) i zobacz, jak powy偶szy kod czasami zwraca t warto.

> **Zadanie 2:** Zmodyfikuj funkcj `walk`, aby nie wracaa do miejsc, w kt贸rych ju偶 wczeniej bya. Zapobiegnie to zaptleniu `walk`, jednak agent nadal mo偶e utkn w miejscu, z kt贸rego nie mo偶e si wydosta.

## Nawigacja

Lepsz polityk nawigacji byaby ta, kt贸r stosowalimy podczas treningu, czca eksploatacj i eksploracj. W tej polityce wybieramy ka偶d akcj z okrelonym prawdopodobiestwem, proporcjonalnym do wartoci w Q-Table. Ta strategia mo偶e nadal prowadzi do powrotu agenta do pozycji, kt贸r ju偶 eksplorowa, ale, jak wida w poni偶szym kodzie, skutkuje bardzo kr贸tk redni cie偶k do po偶danej lokalizacji (pamitaj, 偶e `print_statistics` uruchamia symulacj 100 razy): (blok kodu 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Po uruchomieniu tego kodu powiniene uzyska znacznie kr贸tsz redni dugo cie偶ki ni偶 wczeniej, w zakresie 3-6.

## Badanie procesu uczenia

Jak wspomnielimy, proces uczenia to balans midzy eksploracj a wykorzystaniem zdobytej wiedzy o strukturze przestrzeni problemowej. Widzielimy, 偶e wyniki uczenia (zdolno do pomocy agentowi w znalezieniu kr贸tkiej cie偶ki do celu) poprawiy si, ale interesujce jest r贸wnie偶 obserwowanie, jak rednia dugo cie偶ki zachowuje si podczas procesu uczenia:

## Podsumowanie wniosk贸w:

- **rednia dugo cie偶ki ronie**. Na pocztku rednia dugo cie偶ki ronie. Wynika to prawdopodobnie z faktu, 偶e gdy nic nie wiemy o rodowisku, atwo jest utkn w zych stanach, takich jak woda czy wilk. Gdy uczymy si wicej i zaczynamy korzysta z tej wiedzy, mo偶emy eksplorowa rodowisko du偶ej, ale nadal nie znamy dobrze lokalizacji jabek.

- **Dugo cie偶ki maleje, gdy uczymy si wicej**. Gdy nauczymy si wystarczajco du偶o, agentowi atwiej jest osign cel, a dugo cie偶ki zaczyna si zmniejsza. Jednak nadal jestemy otwarci na eksploracj, wic czsto odbiegamy od najlepszej cie偶ki i eksplorujemy nowe opcje, co wydu偶a cie偶k ponad optymaln.

- **Dugo nagle wzrasta**. Na wykresie mo偶na r贸wnie偶 zauwa偶y, 偶e w pewnym momencie dugo nagle wzrasta. Wskazuje to na stochastyczny charakter procesu i na to, 偶e w pewnym momencie mo偶emy "zepsu" wsp贸czynniki Q-Table, nadpisujc je nowymi wartociami. Idealnie powinno si to minimalizowa, zmniejszajc wsp贸czynnik uczenia (na przykad pod koniec treningu dostosowujemy wartoci Q-Table tylko o niewielk warto).

Og贸lnie rzecz biorc, wa偶ne jest, aby pamita, 偶e sukces i jako procesu uczenia w du偶ej mierze zale偶 od parametr贸w, takich jak wsp贸czynnik uczenia, jego zmniejszanie oraz wsp贸czynnik dyskontowy. Czsto nazywa si je **hiperparametrami**, aby odr贸偶ni je od **parametr贸w**, kt贸re optymalizujemy podczas treningu (na przykad wsp贸czynniki Q-Table). Proces znajdowania najlepszych wartoci hiperparametr贸w nazywa si **optymalizacj hiperparametr贸w** i zasuguje na osobny temat.

## [Quiz po wykadzie](https://ff-quizzes.netlify.app/en/ml/)

## Zadanie 
[Bardziej realistyczny wiat](assignment.md)

---

**Zastrze偶enie**:  
Ten dokument zosta przetumaczony za pomoc usugi tumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chocia偶 dokadamy wszelkich stara, aby tumaczenie byo precyzyjne, prosimy pamita, 偶e automatyczne tumaczenia mog zawiera bdy lub niecisoci. Oryginalny dokument w jego rodzimym jzyku powinien by uznawany za wiarygodne 藕r贸do. W przypadku informacji krytycznych zaleca si skorzystanie z profesjonalnego tumaczenia wykonanego przez czowieka. Nie ponosimy odpowiedzialnoci za jakiekolwiek nieporozumienia lub bdne interpretacje wynikajce z korzystania z tego tumaczenia.