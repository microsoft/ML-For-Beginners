<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T08:20:26+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "pl"
}
-->
# Budowanie rozwiÄ…zaÅ„ Machine Learning z odpowiedzialnÄ… sztucznÄ… inteligencjÄ…

![Podsumowanie odpowiedzialnej AI w Machine Learning na szkicowej notatce](../../../../sketchnotes/ml-fairness.png)
> Szkicowa notatka autorstwa [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz przed wykÅ‚adem](https://ff-quizzes.netlify.app/en/ml/)

## Wprowadzenie

W tym programie nauczania zaczniesz odkrywaÄ‡, jak uczenie maszynowe wpÅ‚ywa na nasze codzienne Å¼ycie. JuÅ¼ teraz systemy i modele sÄ… zaangaÅ¼owane w codzienne zadania decyzyjne, takie jak diagnozy medyczne, zatwierdzanie kredytÃ³w czy wykrywanie oszustw. Dlatego waÅ¼ne jest, aby te modele dziaÅ‚aÅ‚y dobrze i dostarczaÅ‚y wyniki, ktÃ³rym moÅ¼na zaufaÄ‡. Podobnie jak kaÅ¼da aplikacja programowa, systemy AI mogÄ… nie speÅ‚niaÄ‡ oczekiwaÅ„ lub prowadziÄ‡ do niepoÅ¼Ä…danych rezultatÃ³w. Dlatego kluczowe jest zrozumienie i wyjaÅ›nienie zachowania modelu AI.

WyobraÅº sobie, co moÅ¼e siÄ™ staÄ‡, gdy dane uÅ¼ywane do budowy tych modeli nie uwzglÄ™dniajÄ… pewnych grup demograficznych, takich jak rasa, pÅ‚eÄ‡, poglÄ…dy polityczne, religia, lub gdy sÄ… one nadmiernie reprezentowane. Co, jeÅ›li wyniki modelu sÄ… interpretowane w sposÃ³b faworyzujÄ…cy pewne grupy demograficzne? Jakie sÄ… konsekwencje dla aplikacji? Co siÄ™ dzieje, gdy model prowadzi do szkodliwych rezultatÃ³w? Kto jest odpowiedzialny za zachowanie systemÃ³w AI? To sÄ… pytania, ktÃ³re bÄ™dziemy eksplorowaÄ‡ w tym programie nauczania.

W tej lekcji dowiesz siÄ™:

- Dlaczego sprawiedliwoÅ›Ä‡ w uczeniu maszynowym i zwiÄ…zane z niÄ… szkody sÄ… tak waÅ¼ne.
- Jak badaÄ‡ odstajÄ…ce przypadki i nietypowe scenariusze, aby zapewniÄ‡ niezawodnoÅ›Ä‡ i bezpieczeÅ„stwo.
- Dlaczego projektowanie inkluzywnych systemÃ³w jest kluczowe dla wzmocnienia pozycji wszystkich ludzi.
- Jak istotne jest chronienie prywatnoÅ›ci i bezpieczeÅ„stwa danych oraz osÃ³b.
- Dlaczego podejÅ›cie â€szklanej skrzynkiâ€ jest waÅ¼ne dla wyjaÅ›nienia zachowania modeli AI.
- Jak odpowiedzialnoÅ›Ä‡ buduje zaufanie do systemÃ³w AI.

## Wymagania wstÄ™pne

Jako wymaganie wstÄ™pne, zapoznaj siÄ™ z â€Zasadami odpowiedzialnej AIâ€ w Å›cieÅ¼ce nauki i obejrzyj poniÅ¼szy film na ten temat:

Dowiedz siÄ™ wiÄ™cej o odpowiedzialnej AI, korzystajÄ…c z tej [Å›cieÅ¼ki nauki](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![PodejÅ›cie Microsoftu do odpowiedzialnej AI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "PodejÅ›cie Microsoftu do odpowiedzialnej AI")

> ğŸ¥ Kliknij obrazek powyÅ¼ej, aby obejrzeÄ‡ film: PodejÅ›cie Microsoftu do odpowiedzialnej AI

## SprawiedliwoÅ›Ä‡

Systemy AI powinny traktowaÄ‡ wszystkich sprawiedliwie i unikaÄ‡ rÃ³Å¼nic w traktowaniu podobnych grup ludzi. Na przykÅ‚ad, gdy systemy AI udzielajÄ… wskazÃ³wek dotyczÄ…cych leczenia medycznego, aplikacji kredytowych czy zatrudnienia, powinny wydawaÄ‡ takie same rekomendacje wszystkim osobom o podobnych objawach, sytuacji finansowej czy kwalifikacjach zawodowych. KaÅ¼dy z nas jako czÅ‚owiek nosi w sobie odziedziczone uprzedzenia, ktÃ³re wpÅ‚ywajÄ… na nasze decyzje i dziaÅ‚ania. Te uprzedzenia mogÄ… byÄ‡ widoczne w danych, ktÃ³re wykorzystujemy do trenowania systemÃ³w AI. Czasami takie manipulacje zdarzajÄ… siÄ™ nieumyÅ›lnie. CzÄ™sto trudno jest Å›wiadomie zauwaÅ¼yÄ‡, kiedy wprowadzamy uprzedzenia do danych.

**â€NiesprawiedliwoÅ›Ä‡â€** obejmuje negatywne skutki, czyli â€szkodyâ€, dla grupy ludzi, takich jak te zdefiniowane na podstawie rasy, pÅ‚ci, wieku czy statusu niepeÅ‚nosprawnoÅ›ci. GÅ‚Ã³wne szkody zwiÄ…zane ze sprawiedliwoÅ›ciÄ… moÅ¼na sklasyfikowaÄ‡ jako:

- **Alokacja**, gdy na przykÅ‚ad pÅ‚eÄ‡ lub etnicznoÅ›Ä‡ jest faworyzowana kosztem innych.
- **JakoÅ›Ä‡ usÅ‚ug**. JeÅ›li dane sÄ… trenowane dla jednego konkretnego scenariusza, ale rzeczywistoÅ›Ä‡ jest znacznie bardziej zÅ‚oÅ¼ona, prowadzi to do sÅ‚abo dziaÅ‚ajÄ…cej usÅ‚ugi. Na przykÅ‚ad dozownik mydÅ‚a, ktÃ³ry nie potrafiÅ‚ rozpoznaÄ‡ osÃ³b o ciemnej skÃ³rze. [Å¹rÃ³dÅ‚o](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Oczernianie**. Niesprawiedliwe krytykowanie i etykietowanie czegoÅ› lub kogoÅ›. Na przykÅ‚ad technologia etykietowania obrazÃ³w niesÅ‚awnie bÅ‚Ä™dnie oznaczyÅ‚a zdjÄ™cia osÃ³b o ciemnej skÃ³rze jako goryle.
- **Nadmierna lub niedostateczna reprezentacja**. Chodzi o to, Å¼e pewna grupa nie jest widoczna w okreÅ›lonym zawodzie, a kaÅ¼da usÅ‚uga lub funkcja, ktÃ³ra to utrwala, przyczynia siÄ™ do szkody.
- **Stereotypizacja**. Przypisywanie okreÅ›lonej grupie z gÃ³ry ustalonych cech. Na przykÅ‚ad system tÅ‚umaczenia jÄ™zykowego miÄ™dzy angielskim a tureckim moÅ¼e mieÄ‡ nieÅ›cisÅ‚oÅ›ci wynikajÄ…ce ze stereotypowych skojarzeÅ„ z pÅ‚ciÄ….

![tÅ‚umaczenie na turecki](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> tÅ‚umaczenie na turecki

![tÅ‚umaczenie z powrotem na angielski](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> tÅ‚umaczenie z powrotem na angielski

Podczas projektowania i testowania systemÃ³w AI musimy upewniÄ‡ siÄ™, Å¼e AI jest sprawiedliwe i nie jest zaprogramowane do podejmowania stronniczych lub dyskryminujÄ…cych decyzji, ktÃ³rych ludzie rÃ³wnieÅ¼ nie powinni podejmowaÄ‡. Zapewnienie sprawiedliwoÅ›ci w AI i uczeniu maszynowym pozostaje zÅ‚oÅ¼onym wyzwaniem socjotechnicznym.

### NiezawodnoÅ›Ä‡ i bezpieczeÅ„stwo

Aby budowaÄ‡ zaufanie, systemy AI muszÄ… byÄ‡ niezawodne, bezpieczne i spÃ³jne w normalnych i nieoczekiwanych warunkach. WaÅ¼ne jest, aby wiedzieÄ‡, jak systemy AI bÄ™dÄ… siÄ™ zachowywaÄ‡ w rÃ³Å¼nych sytuacjach, zwÅ‚aszcza w przypadku odstajÄ…cych przypadkÃ³w. Podczas budowania rozwiÄ…zaÅ„ AI naleÅ¼y poÅ›wiÄ™ciÄ‡ znacznÄ… uwagÄ™ temu, jak radziÄ‡ sobie z szerokÄ… gamÄ… okolicznoÅ›ci, ktÃ³re mogÄ… napotkaÄ‡ rozwiÄ…zania AI. Na przykÅ‚ad samochÃ³d autonomiczny musi stawiaÄ‡ bezpieczeÅ„stwo ludzi na pierwszym miejscu. W zwiÄ…zku z tym AI napÄ™dzajÄ…ce samochÃ³d musi uwzglÄ™dniaÄ‡ wszystkie moÅ¼liwe scenariusze, ktÃ³re samochÃ³d moÅ¼e napotkaÄ‡, takie jak noc, burze, zamiecie Å›nieÅ¼ne, dzieci biegnÄ…ce przez ulicÄ™, zwierzÄ™ta domowe, roboty drogowe itd. To, jak dobrze system AI radzi sobie z szerokim zakresem warunkÃ³w w sposÃ³b niezawodny i bezpieczny, odzwierciedla poziom przewidywania, ktÃ³ry naukowiec danych lub programista AI uwzglÄ™dniÅ‚ podczas projektowania lub testowania systemu.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### InkluzywnoÅ›Ä‡

Systemy AI powinny byÄ‡ projektowane tak, aby angaÅ¼owaÄ‡ i wzmacniaÄ‡ pozycjÄ™ wszystkich ludzi. Podczas projektowania i wdraÅ¼ania systemÃ³w AI naukowcy danych i programiÅ›ci AI identyfikujÄ… i rozwiÄ…zujÄ… potencjalne bariery w systemie, ktÃ³re mogÅ‚yby nieumyÅ›lnie wykluczyÄ‡ ludzi. Na przykÅ‚ad na Å›wiecie jest 1 miliard osÃ³b z niepeÅ‚nosprawnoÅ›ciami. DziÄ™ki postÄ™powi w dziedzinie AI mogÄ… oni Å‚atwiej uzyskiwaÄ‡ dostÄ™p do szerokiego zakresu informacji i moÅ¼liwoÅ›ci w codziennym Å¼yciu. RozwiÄ…zywanie barier tworzy moÅ¼liwoÅ›ci innowacji i rozwijania produktÃ³w AI z lepszymi doÅ›wiadczeniami, ktÃ³re przynoszÄ… korzyÅ›ci wszystkim.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: inkluzywnoÅ›Ä‡ w AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### BezpieczeÅ„stwo i prywatnoÅ›Ä‡

Systemy AI powinny byÄ‡ bezpieczne i szanowaÄ‡ prywatnoÅ›Ä‡ ludzi. Ludzie majÄ… mniejsze zaufanie do systemÃ³w, ktÃ³re naraÅ¼ajÄ… ich prywatnoÅ›Ä‡, informacje lub Å¼ycie na ryzyko. Podczas trenowania modeli uczenia maszynowego polegamy na danych, aby uzyskaÄ‡ najlepsze wyniki. W zwiÄ…zku z tym naleÅ¼y wziÄ…Ä‡ pod uwagÄ™ pochodzenie danych i ich integralnoÅ›Ä‡. Na przykÅ‚ad, czy dane zostaÅ‚y dostarczone przez uÅ¼ytkownika, czy byÅ‚y publicznie dostÄ™pne? NastÄ™pnie, pracujÄ…c z danymi, kluczowe jest opracowanie systemÃ³w AI, ktÃ³re mogÄ… chroniÄ‡ poufne informacje i opieraÄ‡ siÄ™ atakom. W miarÄ™ jak AI staje siÄ™ coraz bardziej powszechne, ochrona prywatnoÅ›ci i zabezpieczanie waÅ¼nych informacji osobistych i biznesowych staje siÄ™ coraz bardziej istotna i zÅ‚oÅ¼ona. Problemy zwiÄ…zane z prywatnoÅ›ciÄ… i bezpieczeÅ„stwem danych wymagajÄ… szczegÃ³lnej uwagi w przypadku AI, poniewaÅ¼ dostÄ™p do danych jest niezbÄ™dny, aby systemy AI mogÅ‚y dokonywaÄ‡ dokÅ‚adnych i Å›wiadomych prognoz oraz podejmowaÄ‡ decyzje dotyczÄ…ce ludzi.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: bezpieczeÅ„stwo w AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- BranÅ¼a poczyniÅ‚a znaczÄ…ce postÄ™py w zakresie prywatnoÅ›ci i bezpieczeÅ„stwa, w duÅ¼ej mierze dziÄ™ki regulacjom takim jak GDPR (OgÃ³lne RozporzÄ…dzenie o Ochronie Danych).
- Jednak w przypadku systemÃ³w AI musimy uznaÄ‡ napiÄ™cie miÄ™dzy potrzebÄ… wiÄ™kszej iloÅ›ci danych osobowych w celu uczynienia systemÃ³w bardziej osobistymi i skutecznymi â€“ a prywatnoÅ›ciÄ….
- Podobnie jak w przypadku narodzin poÅ‚Ä…czonych komputerÃ³w z internetem, obserwujemy rÃ³wnieÅ¼ ogromny wzrost liczby problemÃ³w zwiÄ…zanych z bezpieczeÅ„stwem w kontekÅ›cie AI.
- JednoczeÅ›nie widzimy, Å¼e AI jest wykorzystywane do poprawy bezpieczeÅ„stwa. Na przykÅ‚ad wiÄ™kszoÅ›Ä‡ nowoczesnych skanerÃ³w antywirusowych jest dziÅ› napÄ™dzana przez heurystyki AI.
- Musimy upewniÄ‡ siÄ™, Å¼e nasze procesy Data Science harmonijnie wspÃ³Å‚grajÄ… z najnowszymi praktykami dotyczÄ…cymi prywatnoÅ›ci i bezpieczeÅ„stwa.

### PrzejrzystoÅ›Ä‡

Systemy AI powinny byÄ‡ zrozumiaÅ‚e. Kluczowym elementem przejrzystoÅ›ci jest wyjaÅ›nienie zachowania systemÃ³w AI i ich komponentÃ³w. Poprawa zrozumienia systemÃ³w AI wymaga, aby interesariusze rozumieli, jak i dlaczego dziaÅ‚ajÄ…, aby mogli zidentyfikowaÄ‡ potencjalne problemy z wydajnoÅ›ciÄ…, obawy dotyczÄ…ce bezpieczeÅ„stwa i prywatnoÅ›ci, uprzedzenia, praktyki wykluczajÄ…ce lub niezamierzone rezultaty. Wierzymy rÃ³wnieÅ¼, Å¼e ci, ktÃ³rzy korzystajÄ… z systemÃ³w AI, powinni byÄ‡ uczciwi i otwarci w kwestii tego, kiedy, dlaczego i jak decydujÄ… siÄ™ je wdraÅ¼aÄ‡. A takÅ¼e w kwestii ograniczeÅ„ systemÃ³w, z ktÃ³rych korzystajÄ…. Na przykÅ‚ad, jeÅ›li bank korzysta z systemu AI wspierajÄ…cego decyzje kredytowe dla konsumentÃ³w, waÅ¼ne jest, aby przeanalizowaÄ‡ wyniki i zrozumieÄ‡, ktÃ³re dane wpÅ‚ywajÄ… na rekomendacje systemu. RzÄ…dy zaczynajÄ… regulowaÄ‡ AI w rÃ³Å¼nych branÅ¼ach, wiÄ™c naukowcy danych i organizacje muszÄ… wyjaÅ›niÄ‡, czy system AI speÅ‚nia wymagania regulacyjne, zwÅ‚aszcza gdy pojawia siÄ™ niepoÅ¼Ä…dany rezultat.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: przejrzystoÅ›Ä‡ w AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- PoniewaÅ¼ systemy AI sÄ… tak zÅ‚oÅ¼one, trudno jest zrozumieÄ‡, jak dziaÅ‚ajÄ… i interpretowaÄ‡ wyniki.
- Ten brak zrozumienia wpÅ‚ywa na sposÃ³b zarzÄ…dzania, operacjonalizacji i dokumentowania tych systemÃ³w.
- Co waÅ¼niejsze, brak zrozumienia wpÅ‚ywa na decyzje podejmowane na podstawie wynikÃ³w, ktÃ³re te systemy produkujÄ….

### OdpowiedzialnoÅ›Ä‡

Osoby projektujÄ…ce i wdraÅ¼ajÄ…ce systemy AI muszÄ… byÄ‡ odpowiedzialne za sposÃ³b, w jaki ich systemy dziaÅ‚ajÄ…. Potrzeba odpowiedzialnoÅ›ci jest szczegÃ³lnie istotna w przypadku technologii wraÅ¼liwych, takich jak rozpoznawanie twarzy. W ostatnim czasie roÅ›nie zapotrzebowanie na technologiÄ™ rozpoznawania twarzy, zwÅ‚aszcza ze strony organizacji zajmujÄ…cych siÄ™ egzekwowaniem prawa, ktÃ³re dostrzegajÄ… potencjaÅ‚ tej technologii w takich zastosowaniach jak odnajdywanie zaginionych dzieci. Jednak te technologie mogÄ… byÄ‡ potencjalnie wykorzystywane przez rzÄ…dy do naruszania podstawowych wolnoÅ›ci obywateli, na przykÅ‚ad poprzez umoÅ¼liwienie ciÄ…gÅ‚ego monitorowania konkretnych osÃ³b. Dlatego naukowcy danych i organizacje muszÄ… byÄ‡ odpowiedzialni za to, jak ich systemy AI wpÅ‚ywajÄ… na jednostki lub spoÅ‚eczeÅ„stwo.

[![CzoÅ‚owy badacz AI ostrzega przed masowÄ… inwigilacjÄ… za pomocÄ… rozpoznawania twarzy](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "PodejÅ›cie Microsoftu do odpowiedzialnej AI")

> ğŸ¥ Kliknij obrazek powyÅ¼ej, aby obejrzeÄ‡ film: OstrzeÅ¼enia przed masowÄ… inwigilacjÄ… za pomocÄ… rozpoznawania twarzy

Ostatecznie jednym z najwiÄ™kszych pytaÅ„ dla naszego pokolenia, jako pierwszego pokolenia wprowadzajÄ…cego AI do spoÅ‚eczeÅ„stwa, jest to, jak zapewniÄ‡, Å¼e komputery pozostanÄ… odpowiedzialne wobec ludzi i jak zapewniÄ‡, Å¼e osoby projektujÄ…ce komputery pozostanÄ… odpowiedzialne wobec innych.

## Ocena wpÅ‚ywu

Przed trenowaniem modelu uczenia maszynowego waÅ¼ne jest przeprowadzenie oceny wpÅ‚ywu, aby zrozumieÄ‡ cel systemu AI; jego zamierzone zastosowanie; miejsce wdroÅ¼enia; oraz osoby, ktÃ³re bÄ™dÄ… wchodziÄ‡ w interakcjÄ™ z systemem. SÄ… to pomocne informacje dla recenzentÃ³w lub testerÃ³w oceniajÄ…cych system, aby wiedzieÄ‡, jakie czynniki naleÅ¼y wziÄ…Ä‡ pod uwagÄ™ przy identyfikowaniu potencjalnych ryzyk i oczekiwanych konsekwencji.

PoniÅ¼ej znajdujÄ… siÄ™ obszary, na ktÃ³rych naleÅ¼y siÄ™ skupiÄ‡ podczas przeprowadzania oceny wpÅ‚ywu:

* **Negatywny wpÅ‚yw na jednostki**. WaÅ¼ne jest, aby byÄ‡ Å›wiadomym wszelkich ograniczeÅ„ lub wymagaÅ„, nieobsÅ‚ugiwanych zastosowaÅ„ lub znanych ograniczeÅ„ utrudniajÄ…cych dziaÅ‚anie systemu, aby upewniÄ‡ siÄ™, Å¼e system nie jest uÅ¼ywany w sposÃ³b, ktÃ³ry mÃ³gÅ‚by zaszkodziÄ‡ jednostkom.
* **Wymagania dotyczÄ…ce danych**. Zrozumienie, jak i gdzie system bÄ™dzie korzystaÅ‚ z danych, pozwala recenzentom zbadaÄ‡ wszelkie wymagania dotyczÄ…ce danych, ktÃ³re naleÅ¼y uwzglÄ™dniÄ‡ (np. regulacje GDPR lub HIPPA). Ponadto naleÅ¼y sprawdziÄ‡, czy ÅºrÃ³dÅ‚o lub iloÅ›Ä‡ danych sÄ… wystarczajÄ…ce do trenowania.
* **Podsumowanie wpÅ‚ywu**. Zbierz listÄ™ potencjalnych szkÃ³d, ktÃ³re mogÄ… wyniknÄ…Ä‡ z uÅ¼ywania systemu. Przez caÅ‚y cykl Å¼ycia ML sprawdzaj, czy zidentyfikowane problemy zostaÅ‚y zÅ‚agodzone lub rozwiÄ…zane.
* **Cele dla kaÅ¼dej z szeÅ›ciu podstawowych zasad**. OceÅ„, czy cele wynikajÄ…ce z kaÅ¼dej zasady zostaÅ‚y osiÄ…gniÄ™te i czy istniejÄ… jakieÅ› luki.

## Debugowanie z odpowiedzialnÄ… AI

Podobnie jak debugowanie aplikacji programowej, debugowanie systemu AI jest niezbÄ™dnym procesem identyfikowania i rozwiÄ…zywania problemÃ³w w systemie. Istnieje wiele czynnikÃ³w, ktÃ³re mogÄ… wpÅ‚ywaÄ‡ na to, Å¼e model nie dziaÅ‚a zgodnie z oczekiwaniami lub odpowiedzialnie. WiÄ™kszoÅ›Ä‡ tradycyjnych metryk wydajnoÅ›ci modelu to iloÅ›ciowe agregaty wydajnoÅ›ci modelu, ktÃ³re nie sÄ… wystarczajÄ…ce do analizy, w jaki sposÃ³b model narusza zasady odpowiedzialnej AI. Ponadto model uczenia maszynowego jest â€czarnÄ… skrzynkÄ…â€, co utrudnia zrozumienie, co napÄ™dza jego wyniki lub wyjaÅ›nienie, dlaczego popeÅ‚nia bÅ‚Ä™dy. W dalszej czÄ™Å›ci tego kursu nauczymy siÄ™, jak korzystaÄ‡ z pulpitu odpowiedzialnej AI, aby pomÃ³c w debugowaniu systemÃ³w AI. Pulpit zapewnia kompleksowe narzÄ™dzie dla naukowcÃ³w danych i programistÃ³w AI do wykonywania:

* **Analizy bÅ‚Ä™dÃ³w**. Aby zidentyfikowaÄ‡ rozkÅ‚ad bÅ‚Ä™dÃ³w modelu, ktÃ³ry moÅ¼e wpÅ‚ywaÄ‡ na sprawiedliwoÅ›Ä‡ lub niezawodnoÅ›Ä‡ systemu.
* **PrzeglÄ…du modelu**. Aby odkryÄ‡, gdzie wystÄ™pujÄ… rÃ³Å¼nice w wydajnoÅ›ci modelu w rÃ³Å¼nych grupach danych.
* **Analizy danych**. Aby zrozumieÄ‡ rozkÅ‚ad danych i zidentyfik
Obejrzyj ten warsztat, aby zgÅ‚Ä™biÄ‡ tematy:

- W pogoni za odpowiedzialnÄ… AI: Wprowadzenie zasad w praktykÄ™ przez BesmirÄ™ Nushi, Mehrnoosh Sameki i Amita SharmÄ™

[![Responsible AI Toolbox: OtwartoÅºrÃ³dÅ‚owe narzÄ™dzie do budowy odpowiedzialnej AI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: OtwartoÅºrÃ³dÅ‚owe narzÄ™dzie do budowy odpowiedzialnej AI")

> ğŸ¥ Kliknij obrazek powyÅ¼ej, aby obejrzeÄ‡ wideo: RAI Toolbox: OtwartoÅºrÃ³dÅ‚owe narzÄ™dzie do budowy odpowiedzialnej AI przez BesmirÄ™ Nushi, Mehrnoosh Sameki i Amita SharmÄ™

Przeczytaj rÃ³wnieÅ¼:

- Centrum zasobÃ³w RAI Microsoftu: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupa badawcza FATE Microsoftu: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Repozytorium GitHub Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Przeczytaj o narzÄ™dziach Azure Machine Learning zapewniajÄ…cych sprawiedliwoÅ›Ä‡:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Zadanie

[Poznaj RAI Toolbox](assignment.md)

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ dokÅ‚adamy wszelkich staraÅ„, aby zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia wykonanego przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z korzystania z tego tÅ‚umaczenia.