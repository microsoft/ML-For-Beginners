<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8f819813b2ca08ec7b9f60a2c9336045",
  "translation_date": "2025-09-03T17:37:05+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "pl"
}
-->
# Budowanie rozwiÄ…zaÅ„ Machine Learning z odpowiedzialnÄ… AI

![Podsumowanie odpowiedzialnej AI w Machine Learning na szkicowniku](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.pl.png)
> Szkicownik autorstwa [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz przed wykÅ‚adem](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Wprowadzenie

W tym kursie zaczniesz odkrywaÄ‡, jak uczenie maszynowe wpÅ‚ywa na nasze codzienne Å¼ycie. JuÅ¼ teraz systemy i modele sÄ… zaangaÅ¼owane w codzienne zadania decyzyjne, takie jak diagnozy medyczne, zatwierdzanie kredytÃ³w czy wykrywanie oszustw. Dlatego waÅ¼ne jest, aby te modele dziaÅ‚aÅ‚y dobrze i dostarczaÅ‚y wyniki, ktÃ³rym moÅ¼na zaufaÄ‡. Podobnie jak kaÅ¼da aplikacja, systemy AI mogÄ… nie speÅ‚niaÄ‡ oczekiwaÅ„ lub prowadziÄ‡ do niepoÅ¼Ä…danych rezultatÃ³w. Dlatego kluczowe jest zrozumienie i wyjaÅ›nienie zachowania modelu AI.

WyobraÅº sobie, co moÅ¼e siÄ™ staÄ‡, gdy dane uÅ¼ywane do budowy tych modeli nie uwzglÄ™dniajÄ… pewnych grup demograficznych, takich jak rasa, pÅ‚eÄ‡, poglÄ…dy polityczne, religia, lub gdy sÄ… one nadmiernie reprezentowane. Co, jeÅ›li wyniki modelu sÄ… interpretowane w sposÃ³b faworyzujÄ…cy pewne grupy? Jakie sÄ… konsekwencje dla aplikacji? Co siÄ™ dzieje, gdy model prowadzi do szkodliwych rezultatÃ³w? Kto jest odpowiedzialny za zachowanie systemu AI? To sÄ… pytania, ktÃ³re bÄ™dziemy eksplorowaÄ‡ w tym kursie.

W tej lekcji dowiesz siÄ™:

- Dlaczego sprawiedliwoÅ›Ä‡ w uczeniu maszynowym i zwiÄ…zane z niÄ… szkody sÄ… tak waÅ¼ne.
- Jak badaÄ‡ odstajÄ…ce przypadki i nietypowe scenariusze, aby zapewniÄ‡ niezawodnoÅ›Ä‡ i bezpieczeÅ„stwo.
- Jak projektowaÄ‡ inkluzywne systemy, ktÃ³re wspierajÄ… wszystkich.
- Dlaczego ochrona prywatnoÅ›ci i bezpieczeÅ„stwa danych oraz ludzi jest kluczowa.
- Jak waÅ¼ne jest wyjaÅ›nianie zachowania modeli AI w sposÃ³b przejrzysty.
- Dlaczego odpowiedzialnoÅ›Ä‡ jest kluczowa dla budowania zaufania do systemÃ³w AI.

## Wymagania wstÄ™pne

Przed rozpoczÄ™ciem, zapoznaj siÄ™ z "Zasadami odpowiedzialnej AI" w Å›cieÅ¼ce edukacyjnej i obejrzyj poniÅ¼szy film na ten temat:

Dowiedz siÄ™ wiÄ™cej o odpowiedzialnej AI, korzystajÄ…c z tej [Å›cieÅ¼ki edukacyjnej](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![PodejÅ›cie Microsoftu do odpowiedzialnej AI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "PodejÅ›cie Microsoftu do odpowiedzialnej AI")

> ğŸ¥ Kliknij obrazek powyÅ¼ej, aby obejrzeÄ‡ film: PodejÅ›cie Microsoftu do odpowiedzialnej AI

## SprawiedliwoÅ›Ä‡

Systemy AI powinny traktowaÄ‡ wszystkich sprawiedliwie i unikaÄ‡ rÃ³Å¼nic w traktowaniu podobnych grup ludzi. Na przykÅ‚ad, gdy systemy AI udzielajÄ… wskazÃ³wek dotyczÄ…cych leczenia, aplikacji kredytowych czy zatrudnienia, powinny wydawaÄ‡ takie same rekomendacje wszystkim osobom o podobnych objawach, sytuacji finansowej czy kwalifikacjach zawodowych. KaÅ¼dy z nas jako czÅ‚owiek nosi w sobie odziedziczone uprzedzenia, ktÃ³re wpÅ‚ywajÄ… na nasze decyzje i dziaÅ‚ania. Te uprzedzenia mogÄ… byÄ‡ widoczne w danych, ktÃ³re wykorzystujemy do trenowania systemÃ³w AI. Czasami takie manipulacje zdarzajÄ… siÄ™ nieumyÅ›lnie. Trudno jest Å›wiadomie zauwaÅ¼yÄ‡, kiedy wprowadzamy uprzedzenia do danych.

**â€NiesprawiedliwoÅ›Ä‡â€** obejmuje negatywne skutki, czyli â€szkodyâ€, dla grup ludzi, takich jak te zdefiniowane na podstawie rasy, pÅ‚ci, wieku czy niepeÅ‚nosprawnoÅ›ci. GÅ‚Ã³wne szkody zwiÄ…zane ze sprawiedliwoÅ›ciÄ… moÅ¼na sklasyfikowaÄ‡ jako:

- **Alokacja**, gdy na przykÅ‚ad pÅ‚eÄ‡ lub etnicznoÅ›Ä‡ jest faworyzowana kosztem innych.
- **JakoÅ›Ä‡ usÅ‚ug**. JeÅ›li dane sÄ… trenowane dla jednego konkretnego scenariusza, ale rzeczywistoÅ›Ä‡ jest znacznie bardziej zÅ‚oÅ¼ona, prowadzi to do sÅ‚abej jakoÅ›ci usÅ‚ug. Na przykÅ‚ad dozownik mydÅ‚a, ktÃ³ry nie potrafiÅ‚ rozpoznaÄ‡ osÃ³b o ciemnej skÃ³rze. [Å¹rÃ³dÅ‚o](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Oczernianie**. Niesprawiedliwe krytykowanie i etykietowanie czegoÅ› lub kogoÅ›. Na przykÅ‚ad technologia etykietowania obrazÃ³w bÅ‚Ä™dnie oznaczyÅ‚a zdjÄ™cia osÃ³b o ciemnej skÃ³rze jako goryle.
- **Nadmierna lub niedostateczna reprezentacja**. Chodzi o to, Å¼e pewna grupa nie jest widoczna w okreÅ›lonym zawodzie, a kaÅ¼da usÅ‚uga lub funkcja, ktÃ³ra to utrwala, przyczynia siÄ™ do szkody.
- **Stereotypizacja**. Przypisywanie okreÅ›lonej grupie z gÃ³ry ustalonych cech. Na przykÅ‚ad system tÅ‚umaczenia jÄ™zykowego miÄ™dzy angielskim a tureckim moÅ¼e mieÄ‡ nieÅ›cisÅ‚oÅ›ci wynikajÄ…ce ze stereotypowych skojarzeÅ„ z pÅ‚ciÄ….

![tÅ‚umaczenie na turecki](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.pl.png)
> tÅ‚umaczenie na turecki

![tÅ‚umaczenie z powrotem na angielski](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.pl.png)
> tÅ‚umaczenie z powrotem na angielski

Podczas projektowania i testowania systemÃ³w AI musimy upewniÄ‡ siÄ™, Å¼e AI jest sprawiedliwe i nie jest zaprogramowane do podejmowania uprzedzonych lub dyskryminujÄ…cych decyzji, ktÃ³rych ludzie rÃ³wnieÅ¼ nie powinni podejmowaÄ‡. Zapewnienie sprawiedliwoÅ›ci w AI i uczeniu maszynowym pozostaje zÅ‚oÅ¼onym wyzwaniem spoÅ‚eczno-technologicznym.

### NiezawodnoÅ›Ä‡ i bezpieczeÅ„stwo

Aby budowaÄ‡ zaufanie, systemy AI muszÄ… byÄ‡ niezawodne, bezpieczne i spÃ³jne w normalnych i nieoczekiwanych warunkach. WaÅ¼ne jest, aby wiedzieÄ‡, jak systemy AI bÄ™dÄ… siÄ™ zachowywaÄ‡ w rÃ³Å¼nych sytuacjach, zwÅ‚aszcza w przypadku odstajÄ…cych przypadkÃ³w. Podczas budowania rozwiÄ…zaÅ„ AI naleÅ¼y poÅ›wiÄ™ciÄ‡ znacznÄ… uwagÄ™ temu, jak radziÄ‡ sobie z szerokÄ… gamÄ… okolicznoÅ›ci, z ktÃ³rymi mogÄ… siÄ™ spotkaÄ‡. Na przykÅ‚ad samochÃ³d autonomiczny musi stawiaÄ‡ bezpieczeÅ„stwo ludzi na pierwszym miejscu. W zwiÄ…zku z tym AI napÄ™dzajÄ…ce samochÃ³d musi uwzglÄ™dniaÄ‡ wszystkie moÅ¼liwe scenariusze, takie jak noc, burze, zamiecie Å›nieÅ¼ne, dzieci przebiegajÄ…ce przez ulicÄ™, zwierzÄ™ta, roboty drogowe itd. To, jak dobrze system AI radzi sobie z szerokim zakresem warunkÃ³w w sposÃ³b niezawodny i bezpieczny, odzwierciedla poziom przewidywania, ktÃ³ry naukowiec danych lub programista AI uwzglÄ™dniÅ‚ podczas projektowania lub testowania systemu.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### InkluzywnoÅ›Ä‡

Systemy AI powinny byÄ‡ projektowane tak, aby angaÅ¼owaÄ‡ i wspieraÄ‡ wszystkich. Podczas projektowania i wdraÅ¼ania systemÃ³w AI naukowcy danych i programiÅ›ci AI identyfikujÄ… i eliminujÄ… potencjalne bariery w systemie, ktÃ³re mogÅ‚yby nieumyÅ›lnie wykluczaÄ‡ ludzi. Na przykÅ‚ad na Å›wiecie jest 1 miliard osÃ³b z niepeÅ‚nosprawnoÅ›ciami. DziÄ™ki postÄ™powi AI mogÄ… oni Å‚atwiej uzyskiwaÄ‡ dostÄ™p do szerokiego zakresu informacji i moÅ¼liwoÅ›ci w codziennym Å¼yciu. Eliminowanie barier tworzy moÅ¼liwoÅ›ci innowacji i rozwijania produktÃ³w AI z lepszymi doÅ›wiadczeniami, ktÃ³re przynoszÄ… korzyÅ›ci wszystkim.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: inkluzywnoÅ›Ä‡ w AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### BezpieczeÅ„stwo i prywatnoÅ›Ä‡

Systemy AI powinny byÄ‡ bezpieczne i szanowaÄ‡ prywatnoÅ›Ä‡ ludzi. Ludzie majÄ… mniejsze zaufanie do systemÃ³w, ktÃ³re naraÅ¼ajÄ… ich prywatnoÅ›Ä‡, informacje lub Å¼ycie na ryzyko. Podczas trenowania modeli uczenia maszynowego polegamy na danych, aby uzyskaÄ‡ najlepsze wyniki. W zwiÄ…zku z tym naleÅ¼y uwzglÄ™dniÄ‡ pochodzenie danych i ich integralnoÅ›Ä‡. Na przykÅ‚ad, czy dane zostaÅ‚y dostarczone przez uÅ¼ytkownika, czy sÄ… publicznie dostÄ™pne? NastÄ™pnie, pracujÄ…c z danymi, kluczowe jest opracowanie systemÃ³w AI, ktÃ³re mogÄ… chroniÄ‡ poufne informacje i opieraÄ‡ siÄ™ atakom. W miarÄ™ jak AI staje siÄ™ coraz bardziej powszechne, ochrona prywatnoÅ›ci i zabezpieczanie waÅ¼nych informacji osobistych i biznesowych staje siÄ™ coraz bardziej krytyczna i zÅ‚oÅ¼ona. Problemy zwiÄ…zane z prywatnoÅ›ciÄ… i bezpieczeÅ„stwem danych wymagajÄ… szczegÃ³lnej uwagi w przypadku AI, poniewaÅ¼ dostÄ™p do danych jest niezbÄ™dny, aby systemy AI mogÅ‚y podejmowaÄ‡ dokÅ‚adne i Å›wiadome decyzje dotyczÄ…ce ludzi.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: bezpieczeÅ„stwo w AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- BranÅ¼a poczyniÅ‚a znaczÄ…ce postÄ™py w zakresie prywatnoÅ›ci i bezpieczeÅ„stwa, w duÅ¼ej mierze dziÄ™ki regulacjom takim jak GDPR (OgÃ³lne RozporzÄ…dzenie o Ochronie Danych).
- Jednak w przypadku systemÃ³w AI musimy uznaÄ‡ napiÄ™cie miÄ™dzy potrzebÄ… wiÄ™kszej iloÅ›ci danych osobowych, aby systemy byÅ‚y bardziej efektywne, a prywatnoÅ›ciÄ….
- Podobnie jak w przypadku narodzin poÅ‚Ä…czonych komputerÃ³w z internetem, obserwujemy rÃ³wnieÅ¼ ogromny wzrost liczby problemÃ³w zwiÄ…zanych z bezpieczeÅ„stwem w kontekÅ›cie AI.
- JednoczeÅ›nie widzimy, Å¼e AI jest wykorzystywane do poprawy bezpieczeÅ„stwa. Na przykÅ‚ad wiÄ™kszoÅ›Ä‡ nowoczesnych skanerÃ³w antywirusowych jest dziÅ› napÄ™dzana przez heurystyki AI.
- Musimy upewniÄ‡ siÄ™, Å¼e nasze procesy Data Science harmonijnie wspÃ³Å‚grajÄ… z najnowszymi praktykami dotyczÄ…cymi prywatnoÅ›ci i bezpieczeÅ„stwa.

### PrzejrzystoÅ›Ä‡

Systemy AI powinny byÄ‡ zrozumiaÅ‚e. Kluczowym elementem przejrzystoÅ›ci jest wyjaÅ›nianie zachowania systemÃ³w AI i ich komponentÃ³w. Poprawa zrozumienia systemÃ³w AI wymaga, aby interesariusze rozumieli, jak i dlaczego dziaÅ‚ajÄ…, aby mogli zidentyfikowaÄ‡ potencjalne problemy z wydajnoÅ›ciÄ…, obawy dotyczÄ…ce bezpieczeÅ„stwa i prywatnoÅ›ci, uprzedzenia, praktyki wykluczajÄ…ce lub niezamierzone rezultaty. Wierzymy rÃ³wnieÅ¼, Å¼e ci, ktÃ³rzy korzystajÄ… z systemÃ³w AI, powinni byÄ‡ uczciwi i otwarci w kwestii tego, kiedy, dlaczego i jak decydujÄ… siÄ™ je wdraÅ¼aÄ‡, a takÅ¼e ograniczeÅ„ systemÃ³w, z ktÃ³rych korzystajÄ…. Na przykÅ‚ad, jeÅ›li bank korzysta z systemu AI, aby wspieraÄ‡ decyzje dotyczÄ…ce kredytÃ³w konsumenckich, waÅ¼ne jest, aby zbadaÄ‡ wyniki i zrozumieÄ‡, ktÃ³re dane wpÅ‚ywajÄ… na rekomendacje systemu. RzÄ…dy zaczynajÄ… regulowaÄ‡ AI w rÃ³Å¼nych branÅ¼ach, wiÄ™c naukowcy danych i organizacje muszÄ… wyjaÅ›niaÄ‡, czy system AI speÅ‚nia wymagania regulacyjne, zwÅ‚aszcza gdy pojawia siÄ™ niepoÅ¼Ä…dany rezultat.

> [ğŸ¥ Kliknij tutaj, aby obejrzeÄ‡ film: przejrzystoÅ›Ä‡ w AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- PoniewaÅ¼ systemy AI sÄ… tak zÅ‚oÅ¼one, trudno jest zrozumieÄ‡, jak dziaÅ‚ajÄ… i interpretowaÄ‡ wyniki.
- Ten brak zrozumienia wpÅ‚ywa na sposÃ³b zarzÄ…dzania, operacjonalizacji i dokumentowania tych systemÃ³w.
- Co waÅ¼niejsze, brak zrozumienia wpÅ‚ywa na decyzje podejmowane na podstawie wynikÃ³w, ktÃ³re te systemy produkujÄ….

### OdpowiedzialnoÅ›Ä‡

Osoby projektujÄ…ce i wdraÅ¼ajÄ…ce systemy AI muszÄ… byÄ‡ odpowiedzialne za sposÃ³b, w jaki ich systemy dziaÅ‚ajÄ…. Potrzeba odpowiedzialnoÅ›ci jest szczegÃ³lnie istotna w przypadku technologii wraÅ¼liwych, takich jak rozpoznawanie twarzy. W ostatnim czasie roÅ›nie zapotrzebowanie na technologiÄ™ rozpoznawania twarzy, zwÅ‚aszcza ze strony organizacji zajmujÄ…cych siÄ™ egzekwowaniem prawa, ktÃ³re dostrzegajÄ… potencjaÅ‚ tej technologii w takich zastosowaniach jak odnajdywanie zaginionych dzieci. Jednak te technologie mogÄ… byÄ‡ potencjalnie wykorzystywane przez rzÄ…dy do naruszania podstawowych wolnoÅ›ci obywateli, na przykÅ‚ad poprzez umoÅ¼liwienie ciÄ…gÅ‚ego monitorowania konkretnych osÃ³b. Dlatego naukowcy danych i organizacje muszÄ… byÄ‡ odpowiedzialni za to, jak ich system AI wpÅ‚ywa na jednostki lub spoÅ‚eczeÅ„stwo.

[![CzoÅ‚owy badacz AI ostrzega przed masowÄ… inwigilacjÄ… za pomocÄ… rozpoznawania twarzy](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.pl.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "PodejÅ›cie Microsoftu do odpowiedzialnej AI")

> ğŸ¥ Kliknij obrazek powyÅ¼ej, aby obejrzeÄ‡ film: OstrzeÅ¼enia przed masowÄ… inwigilacjÄ… za pomocÄ… rozpoznawania twarzy

Ostatecznie jednym z najwiÄ™kszych pytaÅ„ dla naszego pokolenia, jako pierwszego, ktÃ³re wprowadza AI do spoÅ‚eczeÅ„stwa, jest to, jak zapewniÄ‡, Å¼e komputery pozostanÄ… odpowiedzialne wobec ludzi i jak zapewniÄ‡, Å¼e osoby projektujÄ…ce komputery pozostanÄ… odpowiedzialne wobec innych.

## Ocena wpÅ‚ywu

Przed trenowaniem modelu uczenia maszynowego waÅ¼ne jest przeprowadzenie oceny wpÅ‚ywu, aby zrozumieÄ‡ cel systemu AI, jego zamierzone zastosowanie, miejsce wdroÅ¼enia oraz osoby, ktÃ³re bÄ™dÄ… wchodziÄ‡ w interakcjÄ™ z systemem. SÄ… to pomocne informacje dla recenzentÃ³w lub testerÃ³w oceniajÄ…cych system, aby wiedzieÄ‡, jakie czynniki naleÅ¼y wziÄ…Ä‡ pod uwagÄ™ przy identyfikowaniu potencjalnych ryzyk i oczekiwanych konsekwencji.

Oto obszary, na ktÃ³re naleÅ¼y zwrÃ³ciÄ‡ uwagÄ™ podczas przeprowadzania oceny wpÅ‚ywu:

* **Negatywny wpÅ‚yw na jednostki**. WaÅ¼ne jest, aby byÄ‡ Å›wiadomym wszelkich ograniczeÅ„, wymagaÅ„, nieobsÅ‚ugiwanych zastosowaÅ„ lub znanych ograniczeÅ„, ktÃ³re mogÄ… utrudniaÄ‡ dziaÅ‚anie systemu, aby upewniÄ‡ siÄ™, Å¼e system nie jest uÅ¼ywany w sposÃ³b, ktÃ³ry mÃ³gÅ‚by zaszkodziÄ‡ jednostkom.
* **Wymagania dotyczÄ…ce danych**. Zrozumienie, jak i gdzie system bÄ™dzie korzystaÅ‚ z danych, pozwala recenzentom zbadaÄ‡ wszelkie wymagania dotyczÄ…ce danych, ktÃ³re naleÅ¼y uwzglÄ™dniÄ‡ (np. regulacje GDPR lub HIPPA). Ponadto naleÅ¼y sprawdziÄ‡, czy ÅºrÃ³dÅ‚o lub iloÅ›Ä‡ danych sÄ… wystarczajÄ…ce do trenowania.
* **Podsumowanie wpÅ‚ywu**. Zbierz listÄ™ potencjalnych szkÃ³d, ktÃ³re mogÄ… wyniknÄ…Ä‡ z uÅ¼ywania systemu. Przez caÅ‚y cykl Å¼ycia ML sprawdzaj, czy zidentyfikowane problemy zostaÅ‚y zÅ‚agodzone lub rozwiÄ…zane.
* **Cele zwiÄ…zane z szeÅ›cioma podstawowymi zasadami**. OceÅ„, czy cele wynikajÄ…ce z kaÅ¼dej z zasad zostaÅ‚y osiÄ…gniÄ™te i czy istniejÄ… jakieÅ› luki.

## Debugowanie z odpowiedzialnÄ… AI

Podobnie jak debugowanie aplikacji, debugowanie systemu AI jest niezbÄ™dnym procesem identyfikowania i rozwiÄ…zywania problemÃ³w w systemie. Istnieje wiele czynnikÃ³w, ktÃ³re mogÄ… wpÅ‚ywaÄ‡ na to, Å¼e model nie dziaÅ‚a zgodnie z oczekiwaniami lub odpowiedzialnie. WiÄ™kszoÅ›Ä‡ tradycyjnych metryk wydajnoÅ›ci modelu to iloÅ›ciowe agregaty wydajnoÅ›ci modelu, ktÃ³re nie sÄ… wystarczajÄ…ce do analizy, w jaki sposÃ³b model narusza zasady odpowiedzialnej AI. Ponadto model uczenia maszynowego jest czarnÄ… skrzynkÄ…, co utrudnia zrozumienie, co napÄ™dza jego wyniki lub wyjaÅ›nienie, dlaczego popeÅ‚nia bÅ‚Ä™dy. W dalszej czÄ™Å›ci kursu nauczymy siÄ™ korzystaÄ‡ z pulpitu odpowiedzialnej AI, ktÃ³ry pomaga debugowaÄ‡ systemy AI. Pulpit zapewnia kompleksowe narzÄ™dzie dla naukowcÃ³w danych i programistÃ³w AI do wykonywania:

* **Analizy bÅ‚Ä™dÃ³w**. Aby zidentyfikowaÄ‡ rozkÅ‚ad bÅ‚Ä™dÃ³w modelu, ktÃ³ry moÅ¼e wpÅ‚ywaÄ‡ na sprawiedliwoÅ›Ä‡ lub niezawodnoÅ›Ä‡ systemu.
* **PrzeglÄ…du modelu**. Aby odkryÄ‡, gdzie wystÄ™pujÄ… rÃ³Å¼nice w wydajnoÅ›ci modelu w rÃ³Å¼nych grupach danych.
* **Analizy danych**. Aby zrozumieÄ‡ rozkÅ‚ad danych i zidentyfikowaÄ‡ potencjalne uprzedzenia w danych, ktÃ³re mogÄ… prowadziÄ‡ do problemÃ³w ze sprawiedliwoÅ›ciÄ…, inkluzywnoÅ›ciÄ… i niezawodnoÅ›ciÄ….
* **Interpretacji modelu**. Aby zrozumieÄ‡, co wpÅ‚ywa na przewidywania modelu. To pomaga wyjaÅ›niÄ‡ zachowanie modelu, co jest waÅ¼ne dla przejrzystoÅ›ci i odpowiedzial
W tej lekcji nauczyÅ‚eÅ› siÄ™ podstawowych pojÄ™Ä‡ dotyczÄ…cych sprawiedliwoÅ›ci i niesprawiedliwoÅ›ci w uczeniu maszynowym.  

Obejrzyj ten warsztat, aby zgÅ‚Ä™biÄ‡ temat: 

- W pogoni za odpowiedzialnÄ… sztucznÄ… inteligencjÄ…: Wprowadzenie zasad w praktykÄ™, prowadzone przez BesmirÄ™ Nushi, Mehrnoosh Sameki i Amita SharmÄ™

[![Responsible AI Toolbox: OtwartoÅºrÃ³dÅ‚owe narzÄ™dzie do budowy odpowiedzialnej AI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: OtwartoÅºrÃ³dÅ‚owe narzÄ™dzie do budowy odpowiedzialnej AI")


> ğŸ¥ Kliknij obrazek powyÅ¼ej, aby obejrzeÄ‡ wideo: RAI Toolbox: OtwartoÅºrÃ³dÅ‚owe narzÄ™dzie do budowy odpowiedzialnej AI, prowadzone przez BesmirÄ™ Nushi, Mehrnoosh Sameki i Amita SharmÄ™

Przeczytaj rÃ³wnieÅ¼: 

- Centrum zasobÃ³w RAI Microsoftu: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Grupa badawcza FATE Microsoftu: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox: 

- [Repozytorium GitHub Responsible AI Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Przeczytaj o narzÄ™dziach Azure Machine Learning zapewniajÄ…cych sprawiedliwoÅ›Ä‡:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## Zadanie

[Poznaj RAI Toolbox](assignment.md)

---

**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ dokÅ‚adamy wszelkich staraÅ„, aby tÅ‚umaczenie byÅ‚o precyzyjne, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego jÄ™zyku ÅºrÃ³dÅ‚owym powinien byÄ‡ uznawany za autorytatywne ÅºrÃ³dÅ‚o. W przypadku informacji o kluczowym znaczeniu zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.