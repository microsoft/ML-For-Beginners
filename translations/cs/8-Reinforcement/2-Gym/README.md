<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T01:15:06+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "cs"
}
-->
# CartPole BruslenÃ­

ProblÃ©m, kterÃ½ jsme Å™eÅ¡ili v pÅ™edchozÃ­ lekci, se mÅ¯Å¾e zdÃ¡t jako hraÄka, kterÃ¡ nemÃ¡ skuteÄnÃ© vyuÅ¾itÃ­ v reÃ¡lnÃ½ch situacÃ­ch. To vÅ¡ak nenÃ­ pravda, protoÅ¾e mnoho reÃ¡lnÃ½ch problÃ©mÅ¯ mÃ¡ podobnÃ½ scÃ©nÃ¡Å™ â€“ napÅ™Ã­klad hranÃ­ Å¡achÅ¯ nebo Go. Jsou podobnÃ©, protoÅ¾e takÃ© mÃ¡me hracÃ­ desku s danÃ½mi pravidly a **diskrÃ©tnÃ­ stav**.

## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ml/)

## Ãšvod

V tÃ©to lekci pouÅ¾ijeme stejnÃ© principy Q-Learningu na problÃ©m s **kontinuÃ¡lnÃ­m stavem**, tj. stavem, kterÃ½ je definovÃ¡n jednÃ­m nebo vÃ­ce reÃ¡lnÃ½mi ÄÃ­sly. Budeme se zabÃ½vat nÃ¡sledujÃ­cÃ­m problÃ©mem:

> **ProblÃ©m**: Pokud chce Petr utÃ©ct vlkovi, musÃ­ se nauÄit pohybovat rychleji. UvidÃ­me, jak se Petr mÅ¯Å¾e nauÄit bruslit, konkrÃ©tnÄ› udrÅ¾ovat rovnovÃ¡hu, pomocÃ­ Q-Learningu.

![VelkÃ½ ÃºtÄ›k!](../../../../8-Reinforcement/2-Gym/images/escape.png)

> Petr a jeho pÅ™Ã¡telÃ© jsou kreativnÃ­, aby unikli vlkovi! ObrÃ¡zek od [Jen Looper](https://twitter.com/jenlooper)

PouÅ¾ijeme zjednoduÅ¡enou verzi udrÅ¾ovÃ¡nÃ­ rovnovÃ¡hy znÃ¡mou jako problÃ©m **CartPole**. Ve svÄ›tÄ› CartPole mÃ¡me horizontÃ¡lnÃ­ jezdec, kterÃ½ se mÅ¯Å¾e pohybovat doleva nebo doprava, a cÃ­lem je udrÅ¾et vertikÃ¡lnÃ­ tyÄ na vrcholu jezdce.

## PÅ™edpoklady

V tÃ©to lekci budeme pouÅ¾Ã­vat knihovnu **OpenAI Gym** k simulaci rÅ¯znÃ½ch **prostÅ™edÃ­**. KÃ³d tÃ©to lekce mÅ¯Å¾ete spustit lokÃ¡lnÄ› (napÅ™. z Visual Studio Code), v takovÃ©m pÅ™Ã­padÄ› se simulace otevÅ™e v novÃ©m oknÄ›. PÅ™i spuÅ¡tÄ›nÃ­ kÃ³du online mÅ¯Å¾e bÃ½t nutnÃ© provÃ©st nÄ›kterÃ© Ãºpravy kÃ³du, jak je popsÃ¡no [zde](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

V pÅ™edchozÃ­ lekci byly pravidla hry a stav definovÃ¡ny tÅ™Ã­dou `Board`, kterou jsme si sami vytvoÅ™ili. Zde pouÅ¾ijeme speciÃ¡lnÃ­ **simulaÄnÃ­ prostÅ™edÃ­**, kterÃ© bude simulovat fyziku za udrÅ¾ovÃ¡nÃ­m rovnovÃ¡hy tyÄe. Jedno z nejpopulÃ¡rnÄ›jÅ¡Ã­ch simulaÄnÃ­ch prostÅ™edÃ­ pro trÃ©novÃ¡nÃ­ algoritmÅ¯ posilovanÃ©ho uÄenÃ­ se nazÃ½vÃ¡ [Gym](https://gym.openai.com/), kterÃ© spravuje [OpenAI](https://openai.com/). PomocÃ­ tohoto gymu mÅ¯Å¾eme vytvoÅ™it rÅ¯znÃ¡ **prostÅ™edÃ­** od simulace CartPole aÅ¾ po hry Atari.

> **PoznÃ¡mka**: DalÅ¡Ã­ prostÅ™edÃ­ dostupnÃ¡ v OpenAI Gym si mÅ¯Å¾ete prohlÃ©dnout [zde](https://gym.openai.com/envs/#classic_control).

Nejprve nainstalujeme gym a importujeme potÅ™ebnÃ© knihovny (blok kÃ³du 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## CviÄenÃ­ â€“ inicializace prostÅ™edÃ­ CartPole

Pro prÃ¡ci s problÃ©mem udrÅ¾ovÃ¡nÃ­ rovnovÃ¡hy CartPole musÃ­me inicializovat odpovÃ­dajÃ­cÃ­ prostÅ™edÃ­. KaÅ¾dÃ© prostÅ™edÃ­ je spojeno s:

- **Prostor pozorovÃ¡nÃ­**, kterÃ½ definuje strukturu informacÃ­, kterÃ© zÃ­skÃ¡vÃ¡me z prostÅ™edÃ­. U problÃ©mu CartPole zÃ­skÃ¡vÃ¡me polohu tyÄe, rychlost a nÄ›kterÃ© dalÅ¡Ã­ hodnoty.

- **Prostor akcÃ­**, kterÃ½ definuje moÅ¾nÃ© akce. V naÅ¡em pÅ™Ã­padÄ› je prostor akcÃ­ diskrÃ©tnÃ­ a sklÃ¡dÃ¡ se ze dvou akcÃ­ â€“ **doleva** a **doprava**. (blok kÃ³du 2)

1. Pro inicializaci napiÅ¡te nÃ¡sledujÃ­cÃ­ kÃ³d:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

Abychom vidÄ›li, jak prostÅ™edÃ­ funguje, spusÅ¥me krÃ¡tkou simulaci na 100 krokÅ¯. V kaÅ¾dÃ©m kroku poskytujeme jednu z akcÃ­, kterÃ© majÃ­ bÃ½t provedeny â€“ v tÃ©to simulaci nÃ¡hodnÄ› vybÃ­rÃ¡me akci z `action_space`.

1. SpusÅ¥te nÃ­Å¾e uvedenÃ½ kÃ³d a podÃ­vejte se, k Äemu vede.

    âœ… Pamatujte, Å¾e je preferovÃ¡no spustit tento kÃ³d na lokÃ¡lnÃ­ instalaci Pythonu! (blok kÃ³du 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    MÄ›li byste vidÄ›t nÄ›co podobnÃ©ho tomuto obrÃ¡zku:

    ![nevyvÃ¡Å¾enÃ½ CartPole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. BÄ›hem simulace potÅ™ebujeme zÃ­skat pozorovÃ¡nÃ­, abychom rozhodli, jak jednat. Ve skuteÄnosti funkce `step` vracÃ­ aktuÃ¡lnÃ­ pozorovÃ¡nÃ­, funkci odmÄ›ny a pÅ™Ã­znak `done`, kterÃ½ oznaÄuje, zda mÃ¡ smysl pokraÄovat v simulaci nebo ne: (blok kÃ³du 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    V notebooku byste mÄ›li vidÄ›t nÄ›co podobnÃ©ho tomuto vÃ½stupu:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Vektor pozorovÃ¡nÃ­, kterÃ½ je vrÃ¡cen pÅ™i kaÅ¾dÃ©m kroku simulace, obsahuje nÃ¡sledujÃ­cÃ­ hodnoty:
    - Poloha vozÃ­ku
    - Rychlost vozÃ­ku
    - Ãšhel tyÄe
    - Rychlost otÃ¡ÄenÃ­ tyÄe

1. ZÃ­skejte minimÃ¡lnÃ­ a maximÃ¡lnÃ­ hodnotu tÄ›chto ÄÃ­sel: (blok kÃ³du 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    MÅ¯Å¾ete si takÃ© vÅ¡imnout, Å¾e hodnota odmÄ›ny pÅ™i kaÅ¾dÃ©m kroku simulace je vÅ¾dy 1. To je proto, Å¾e naÅ¡Ã­m cÃ­lem je pÅ™eÅ¾Ã­t co nejdÃ©le, tj. udrÅ¾et tyÄ v pÅ™imÄ›Å™enÄ› vertikÃ¡lnÃ­ poloze po co nejdelÅ¡Ã­ dobu.

    âœ… Ve skuteÄnosti je simulace CartPole povaÅ¾ovÃ¡na za vyÅ™eÅ¡enou, pokud se nÃ¡m podaÅ™Ã­ zÃ­skat prÅ¯mÄ›rnou odmÄ›nu 195 bÄ›hem 100 po sobÄ› jdoucÃ­ch pokusÅ¯.

## Diskretizace stavu

V Q-Learningu potÅ™ebujeme vytvoÅ™it Q-Tabulku, kterÃ¡ definuje, co dÄ›lat v kaÅ¾dÃ©m stavu. Abychom toho dosÃ¡hli, musÃ­ bÃ½t stav **diskrÃ©tnÃ­**, pÅ™esnÄ›ji Å™eÄeno, musÃ­ obsahovat koneÄnÃ½ poÄet diskrÃ©tnÃ­ch hodnot. Proto musÃ­me nÄ›jak **diskretizovat** naÅ¡e pozorovÃ¡nÃ­, mapovat je na koneÄnou mnoÅ¾inu stavÅ¯.

Existuje nÄ›kolik zpÅ¯sobÅ¯, jak to udÄ›lat:

- **RozdÄ›lenÃ­ na intervaly**. Pokud znÃ¡me interval urÄitÃ© hodnoty, mÅ¯Å¾eme tento interval rozdÄ›lit na nÄ›kolik **intervalÅ¯** a potÃ© nahradit hodnotu ÄÃ­slem intervalu, do kterÃ©ho patÅ™Ã­. To lze provÃ©st pomocÃ­ metody numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). V tomto pÅ™Ã­padÄ› budeme pÅ™esnÄ› znÃ¡t velikost stavu, protoÅ¾e bude zÃ¡viset na poÄtu intervalÅ¯, kterÃ© vybereme pro digitalizaci.

âœ… MÅ¯Å¾eme pouÅ¾Ã­t lineÃ¡rnÃ­ interpolaci k pÅ™ivedenÃ­ hodnot na nÄ›jakÃ½ koneÄnÃ½ interval (napÅ™. od -20 do 20) a potÃ© pÅ™evÃ©st ÄÃ­sla na celÃ¡ ÄÃ­sla zaokrouhlenÃ­m. To nÃ¡m dÃ¡vÃ¡ o nÄ›co menÅ¡Ã­ kontrolu nad velikostÃ­ stavu, zejmÃ©na pokud neznÃ¡me pÅ™esnÃ© rozsahy vstupnÃ­ch hodnot. NapÅ™Ã­klad v naÅ¡em pÅ™Ã­padÄ› 2 ze 4 hodnot nemajÃ­ hornÃ­/dolnÃ­ hranice svÃ½ch hodnot, coÅ¾ mÅ¯Å¾e vÃ©st k nekoneÄnÃ©mu poÄtu stavÅ¯.

V naÅ¡em pÅ™Ã­kladu pouÅ¾ijeme druhÃ½ pÅ™Ã­stup. Jak si moÅ¾nÃ¡ pozdÄ›ji vÅ¡imnete, navzdory nedefinovanÃ½m hornÃ­m/dolnÃ­m hranicÃ­m tyto hodnoty zÅ™Ã­dka nabÃ½vajÃ­ hodnot mimo urÄitÃ© koneÄnÃ© intervaly, takÅ¾e tyto stavy s extrÃ©mnÃ­mi hodnotami budou velmi vzÃ¡cnÃ©.

1. Zde je funkce, kterÃ¡ vezme pozorovÃ¡nÃ­ z naÅ¡eho modelu a vytvoÅ™Ã­ ÄtveÅ™ici 4 celÃ½ch ÄÃ­sel: (blok kÃ³du 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Prozkoumejme takÃ© jinou metodu diskretizace pomocÃ­ intervalÅ¯: (blok kÃ³du 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. NynÃ­ spusÅ¥me krÃ¡tkou simulaci a pozorujme tyto diskrÃ©tnÃ­ hodnoty prostÅ™edÃ­. VyzkouÅ¡ejte `discretize` i `discretize_bins` a podÃ­vejte se, zda je mezi nimi rozdÃ­l.

    âœ… `discretize_bins` vracÃ­ ÄÃ­slo intervalu, kterÃ© je 0-based. TakÅ¾e pro hodnoty vstupnÃ­ promÄ›nnÃ© kolem 0 vracÃ­ ÄÃ­slo ze stÅ™edu intervalu (10). U `discretize` jsme se nestarali o rozsah vÃ½stupnÃ­ch hodnot, coÅ¾ umoÅ¾Åˆuje, aby byly negativnÃ­, takÅ¾e hodnoty stavu nejsou posunutÃ© a 0 odpovÃ­dÃ¡ 0. (blok kÃ³du 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    âœ… Odkomentujte Å™Ã¡dek zaÄÃ­najÃ­cÃ­ `env.render`, pokud chcete vidÄ›t, jak prostÅ™edÃ­ funguje. Jinak jej mÅ¯Å¾ete spustit na pozadÃ­, coÅ¾ je rychlejÅ¡Ã­. Tento "neviditelnÃ½" zpÅ¯sob provÃ¡dÄ›nÃ­ pouÅ¾ijeme bÄ›hem procesu Q-Learningu.

## Struktura Q-Tabulky

V naÅ¡Ã­ pÅ™edchozÃ­ lekci byl stav jednoduchÃ½ pÃ¡r ÄÃ­sel od 0 do 8, a proto bylo pohodlnÃ© reprezentovat Q-Tabulku pomocÃ­ numpy tenzoru s tvarem 8x8x2. Pokud pouÅ¾ijeme diskretizaci pomocÃ­ intervalÅ¯, velikost naÅ¡eho stavovÃ©ho vektoru je takÃ© znÃ¡mÃ¡, takÅ¾e mÅ¯Å¾eme pouÅ¾Ã­t stejnÃ½ pÅ™Ã­stup a reprezentovat stav pomocÃ­ pole tvaru 20x20x10x10x2 (zde 2 je dimenze prostoru akcÃ­ a prvnÃ­ dimenze odpovÃ­dajÃ­ poÄtu intervalÅ¯, kterÃ© jsme vybrali pro kaÅ¾dou z hodnot v prostoru pozorovÃ¡nÃ­).

NicmÃ©nÄ› nÄ›kdy pÅ™esnÃ© rozmÄ›ry prostoru pozorovÃ¡nÃ­ nejsou znÃ¡mÃ©. V pÅ™Ã­padÄ› funkce `discretize` si nikdy nemÅ¯Å¾eme bÃ½t jisti, Å¾e nÃ¡Å¡ stav zÅ¯stane v urÄitÃ½ch mezÃ­ch, protoÅ¾e nÄ›kterÃ© z pÅ¯vodnÃ­ch hodnot nejsou omezenÃ©. Proto pouÅ¾ijeme mÃ­rnÄ› odliÅ¡nÃ½ pÅ™Ã­stup a reprezentujeme Q-Tabulku pomocÃ­ slovnÃ­ku.

1. PouÅ¾ijte dvojici *(state,action)* jako klÃ­Ä slovnÃ­ku a hodnota by odpovÃ­dala hodnotÄ› poloÅ¾ky Q-Tabulky. (blok kÃ³du 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    Zde takÃ© definujeme funkci `qvalues()`, kterÃ¡ vracÃ­ seznam hodnot Q-Tabulky pro danÃ½ stav, kterÃ½ odpovÃ­dÃ¡ vÅ¡em moÅ¾nÃ½m akcÃ­m. Pokud poloÅ¾ka nenÃ­ pÅ™Ã­tomna v Q-Tabulce, vrÃ¡tÃ­me 0 jako vÃ½chozÃ­ hodnotu.

## ZaÄnÄ›me Q-Learning

NynÃ­ jsme pÅ™ipraveni nauÄit Petra udrÅ¾ovat rovnovÃ¡hu!

1. Nejprve nastavme nÄ›kterÃ© hyperparametry: (blok kÃ³du 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    Zde `alpha` je **rychlost uÄenÃ­**, kterÃ¡ urÄuje, do jakÃ© mÃ­ry bychom mÄ›li upravit aktuÃ¡lnÃ­ hodnoty Q-Tabulky pÅ™i kaÅ¾dÃ©m kroku. V pÅ™edchozÃ­ lekci jsme zaÄali s hodnotou 1 a potÃ© jsme `alpha` snÃ­Å¾ili na niÅ¾Å¡Ã­ hodnoty bÄ›hem trÃ©ninku. V tomto pÅ™Ã­kladu ji ponechÃ¡me konstantnÃ­ jen pro jednoduchost, a mÅ¯Å¾ete experimentovat s Ãºpravou hodnot `alpha` pozdÄ›ji.

    `gamma` je **faktor diskontovÃ¡nÃ­**, kterÃ½ ukazuje, do jakÃ© mÃ­ry bychom mÄ›li upÅ™ednostÅˆovat budoucÃ­ odmÄ›nu pÅ™ed aktuÃ¡lnÃ­ odmÄ›nou.

    `epsilon` je **faktor prÅ¯zkumu/vyuÅ¾itÃ­**, kterÃ½ urÄuje, zda bychom mÄ›li preferovat prÅ¯zkum pÅ™ed vyuÅ¾itÃ­m nebo naopak. V naÅ¡em algoritmu budeme v `epsilon` procentech pÅ™Ã­padÅ¯ vybÃ­rat dalÅ¡Ã­ akci podle hodnot Q-Tabulky a ve zbÃ½vajÃ­cÃ­m poÄtu pÅ™Ã­padÅ¯ provedeme nÃ¡hodnou akci. To nÃ¡m umoÅ¾nÃ­ prozkoumat oblasti prostoru hledÃ¡nÃ­, kterÃ© jsme dosud nevidÄ›li.

    âœ… Z hlediska udrÅ¾ovÃ¡nÃ­ rovnovÃ¡hy â€“ vÃ½bÄ›r nÃ¡hodnÃ© akce (prÅ¯zkum) by pÅ¯sobil jako nÃ¡hodnÃ½ Ãºder Å¡patnÃ½m smÄ›rem a tyÄ by se musela nauÄit, jak obnovit rovnovÃ¡hu z tÄ›chto "chyb".

### VylepÅ¡enÃ­ algoritmu

MÅ¯Å¾eme takÃ© provÃ©st dvÄ› vylepÅ¡enÃ­ naÅ¡eho algoritmu z pÅ™edchozÃ­ lekce:

- **VÃ½poÄet prÅ¯mÄ›rnÃ© kumulativnÃ­ odmÄ›ny** bÄ›hem nÄ›kolika simulacÃ­. Pokrok budeme tisknout kaÅ¾dÃ½ch 5000 iteracÃ­ a prÅ¯mÄ›rnou kumulativnÃ­ odmÄ›nu za toto obdobÃ­ zprÅ¯mÄ›rujeme. To znamenÃ¡, Å¾e pokud zÃ­skÃ¡me vÃ­ce neÅ¾ 195 bodÅ¯, mÅ¯Å¾eme problÃ©m povaÅ¾ovat za vyÅ™eÅ¡enÃ½, a to s jeÅ¡tÄ› vyÅ¡Å¡Ã­ kvalitou, neÅ¾ je poÅ¾adovÃ¡no.

- **VÃ½poÄet maximÃ¡lnÃ­ho prÅ¯mÄ›rnÃ©ho kumulativnÃ­ho vÃ½sledku**, `Qmax`, a uloÅ¾Ã­me Q-Tabulku odpovÃ­dajÃ­cÃ­ tomuto vÃ½sledku. KdyÅ¾ spustÃ­te trÃ©nink, vÅ¡imnete si, Å¾e nÄ›kdy prÅ¯mÄ›rnÃ½ kumulativnÃ­ vÃ½sledek zaÄne klesat, a chceme si uchovat hodnoty Q-Tabulky, kterÃ© odpovÃ­dajÃ­ nejlepÅ¡Ã­mu modelu pozorovanÃ©mu bÄ›hem trÃ©ninku.

1. SbÃ­rejte vÅ¡echny kumulativnÃ­ odmÄ›ny pÅ™i kaÅ¾dÃ© simulaci do vektoru `rewards` pro dalÅ¡Ã­ vykreslenÃ­. (blok kÃ³du 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Co si mÅ¯Å¾ete vÅ¡imnout z tÄ›chto vÃ½sledkÅ¯:

- **BlÃ­zko naÅ¡eho cÃ­le**. Jsme velmi blÃ­zko dosaÅ¾enÃ­ cÃ­le zÃ­skat 195 kumulativnÃ­ch odmÄ›n bÄ›hem 100+ po sobÄ› jdoucÃ­ch bÄ›hÅ¯ simulace, nebo jsme toho moÅ¾nÃ¡ jiÅ¾ dosÃ¡hli! I kdyÅ¾ zÃ­skÃ¡me menÅ¡Ã­ ÄÃ­sla, stÃ¡le to nevÃ­me, protoÅ¾e prÅ¯mÄ›rujeme pÅ™es 5000 bÄ›hÅ¯ a pouze 100 bÄ›hÅ¯ je poÅ¾adovÃ¡no v rÃ¡mci formÃ¡lnÃ­ch kritÃ©riÃ­.

- **OdmÄ›na zaÄÃ­nÃ¡ klesat**. NÄ›kdy odmÄ›na zaÄne klesat, coÅ¾ znamenÃ¡, Å¾e mÅ¯Å¾eme "zniÄit" jiÅ¾ nauÄenÃ© hodnoty v Q-Tabulce tÄ›mi, kterÃ© situaci zhorÅ¡ujÃ­.

Toto pozorovÃ¡nÃ­ je jasnÄ›ji viditelnÃ©, pokud vykreslÃ­me prÅ¯bÄ›h trÃ©ninku.

## VykreslenÃ­ prÅ¯bÄ›hu trÃ©ninku

BÄ›hem trÃ©ninku jsme sbÃ­rali hodnotu kumulativnÃ­ odmÄ›ny pÅ™i kaÅ¾dÃ© iteraci do vektoru `rewards`. Takto to vypadÃ¡, kdyÅ¾ to vykreslÃ­me proti ÄÃ­slu iterace:

```python
plt.plot(rewards)
```

![surovÃ½ prÅ¯bÄ›h](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Z tohoto grafu nenÃ­ moÅ¾nÃ© nic Å™Ã­ct, protoÅ¾e kvÅ¯li povaze stochastickÃ©ho procesu trÃ©ninku se dÃ©lka trÃ©ninkovÃ½ch sezenÃ­ znaÄnÄ› liÅ¡Ã­. Aby mÄ›l tento graf vÄ›tÅ¡Ã­ smysl, mÅ¯Å¾eme vypoÄÃ­tat **bÄ›Å¾nÃ½ prÅ¯mÄ›r** pÅ™es sÃ©rii experimentÅ¯, Å™eknÄ›me 100. To lze pohodlnÄ› provÃ©st pomocÃ­ `np.convolve`: (blok kÃ³du 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![prÅ¯bÄ›h trÃ©ninku](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Ãšprava hyperparametrÅ¯

Aby bylo uÄenÃ­ stabilnÄ›jÅ¡Ã­, mÃ¡ smysl upravit nÄ›kterÃ© z naÅ¡ich hyperparametrÅ¯ bÄ›hem trÃ©ninku. KonkrÃ©tnÄ›:

- **Pro rychlost uÄenÃ­**, `alpha`, mÅ¯Å¾eme zaÄÃ­t s hodnotami blÃ­zkÃ½mi 1 a potÃ© tento parametr postupnÄ› sniÅ¾ovat. S Äasem budeme zÃ­skÃ¡vat dobrÃ© pravdÄ›podobnostnÃ­ hodnoty v Q-Tabulce, a proto bychom je mÄ›li upravovat mÃ­rnÄ›, a ne zcela pÅ™episovat novÃ½mi hodnotami.

- **ZvÃ½Å¡enÃ­ epsilon**. MÅ¯Å¾eme chtÃ­t `epsilon` pomalu zvyÅ¡ovat, aby se mÃ©nÄ› prozkoumÃ¡valo a vÃ­ce vyuÅ¾Ã­valo. PravdÄ›podobnÄ› mÃ¡ smysl zaÄÃ­t s niÅ¾Å¡Ã­ hodnotou `epsilon` a postupnÄ› ji zvÃ½Å¡it tÃ©mÄ›Å™ na 1.
> **Ãškol 1**: Experimentujte s hodnotami hyperparametrÅ¯ a zjistÄ›te, zda mÅ¯Å¾ete dosÃ¡hnout vyÅ¡Å¡Ã­ho kumulativnÃ­ho odmÄ›ny. Dosahujete vÃ­ce neÅ¾ 195?
> **Ãškol 2**: Aby bylo moÅ¾nÃ© problÃ©m formÃ¡lnÄ› vyÅ™eÅ¡it, je potÅ™eba dosÃ¡hnout prÅ¯mÄ›rnÃ© odmÄ›ny 195 bÄ›hem 100 po sobÄ› jdoucÃ­ch bÄ›hÅ¯. MÄ›Å™te to bÄ›hem trÃ©ninku a ujistÄ›te se, Å¾e jste problÃ©m formÃ¡lnÄ› vyÅ™eÅ¡ili!

## VidÄ›t vÃ½sledek v akci

Bylo by zajÃ­mavÃ© skuteÄnÄ› vidÄ›t, jak se nauÄenÃ½ model chovÃ¡. SpusÅ¥me simulaci a pouÅ¾ijme stejnou strategii vÃ½bÄ›ru akcÃ­ jako bÄ›hem trÃ©ninku, tedy vzorkovÃ¡nÃ­ podle pravdÄ›podobnostnÃ­ho rozdÄ›lenÃ­ v Q-Tabulce: (blok kÃ³du 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

MÄ›li byste vidÄ›t nÄ›co podobnÃ©ho:

![balancujÃ­cÃ­ cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## ğŸš€VÃ½zva

> **Ãškol 3**: Zde jsme pouÅ¾Ã­vali finÃ¡lnÃ­ verzi Q-Tabulky, kterÃ¡ nemusÃ­ bÃ½t ta nejlepÅ¡Ã­. Pamatujte, Å¾e jsme uloÅ¾ili nejlÃ©pe fungujÃ­cÃ­ Q-Tabulku do promÄ›nnÃ© `Qbest`! VyzkouÅ¡ejte stejnÃ½ pÅ™Ã­klad s nejlÃ©pe fungujÃ­cÃ­ Q-Tabulkou tÃ­m, Å¾e zkopÃ­rujete `Qbest` do `Q`, a sledujte, zda zaznamenÃ¡te rozdÃ­l.

> **Ãškol 4**: Zde jsme na kaÅ¾dÃ©m kroku nevybÃ­rali nejlepÅ¡Ã­ akci, ale spÃ­Å¡e vzorkovali podle odpovÃ­dajÃ­cÃ­ho pravdÄ›podobnostnÃ­ho rozdÄ›lenÃ­. MÄ›lo by vÄ›tÅ¡Ã­ smysl vÅ¾dy vybÃ­rat nejlepÅ¡Ã­ akci s nejvyÅ¡Å¡Ã­ hodnotou v Q-Tabulce? To lze provÃ©st pomocÃ­ funkce `np.argmax`, kterÃ¡ zjistÃ­ ÄÃ­slo akce odpovÃ­dajÃ­cÃ­ nejvyÅ¡Å¡Ã­ hodnotÄ› v Q-Tabulce. Implementujte tuto strategii a sledujte, zda zlepÅ¡Ã­ balancovÃ¡nÃ­.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ml/)

## ZadÃ¡nÃ­
[VytrÃ©nujte Mountain Car](assignment.md)

## ZÃ¡vÄ›r

NynÃ­ jsme se nauÄili, jak trÃ©novat agenty, aby dosÃ¡hli dobrÃ½ch vÃ½sledkÅ¯ pouze tÃ­m, Å¾e jim poskytneme funkci odmÄ›ny, kterÃ¡ definuje poÅ¾adovanÃ½ stav hry, a dÃ¡me jim pÅ™Ã­leÅ¾itost inteligentnÄ› prozkoumat prostor hledÃ¡nÃ­. ÃšspÄ›Å¡nÄ› jsme aplikovali algoritmus Q-Learning v pÅ™Ã­padech diskrÃ©tnÃ­ch i spojitÃ½ch prostÅ™edÃ­, ale s diskrÃ©tnÃ­mi akcemi.

Je takÃ© dÅ¯leÅ¾itÃ© studovat situace, kdy je stav akcÃ­ spojitÃ½ a kdy je prostor pozorovÃ¡nÃ­ mnohem sloÅ¾itÄ›jÅ¡Ã­, napÅ™Ã­klad obraz z obrazovky hry Atari. V tÄ›chto problÃ©mech Äasto potÅ™ebujeme pouÅ¾Ã­t vÃ½konnÄ›jÅ¡Ã­ techniky strojovÃ©ho uÄenÃ­, jako jsou neuronovÃ© sÃ­tÄ›, abychom dosÃ¡hli dobrÃ½ch vÃ½sledkÅ¯. Tyto pokroÄilejÅ¡Ã­ tÃ©mata jsou pÅ™edmÄ›tem naÅ¡eho nadchÃ¡zejÃ­cÃ­ho pokroÄilÃ©ho kurzu AI.

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.