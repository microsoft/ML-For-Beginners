<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T01:03:44+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "cs"
}
-->
# Ãšvod do posilovanÃ©ho uÄenÃ­

PosilovanÃ© uÄenÃ­, RL, je povaÅ¾ovÃ¡no za jeden ze zÃ¡kladnÃ­ch paradigmat strojovÃ©ho uÄenÃ­, vedle uÄenÃ­ s uÄitelem a uÄenÃ­ bez uÄitele. RL se zamÄ›Å™uje na rozhodovÃ¡nÃ­: poskytovÃ¡nÃ­ sprÃ¡vnÃ½ch rozhodnutÃ­ nebo alespoÅˆ uÄenÃ­ se z nich.

PÅ™edstavte si, Å¾e mÃ¡te simulovanÃ© prostÅ™edÃ­, napÅ™Ã­klad akciovÃ½ trh. Co se stane, pokud zavedete urÄitou regulaci? MÃ¡ to pozitivnÃ­ nebo negativnÃ­ dopad? Pokud se stane nÄ›co negativnÃ­ho, musÃ­te vzÃ­t tento _negativnÃ­ posilovacÃ­ podnÄ›t_, pouÄit se z nÄ›j a zmÄ›nit smÄ›r. Pokud je vÃ½sledek pozitivnÃ­, musÃ­te na tomto _pozitivnÃ­m posilovacÃ­m podnÄ›tu_ stavÄ›t.

![peter a vlk](../../../8-Reinforcement/images/peter.png)

> Petr a jeho pÅ™Ã¡telÃ© musÃ­ utÃ©ct hladovÃ©mu vlkovi! ObrÃ¡zek od [Jen Looper](https://twitter.com/jenlooper)

## RegionÃ¡lnÃ­ tÃ©ma: Petr a vlk (Rusko)

[Petr a vlk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) je hudebnÃ­ pohÃ¡dka napsanÃ¡ ruskÃ½m skladatelem [Sergejem Prokofjevem](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Je to pÅ™Ã­bÄ›h o mladÃ©m pionÃ½rovi Petrovi, kterÃ½ odvÃ¡Å¾nÄ› vyjde z domu na lesnÃ­ mÃ½tinu, aby pronÃ¡sledoval vlka. V tÃ©to ÄÃ¡sti budeme trÃ©novat algoritmy strojovÃ©ho uÄenÃ­, kterÃ© Petrovi pomohou:

- **Prozkoumat** okolnÃ­ oblast a vytvoÅ™it optimÃ¡lnÃ­ navigaÄnÃ­ mapu
- **NauÄit se** jezdit na skateboardu a udrÅ¾ovat rovnovÃ¡hu, aby se mohl pohybovat rychleji.

[![Petr a vlk](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e a poslechnÄ›te si Petra a vlka od Prokofjeva

## PosilovanÃ© uÄenÃ­

V pÅ™edchozÃ­ch ÄÃ¡stech jste vidÄ›li dva pÅ™Ã­klady problÃ©mÅ¯ strojovÃ©ho uÄenÃ­:

- **S uÄitelem**, kde mÃ¡me datovÃ© sady, kterÃ© naznaÄujÃ­ vzorovÃ¡ Å™eÅ¡enÃ­ problÃ©mu, kterÃ½ chceme vyÅ™eÅ¡it. [Klasifikace](../4-Classification/README.md) a [regrese](../2-Regression/README.md) jsou Ãºkoly uÄenÃ­ s uÄitelem.
- **Bez uÄitele**, kde nemÃ¡me oznaÄenÃ¡ trÃ©ninkovÃ¡ data. HlavnÃ­m pÅ™Ã­kladem uÄenÃ­ bez uÄitele je [shlukovÃ¡nÃ­](../5-Clustering/README.md).

V tÃ©to ÄÃ¡sti vÃ¡s seznÃ¡mÃ­me s novÃ½m typem problÃ©mu uÄenÃ­, kterÃ½ nevyÅ¾aduje oznaÄenÃ¡ trÃ©ninkovÃ¡ data. Existuje nÄ›kolik typÅ¯ takovÃ½ch problÃ©mÅ¯:

- **[PolouÄenÃ­ s uÄitelem](https://wikipedia.org/wiki/Semi-supervised_learning)**, kde mÃ¡me velkÃ© mnoÅ¾stvÃ­ neoznaÄenÃ½ch dat, kterÃ¡ mohou bÃ½t pouÅ¾ita k pÅ™edtrÃ©novÃ¡nÃ­ modelu.
- **[PosilovanÃ© uÄenÃ­](https://wikipedia.org/wiki/Reinforcement_learning)**, pÅ™i kterÃ©m se agent uÄÃ­, jak se chovat, provÃ¡dÄ›nÃ­m experimentÅ¯ v nÄ›jakÃ©m simulovanÃ©m prostÅ™edÃ­.

### PÅ™Ã­klad - poÄÃ­taÄovÃ¡ hra

PÅ™edstavte si, Å¾e chcete nauÄit poÄÃ­taÄ hrÃ¡t hru, napÅ™Ã­klad Å¡achy nebo [Super Mario](https://wikipedia.org/wiki/Super_Mario). Aby poÄÃ­taÄ mohl hrÃ¡t hru, potÅ™ebujeme, aby pÅ™edpovÄ›dÄ›l, jakÃ½ tah udÄ›lat v kaÅ¾dÃ©m hernÃ­m stavu. I kdyÅ¾ se to mÅ¯Å¾e zdÃ¡t jako problÃ©m klasifikace, nenÃ­ tomu tak - protoÅ¾e nemÃ¡me datovou sadu se stavy a odpovÃ­dajÃ­cÃ­mi akcemi. I kdyÅ¾ mÅ¯Å¾eme mÃ­t nÄ›jakÃ¡ data, jako jsou existujÃ­cÃ­ Å¡achovÃ© partie nebo zÃ¡znamy hrÃ¡ÄÅ¯ hrajÃ­cÃ­ch Super Mario, je pravdÄ›podobnÃ©, Å¾e tato data nebudou dostateÄnÄ› pokrÃ½vat velkÃ© mnoÅ¾stvÃ­ moÅ¾nÃ½ch stavÅ¯.

MÃ­sto hledÃ¡nÃ­ existujÃ­cÃ­ch hernÃ­ch dat je **posilovanÃ© uÄenÃ­** (RL) zaloÅ¾eno na myÅ¡lence *nechat poÄÃ­taÄ hrÃ¡t* mnohokrÃ¡t a pozorovat vÃ½sledek. Abychom mohli aplikovat posilovanÃ© uÄenÃ­, potÅ™ebujeme dvÄ› vÄ›ci:

- **ProstÅ™edÃ­** a **simulÃ¡tor**, kterÃ© nÃ¡m umoÅ¾nÃ­ hru hrÃ¡t mnohokrÃ¡t. Tento simulÃ¡tor by definoval vÅ¡echna pravidla hry, stejnÄ› jako moÅ¾nÃ© stavy a akce.

- **Funkci odmÄ›ny**, kterÃ¡ nÃ¡m Å™ekne, jak dobÅ™e jsme si vedli bÄ›hem kaÅ¾dÃ©ho tahu nebo hry.

HlavnÃ­ rozdÃ­l mezi ostatnÃ­mi typy strojovÃ©ho uÄenÃ­ a RL je ten, Å¾e v RL obvykle nevÃ­me, zda vyhrajeme nebo prohrajeme, dokud nedokonÄÃ­me hru. NemÅ¯Å¾eme tedy Å™Ã­ci, zda je urÄitÃ½ tah sÃ¡m o sobÄ› dobrÃ½ nebo ne - odmÄ›nu dostÃ¡vÃ¡me aÅ¾ na konci hry. NaÅ¡Ã­m cÃ­lem je navrhnout algoritmy, kterÃ© nÃ¡m umoÅ¾nÃ­ trÃ©novat model za nejistÃ½ch podmÃ­nek. NauÄÃ­me se o jednom RL algoritmu nazvanÃ©m **Q-learning**.

## Lekce

1. [Ãšvod do posilovanÃ©ho uÄenÃ­ a Q-Learningu](1-QLearning/README.md)
2. [PouÅ¾itÃ­ simulovanÃ©ho prostÅ™edÃ­ Gym](2-Gym/README.md)

## PodÄ›kovÃ¡nÃ­

"Ãšvod do posilovanÃ©ho uÄenÃ­" byl napsÃ¡n s â™¥ï¸ od [Dmitry Soshnikov](http://soshnikov.com)

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.