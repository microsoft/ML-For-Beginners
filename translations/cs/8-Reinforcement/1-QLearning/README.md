<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T01:06:51+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "cs"
}
-->
# Ãšvod do posilovanÃ©ho uÄenÃ­ a Q-Learningu

![ShrnutÃ­ posilovanÃ©ho uÄenÃ­ v oblasti strojovÃ©ho uÄenÃ­ ve sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote od [Tomomi Imura](https://www.twitter.com/girlie_mac)

PosilovanÃ© uÄenÃ­ zahrnuje tÅ™i dÅ¯leÅ¾itÃ© koncepty: agenta, stavy a sadu akcÃ­ pro kaÅ¾dÃ½ stav. ProvÃ¡dÄ›nÃ­m akce ve specifickÃ©m stavu zÃ­skÃ¡vÃ¡ agent odmÄ›nu. PÅ™edstavte si opÄ›t poÄÃ­taÄovou hru Super Mario. Vy jste Mario, nachÃ¡zÃ­te se v Ãºrovni hry, stojÃ­te vedle okraje Ãºtesu. Nad vÃ¡mi je mince. Vy jako Mario, v hernÃ­ Ãºrovni, na konkrÃ©tnÃ­ pozici... to je vÃ¡Å¡ stav. Posun o krok doprava (akce) vÃ¡s pÅ™ivede pÅ™es okraj, coÅ¾ by vÃ¡m pÅ™ineslo nÃ­zkÃ© ÄÃ­selnÃ© skÃ³re. StisknutÃ­ tlaÄÃ­tka skoku by vÃ¡m vÅ¡ak umoÅ¾nilo zÃ­skat bod a zÅ¯stat naÅ¾ivu. To je pozitivnÃ­ vÃ½sledek, kterÃ½ by vÃ¡m mÄ›l pÅ™inÃ©st pozitivnÃ­ ÄÃ­selnÃ© skÃ³re.

PomocÃ­ posilovanÃ©ho uÄenÃ­ a simulÃ¡toru (hry) se mÅ¯Å¾ete nauÄit, jak hru hrÃ¡t, abyste maximalizovali odmÄ›nu, coÅ¾ znamenÃ¡ zÅ¯stat naÅ¾ivu a zÃ­skat co nejvÃ­ce bodÅ¯.

[![Ãšvod do posilovanÃ©ho uÄenÃ­](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e a poslechnÄ›te si Dmitryho, jak diskutuje o posilovanÃ©m uÄenÃ­.

## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ml/)

## PÅ™edpoklady a nastavenÃ­

V tÃ©to lekci budeme experimentovat s kÃ³dem v Pythonu. MÄ›li byste bÃ½t schopni spustit kÃ³d v Jupyter Notebooku z tÃ©to lekce, buÄ na svÃ©m poÄÃ­taÄi, nebo nÄ›kde v cloudu.

MÅ¯Å¾ete otevÅ™Ã­t [notebook lekce](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) a projÃ­t si tuto lekci krok za krokem.

> **PoznÃ¡mka:** Pokud otevÃ­rÃ¡te tento kÃ³d z cloudu, musÃ­te takÃ© stÃ¡hnout soubor [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kterÃ½ se pouÅ¾Ã­vÃ¡ v kÃ³du notebooku. PÅ™idejte jej do stejnÃ©ho adresÃ¡Å™e jako notebook.

## Ãšvod

V tÃ©to lekci prozkoumÃ¡me svÄ›t **[Petr a vlk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirovanÃ½ hudebnÃ­ pohÃ¡dkou ruskÃ©ho skladatele [Sergeje Prokofjeva](https://en.wikipedia.org/wiki/Sergei_Prokofiev). PouÅ¾ijeme **posilovanÃ© uÄenÃ­**, aby Petr mohl prozkoumat svÃ© prostÅ™edÃ­, sbÃ­rat chutnÃ¡ jablka a vyhnout se setkÃ¡nÃ­ s vlkem.

**PosilovanÃ© uÄenÃ­** (RL) je technika uÄenÃ­, kterÃ¡ nÃ¡m umoÅ¾Åˆuje nauÄit se optimÃ¡lnÃ­ chovÃ¡nÃ­ **agenta** v nÄ›jakÃ©m **prostÅ™edÃ­** prostÅ™ednictvÃ­m mnoha experimentÅ¯. Agent v tomto prostÅ™edÃ­ by mÄ›l mÃ­t nÄ›jakÃ½ **cÃ­l**, definovanÃ½ pomocÃ­ **funkce odmÄ›ny**.

## ProstÅ™edÃ­

Pro jednoduchost si pÅ™edstavme Petrov svÄ›t jako Ätvercovou desku o velikosti `Å¡Ã­Å™ka` x `vÃ½Å¡ka`, jako je tato:

![Petrovo prostÅ™edÃ­](../../../../8-Reinforcement/1-QLearning/images/environment.png)

KaÅ¾dÃ¡ buÅˆka na tÃ©to desce mÅ¯Å¾e bÃ½t:

* **zem**, po kterÃ© Petr a dalÅ¡Ã­ bytosti mohou chodit.
* **voda**, po kterÃ© samozÅ™ejmÄ› nemÅ¯Å¾ete chodit.
* **strom** nebo **trÃ¡va**, mÃ­sto, kde si mÅ¯Å¾ete odpoÄinout.
* **jablko**, coÅ¾ pÅ™edstavuje nÄ›co, co by Petr rÃ¡d naÅ¡el, aby se nakrmil.
* **vlk**, kterÃ½ je nebezpeÄnÃ½ a mÄ›l by bÃ½t vyhnut.

Existuje samostatnÃ½ modul v Pythonu, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kterÃ½ obsahuje kÃ³d pro prÃ¡ci s tÃ­mto prostÅ™edÃ­m. ProtoÅ¾e tento kÃ³d nenÃ­ dÅ¯leÅ¾itÃ½ pro pochopenÃ­ naÅ¡ich konceptÅ¯, importujeme modul a pouÅ¾ijeme jej k vytvoÅ™enÃ­ vzorovÃ© desky (blok kÃ³du 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Tento kÃ³d by mÄ›l vytisknout obrÃ¡zek prostÅ™edÃ­ podobnÃ½ tomu vÃ½Å¡e.

## Akce a politika

V naÅ¡em pÅ™Ã­kladu by PetrovÃ½m cÃ­lem bylo najÃ­t jablko, zatÃ­mco se vyhÃ½bÃ¡ vlkovi a dalÅ¡Ã­m pÅ™ekÃ¡Å¾kÃ¡m. K tomu mÅ¯Å¾e v podstatÄ› chodit, dokud nenajde jablko.

Proto mÅ¯Å¾e na jakÃ©koli pozici zvolit jednu z nÃ¡sledujÃ­cÃ­ch akcÃ­: nahoru, dolÅ¯, doleva a doprava.

Tyto akce definujeme jako slovnÃ­k a mapujeme je na dvojice odpovÃ­dajÃ­cÃ­ch zmÄ›n souÅ™adnic. NapÅ™Ã­klad pohyb doprava (`R`) by odpovÃ­dal dvojici `(1,0)`. (blok kÃ³du 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Shrneme-li, strategie a cÃ­l tohoto scÃ©nÃ¡Å™e jsou nÃ¡sledujÃ­cÃ­:

- **Strategie** naÅ¡eho agenta (Petra) je definovÃ¡na tzv. **politikou**. Politika je funkce, kterÃ¡ vracÃ­ akci v danÃ©m stavu. V naÅ¡em pÅ™Ã­padÄ› je stav problÃ©mu reprezentovÃ¡n deskou, vÄetnÄ› aktuÃ¡lnÃ­ pozice hrÃ¡Äe.

- **CÃ­l** posilovanÃ©ho uÄenÃ­ je nakonec nauÄit se dobrou politiku, kterÃ¡ nÃ¡m umoÅ¾nÃ­ problÃ©m efektivnÄ› vyÅ™eÅ¡it. Jako zÃ¡klad vÅ¡ak zvaÅ¾me nejjednoduÅ¡Å¡Ã­ politiku nazvanou **nÃ¡hodnÃ¡ chÅ¯ze**.

## NÃ¡hodnÃ¡ chÅ¯ze

Nejprve vyÅ™eÅ¡Ã­me nÃ¡Å¡ problÃ©m implementacÃ­ strategie nÃ¡hodnÃ© chÅ¯ze. PÅ™i nÃ¡hodnÃ© chÅ¯zi budeme nÃ¡hodnÄ› vybÃ­rat dalÅ¡Ã­ akci z povolenÃ½ch akcÃ­, dokud nedosÃ¡hneme jablka (blok kÃ³du 3).

1. Implementujte nÃ¡hodnou chÅ¯zi pomocÃ­ nÃ­Å¾e uvedenÃ©ho kÃ³du:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    VolÃ¡nÃ­ `walk` by mÄ›lo vrÃ¡tit dÃ©lku odpovÃ­dajÃ­cÃ­ cesty, kterÃ¡ se mÅ¯Å¾e liÅ¡it od jednoho spuÅ¡tÄ›nÃ­ k druhÃ©mu.

1. SpusÅ¥te experiment chÅ¯ze nÄ›kolikrÃ¡t (Å™eknÄ›me 100krÃ¡t) a vytisknÄ›te vÃ½slednÃ© statistiky (blok kÃ³du 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    VÅ¡imnÄ›te si, Å¾e prÅ¯mÄ›rnÃ¡ dÃ©lka cesty je kolem 30-40 krokÅ¯, coÅ¾ je pomÄ›rnÄ› hodnÄ›, vzhledem k tomu, Å¾e prÅ¯mÄ›rnÃ¡ vzdÃ¡lenost k nejbliÅ¾Å¡Ã­mu jablku je kolem 5-6 krokÅ¯.

    MÅ¯Å¾ete takÃ© vidÄ›t, jak vypadÃ¡ Petrov pohyb bÄ›hem nÃ¡hodnÃ© chÅ¯ze:

    ![Petrova nÃ¡hodnÃ¡ chÅ¯ze](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funkce odmÄ›ny

Aby byla naÅ¡e politika inteligentnÄ›jÅ¡Ã­, musÃ­me pochopit, kterÃ© kroky jsou "lepÅ¡Ã­" neÅ¾ jinÃ©. K tomu musÃ­me definovat nÃ¡Å¡ cÃ­l.

CÃ­l mÅ¯Å¾e bÃ½t definovÃ¡n pomocÃ­ **funkce odmÄ›ny**, kterÃ¡ vrÃ¡tÃ­ nÄ›jakou hodnotu skÃ³re pro kaÅ¾dÃ½ stav. ÄŒÃ­m vyÅ¡Å¡Ã­ ÄÃ­slo, tÃ­m lepÅ¡Ã­ funkce odmÄ›ny. (blok kÃ³du 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

ZajÃ­mavÃ© na funkcÃ­ch odmÄ›ny je, Å¾e ve vÄ›tÅ¡inÄ› pÅ™Ã­padÅ¯ *dostÃ¡vÃ¡me podstatnou odmÄ›nu aÅ¾ na konci hry*. To znamenÃ¡, Å¾e nÃ¡Å¡ algoritmus by mÄ›l nÄ›jakÃ½m zpÅ¯sobem zapamatovat "dobrÃ©" kroky, kterÃ© vedou k pozitivnÃ­ odmÄ›nÄ› na konci, a zvÃ½Å¡it jejich dÅ¯leÅ¾itost. PodobnÄ› by mÄ›ly bÃ½t odrazeny vÅ¡echny kroky, kterÃ© vedou k Å¡patnÃ½m vÃ½sledkÅ¯m.

## Q-Learning

Algoritmus, kterÃ½ zde budeme diskutovat, se nazÃ½vÃ¡ **Q-Learning**. V tomto algoritmu je politika definovÃ¡na funkcÃ­ (nebo datovou strukturou) nazvanou **Q-Tabulka**. Ta zaznamenÃ¡vÃ¡ "kvalitu" kaÅ¾dÃ© akce v danÃ©m stavu.

NazÃ½vÃ¡ se Q-Tabulka, protoÅ¾e je Äasto vÃ½hodnÃ© ji reprezentovat jako tabulku nebo vÃ­cerozmÄ›rnÃ© pole. ProtoÅ¾e naÅ¡e deska mÃ¡ rozmÄ›ry `Å¡Ã­Å™ka` x `vÃ½Å¡ka`, mÅ¯Å¾eme Q-Tabulku reprezentovat pomocÃ­ numpy pole s tvarem `Å¡Ã­Å™ka` x `vÃ½Å¡ka` x `len(actions)`: (blok kÃ³du 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

VÅ¡imnÄ›te si, Å¾e inicializujeme vÅ¡echny hodnoty Q-Tabulky stejnou hodnotou, v naÅ¡em pÅ™Ã­padÄ› - 0.25. To odpovÃ­dÃ¡ politice "nÃ¡hodnÃ© chÅ¯ze", protoÅ¾e vÅ¡echny kroky v kaÅ¾dÃ©m stavu jsou stejnÄ› dobrÃ©. Q-Tabulku mÅ¯Å¾eme pÅ™edat funkci `plot`, abychom ji vizualizovali na desce: `m.plot(Q)`.

![Petrovo prostÅ™edÃ­](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

UprostÅ™ed kaÅ¾dÃ© buÅˆky je "Å¡ipka", kterÃ¡ oznaÄuje preferovanÃ½ smÄ›r pohybu. ProtoÅ¾e vÅ¡echny smÄ›ry jsou stejnÃ©, je zobrazen bod.

NynÃ­ musÃ­me spustit simulaci, prozkoumat naÅ¡e prostÅ™edÃ­ a nauÄit se lepÅ¡Ã­ rozloÅ¾enÃ­ hodnot Q-Tabulky, kterÃ© nÃ¡m umoÅ¾nÃ­ najÃ­t cestu k jablku mnohem rychleji.

## Podstata Q-Learningu: Bellmanova rovnice

Jakmile se zaÄneme pohybovat, kaÅ¾dÃ¡ akce bude mÃ­t odpovÃ­dajÃ­cÃ­ odmÄ›nu, tj. teoreticky mÅ¯Å¾eme vybrat dalÅ¡Ã­ akci na zÃ¡kladÄ› nejvyÅ¡Å¡Ã­ okamÅ¾itÃ© odmÄ›ny. Ve vÄ›tÅ¡inÄ› stavÅ¯ vÅ¡ak krok nedosÃ¡hne naÅ¡eho cÃ­le dosÃ¡hnout jablka, a proto nemÅ¯Å¾eme okamÅ¾itÄ› rozhodnout, kterÃ½ smÄ›r je lepÅ¡Ã­.

> Pamatujte, Å¾e nezÃ¡leÅ¾Ã­ na okamÅ¾itÃ©m vÃ½sledku, ale spÃ­Å¡e na koneÄnÃ©m vÃ½sledku, kterÃ½ zÃ­skÃ¡me na konci simulace.

Abychom zohlednili tuto zpoÅ¾dÄ›nou odmÄ›nu, musÃ­me pouÅ¾Ã­t principy **[dynamickÃ©ho programovÃ¡nÃ­](https://en.wikipedia.org/wiki/Dynamic_programming)**, kterÃ© nÃ¡m umoÅ¾ÅˆujÃ­ pÅ™emÃ½Å¡let o naÅ¡em problÃ©mu rekurzivnÄ›.

PÅ™edpoklÃ¡dejme, Å¾e se nynÃ­ nachÃ¡zÃ­me ve stavu *s* a chceme se pÅ™esunout do dalÅ¡Ã­ho stavu *s'*. TÃ­m zÃ­skÃ¡me okamÅ¾itou odmÄ›nu *r(s,a)*, definovanou funkcÃ­ odmÄ›ny, plus nÄ›jakou budoucÃ­ odmÄ›nu. Pokud pÅ™edpoklÃ¡dÃ¡me, Å¾e naÅ¡e Q-Tabulka sprÃ¡vnÄ› odrÃ¡Å¾Ã­ "atraktivitu" kaÅ¾dÃ© akce, pak ve stavu *s'* zvolÃ­me akci *a*, kterÃ¡ odpovÃ­dÃ¡ maximÃ¡lnÃ­ hodnotÄ› *Q(s',a')*. TÃ­m pÃ¡dem nejlepÅ¡Ã­ moÅ¾nÃ¡ budoucÃ­ odmÄ›na, kterou bychom mohli zÃ­skat ve stavu *s*, bude definovÃ¡na jako `max`

## Kontrola politiky

ProtoÅ¾e Q-Tabulka uvÃ¡dÃ­ "atraktivitu" kaÅ¾dÃ© akce v kaÅ¾dÃ©m stavu, je pomÄ›rnÄ› snadnÃ© ji pouÅ¾Ã­t k definovÃ¡nÃ­ efektivnÃ­ navigace v naÅ¡em svÄ›tÄ›. V nejjednoduÅ¡Å¡Ã­m pÅ™Ã­padÄ› mÅ¯Å¾eme vybrat akci odpovÃ­dajÃ­cÃ­ nejvyÅ¡Å¡Ã­ hodnotÄ› v Q-Tabulce: (kÃ³dovÃ½ blok 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Pokud vÃ½Å¡e uvedenÃ½ kÃ³d vyzkouÅ¡Ã­te nÄ›kolikrÃ¡t, mÅ¯Å¾ete si vÅ¡imnout, Å¾e se nÄ›kdy "zasekne" a je tÅ™eba stisknout tlaÄÃ­tko STOP v notebooku, abyste ho pÅ™eruÅ¡ili. K tomu dochÃ¡zÃ­, protoÅ¾e mohou nastat situace, kdy si dva stavy "ukazujÃ­" na sebe z hlediska optimÃ¡lnÃ­ hodnoty Q, coÅ¾ vede k tomu, Å¾e agent se mezi tÄ›mito stavy pohybuje nekoneÄnÄ›.

## ğŸš€VÃ½zva

> **Ãškol 1:** Upravte funkci `walk` tak, aby omezila maximÃ¡lnÃ­ dÃ©lku cesty na urÄitÃ½ poÄet krokÅ¯ (napÅ™Ã­klad 100), a sledujte, jak vÃ½Å¡e uvedenÃ½ kÃ³d tuto hodnotu Äas od Äasu vracÃ­.

> **Ãškol 2:** Upravte funkci `walk` tak, aby se nevracela na mÃ­sta, kde jiÅ¾ byla. TÃ­m se zabrÃ¡nÃ­ tomu, aby se `walk` opakovala, nicmÃ©nÄ› agent mÅ¯Å¾e stÃ¡le skonÄit "uvÄ›znÄ›nÃ½" na mÃ­stÄ›, odkud se nemÅ¯Å¾e dostat.

## Navigace

LepÅ¡Ã­ navigaÄnÃ­ politika by byla ta, kterou jsme pouÅ¾ili bÄ›hem trÃ©ninku, kterÃ¡ kombinuje vyuÅ¾Ã­vÃ¡nÃ­ a zkoumÃ¡nÃ­. V tÃ©to politice budeme vybÃ­rat kaÅ¾dou akci s urÄitou pravdÄ›podobnostÃ­, ÃºmÄ›rnou hodnotÃ¡m v Q-Tabulce. Tato strategie mÅ¯Å¾e stÃ¡le vÃ©st k tomu, Å¾e se agent vrÃ¡tÃ­ na pozici, kterou jiÅ¾ prozkoumal, ale jak mÅ¯Å¾ete vidÄ›t z nÃ­Å¾e uvedenÃ©ho kÃ³du, vede k velmi krÃ¡tkÃ© prÅ¯mÄ›rnÃ© cestÄ› k poÅ¾adovanÃ©mu mÃ­stu (pamatujte, Å¾e `print_statistics` spouÅ¡tÃ­ simulaci 100krÃ¡t): (kÃ³dovÃ½ blok 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Po spuÅ¡tÄ›nÃ­ tohoto kÃ³du byste mÄ›li zÃ­skat mnohem kratÅ¡Ã­ prÅ¯mÄ›rnou dÃ©lku cesty neÅ¾ dÅ™Ã­ve, v rozmezÃ­ 3-6.

## ZkoumÃ¡nÃ­ procesu uÄenÃ­

Jak jsme zmÃ­nili, proces uÄenÃ­ je rovnovÃ¡hou mezi zkoumÃ¡nÃ­m a vyuÅ¾Ã­vÃ¡nÃ­m zÃ­skanÃ½ch znalostÃ­ o struktuÅ™e prostoru problÃ©mu. VidÄ›li jsme, Å¾e vÃ½sledky uÄenÃ­ (schopnost pomoci agentovi najÃ­t krÃ¡tkou cestu k cÃ­li) se zlepÅ¡ily, ale je takÃ© zajÃ­mavÃ© sledovat, jak se prÅ¯mÄ›rnÃ¡ dÃ©lka cesty chovÃ¡ bÄ›hem procesu uÄenÃ­:

## ShrnutÃ­ poznatkÅ¯:

- **PrÅ¯mÄ›rnÃ¡ dÃ©lka cesty se zvyÅ¡uje**. Na zaÄÃ¡tku vidÃ­me, Å¾e prÅ¯mÄ›rnÃ¡ dÃ©lka cesty roste. PravdÄ›podobnÄ› je to zpÅ¯sobeno tÃ­m, Å¾e kdyÅ¾ o prostÅ™edÃ­ nic nevÃ­me, mÃ¡me tendenci uvÃ­znout ve Å¡patnÃ½ch stavech, jako je voda nebo vlk. Jak se dozvÃ­dÃ¡me vÃ­ce a zaÄneme tyto znalosti vyuÅ¾Ã­vat, mÅ¯Å¾eme prostÅ™edÃ­ prozkoumÃ¡vat dÃ©le, ale stÃ¡le nevÃ­me, kde pÅ™esnÄ› jsou jablka.

- **DÃ©lka cesty se s uÄenÃ­m zkracuje**. Jakmile se nauÄÃ­me dostateÄnÄ›, je pro agenta snazÅ¡Ã­ dosÃ¡hnout cÃ­le a dÃ©lka cesty se zaÄne zkracovat. StÃ¡le vÅ¡ak zÅ¯stÃ¡vÃ¡me otevÅ™enÃ­ zkoumÃ¡nÃ­, takÅ¾e Äasto odboÄÃ­me od nejlepÅ¡Ã­ cesty a zkoumÃ¡me novÃ© moÅ¾nosti, coÅ¾ cestu prodluÅ¾uje nad optimÃ¡lnÃ­ dÃ©lku.

- **DÃ©lka se nÃ¡hle zvÃ½Å¡Ã­**. Na grafu takÃ© pozorujeme, Å¾e v urÄitÃ©m bodÄ› se dÃ©lka nÃ¡hle zvÃ½Å¡Ã­. To ukazuje na stochastickou povahu procesu a na to, Å¾e mÅ¯Å¾eme v urÄitÃ©m okamÅ¾iku "zkazit" koeficienty Q-Tabulky jejich pÅ™epsÃ¡nÃ­m novÃ½mi hodnotami. IdeÃ¡lnÄ› by se tomu mÄ›lo zabrÃ¡nit snÃ­Å¾enÃ­m rychlosti uÄenÃ­ (napÅ™Ã­klad ke konci trÃ©ninku upravujeme hodnoty Q-Tabulky pouze o malou hodnotu).

CelkovÄ› je dÅ¯leÅ¾itÃ© si uvÄ›domit, Å¾e ÃºspÄ›ch a kvalita procesu uÄenÃ­ vÃ½znamnÄ› zÃ¡visÃ­ na parametrech, jako je rychlost uÄenÃ­, pokles rychlosti uÄenÃ­ a diskontnÃ­ faktor. Tyto parametry se Äasto nazÃ½vajÃ­ **hyperparametry**, aby se odliÅ¡ily od **parametrÅ¯**, kterÃ© optimalizujeme bÄ›hem trÃ©ninku (napÅ™Ã­klad koeficienty Q-Tabulky). Proces hledÃ¡nÃ­ nejlepÅ¡Ã­ch hodnot hyperparametrÅ¯ se nazÃ½vÃ¡ **optimalizace hyperparametrÅ¯** a zaslouÅ¾Ã­ si samostatnÃ© tÃ©ma.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ml/)

## ZadÃ¡nÃ­ 
[RealistiÄtÄ›jÅ¡Ã­ svÄ›t](assignment.md)

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.