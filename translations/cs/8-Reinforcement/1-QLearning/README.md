<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T01:06:51+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "cs"
}
-->
# √övod do posilovan√©ho uƒçen√≠ a Q-Learningu

![Shrnut√≠ posilovan√©ho uƒçen√≠ v oblasti strojov√©ho uƒçen√≠ ve sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote od [Tomomi Imura](https://www.twitter.com/girlie_mac)

Posilovan√© uƒçen√≠ zahrnuje t≈ôi d≈Øle≈æit√© koncepty: agenta, stavy a sadu akc√≠ pro ka≈æd√Ω stav. Prov√°dƒõn√≠m akce ve specifick√©m stavu z√≠sk√°v√° agent odmƒõnu. P≈ôedstavte si opƒõt poƒç√≠taƒçovou hru Super Mario. Vy jste Mario, nach√°z√≠te se v √∫rovni hry, stoj√≠te vedle okraje √∫tesu. Nad v√°mi je mince. Vy jako Mario, v hern√≠ √∫rovni, na konkr√©tn√≠ pozici... to je v√°≈° stav. Posun o krok doprava (akce) v√°s p≈ôivede p≈ôes okraj, co≈æ by v√°m p≈ôineslo n√≠zk√© ƒç√≠seln√© sk√≥re. Stisknut√≠ tlaƒç√≠tka skoku by v√°m v≈°ak umo≈ænilo z√≠skat bod a z≈Østat na≈æivu. To je pozitivn√≠ v√Ωsledek, kter√Ω by v√°m mƒõl p≈ôin√©st pozitivn√≠ ƒç√≠seln√© sk√≥re.

Pomoc√≠ posilovan√©ho uƒçen√≠ a simul√°toru (hry) se m≈Ø≈æete nauƒçit, jak hru hr√°t, abyste maximalizovali odmƒõnu, co≈æ znamen√° z≈Østat na≈æivu a z√≠skat co nejv√≠ce bod≈Ø.

[![√övod do posilovan√©ho uƒçen√≠](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Kliknƒõte na obr√°zek v√Ω≈°e a poslechnƒõte si Dmitryho, jak diskutuje o posilovan√©m uƒçen√≠.

## [Kv√≠z p≈ôed lekc√≠](https://ff-quizzes.netlify.app/en/ml/)

## P≈ôedpoklady a nastaven√≠

V t√©to lekci budeme experimentovat s k√≥dem v Pythonu. Mƒõli byste b√Ωt schopni spustit k√≥d v Jupyter Notebooku z t√©to lekce, buƒè na sv√©m poƒç√≠taƒçi, nebo nƒõkde v cloudu.

M≈Ø≈æete otev≈ô√≠t [notebook lekce](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) a proj√≠t si tuto lekci krok za krokem.

> **Pozn√°mka:** Pokud otev√≠r√°te tento k√≥d z cloudu, mus√≠te tak√© st√°hnout soubor [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kter√Ω se pou≈æ√≠v√° v k√≥du notebooku. P≈ôidejte jej do stejn√©ho adres√°≈ôe jako notebook.

## √övod

V t√©to lekci prozkoum√°me svƒõt **[Petr a vlk](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirovan√Ω hudebn√≠ poh√°dkou rusk√©ho skladatele [Sergeje Prokofjeva](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Pou≈æijeme **posilovan√© uƒçen√≠**, aby Petr mohl prozkoumat sv√© prost≈ôed√≠, sb√≠rat chutn√° jablka a vyhnout se setk√°n√≠ s vlkem.

**Posilovan√© uƒçen√≠** (RL) je technika uƒçen√≠, kter√° n√°m umo≈æ≈àuje nauƒçit se optim√°ln√≠ chov√°n√≠ **agenta** v nƒõjak√©m **prost≈ôed√≠** prost≈ôednictv√≠m mnoha experiment≈Ø. Agent v tomto prost≈ôed√≠ by mƒõl m√≠t nƒõjak√Ω **c√≠l**, definovan√Ω pomoc√≠ **funkce odmƒõny**.

## Prost≈ôed√≠

Pro jednoduchost si p≈ôedstavme Petrov svƒõt jako ƒçtvercovou desku o velikosti `≈°√≠≈ôka` x `v√Ω≈°ka`, jako je tato:

![Petrovo prost≈ôed√≠](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Ka≈æd√° bu≈àka na t√©to desce m≈Ø≈æe b√Ωt:

* **zem**, po kter√© Petr a dal≈°√≠ bytosti mohou chodit.
* **voda**, po kter√© samoz≈ôejmƒõ nem≈Ø≈æete chodit.
* **strom** nebo **tr√°va**, m√≠sto, kde si m≈Ø≈æete odpoƒçinout.
* **jablko**, co≈æ p≈ôedstavuje nƒõco, co by Petr r√°d na≈°el, aby se nakrmil.
* **vlk**, kter√Ω je nebezpeƒçn√Ω a mƒõl by b√Ωt vyhnut.

Existuje samostatn√Ω modul v Pythonu, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), kter√Ω obsahuje k√≥d pro pr√°ci s t√≠mto prost≈ôed√≠m. Proto≈æe tento k√≥d nen√≠ d≈Øle≈æit√Ω pro pochopen√≠ na≈°ich koncept≈Ø, importujeme modul a pou≈æijeme jej k vytvo≈ôen√≠ vzorov√© desky (blok k√≥du 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Tento k√≥d by mƒõl vytisknout obr√°zek prost≈ôed√≠ podobn√Ω tomu v√Ω≈°e.

## Akce a politika

V na≈°em p≈ô√≠kladu by Petrov√Ωm c√≠lem bylo naj√≠t jablko, zat√≠mco se vyh√Ωb√° vlkovi a dal≈°√≠m p≈ôek√°≈æk√°m. K tomu m≈Ø≈æe v podstatƒõ chodit, dokud nenajde jablko.

Proto m≈Ø≈æe na jak√©koli pozici zvolit jednu z n√°sleduj√≠c√≠ch akc√≠: nahoru, dol≈Ø, doleva a doprava.

Tyto akce definujeme jako slovn√≠k a mapujeme je na dvojice odpov√≠daj√≠c√≠ch zmƒõn sou≈ôadnic. Nap≈ô√≠klad pohyb doprava (`R`) by odpov√≠dal dvojici `(1,0)`. (blok k√≥du 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Shrneme-li, strategie a c√≠l tohoto sc√©n√°≈ôe jsou n√°sleduj√≠c√≠:

- **Strategie** na≈°eho agenta (Petra) je definov√°na tzv. **politikou**. Politika je funkce, kter√° vrac√≠ akci v dan√©m stavu. V na≈°em p≈ô√≠padƒõ je stav probl√©mu reprezentov√°n deskou, vƒçetnƒõ aktu√°ln√≠ pozice hr√°ƒçe.

- **C√≠l** posilovan√©ho uƒçen√≠ je nakonec nauƒçit se dobrou politiku, kter√° n√°m umo≈æn√≠ probl√©m efektivnƒõ vy≈ôe≈°it. Jako z√°klad v≈°ak zva≈æme nejjednodu≈°≈°√≠ politiku nazvanou **n√°hodn√° ch≈Øze**.

## N√°hodn√° ch≈Øze

Nejprve vy≈ôe≈°√≠me n√°≈° probl√©m implementac√≠ strategie n√°hodn√© ch≈Øze. P≈ôi n√°hodn√© ch≈Øzi budeme n√°hodnƒõ vyb√≠rat dal≈°√≠ akci z povolen√Ωch akc√≠, dokud nedos√°hneme jablka (blok k√≥du 3).

1. Implementujte n√°hodnou ch≈Øzi pomoc√≠ n√≠≈æe uveden√©ho k√≥du:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Vol√°n√≠ `walk` by mƒõlo vr√°tit d√©lku odpov√≠daj√≠c√≠ cesty, kter√° se m≈Ø≈æe li≈°it od jednoho spu≈°tƒõn√≠ k druh√©mu.

1. Spus≈•te experiment ch≈Øze nƒõkolikr√°t (≈ôeknƒõme 100kr√°t) a vytisknƒõte v√Ωsledn√© statistiky (blok k√≥du 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    V≈°imnƒõte si, ≈æe pr≈Ømƒõrn√° d√©lka cesty je kolem 30-40 krok≈Ø, co≈æ je pomƒõrnƒõ hodnƒõ, vzhledem k tomu, ≈æe pr≈Ømƒõrn√° vzd√°lenost k nejbli≈æ≈°√≠mu jablku je kolem 5-6 krok≈Ø.

    M≈Ø≈æete tak√© vidƒõt, jak vypad√° Petrov pohyb bƒõhem n√°hodn√© ch≈Øze:

    ![Petrova n√°hodn√° ch≈Øze](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Funkce odmƒõny

Aby byla na≈°e politika inteligentnƒõj≈°√≠, mus√≠me pochopit, kter√© kroky jsou "lep≈°√≠" ne≈æ jin√©. K tomu mus√≠me definovat n√°≈° c√≠l.

C√≠l m≈Ø≈æe b√Ωt definov√°n pomoc√≠ **funkce odmƒõny**, kter√° vr√°t√≠ nƒõjakou hodnotu sk√≥re pro ka≈æd√Ω stav. ƒå√≠m vy≈°≈°√≠ ƒç√≠slo, t√≠m lep≈°√≠ funkce odmƒõny. (blok k√≥du 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Zaj√≠mav√© na funkc√≠ch odmƒõny je, ≈æe ve vƒõt≈°inƒõ p≈ô√≠pad≈Ø *dost√°v√°me podstatnou odmƒõnu a≈æ na konci hry*. To znamen√°, ≈æe n√°≈° algoritmus by mƒõl nƒõjak√Ωm zp≈Øsobem zapamatovat "dobr√©" kroky, kter√© vedou k pozitivn√≠ odmƒõnƒõ na konci, a zv√Ω≈°it jejich d≈Øle≈æitost. Podobnƒõ by mƒõly b√Ωt odrazeny v≈°echny kroky, kter√© vedou k ≈°patn√Ωm v√Ωsledk≈Øm.

## Q-Learning

Algoritmus, kter√Ω zde budeme diskutovat, se naz√Ωv√° **Q-Learning**. V tomto algoritmu je politika definov√°na funkc√≠ (nebo datovou strukturou) nazvanou **Q-Tabulka**. Ta zaznamen√°v√° "kvalitu" ka≈æd√© akce v dan√©m stavu.

Naz√Ωv√° se Q-Tabulka, proto≈æe je ƒçasto v√Ωhodn√© ji reprezentovat jako tabulku nebo v√≠cerozmƒõrn√© pole. Proto≈æe na≈°e deska m√° rozmƒõry `≈°√≠≈ôka` x `v√Ω≈°ka`, m≈Ø≈æeme Q-Tabulku reprezentovat pomoc√≠ numpy pole s tvarem `≈°√≠≈ôka` x `v√Ω≈°ka` x `len(actions)`: (blok k√≥du 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

V≈°imnƒõte si, ≈æe inicializujeme v≈°echny hodnoty Q-Tabulky stejnou hodnotou, v na≈°em p≈ô√≠padƒõ - 0.25. To odpov√≠d√° politice "n√°hodn√© ch≈Øze", proto≈æe v≈°echny kroky v ka≈æd√©m stavu jsou stejnƒõ dobr√©. Q-Tabulku m≈Ø≈æeme p≈ôedat funkci `plot`, abychom ji vizualizovali na desce: `m.plot(Q)`.

![Petrovo prost≈ôed√≠](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

Uprost≈ôed ka≈æd√© bu≈àky je "≈°ipka", kter√° oznaƒçuje preferovan√Ω smƒõr pohybu. Proto≈æe v≈°echny smƒõry jsou stejn√©, je zobrazen bod.

Nyn√≠ mus√≠me spustit simulaci, prozkoumat na≈°e prost≈ôed√≠ a nauƒçit se lep≈°√≠ rozlo≈æen√≠ hodnot Q-Tabulky, kter√© n√°m umo≈æn√≠ naj√≠t cestu k jablku mnohem rychleji.

## Podstata Q-Learningu: Bellmanova rovnice

Jakmile se zaƒçneme pohybovat, ka≈æd√° akce bude m√≠t odpov√≠daj√≠c√≠ odmƒõnu, tj. teoreticky m≈Ø≈æeme vybrat dal≈°√≠ akci na z√°kladƒõ nejvy≈°≈°√≠ okam≈æit√© odmƒõny. Ve vƒõt≈°inƒõ stav≈Ø v≈°ak krok nedos√°hne na≈°eho c√≠le dos√°hnout jablka, a proto nem≈Ø≈æeme okam≈æitƒõ rozhodnout, kter√Ω smƒõr je lep≈°√≠.

> Pamatujte, ≈æe nez√°le≈æ√≠ na okam≈æit√©m v√Ωsledku, ale sp√≠≈°e na koneƒçn√©m v√Ωsledku, kter√Ω z√≠sk√°me na konci simulace.

Abychom zohlednili tuto zpo≈ædƒõnou odmƒõnu, mus√≠me pou≈æ√≠t principy **[dynamick√©ho programov√°n√≠](https://en.wikipedia.org/wiki/Dynamic_programming)**, kter√© n√°m umo≈æ≈àuj√≠ p≈ôem√Ω≈°let o na≈°em probl√©mu rekurzivnƒõ.

P≈ôedpokl√°dejme, ≈æe se nyn√≠ nach√°z√≠me ve stavu *s* a chceme se p≈ôesunout do dal≈°√≠ho stavu *s'*. T√≠m z√≠sk√°me okam≈æitou odmƒõnu *r(s,a)*, definovanou funkc√≠ odmƒõny, plus nƒõjakou budouc√≠ odmƒõnu. Pokud p≈ôedpokl√°d√°me, ≈æe na≈°e Q-Tabulka spr√°vnƒõ odr√°≈æ√≠ "atraktivitu" ka≈æd√© akce, pak ve stavu *s'* zvol√≠me akci *a*, kter√° odpov√≠d√° maxim√°ln√≠ hodnotƒõ *Q(s',a')*. T√≠m p√°dem nejlep≈°√≠ mo≈æn√° budouc√≠ odmƒõna, kterou bychom mohli z√≠skat ve stavu *s*, bude definov√°na jako `max`

## Kontrola politiky

Proto≈æe Q-Tabulka uv√°d√≠ "atraktivitu" ka≈æd√© akce v ka≈æd√©m stavu, je pomƒõrnƒõ snadn√© ji pou≈æ√≠t k definov√°n√≠ efektivn√≠ navigace v na≈°em svƒõtƒõ. V nejjednodu≈°≈°√≠m p≈ô√≠padƒõ m≈Ø≈æeme vybrat akci odpov√≠daj√≠c√≠ nejvy≈°≈°√≠ hodnotƒõ v Q-Tabulce: (k√≥dov√Ω blok 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Pokud v√Ω≈°e uveden√Ω k√≥d vyzkou≈°√≠te nƒõkolikr√°t, m≈Ø≈æete si v≈°imnout, ≈æe se nƒõkdy "zasekne" a je t≈ôeba stisknout tlaƒç√≠tko STOP v notebooku, abyste ho p≈ôeru≈°ili. K tomu doch√°z√≠, proto≈æe mohou nastat situace, kdy si dva stavy "ukazuj√≠" na sebe z hlediska optim√°ln√≠ hodnoty Q, co≈æ vede k tomu, ≈æe agent se mezi tƒõmito stavy pohybuje nekoneƒçnƒõ.

## üöÄV√Ωzva

> **√ökol 1:** Upravte funkci `walk` tak, aby omezila maxim√°ln√≠ d√©lku cesty na urƒçit√Ω poƒçet krok≈Ø (nap≈ô√≠klad 100), a sledujte, jak v√Ω≈°e uveden√Ω k√≥d tuto hodnotu ƒças od ƒçasu vrac√≠.

> **√ökol 2:** Upravte funkci `walk` tak, aby se nevracela na m√≠sta, kde ji≈æ byla. T√≠m se zabr√°n√≠ tomu, aby se `walk` opakovala, nicm√©nƒõ agent m≈Ø≈æe st√°le skonƒçit "uvƒõznƒõn√Ω" na m√≠stƒõ, odkud se nem≈Ø≈æe dostat.

## Navigace

Lep≈°√≠ navigaƒçn√≠ politika by byla ta, kterou jsme pou≈æili bƒõhem tr√©ninku, kter√° kombinuje vyu≈æ√≠v√°n√≠ a zkoum√°n√≠. V t√©to politice budeme vyb√≠rat ka≈ædou akci s urƒçitou pravdƒõpodobnost√≠, √∫mƒõrnou hodnot√°m v Q-Tabulce. Tato strategie m≈Ø≈æe st√°le v√©st k tomu, ≈æe se agent vr√°t√≠ na pozici, kterou ji≈æ prozkoumal, ale jak m≈Ø≈æete vidƒõt z n√≠≈æe uveden√©ho k√≥du, vede k velmi kr√°tk√© pr≈Ømƒõrn√© cestƒõ k po≈æadovan√©mu m√≠stu (pamatujte, ≈æe `print_statistics` spou≈°t√≠ simulaci 100kr√°t): (k√≥dov√Ω blok 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Po spu≈°tƒõn√≠ tohoto k√≥du byste mƒõli z√≠skat mnohem krat≈°√≠ pr≈Ømƒõrnou d√©lku cesty ne≈æ d≈ô√≠ve, v rozmez√≠ 3-6.

## Zkoum√°n√≠ procesu uƒçen√≠

Jak jsme zm√≠nili, proces uƒçen√≠ je rovnov√°hou mezi zkoum√°n√≠m a vyu≈æ√≠v√°n√≠m z√≠skan√Ωch znalost√≠ o struktu≈ôe prostoru probl√©mu. Vidƒõli jsme, ≈æe v√Ωsledky uƒçen√≠ (schopnost pomoci agentovi naj√≠t kr√°tkou cestu k c√≠li) se zlep≈°ily, ale je tak√© zaj√≠mav√© sledovat, jak se pr≈Ømƒõrn√° d√©lka cesty chov√° bƒõhem procesu uƒçen√≠:

## Shrnut√≠ poznatk≈Ø:

- **Pr≈Ømƒõrn√° d√©lka cesty se zvy≈°uje**. Na zaƒç√°tku vid√≠me, ≈æe pr≈Ømƒõrn√° d√©lka cesty roste. Pravdƒõpodobnƒõ je to zp≈Øsobeno t√≠m, ≈æe kdy≈æ o prost≈ôed√≠ nic nev√≠me, m√°me tendenci uv√≠znout ve ≈°patn√Ωch stavech, jako je voda nebo vlk. Jak se dozv√≠d√°me v√≠ce a zaƒçneme tyto znalosti vyu≈æ√≠vat, m≈Ø≈æeme prost≈ôed√≠ prozkoum√°vat d√©le, ale st√°le nev√≠me, kde p≈ôesnƒõ jsou jablka.

- **D√©lka cesty se s uƒçen√≠m zkracuje**. Jakmile se nauƒç√≠me dostateƒçnƒõ, je pro agenta snaz≈°√≠ dos√°hnout c√≠le a d√©lka cesty se zaƒçne zkracovat. St√°le v≈°ak z≈Øst√°v√°me otev≈ôen√≠ zkoum√°n√≠, tak≈æe ƒçasto odboƒç√≠me od nejlep≈°√≠ cesty a zkoum√°me nov√© mo≈ænosti, co≈æ cestu prodlu≈æuje nad optim√°ln√≠ d√©lku.

- **D√©lka se n√°hle zv√Ω≈°√≠**. Na grafu tak√© pozorujeme, ≈æe v urƒçit√©m bodƒõ se d√©lka n√°hle zv√Ω≈°√≠. To ukazuje na stochastickou povahu procesu a na to, ≈æe m≈Ø≈æeme v urƒçit√©m okam≈æiku "zkazit" koeficienty Q-Tabulky jejich p≈ôeps√°n√≠m nov√Ωmi hodnotami. Ide√°lnƒõ by se tomu mƒõlo zabr√°nit sn√≠≈æen√≠m rychlosti uƒçen√≠ (nap≈ô√≠klad ke konci tr√©ninku upravujeme hodnoty Q-Tabulky pouze o malou hodnotu).

Celkovƒõ je d≈Øle≈æit√© si uvƒõdomit, ≈æe √∫spƒõch a kvalita procesu uƒçen√≠ v√Ωznamnƒõ z√°vis√≠ na parametrech, jako je rychlost uƒçen√≠, pokles rychlosti uƒçen√≠ a diskontn√≠ faktor. Tyto parametry se ƒçasto naz√Ωvaj√≠ **hyperparametry**, aby se odli≈°ily od **parametr≈Ø**, kter√© optimalizujeme bƒõhem tr√©ninku (nap≈ô√≠klad koeficienty Q-Tabulky). Proces hled√°n√≠ nejlep≈°√≠ch hodnot hyperparametr≈Ø se naz√Ωv√° **optimalizace hyperparametr≈Ø** a zaslou≈æ√≠ si samostatn√© t√©ma.

## [Kv√≠z po p≈ôedn√°≈°ce](https://ff-quizzes.netlify.app/en/ml/)

## Zad√°n√≠ 
[Realistiƒçtƒõj≈°√≠ svƒõt](assignment.md)

---

**Prohl√°≈°en√≠**:  
Tento dokument byl p≈ôelo≈æen pomoc√≠ slu≈æby pro automatick√Ω p≈ôeklad [Co-op Translator](https://github.com/Azure/co-op-translator). Aƒçkoli se sna≈æ√≠me o p≈ôesnost, mƒõjte pros√≠m na pamƒõti, ≈æe automatick√© p≈ôeklady mohou obsahovat chyby nebo nep≈ôesnosti. P≈Øvodn√≠ dokument v jeho p≈Øvodn√≠m jazyce by mƒõl b√Ωt pova≈æov√°n za autoritativn√≠ zdroj. Pro d≈Øle≈æit√© informace se doporuƒçuje profesion√°ln√≠ lidsk√Ω p≈ôeklad. Neodpov√≠d√°me za ≈æ√°dn√° nedorozumƒõn√≠ nebo nespr√°vn√© interpretace vypl√Ωvaj√≠c√≠ z pou≈æit√≠ tohoto p≈ôekladu.