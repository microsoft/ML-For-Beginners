<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T00:05:07+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "cs"
}
-->
# K-Means clustering

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

V tÃ©to lekci se nauÄÃ­te, jak vytvÃ¡Å™et klastery pomocÃ­ Scikit-learn a nigerijskÃ©ho hudebnÃ­ho datasetu, kterÃ½ jste importovali dÅ™Ã­ve. Probereme zÃ¡klady K-Means pro klastrovÃ¡nÃ­. MÄ›jte na pamÄ›ti, Å¾e jak jste se nauÄili v pÅ™edchozÃ­ lekci, existuje mnoho zpÅ¯sobÅ¯, jak pracovat s klastery, a metoda, kterou pouÅ¾ijete, zÃ¡visÃ­ na vaÅ¡ich datech. VyzkouÅ¡Ã­me K-Means, protoÅ¾e je to nejbÄ›Å¾nÄ›jÅ¡Ã­ technika klastrovÃ¡nÃ­. PojÄme zaÄÃ­t!

Pojmy, o kterÃ½ch se dozvÃ­te:

- Silhouette skÃ³re
- Metoda lokte
- InerciÃ¡lnÃ­ hodnota
- Variance

## Ãšvod

[K-Means Clustering](https://wikipedia.org/wiki/K-means_clustering) je metoda odvozenÃ¡ z oblasti zpracovÃ¡nÃ­ signÃ¡lÅ¯. PouÅ¾Ã­vÃ¡ se k rozdÄ›lenÃ­ a seskupenÃ­ dat do 'k' klastrÅ¯ pomocÃ­ sÃ©rie pozorovÃ¡nÃ­. KaÅ¾dÃ© pozorovÃ¡nÃ­ pracuje na seskupenÃ­ danÃ©ho datovÃ©ho bodu k nejbliÅ¾Å¡Ã­mu 'prÅ¯mÄ›ru', tedy stÅ™edovÃ©mu bodu klastru.

Klastery lze vizualizovat jako [Voronoi diagramy](https://wikipedia.org/wiki/Voronoi_diagram), kterÃ© zahrnujÃ­ bod (nebo 'semÃ­nko') a jeho odpovÃ­dajÃ­cÃ­ oblast.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografika od [Jen Looper](https://twitter.com/jenlooper)

Proces K-Means klastrovÃ¡nÃ­ [probÃ­hÃ¡ ve tÅ™ech krocÃ­ch](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritmus vybere k-poÄet stÅ™edovÃ½ch bodÅ¯ vzorkovÃ¡nÃ­m z datasetu. PotÃ© opakuje:
    1. PÅ™iÅ™adÃ­ kaÅ¾dÃ½ vzorek k nejbliÅ¾Å¡Ã­mu centroidu.
    2. VytvoÅ™Ã­ novÃ© centroidy vypoÄÃ­tÃ¡nÃ­m prÅ¯mÄ›rnÃ© hodnoty vÅ¡ech vzorkÅ¯ pÅ™iÅ™azenÃ½ch k pÅ™edchozÃ­m centroidÅ¯m.
    3. PotÃ© vypoÄÃ­tÃ¡ rozdÃ­l mezi novÃ½mi a starÃ½mi centroidy a opakuje, dokud se centroidy nestabilizujÃ­.

Jednou z nevÃ½hod pouÅ¾itÃ­ K-Means je nutnost stanovit 'k', tedy poÄet centroidÅ¯. NaÅ¡tÄ›stÃ­ metoda 'lokte' pomÃ¡hÃ¡ odhadnout dobrÃ½ vÃ½chozÃ­ poÄet 'k'. Za chvÃ­li si ji vyzkouÅ¡Ã­te.

## PÅ™edpoklady

Budete pracovat v souboru [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb), kterÃ½ obsahuje import dat a pÅ™edbÄ›Å¾nÃ© ÄiÅ¡tÄ›nÃ­, kterÃ© jste provedli v minulÃ© lekci.

## CviÄenÃ­ - pÅ™Ã­prava

ZaÄnÄ›te tÃ­m, Å¾e se znovu podÃ­vÃ¡te na data o pÃ­snÃ­ch.

1. VytvoÅ™te boxplot, zavolejte `boxplot()` pro kaÅ¾dÃ½ sloupec:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Tato data jsou trochu hluÄnÃ¡: pÅ™i pozorovÃ¡nÃ­ kaÅ¾dÃ©ho sloupce jako boxplotu mÅ¯Å¾ete vidÄ›t odlehlÃ© hodnoty.

    ![outliers](../../../../5-Clustering/2-K-Means/images/boxplots.png)

MÅ¯Å¾ete projÃ­t dataset a odstranit tyto odlehlÃ© hodnoty, ale to by data znaÄnÄ› zredukovalo.

1. ProzatÃ­m vyberte, kterÃ© sloupce pouÅ¾ijete pro svÃ© cviÄenÃ­ klastrovÃ¡nÃ­. Vyberte ty s podobnÃ½mi rozsahy a zakÃ³dujte sloupec `artist_top_genre` jako ÄÃ­selnÃ¡ data:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. NynÃ­ musÃ­te vybrat, kolik klastrÅ¯ budete cÃ­lit. VÃ­te, Å¾e existujÃ­ 3 hudebnÃ­ Å¾Ã¡nry, kterÃ© jsme vyÄlenili z datasetu, takÅ¾e zkusme 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

VidÃ­te vytiÅ¡tÄ›nÃ© pole s pÅ™edpovÄ›zenÃ½mi klastery (0, 1 nebo 2) pro kaÅ¾dÃ½ Å™Ã¡dek datovÃ©ho rÃ¡mce.

1. PouÅ¾ijte toto pole k vÃ½poÄtu 'silhouette skÃ³re':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhouette skÃ³re

Hledejte silhouette skÃ³re blÃ­Å¾e k 1. Toto skÃ³re se pohybuje od -1 do 1, a pokud je skÃ³re 1, klastr je hustÃ½ a dobÅ™e oddÄ›lenÃ½ od ostatnÃ­ch klastrÅ¯. Hodnota blÃ­zko 0 pÅ™edstavuje pÅ™ekrÃ½vajÃ­cÃ­ se klastery s vzorky velmi blÃ­zko rozhodovacÃ­ hranice sousednÃ­ch klastrÅ¯. [(Zdroj)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

NaÅ¡e skÃ³re je **.53**, tedy pÅ™Ã­mo uprostÅ™ed. To naznaÄuje, Å¾e naÅ¡e data nejsou pro tento typ klastrovÃ¡nÃ­ pÅ™Ã­liÅ¡ vhodnÃ¡, ale pokraÄujme.

### CviÄenÃ­ - vytvoÅ™enÃ­ modelu

1. Importujte `KMeans` a zaÄnÄ›te proces klastrovÃ¡nÃ­.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    NÄ›kolik ÄÃ¡stÃ­ zde si zaslouÅ¾Ã­ vysvÄ›tlenÃ­.

    > ğŸ“ range: Toto jsou iterace procesu klastrovÃ¡nÃ­.

    > ğŸ“ random_state: "UrÄuje generovÃ¡nÃ­ nÃ¡hodnÃ½ch ÄÃ­sel pro inicializaci centroidÅ¯." [Zdroj](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > ğŸ“ WCSS: "souÄet ÄtvercÅ¯ uvnitÅ™ klastrÅ¯" mÄ›Å™Ã­ prÅ¯mÄ›rnou Ätvercovou vzdÃ¡lenost vÅ¡ech bodÅ¯ v rÃ¡mci klastru od centroidu klastru. [Zdroj](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > ğŸ“ InerciÃ¡lnÃ­ hodnota: Algoritmy K-Means se snaÅ¾Ã­ vybrat centroidy tak, aby minimalizovaly 'inerciÃ¡lnÃ­ hodnotu', "mÄ›Å™Ã­tko toho, jak jsou klastery internÄ› koherentnÃ­." [Zdroj](https://scikit-learn.org/stable/modules/clustering.html). Hodnota je pÅ™ipojena k promÄ›nnÃ© wcss pÅ™i kaÅ¾dÃ© iteraci.

    > ğŸ“ k-means++: V [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) mÅ¯Å¾ete pouÅ¾Ã­t optimalizaci 'k-means++', kterÃ¡ "inicializuje centroidy tak, aby byly (obecnÄ›) vzdÃ¡lenÃ© od sebe, coÅ¾ vede pravdÄ›podobnÄ› k lepÅ¡Ã­m vÃ½sledkÅ¯m neÅ¾ nÃ¡hodnÃ¡ inicializace."

### Metoda lokte

DÅ™Ã­ve jste pÅ™edpoklÃ¡dali, Å¾e protoÅ¾e jste cÃ­lovali 3 hudebnÃ­ Å¾Ã¡nry, mÄ›li byste zvolit 3 klastery. Ale je tomu tak?

1. PouÅ¾ijte metodu 'lokte', abyste si byli jistÃ­.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    PouÅ¾ijte promÄ›nnou `wcss`, kterou jste vytvoÅ™ili v pÅ™edchozÃ­m kroku, k vytvoÅ™enÃ­ grafu ukazujÃ­cÃ­ho, kde je 'ohyb' v lokti, coÅ¾ naznaÄuje optimÃ¡lnÃ­ poÄet klastrÅ¯. MoÅ¾nÃ¡ to **opravdu jsou** 3!

    ![elbow method](../../../../5-Clustering/2-K-Means/images/elbow.png)

## CviÄenÃ­ - zobrazenÃ­ klastrÅ¯

1. Zkuste proces znovu, tentokrÃ¡t nastavte tÅ™i klastery a zobrazte klastery jako scatterplot:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Zkontrolujte pÅ™esnost modelu:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    PÅ™esnost tohoto modelu nenÃ­ pÅ™Ã­liÅ¡ dobrÃ¡ a tvar klastrÅ¯ vÃ¡m naznaÄuje proÄ.

    ![clusters](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Tato data jsou pÅ™Ã­liÅ¡ nevyvÃ¡Å¾enÃ¡, mÃ¡lo korelovanÃ¡ a mezi hodnotami sloupcÅ¯ je pÅ™Ã­liÅ¡ velkÃ¡ variance na to, aby se dobÅ™e klastrovala. Ve skuteÄnosti jsou klastery, kterÃ© se tvoÅ™Ã­, pravdÄ›podobnÄ› silnÄ› ovlivnÄ›ny nebo zkresleny tÅ™emi kategoriemi Å¾Ã¡nrÅ¯, kterÃ© jsme definovali vÃ½Å¡e. To byl proces uÄenÃ­!

    V dokumentaci Scikit-learn mÅ¯Å¾ete vidÄ›t, Å¾e model jako tento, s klastery, kterÃ© nejsou pÅ™Ã­liÅ¡ dobÅ™e vymezenÃ©, mÃ¡ problÃ©m s 'variancÃ­':

    ![problem models](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografika ze Scikit-learn

## Variance

Variance je definovÃ¡na jako "prÅ¯mÄ›r ÄtvercovÃ½ch rozdÃ­lÅ¯ od prÅ¯mÄ›ru" [(Zdroj)](https://www.mathsisfun.com/data/standard-deviation.html). V kontextu tohoto problÃ©mu klastrovÃ¡nÃ­ se jednÃ¡ o data, kde ÄÃ­sla naÅ¡eho datasetu majÃ­ tendenci se pÅ™Ã­liÅ¡ odchylovat od prÅ¯mÄ›ru.

âœ… Toto je skvÄ›lÃ½ moment k zamyÅ¡lenÃ­ nad vÅ¡emi zpÅ¯soby, jak byste mohli tento problÃ©m napravit. Upravit data trochu vÃ­ce? PouÅ¾Ã­t jinÃ© sloupce? PouÅ¾Ã­t jinÃ½ algoritmus? Tip: Zkuste [Å¡kÃ¡lovat svÃ¡ data](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) pro jejich normalizaci a otestujte jinÃ© sloupce.

> VyzkouÅ¡ejte tento '[kalkulÃ¡tor variance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)', abyste lÃ©pe pochopili tento koncept.

---

## ğŸš€VÃ½zva

Stravte nÄ›jakÃ½ Äas s tÃ­mto notebookem a upravujte parametry. DokÃ¡Å¾ete zlepÅ¡it pÅ™esnost modelu tÃ­m, Å¾e data vÃ­ce vyÄistÃ­te (napÅ™Ã­klad odstranÃ­te odlehlÃ© hodnoty)? MÅ¯Å¾ete pouÅ¾Ã­t vÃ¡hy, abyste dali vÄ›tÅ¡Ã­ vÃ¡hu urÄitÃ½m vzorkÅ¯m dat. Co dalÅ¡Ã­ho mÅ¯Å¾ete udÄ›lat pro vytvoÅ™enÃ­ lepÅ¡Ã­ch klastrÅ¯?

Tip: Zkuste Å¡kÃ¡lovat svÃ¡ data. V notebooku je komentovanÃ½ kÃ³d, kterÃ½ pÅ™idÃ¡vÃ¡ standardnÃ­ Å¡kÃ¡lovÃ¡nÃ­, aby se sloupce dat vÃ­ce podobaly z hlediska rozsahu. ZjistÃ­te, Å¾e zatÃ­mco silhouette skÃ³re klesÃ¡, 'ohyb' v grafu lokte se vyhlazuje. To je proto, Å¾e ponechÃ¡nÃ­ dat neÅ¡kÃ¡lovanÃ½ch umoÅ¾Åˆuje datÅ¯m s menÅ¡Ã­ variancÃ­ mÃ­t vÄ›tÅ¡Ã­ vÃ¡hu. PÅ™eÄtÄ›te si o tomto problÃ©mu vÃ­ce [zde](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## PÅ™ehled & Samostudium

PodÃ­vejte se na simulÃ¡tor K-Means [jako je tento](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). MÅ¯Å¾ete pouÅ¾Ã­t tento nÃ¡stroj k vizualizaci vzorovÃ½ch datovÃ½ch bodÅ¯ a urÄenÃ­ jejich centroidÅ¯. MÅ¯Å¾ete upravit nÃ¡hodnost dat, poÄet klastrÅ¯ a poÄet centroidÅ¯. PomÃ¡hÃ¡ vÃ¡m to zÃ­skat pÅ™edstavu o tom, jak lze data seskupit?

TakÃ© se podÃ­vejte na [tento materiÃ¡l o K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) ze Stanfordu.

## ZadÃ¡nÃ­

[VyzkouÅ¡ejte rÅ¯znÃ© metody klastrovÃ¡nÃ­](assignment.md)

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.