<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-04T23:27:47+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "cs"
}
-->
# Logistick√° regrese pro predikci kategori√≠

![Infografika: Logistick√° vs. line√°rn√≠ regrese](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Kv√≠z p≈ôed lekc√≠](https://ff-quizzes.netlify.app/en/ml/)

> ### [Tato lekce je dostupn√° v R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## √övod

V t√©to posledn√≠ lekci o regresi, jedn√© ze z√°kladn√≠ch _klasick√Ωch_ technik strojov√©ho uƒçen√≠, se pod√≠v√°me na logistickou regresi. Tuto techniku byste pou≈æili k odhalen√≠ vzorc≈Ø pro predikci bin√°rn√≠ch kategori√≠. Je tato cukrovinka ƒçokol√°dov√° nebo ne? Je tato nemoc naka≈æliv√° nebo ne? Vybere si tento z√°kazn√≠k tento produkt nebo ne?

V t√©to lekci se nauƒç√≠te:

- Novou knihovnu pro vizualizaci dat
- Techniky logistick√© regrese

‚úÖ Prohlubte sv√© znalosti pr√°ce s t√≠mto typem regrese v tomto [modulu Learn](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## P≈ôedpoklady

Po pr√°ci s daty o d√Ωn√≠ch jsme nyn√≠ dostateƒçnƒõ obezn√°meni s t√≠m, ≈æe existuje jedna bin√°rn√≠ kategorie, se kterou m≈Ø≈æeme pracovat: `Barva`.

Postavme model logistick√© regrese, kter√Ω bude predikovat, na z√°kladƒõ nƒõkter√Ωch promƒõnn√Ωch, _jakou barvu bude m√≠t dan√° d√Ωnƒõ_ (oran≈æov√° üéÉ nebo b√≠l√° üëª).

> Proƒç mluv√≠me o bin√°rn√≠ klasifikaci v lekci o regresi? Pouze z jazykov√©ho pohodl√≠, proto≈æe logistick√° regrese je [ve skuteƒçnosti klasifikaƒçn√≠ metoda](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), i kdy≈æ zalo≈æen√° na line√°rn√≠ regresi. O dal≈°√≠ch zp≈Øsobech klasifikace dat se dozv√≠te v dal≈°√≠ skupinƒõ lekc√≠.

## Definujte ot√°zku

Pro na≈°e √∫ƒçely to vyj√°d≈ô√≠me jako bin√°rn√≠: 'B√≠l√°' nebo 'Ne b√≠l√°'. V na≈°em datasetu je tak√© kategorie 'pruhovan√°', ale m√° m√°lo z√°znam≈Ø, tak≈æe ji nebudeme pou≈æ√≠vat. Stejnƒõ zmiz√≠, jakmile odstran√≠me nulov√© hodnoty z datasetu.

> üéÉ Zaj√≠mavost: b√≠l√© d√Ωnƒõ nƒõkdy naz√Ωv√°me 'duchov√©' d√Ωnƒõ. Nejsou p≈ô√≠li≈° snadn√© na vy≈ôez√°v√°n√≠, tak≈æe nejsou tak popul√°rn√≠ jako oran≈æov√©, ale vypadaj√≠ zaj√≠mavƒõ! Mohli bychom tedy tak√© formulovat na≈°i ot√°zku jako: 'Duch' nebo 'Ne duch'. üëª

## O logistick√© regresi

Logistick√° regrese se li≈°√≠ od line√°rn√≠ regrese, kterou jste se nauƒçili d≈ô√≠ve, v nƒõkolika d≈Øle≈æit√Ωch ohledech.

[![ML pro zaƒç√°teƒçn√≠ky - Porozumƒõn√≠ logistick√© regresi pro klasifikaci strojov√©ho uƒçen√≠](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML pro zaƒç√°teƒçn√≠ky - Porozumƒõn√≠ logistick√© regresi pro klasifikaci strojov√©ho uƒçen√≠")

> üé• Kliknƒõte na obr√°zek v√Ω≈°e pro kr√°tk√Ω video p≈ôehled logistick√© regrese.

### Bin√°rn√≠ klasifikace

Logistick√° regrese nenab√≠z√≠ stejn√© funkce jako line√°rn√≠ regrese. Prvn√≠ z nich nab√≠z√≠ predikci bin√°rn√≠ kategorie ("b√≠l√° nebo ne b√≠l√°"), zat√≠mco druh√° je schopna predikovat kontinu√°ln√≠ hodnoty, nap≈ô√≠klad na z√°kladƒõ p≈Øvodu d√Ωnƒõ a ƒçasu skliznƒõ, _o kolik se zv√Ω≈°√≠ jej√≠ cena_.

![Model klasifikace d√Ωn√≠](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Dal≈°√≠ klasifikace

Existuj√≠ i jin√© typy logistick√© regrese, vƒçetnƒõ multinomi√°ln√≠ a ordin√°ln√≠:

- **Multinomi√°ln√≠**, kter√° zahrnuje v√≠ce ne≈æ jednu kategorii - "Oran≈æov√°, B√≠l√° a Pruhovan√°".
- **Ordin√°ln√≠**, kter√° zahrnuje uspo≈ô√°dan√© kategorie, u≈æiteƒçn√©, pokud bychom chtƒõli uspo≈ô√°dat na≈°e v√Ωsledky logicky, nap≈ô√≠klad na≈°e d√Ωnƒõ, kter√© jsou uspo≈ô√°d√°ny podle koneƒçn√©ho poƒçtu velikost√≠ (mini, sm, med, lg, xl, xxl).

![Multinomi√°ln√≠ vs ordin√°ln√≠ regrese](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Promƒõnn√© NEMUS√ç b√Ωt korelovan√©

Pamatujete si, jak line√°rn√≠ regrese fungovala l√©pe s v√≠ce korelovan√Ωmi promƒõnn√Ωmi? Logistick√° regrese je opakem - promƒõnn√© nemus√≠ b√Ωt v souladu. To funguje pro tato data, kter√° maj√≠ pomƒõrnƒõ slab√© korelace.

### Pot≈ôebujete hodnƒõ ƒçist√Ωch dat

Logistick√° regrese poskytne p≈ôesnƒõj≈°√≠ v√Ωsledky, pokud pou≈æijete v√≠ce dat; n√°≈° mal√Ω dataset nen√≠ pro tento √∫kol optim√°ln√≠, tak≈æe to mƒõjte na pamƒõti.

[![ML pro zaƒç√°teƒçn√≠ky - Anal√Ωza a p≈ô√≠prava dat pro logistickou regresi](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML pro zaƒç√°teƒçn√≠ky - Anal√Ωza a p≈ô√≠prava dat pro logistickou regresi")

> üé• Kliknƒõte na obr√°zek v√Ω≈°e pro kr√°tk√Ω video p≈ôehled p≈ô√≠pravy dat pro line√°rn√≠ regresi

‚úÖ Zamyslete se nad typy dat, kter√© by se dob≈ôe hodily pro logistickou regresi

## Cviƒçen√≠ - √∫prava dat

Nejprve data trochu vyƒçistƒõte, odstra≈àte nulov√© hodnoty a vyberte pouze nƒõkter√© sloupce:

1. P≈ôidejte n√°sleduj√≠c√≠ k√≥d:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    V≈ædy se m≈Ø≈æete pod√≠vat na sv≈Øj nov√Ω dataframe:

    ```python
    pumpkins.info
    ```

### Vizualizace - kategorick√Ω graf

Nyn√≠ jste naƒçetli [startovac√≠ notebook](../../../../2-Regression/4-Logistic/notebook.ipynb) s daty o d√Ωn√≠ch a vyƒçistili jej tak, aby obsahoval dataset s nƒõkolika promƒõnn√Ωmi, vƒçetnƒõ `Barva`. Vizualizujme dataframe v notebooku pomoc√≠ jin√© knihovny: [Seaborn](https://seaborn.pydata.org/index.html), kter√° je postavena na Matplotlib, kter√Ω jsme pou≈æili d≈ô√≠ve.

Seaborn nab√≠z√≠ zaj√≠mav√© zp≈Øsoby vizualizace va≈°ich dat. Nap≈ô√≠klad m≈Ø≈æete porovnat distribuce dat pro ka≈ædou `Variety` a `Color` v kategorick√©m grafu.

1. Vytvo≈ôte takov√Ω graf pomoc√≠ funkce `catplot`, pou≈æijte na≈°e data o d√Ωn√≠ch `pumpkins` a specifikujte barevn√© mapov√°n√≠ pro ka≈ædou kategorii d√Ωn√≠ (oran≈æov√° nebo b√≠l√°):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![M≈ô√≠≈æka vizualizovan√Ωch dat](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Pozorov√°n√≠m dat m≈Ø≈æete vidƒõt, jak se data o barvƒõ vztahuj√≠ k odr≈Ødƒõ.

    ‚úÖ Na z√°kladƒõ tohoto kategorick√©ho grafu, jak√© zaj√≠mav√© pr≈Øzkumy si dok√°≈æete p≈ôedstavit?

### P≈ôedzpracov√°n√≠ dat: k√≥dov√°n√≠ vlastnost√≠ a ≈°t√≠tk≈Ø
N√°≈° dataset o d√Ωn√≠ch obsahuje textov√© hodnoty pro v≈°echny sv√© sloupce. Pr√°ce s kategorick√Ωmi daty je intuitivn√≠ pro lidi, ale ne pro stroje. Algoritmy strojov√©ho uƒçen√≠ dob≈ôe pracuj√≠ s ƒç√≠sly. Proto je k√≥dov√°n√≠ velmi d≈Øle≈æit√Ωm krokem ve f√°zi p≈ôedzpracov√°n√≠ dat, proto≈æe n√°m umo≈æ≈àuje p≈ôev√©st kategorick√° data na ƒç√≠seln√° data, ani≈æ bychom ztratili jak√©koli informace. Dobr√© k√≥dov√°n√≠ vede k vytvo≈ôen√≠ dobr√©ho modelu.

Pro k√≥dov√°n√≠ vlastnost√≠ existuj√≠ dva hlavn√≠ typy kod√©r≈Ø:

1. Ordinal encoder: hod√≠ se dob≈ôe pro ordin√°ln√≠ promƒõnn√©, co≈æ jsou kategorick√© promƒõnn√©, kde jejich data n√°sleduj√≠ logick√© po≈ôad√≠, jako je sloupec `Item Size` v na≈°em datasetu. Vytv√°≈ô√≠ mapov√°n√≠ tak, ≈æe ka≈æd√° kategorie je reprezentov√°na ƒç√≠slem, kter√© odpov√≠d√° po≈ôad√≠ kategorie ve sloupci.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Categorical encoder: hod√≠ se dob≈ôe pro nomin√°ln√≠ promƒõnn√©, co≈æ jsou kategorick√© promƒõnn√©, kde jejich data nen√°sleduj√≠ logick√© po≈ôad√≠, jako v≈°echny vlastnosti kromƒõ `Item Size` v na≈°em datasetu. Jedn√° se o one-hot k√≥dov√°n√≠, co≈æ znamen√°, ≈æe ka≈æd√° kategorie je reprezentov√°na bin√°rn√≠m sloupcem: k√≥dovan√° promƒõnn√° je rovna 1, pokud d√Ωnƒõ pat≈ô√≠ do dan√© odr≈Ødy, a 0 jinak.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Pot√© se `ColumnTransformer` pou≈æije k kombinaci v√≠ce kod√©r≈Ø do jednoho kroku a jejich aplikaci na p≈ô√≠slu≈°n√© sloupce.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
Na druhou stranu, pro k√≥dov√°n√≠ ≈°t√≠tku pou≈æ√≠v√°me t≈ô√≠du `LabelEncoder` ze scikit-learn, co≈æ je u≈æiteƒçn√° t≈ô√≠da pro normalizaci ≈°t√≠tk≈Ø tak, aby obsahovaly pouze hodnoty mezi 0 a n_classes-1 (zde 0 a 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
Jakmile jsme zak√≥dovali vlastnosti a ≈°t√≠tek, m≈Ø≈æeme je slouƒçit do nov√©ho dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
‚úÖ Jak√© jsou v√Ωhody pou≈æit√≠ ordinal encoderu pro sloupec `Item Size`?

### Anal√Ωza vztah≈Ø mezi promƒõnn√Ωmi

Nyn√≠, kdy≈æ jsme p≈ôedzpracovali na≈°e data, m≈Ø≈æeme analyzovat vztahy mezi vlastnostmi a ≈°t√≠tkem, abychom z√≠skali p≈ôedstavu o tom, jak dob≈ôe bude model schopen predikovat ≈°t√≠tek na z√°kladƒõ vlastnost√≠.
Nejlep≈°√≠ zp≈Øsob, jak prov√©st tento typ anal√Ωzy, je vykreslen√≠ dat. Opƒõt pou≈æijeme funkci `catplot` ze Seaborn, abychom vizualizovali vztahy mezi `Item Size`, `Variety` a `Color` v kategorick√©m grafu. Pro lep≈°√≠ vykreslen√≠ dat pou≈æijeme zak√≥dovan√Ω sloupec `Item Size` a nezak√≥dovan√Ω sloupec `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![Kategorick√Ω graf vizualizovan√Ωch dat](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Pou≈æit√≠ swarm plotu

Proto≈æe `Color` je bin√°rn√≠ kategorie (B√≠l√° nebo Ne), pot≈ôebuje '[specializovan√Ω p≈ô√≠stup](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) k vizualizaci'. Existuj√≠ i jin√© zp≈Øsoby vizualizace vztahu t√©to kategorie s ostatn√≠mi promƒõnn√Ωmi.

M≈Ø≈æete vizualizovat promƒõnn√© vedle sebe pomoc√≠ graf≈Ø Seaborn.

1. Vyzkou≈°ejte 'swarm' plot pro zobrazen√≠ distribuce hodnot:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Swarm plot vizualizovan√Ωch dat](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Pozor**: v√Ω≈°e uveden√Ω k√≥d m≈Ø≈æe generovat varov√°n√≠, proto≈æe Seaborn nedok√°≈æe reprezentovat takov√© mno≈æstv√≠ datov√Ωch bod≈Ø ve swarm plotu. Mo≈æn√Ωm ≈ôe≈°en√≠m je zmen≈°en√≠ velikosti markeru pomoc√≠ parametru 'size'. Mƒõjte v≈°ak na pamƒõti, ≈æe to ovliv≈àuje ƒçitelnost grafu.

> **üßÆ Matematika**
>
> Logistick√° regrese se op√≠r√° o koncept 'maxim√°ln√≠ vƒõrohodnosti' pomoc√≠ [sigmoidn√≠ch funkc√≠](https://wikipedia.org/wiki/Sigmoid_function). 'Sigmoidn√≠ funkce' na grafu vypad√° jako tvar 'S'. Bere hodnotu a mapuje ji na nƒõco mezi 0 a 1. Jej√≠ k≈ôivka se tak√© naz√Ωv√° 'logistick√° k≈ôivka'. Jej√≠ vzorec vypad√° takto:
>
> ![logistick√° funkce](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> kde st≈ôed sigmoidn√≠ k≈ôivky se nach√°z√≠ na bodƒõ 0 osy x, L je maxim√°ln√≠ hodnota k≈ôivky a k je strmost k≈ôivky. Pokud je v√Ωsledek funkce vƒõt≈°√≠ ne≈æ 0,5, dan√Ω ≈°t√≠tek bude p≈ôi≈ôazen t≈ô√≠dƒõ '1' bin√°rn√≠ volby. Pokud ne, bude klasifikov√°n jako '0'.

## Vytvo≈ôte sv≈Øj model

Vytvo≈ôen√≠ modelu pro nalezen√≠ tƒõchto bin√°rn√≠ch klasifikac√≠ je p≈ôekvapivƒõ jednoduch√© ve Scikit-learn.

[![ML pro zaƒç√°teƒçn√≠ky - Logistick√° regrese pro klasifikaci dat](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML pro zaƒç√°teƒçn√≠ky - Logistick√° regrese pro klasifikaci dat")

> üé• Kliknƒõte na obr√°zek v√Ω≈°e pro kr√°tk√Ω video p≈ôehled vytvo≈ôen√≠ modelu line√°rn√≠ regrese

1. Vyberte promƒõnn√©, kter√© chcete pou≈æ√≠t ve sv√©m klasifikaƒçn√≠m modelu, a rozdƒõlte tr√©novac√≠ a testovac√≠ sady pomoc√≠ `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Nyn√≠ m≈Ø≈æete tr√©novat sv≈Øj model, zavolejte `fit()` s tr√©novac√≠mi daty a vytisknƒõte jeho v√Ωsledek:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Pod√≠vejte se na sk√≥re sv√©ho modelu. Nen√≠ ≈°patn√©, vzhledem k tomu, ≈æe m√°te pouze asi 1000 ≈ô√°dk≈Ø dat:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Lep≈°√≠ pochopen√≠ pomoc√≠ matice z√°mƒõn

Zat√≠mco m≈Ø≈æete z√≠skat zpr√°vu o sk√≥re [term√≠ny](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) vyti≈°tƒõn√≠m v√Ω≈°e uveden√Ωch polo≈æek, m≈Ø≈æete sv≈Øj model l√©pe pochopit pomoc√≠ [matice z√°mƒõn](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), kter√° n√°m pom≈Ø≈æe pochopit, jak model funguje.

> üéì '[Matice z√°mƒõn](https://wikipedia.org/wiki/Confusion_matrix)' (nebo 'matice chyb') je tabulka, kter√° vyjad≈ôuje skuteƒçn√© vs. fale≈°n√© pozitivn√≠ a negativn√≠ v√Ωsledky va≈°eho modelu, ƒç√≠m≈æ hodnot√≠ p≈ôesnost predikc√≠.

1. Pro pou≈æit√≠ matice z√°mƒõn zavolejte `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Pod√≠vejte se na matici z√°mƒõn sv√©ho modelu:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

Ve Scikit-learn matice z√°mƒõn: ≈ô√°dky (osa 0) jsou skuteƒçn√© ≈°t√≠tky a sloupce (osa 1) jsou predikovan√© ≈°t√≠tky.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Co se zde dƒõje? ≈òeknƒõme, ≈æe n√°≈° model je po≈æ√°d√°n, aby klasifikoval d√Ωnƒõ mezi dvƒõ bin√°rn√≠ kategorie, kategorii 'b√≠l√°' a kategorii 'ne b√≠l√°'.

- Pokud v√°≈° model predikuje d√Ωni jako ne b√≠lou a ve skuteƒçnosti pat≈ô√≠ do kategorie 'ne b√≠l√°', naz√Ωv√°me to prav√Ω negativn√≠ v√Ωsledek, zobrazen√Ω horn√≠m lev√Ωm ƒç√≠slem.
- Pokud v√°≈° model predikuje d√Ωni jako b√≠lou a ve skuteƒçnosti pat≈ô√≠ do kategorie 'ne b√≠l√°', naz√Ωv√°me to fale≈°n√Ω negativn√≠ v√Ωsledek, zobrazen√Ω doln√≠m lev√Ωm ƒç√≠slem.
- Pokud v√°≈° model predikuje d√Ωni jako ne b√≠lou a ve skuteƒçnosti pat≈ô√≠ do kategorie 'b√≠l√°', naz√Ωv√°me to fale≈°n√Ω pozitivn√≠ v√Ωsledek, zobrazen√Ω horn√≠m prav√Ωm ƒç√≠slem.
- Pokud v√°≈° model predikuje d√Ωni jako b√≠lou a ve skuteƒçnosti pat≈ô√≠ do kategorie 'b√≠l√°', naz√Ωv√°me to prav√Ω pozitivn√≠ v√Ωsledek, zobrazen√Ω doln√≠m prav√Ωm ƒç√≠slem.

Jak jste mo≈æn√° uhodli, je preferov√°no m√≠t vƒõt≈°√≠ poƒçet prav√Ωch pozitivn√≠ch a prav√Ωch negativn√≠ch v√Ωsledk≈Ø a ni≈æ≈°√≠ poƒçet fale≈°n√Ωch pozitivn√≠ch a fale≈°n√Ωch negativn√≠ch v√Ωsledk≈Ø, co≈æ znamen√°, ≈æe model funguje l√©pe.
Jak souvis√≠ matice z√°mƒõny s p≈ôesnost√≠ a √∫plnost√≠? Pamatujte, ≈æe v√Ω≈°e uveden√° zpr√°va o klasifikaci uk√°zala p≈ôesnost (0,85) a √∫plnost (0,67).

P≈ôesnost = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

√öplnost = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

‚úÖ Ot√°zka: Jak si model vedl podle matice z√°mƒõny? Odpovƒõƒè: Docela dob≈ôe; je zde znaƒçn√Ω poƒçet spr√°vnƒõ negativn√≠ch, ale tak√© nƒõkolik fale≈°nƒõ negativn√≠ch.

Pojƒème si znovu proj√≠t pojmy, kter√© jsme vidƒõli d≈ô√≠ve, s pomoc√≠ mapov√°n√≠ TP/TN a FP/FN v matici z√°mƒõny:

üéì P≈ôesnost: TP/(TP + FP) Pod√≠l relevantn√≠ch instanc√≠ mezi z√≠skan√Ωmi instancemi (nap≈ô. kter√© ≈°t√≠tky byly spr√°vnƒõ oznaƒçeny)

üéì √öplnost: TP/(TP + FN) Pod√≠l relevantn√≠ch instanc√≠, kter√© byly z√≠sk√°ny, a≈• u≈æ spr√°vnƒõ oznaƒçen√© nebo ne

üéì f1-sk√≥re: (2 * p≈ôesnost * √∫plnost)/(p≈ôesnost + √∫plnost) V√°≈æen√Ω pr≈Ømƒõr p≈ôesnosti a √∫plnosti, p≈ôiƒçem≈æ nejlep≈°√≠ je 1 a nejhor≈°√≠ 0

üéì Podpora: Poƒçet v√Ωskyt≈Ø ka≈æd√©ho z√≠skan√©ho ≈°t√≠tku

üéì P≈ôesnost: (TP + TN)/(TP + TN + FP + FN) Procento ≈°t√≠tk≈Ø spr√°vnƒõ p≈ôedpovƒõzen√Ωch pro vzorek.

üéì Makro pr≈Ømƒõr: V√Ωpoƒçet nev√°≈æen√©ho pr≈Ømƒõru metrik pro ka≈æd√Ω ≈°t√≠tek, bez ohledu na nerovnov√°hu ≈°t√≠tk≈Ø.

üéì V√°≈æen√Ω pr≈Ømƒõr: V√Ωpoƒçet pr≈Ømƒõru metrik pro ka≈æd√Ω ≈°t√≠tek, p≈ôiƒçem≈æ se bere v √∫vahu nerovnov√°ha ≈°t√≠tk≈Ø jejich v√°≈æen√≠m podle podpory (poƒçtu skuteƒçn√Ωch instanc√≠ pro ka≈æd√Ω ≈°t√≠tek).

‚úÖ Dok√°≈æete si p≈ôedstavit, kterou metriku byste mƒõli sledovat, pokud chcete, aby v√°≈° model sn√≠≈æil poƒçet fale≈°nƒõ negativn√≠ch?

## Vizualizace ROC k≈ôivky tohoto modelu

[![ML pro zaƒç√°teƒçn√≠ky - Anal√Ωza v√Ωkonu logistick√© regrese pomoc√≠ ROC k≈ôivek](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML pro zaƒç√°teƒçn√≠ky - Anal√Ωza v√Ωkonu logistick√© regrese pomoc√≠ ROC k≈ôivek")


> üé• Kliknƒõte na obr√°zek v√Ω≈°e pro kr√°tk√Ω video p≈ôehled ROC k≈ôivek

Pojƒème udƒõlat je≈°tƒõ jednu vizualizaci, abychom vidƒõli tzv. 'ROC' k≈ôivku:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Pomoc√≠ Matplotlibu vykreslete [Receiver Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) nebo ROC modelu. ROC k≈ôivky se ƒçasto pou≈æ√≠vaj√≠ k z√≠sk√°n√≠ pohledu na v√Ωstup klasifik√°toru z hlediska jeho spr√°vnƒõ vs. fale≈°nƒõ pozitivn√≠ch. "ROC k≈ôivky obvykle zobrazuj√≠ m√≠ru spr√°vnƒõ pozitivn√≠ch na ose Y a m√≠ru fale≈°nƒõ pozitivn√≠ch na ose X." Proto z√°le≈æ√≠ na strmosti k≈ôivky a prostoru mezi st≈ôedovou ƒçarou a k≈ôivkou: chcete k≈ôivku, kter√° rychle stoup√° a p≈ôech√°z√≠ p≈ôes ƒç√°ru. V na≈°em p≈ô√≠padƒõ jsou na zaƒç√°tku fale≈°nƒõ pozitivn√≠, a pot√© ƒç√°ra spr√°vnƒõ stoup√° a p≈ôech√°z√≠ p≈ôes ƒç√°ru:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Nakonec pou≈æijte Scikit-learn [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) k v√Ωpoƒçtu skuteƒçn√© 'plochy pod k≈ôivkou' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
V√Ωsledek je `0.9749908725812341`. Vzhledem k tomu, ≈æe AUC se pohybuje od 0 do 1, chcete vysok√© sk√≥re, proto≈æe model, kter√Ω je ve sv√Ωch p≈ôedpovƒõd√≠ch 100% spr√°vn√Ω, bude m√≠t AUC 1; v tomto p≈ô√≠padƒõ je model _docela dobr√Ω_. 

V budouc√≠ch lekc√≠ch o klasifikac√≠ch se nauƒç√≠te, jak iterovat a zlep≈°ovat sk√≥re sv√©ho modelu. Ale prozat√≠m gratulujeme! Dokonƒçili jste tyto lekce o regresi!

---
## üöÄV√Ωzva

Logistick√° regrese nab√≠z√≠ mnoho dal≈°√≠ch mo≈ænost√≠! Nejlep≈°√≠ zp≈Øsob, jak se uƒçit, je experimentovat. Najdƒõte datovou sadu, kter√° se hod√≠ k tomuto typu anal√Ωzy, a vytvo≈ôte s n√≠ model. Co jste se nauƒçili? tip: zkuste [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) pro zaj√≠mav√© datov√© sady.

## [Kv√≠z po p≈ôedn√°≈°ce](https://ff-quizzes.netlify.app/en/ml/)

## P≈ôehled & Samostudium

P≈ôeƒçtƒõte si prvn√≠ nƒõkolik str√°nek [tohoto ƒçl√°nku ze Stanfordu](https://web.stanford.edu/~jurafsky/slp3/5.pdf) o nƒõkter√Ωch praktick√Ωch vyu≈æit√≠ch logistick√© regrese. P≈ôem√Ω≈°lejte o √∫loh√°ch, kter√© jsou l√©pe vhodn√© pro jeden nebo druh√Ω typ regresn√≠ch √∫loh, kter√© jsme dosud studovali. Co by fungovalo nejl√©pe?

## √ökol 

[Znovu vyzkou≈°ejte tuto regresi](assignment.md)

---

**Prohl√°≈°en√≠**:  
Tento dokument byl p≈ôelo≈æen pomoc√≠ slu≈æby pro automatick√Ω p≈ôeklad [Co-op Translator](https://github.com/Azure/co-op-translator). Aƒçkoli se sna≈æ√≠me o p≈ôesnost, mƒõjte pros√≠m na pamƒõti, ≈æe automatick√© p≈ôeklady mohou obsahovat chyby nebo nep≈ôesnosti. P≈Øvodn√≠ dokument v jeho p≈Øvodn√≠m jazyce by mƒõl b√Ωt pova≈æov√°n za autoritativn√≠ zdroj. Pro d≈Øle≈æit√© informace doporuƒçujeme profesion√°ln√≠ lidsk√Ω p≈ôeklad. Neodpov√≠d√°me za ≈æ√°dn√° nedorozumƒõn√≠ nebo nespr√°vn√© interpretace vypl√Ωvaj√≠c√≠ z pou≈æit√≠ tohoto p≈ôekladu.