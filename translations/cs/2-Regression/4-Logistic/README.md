<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-04T23:27:47+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "cs"
}
-->
# LogistickÃ¡ regrese pro predikci kategoriÃ­

![Infografika: LogistickÃ¡ vs. lineÃ¡rnÃ­ regrese](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ml/)

> ### [Tato lekce je dostupnÃ¡ v R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Ãšvod

V tÃ©to poslednÃ­ lekci o regresi, jednÃ© ze zÃ¡kladnÃ­ch _klasickÃ½ch_ technik strojovÃ©ho uÄenÃ­, se podÃ­vÃ¡me na logistickou regresi. Tuto techniku byste pouÅ¾ili k odhalenÃ­ vzorcÅ¯ pro predikci binÃ¡rnÃ­ch kategoriÃ­. Je tato cukrovinka ÄokolÃ¡dovÃ¡ nebo ne? Je tato nemoc nakaÅ¾livÃ¡ nebo ne? Vybere si tento zÃ¡kaznÃ­k tento produkt nebo ne?

V tÃ©to lekci se nauÄÃ­te:

- Novou knihovnu pro vizualizaci dat
- Techniky logistickÃ© regrese

âœ… Prohlubte svÃ© znalosti prÃ¡ce s tÃ­mto typem regrese v tomto [modulu Learn](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## PÅ™edpoklady

Po prÃ¡ci s daty o dÃ½nÃ­ch jsme nynÃ­ dostateÄnÄ› obeznÃ¡meni s tÃ­m, Å¾e existuje jedna binÃ¡rnÃ­ kategorie, se kterou mÅ¯Å¾eme pracovat: `Barva`.

Postavme model logistickÃ© regrese, kterÃ½ bude predikovat, na zÃ¡kladÄ› nÄ›kterÃ½ch promÄ›nnÃ½ch, _jakou barvu bude mÃ­t danÃ¡ dÃ½nÄ›_ (oranÅ¾ovÃ¡ ğŸƒ nebo bÃ­lÃ¡ ğŸ‘»).

> ProÄ mluvÃ­me o binÃ¡rnÃ­ klasifikaci v lekci o regresi? Pouze z jazykovÃ©ho pohodlÃ­, protoÅ¾e logistickÃ¡ regrese je [ve skuteÄnosti klasifikaÄnÃ­ metoda](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), i kdyÅ¾ zaloÅ¾enÃ¡ na lineÃ¡rnÃ­ regresi. O dalÅ¡Ã­ch zpÅ¯sobech klasifikace dat se dozvÃ­te v dalÅ¡Ã­ skupinÄ› lekcÃ­.

## Definujte otÃ¡zku

Pro naÅ¡e ÃºÄely to vyjÃ¡dÅ™Ã­me jako binÃ¡rnÃ­: 'BÃ­lÃ¡' nebo 'Ne bÃ­lÃ¡'. V naÅ¡em datasetu je takÃ© kategorie 'pruhovanÃ¡', ale mÃ¡ mÃ¡lo zÃ¡znamÅ¯, takÅ¾e ji nebudeme pouÅ¾Ã­vat. StejnÄ› zmizÃ­, jakmile odstranÃ­me nulovÃ© hodnoty z datasetu.

> ğŸƒ ZajÃ­mavost: bÃ­lÃ© dÃ½nÄ› nÄ›kdy nazÃ½vÃ¡me 'duchovÃ©' dÃ½nÄ›. Nejsou pÅ™Ã­liÅ¡ snadnÃ© na vyÅ™ezÃ¡vÃ¡nÃ­, takÅ¾e nejsou tak populÃ¡rnÃ­ jako oranÅ¾ovÃ©, ale vypadajÃ­ zajÃ­mavÄ›! Mohli bychom tedy takÃ© formulovat naÅ¡i otÃ¡zku jako: 'Duch' nebo 'Ne duch'. ğŸ‘»

## O logistickÃ© regresi

LogistickÃ¡ regrese se liÅ¡Ã­ od lineÃ¡rnÃ­ regrese, kterou jste se nauÄili dÅ™Ã­ve, v nÄ›kolika dÅ¯leÅ¾itÃ½ch ohledech.

[![ML pro zaÄÃ¡teÄnÃ­ky - PorozumÄ›nÃ­ logistickÃ© regresi pro klasifikaci strojovÃ©ho uÄenÃ­](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML pro zaÄÃ¡teÄnÃ­ky - PorozumÄ›nÃ­ logistickÃ© regresi pro klasifikaci strojovÃ©ho uÄenÃ­")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled logistickÃ© regrese.

### BinÃ¡rnÃ­ klasifikace

LogistickÃ¡ regrese nenabÃ­zÃ­ stejnÃ© funkce jako lineÃ¡rnÃ­ regrese. PrvnÃ­ z nich nabÃ­zÃ­ predikci binÃ¡rnÃ­ kategorie ("bÃ­lÃ¡ nebo ne bÃ­lÃ¡"), zatÃ­mco druhÃ¡ je schopna predikovat kontinuÃ¡lnÃ­ hodnoty, napÅ™Ã­klad na zÃ¡kladÄ› pÅ¯vodu dÃ½nÄ› a Äasu skliznÄ›, _o kolik se zvÃ½Å¡Ã­ jejÃ­ cena_.

![Model klasifikace dÃ½nÃ­](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)

### DalÅ¡Ã­ klasifikace

ExistujÃ­ i jinÃ© typy logistickÃ© regrese, vÄetnÄ› multinomiÃ¡lnÃ­ a ordinÃ¡lnÃ­:

- **MultinomiÃ¡lnÃ­**, kterÃ¡ zahrnuje vÃ­ce neÅ¾ jednu kategorii - "OranÅ¾ovÃ¡, BÃ­lÃ¡ a PruhovanÃ¡".
- **OrdinÃ¡lnÃ­**, kterÃ¡ zahrnuje uspoÅ™Ã¡danÃ© kategorie, uÅ¾iteÄnÃ©, pokud bychom chtÄ›li uspoÅ™Ã¡dat naÅ¡e vÃ½sledky logicky, napÅ™Ã­klad naÅ¡e dÃ½nÄ›, kterÃ© jsou uspoÅ™Ã¡dÃ¡ny podle koneÄnÃ©ho poÄtu velikostÃ­ (mini, sm, med, lg, xl, xxl).

![MultinomiÃ¡lnÃ­ vs ordinÃ¡lnÃ­ regrese](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### PromÄ›nnÃ© NEMUSÃ bÃ½t korelovanÃ©

Pamatujete si, jak lineÃ¡rnÃ­ regrese fungovala lÃ©pe s vÃ­ce korelovanÃ½mi promÄ›nnÃ½mi? LogistickÃ¡ regrese je opakem - promÄ›nnÃ© nemusÃ­ bÃ½t v souladu. To funguje pro tato data, kterÃ¡ majÃ­ pomÄ›rnÄ› slabÃ© korelace.

### PotÅ™ebujete hodnÄ› ÄistÃ½ch dat

LogistickÃ¡ regrese poskytne pÅ™esnÄ›jÅ¡Ã­ vÃ½sledky, pokud pouÅ¾ijete vÃ­ce dat; nÃ¡Å¡ malÃ½ dataset nenÃ­ pro tento Ãºkol optimÃ¡lnÃ­, takÅ¾e to mÄ›jte na pamÄ›ti.

[![ML pro zaÄÃ¡teÄnÃ­ky - AnalÃ½za a pÅ™Ã­prava dat pro logistickou regresi](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML pro zaÄÃ¡teÄnÃ­ky - AnalÃ½za a pÅ™Ã­prava dat pro logistickou regresi")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled pÅ™Ã­pravy dat pro lineÃ¡rnÃ­ regresi

âœ… Zamyslete se nad typy dat, kterÃ© by se dobÅ™e hodily pro logistickou regresi

## CviÄenÃ­ - Ãºprava dat

Nejprve data trochu vyÄistÄ›te, odstraÅˆte nulovÃ© hodnoty a vyberte pouze nÄ›kterÃ© sloupce:

1. PÅ™idejte nÃ¡sledujÃ­cÃ­ kÃ³d:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    VÅ¾dy se mÅ¯Å¾ete podÃ­vat na svÅ¯j novÃ½ dataframe:

    ```python
    pumpkins.info
    ```

### Vizualizace - kategorickÃ½ graf

NynÃ­ jste naÄetli [startovacÃ­ notebook](../../../../2-Regression/4-Logistic/notebook.ipynb) s daty o dÃ½nÃ­ch a vyÄistili jej tak, aby obsahoval dataset s nÄ›kolika promÄ›nnÃ½mi, vÄetnÄ› `Barva`. Vizualizujme dataframe v notebooku pomocÃ­ jinÃ© knihovny: [Seaborn](https://seaborn.pydata.org/index.html), kterÃ¡ je postavena na Matplotlib, kterÃ½ jsme pouÅ¾ili dÅ™Ã­ve.

Seaborn nabÃ­zÃ­ zajÃ­mavÃ© zpÅ¯soby vizualizace vaÅ¡ich dat. NapÅ™Ã­klad mÅ¯Å¾ete porovnat distribuce dat pro kaÅ¾dou `Variety` a `Color` v kategorickÃ©m grafu.

1. VytvoÅ™te takovÃ½ graf pomocÃ­ funkce `catplot`, pouÅ¾ijte naÅ¡e data o dÃ½nÃ­ch `pumpkins` a specifikujte barevnÃ© mapovÃ¡nÃ­ pro kaÅ¾dou kategorii dÃ½nÃ­ (oranÅ¾ovÃ¡ nebo bÃ­lÃ¡):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![MÅ™Ã­Å¾ka vizualizovanÃ½ch dat](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    PozorovÃ¡nÃ­m dat mÅ¯Å¾ete vidÄ›t, jak se data o barvÄ› vztahujÃ­ k odrÅ¯dÄ›.

    âœ… Na zÃ¡kladÄ› tohoto kategorickÃ©ho grafu, jakÃ© zajÃ­mavÃ© prÅ¯zkumy si dokÃ¡Å¾ete pÅ™edstavit?

### PÅ™edzpracovÃ¡nÃ­ dat: kÃ³dovÃ¡nÃ­ vlastnostÃ­ a Å¡tÃ­tkÅ¯
NÃ¡Å¡ dataset o dÃ½nÃ­ch obsahuje textovÃ© hodnoty pro vÅ¡echny svÃ© sloupce. PrÃ¡ce s kategorickÃ½mi daty je intuitivnÃ­ pro lidi, ale ne pro stroje. Algoritmy strojovÃ©ho uÄenÃ­ dobÅ™e pracujÃ­ s ÄÃ­sly. Proto je kÃ³dovÃ¡nÃ­ velmi dÅ¯leÅ¾itÃ½m krokem ve fÃ¡zi pÅ™edzpracovÃ¡nÃ­ dat, protoÅ¾e nÃ¡m umoÅ¾Åˆuje pÅ™evÃ©st kategorickÃ¡ data na ÄÃ­selnÃ¡ data, aniÅ¾ bychom ztratili jakÃ©koli informace. DobrÃ© kÃ³dovÃ¡nÃ­ vede k vytvoÅ™enÃ­ dobrÃ©ho modelu.

Pro kÃ³dovÃ¡nÃ­ vlastnostÃ­ existujÃ­ dva hlavnÃ­ typy kodÃ©rÅ¯:

1. Ordinal encoder: hodÃ­ se dobÅ™e pro ordinÃ¡lnÃ­ promÄ›nnÃ©, coÅ¾ jsou kategorickÃ© promÄ›nnÃ©, kde jejich data nÃ¡sledujÃ­ logickÃ© poÅ™adÃ­, jako je sloupec `Item Size` v naÅ¡em datasetu. VytvÃ¡Å™Ã­ mapovÃ¡nÃ­ tak, Å¾e kaÅ¾dÃ¡ kategorie je reprezentovÃ¡na ÄÃ­slem, kterÃ© odpovÃ­dÃ¡ poÅ™adÃ­ kategorie ve sloupci.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Categorical encoder: hodÃ­ se dobÅ™e pro nominÃ¡lnÃ­ promÄ›nnÃ©, coÅ¾ jsou kategorickÃ© promÄ›nnÃ©, kde jejich data nenÃ¡sledujÃ­ logickÃ© poÅ™adÃ­, jako vÅ¡echny vlastnosti kromÄ› `Item Size` v naÅ¡em datasetu. JednÃ¡ se o one-hot kÃ³dovÃ¡nÃ­, coÅ¾ znamenÃ¡, Å¾e kaÅ¾dÃ¡ kategorie je reprezentovÃ¡na binÃ¡rnÃ­m sloupcem: kÃ³dovanÃ¡ promÄ›nnÃ¡ je rovna 1, pokud dÃ½nÄ› patÅ™Ã­ do danÃ© odrÅ¯dy, a 0 jinak.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
PotÃ© se `ColumnTransformer` pouÅ¾ije k kombinaci vÃ­ce kodÃ©rÅ¯ do jednoho kroku a jejich aplikaci na pÅ™Ã­sluÅ¡nÃ© sloupce.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
Na druhou stranu, pro kÃ³dovÃ¡nÃ­ Å¡tÃ­tku pouÅ¾Ã­vÃ¡me tÅ™Ã­du `LabelEncoder` ze scikit-learn, coÅ¾ je uÅ¾iteÄnÃ¡ tÅ™Ã­da pro normalizaci Å¡tÃ­tkÅ¯ tak, aby obsahovaly pouze hodnoty mezi 0 a n_classes-1 (zde 0 a 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
Jakmile jsme zakÃ³dovali vlastnosti a Å¡tÃ­tek, mÅ¯Å¾eme je slouÄit do novÃ©ho dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
âœ… JakÃ© jsou vÃ½hody pouÅ¾itÃ­ ordinal encoderu pro sloupec `Item Size`?

### AnalÃ½za vztahÅ¯ mezi promÄ›nnÃ½mi

NynÃ­, kdyÅ¾ jsme pÅ™edzpracovali naÅ¡e data, mÅ¯Å¾eme analyzovat vztahy mezi vlastnostmi a Å¡tÃ­tkem, abychom zÃ­skali pÅ™edstavu o tom, jak dobÅ™e bude model schopen predikovat Å¡tÃ­tek na zÃ¡kladÄ› vlastnostÃ­.
NejlepÅ¡Ã­ zpÅ¯sob, jak provÃ©st tento typ analÃ½zy, je vykreslenÃ­ dat. OpÄ›t pouÅ¾ijeme funkci `catplot` ze Seaborn, abychom vizualizovali vztahy mezi `Item Size`, `Variety` a `Color` v kategorickÃ©m grafu. Pro lepÅ¡Ã­ vykreslenÃ­ dat pouÅ¾ijeme zakÃ³dovanÃ½ sloupec `Item Size` a nezakÃ³dovanÃ½ sloupec `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![KategorickÃ½ graf vizualizovanÃ½ch dat](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### PouÅ¾itÃ­ swarm plotu

ProtoÅ¾e `Color` je binÃ¡rnÃ­ kategorie (BÃ­lÃ¡ nebo Ne), potÅ™ebuje '[specializovanÃ½ pÅ™Ã­stup](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) k vizualizaci'. ExistujÃ­ i jinÃ© zpÅ¯soby vizualizace vztahu tÃ©to kategorie s ostatnÃ­mi promÄ›nnÃ½mi.

MÅ¯Å¾ete vizualizovat promÄ›nnÃ© vedle sebe pomocÃ­ grafÅ¯ Seaborn.

1. VyzkouÅ¡ejte 'swarm' plot pro zobrazenÃ­ distribuce hodnot:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Swarm plot vizualizovanÃ½ch dat](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Pozor**: vÃ½Å¡e uvedenÃ½ kÃ³d mÅ¯Å¾e generovat varovÃ¡nÃ­, protoÅ¾e Seaborn nedokÃ¡Å¾e reprezentovat takovÃ© mnoÅ¾stvÃ­ datovÃ½ch bodÅ¯ ve swarm plotu. MoÅ¾nÃ½m Å™eÅ¡enÃ­m je zmenÅ¡enÃ­ velikosti markeru pomocÃ­ parametru 'size'. MÄ›jte vÅ¡ak na pamÄ›ti, Å¾e to ovlivÅˆuje Äitelnost grafu.

> **ğŸ§® Matematika**
>
> LogistickÃ¡ regrese se opÃ­rÃ¡ o koncept 'maximÃ¡lnÃ­ vÄ›rohodnosti' pomocÃ­ [sigmoidnÃ­ch funkcÃ­](https://wikipedia.org/wiki/Sigmoid_function). 'SigmoidnÃ­ funkce' na grafu vypadÃ¡ jako tvar 'S'. Bere hodnotu a mapuje ji na nÄ›co mezi 0 a 1. JejÃ­ kÅ™ivka se takÃ© nazÃ½vÃ¡ 'logistickÃ¡ kÅ™ivka'. JejÃ­ vzorec vypadÃ¡ takto:
>
> ![logistickÃ¡ funkce](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> kde stÅ™ed sigmoidnÃ­ kÅ™ivky se nachÃ¡zÃ­ na bodÄ› 0 osy x, L je maximÃ¡lnÃ­ hodnota kÅ™ivky a k je strmost kÅ™ivky. Pokud je vÃ½sledek funkce vÄ›tÅ¡Ã­ neÅ¾ 0,5, danÃ½ Å¡tÃ­tek bude pÅ™iÅ™azen tÅ™Ã­dÄ› '1' binÃ¡rnÃ­ volby. Pokud ne, bude klasifikovÃ¡n jako '0'.

## VytvoÅ™te svÅ¯j model

VytvoÅ™enÃ­ modelu pro nalezenÃ­ tÄ›chto binÃ¡rnÃ­ch klasifikacÃ­ je pÅ™ekvapivÄ› jednoduchÃ© ve Scikit-learn.

[![ML pro zaÄÃ¡teÄnÃ­ky - LogistickÃ¡ regrese pro klasifikaci dat](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML pro zaÄÃ¡teÄnÃ­ky - LogistickÃ¡ regrese pro klasifikaci dat")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled vytvoÅ™enÃ­ modelu lineÃ¡rnÃ­ regrese

1. Vyberte promÄ›nnÃ©, kterÃ© chcete pouÅ¾Ã­t ve svÃ©m klasifikaÄnÃ­m modelu, a rozdÄ›lte trÃ©novacÃ­ a testovacÃ­ sady pomocÃ­ `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. NynÃ­ mÅ¯Å¾ete trÃ©novat svÅ¯j model, zavolejte `fit()` s trÃ©novacÃ­mi daty a vytisknÄ›te jeho vÃ½sledek:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    PodÃ­vejte se na skÃ³re svÃ©ho modelu. NenÃ­ Å¡patnÃ©, vzhledem k tomu, Å¾e mÃ¡te pouze asi 1000 Å™Ã¡dkÅ¯ dat:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## LepÅ¡Ã­ pochopenÃ­ pomocÃ­ matice zÃ¡mÄ›n

ZatÃ­mco mÅ¯Å¾ete zÃ­skat zprÃ¡vu o skÃ³re [termÃ­ny](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) vytiÅ¡tÄ›nÃ­m vÃ½Å¡e uvedenÃ½ch poloÅ¾ek, mÅ¯Å¾ete svÅ¯j model lÃ©pe pochopit pomocÃ­ [matice zÃ¡mÄ›n](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix), kterÃ¡ nÃ¡m pomÅ¯Å¾e pochopit, jak model funguje.

> ğŸ“ '[Matice zÃ¡mÄ›n](https://wikipedia.org/wiki/Confusion_matrix)' (nebo 'matice chyb') je tabulka, kterÃ¡ vyjadÅ™uje skuteÄnÃ© vs. faleÅ¡nÃ© pozitivnÃ­ a negativnÃ­ vÃ½sledky vaÅ¡eho modelu, ÄÃ­mÅ¾ hodnotÃ­ pÅ™esnost predikcÃ­.

1. Pro pouÅ¾itÃ­ matice zÃ¡mÄ›n zavolejte `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    PodÃ­vejte se na matici zÃ¡mÄ›n svÃ©ho modelu:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

Ve Scikit-learn matice zÃ¡mÄ›n: Å™Ã¡dky (osa 0) jsou skuteÄnÃ© Å¡tÃ­tky a sloupce (osa 1) jsou predikovanÃ© Å¡tÃ­tky.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Co se zde dÄ›je? Å˜eknÄ›me, Å¾e nÃ¡Å¡ model je poÅ¾Ã¡dÃ¡n, aby klasifikoval dÃ½nÄ› mezi dvÄ› binÃ¡rnÃ­ kategorie, kategorii 'bÃ­lÃ¡' a kategorii 'ne bÃ­lÃ¡'.

- Pokud vÃ¡Å¡ model predikuje dÃ½ni jako ne bÃ­lou a ve skuteÄnosti patÅ™Ã­ do kategorie 'ne bÃ­lÃ¡', nazÃ½vÃ¡me to pravÃ½ negativnÃ­ vÃ½sledek, zobrazenÃ½ hornÃ­m levÃ½m ÄÃ­slem.
- Pokud vÃ¡Å¡ model predikuje dÃ½ni jako bÃ­lou a ve skuteÄnosti patÅ™Ã­ do kategorie 'ne bÃ­lÃ¡', nazÃ½vÃ¡me to faleÅ¡nÃ½ negativnÃ­ vÃ½sledek, zobrazenÃ½ dolnÃ­m levÃ½m ÄÃ­slem.
- Pokud vÃ¡Å¡ model predikuje dÃ½ni jako ne bÃ­lou a ve skuteÄnosti patÅ™Ã­ do kategorie 'bÃ­lÃ¡', nazÃ½vÃ¡me to faleÅ¡nÃ½ pozitivnÃ­ vÃ½sledek, zobrazenÃ½ hornÃ­m pravÃ½m ÄÃ­slem.
- Pokud vÃ¡Å¡ model predikuje dÃ½ni jako bÃ­lou a ve skuteÄnosti patÅ™Ã­ do kategorie 'bÃ­lÃ¡', nazÃ½vÃ¡me to pravÃ½ pozitivnÃ­ vÃ½sledek, zobrazenÃ½ dolnÃ­m pravÃ½m ÄÃ­slem.

Jak jste moÅ¾nÃ¡ uhodli, je preferovÃ¡no mÃ­t vÄ›tÅ¡Ã­ poÄet pravÃ½ch pozitivnÃ­ch a pravÃ½ch negativnÃ­ch vÃ½sledkÅ¯ a niÅ¾Å¡Ã­ poÄet faleÅ¡nÃ½ch pozitivnÃ­ch a faleÅ¡nÃ½ch negativnÃ­ch vÃ½sledkÅ¯, coÅ¾ znamenÃ¡, Å¾e model funguje lÃ©pe.
Jak souvisÃ­ matice zÃ¡mÄ›ny s pÅ™esnostÃ­ a ÃºplnostÃ­? Pamatujte, Å¾e vÃ½Å¡e uvedenÃ¡ zprÃ¡va o klasifikaci ukÃ¡zala pÅ™esnost (0,85) a Ãºplnost (0,67).

PÅ™esnost = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

Ãšplnost = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

âœ… OtÃ¡zka: Jak si model vedl podle matice zÃ¡mÄ›ny? OdpovÄ›Ä: Docela dobÅ™e; je zde znaÄnÃ½ poÄet sprÃ¡vnÄ› negativnÃ­ch, ale takÃ© nÄ›kolik faleÅ¡nÄ› negativnÃ­ch.

PojÄme si znovu projÃ­t pojmy, kterÃ© jsme vidÄ›li dÅ™Ã­ve, s pomocÃ­ mapovÃ¡nÃ­ TP/TN a FP/FN v matici zÃ¡mÄ›ny:

ğŸ“ PÅ™esnost: TP/(TP + FP) PodÃ­l relevantnÃ­ch instancÃ­ mezi zÃ­skanÃ½mi instancemi (napÅ™. kterÃ© Å¡tÃ­tky byly sprÃ¡vnÄ› oznaÄeny)

ğŸ“ Ãšplnost: TP/(TP + FN) PodÃ­l relevantnÃ­ch instancÃ­, kterÃ© byly zÃ­skÃ¡ny, aÅ¥ uÅ¾ sprÃ¡vnÄ› oznaÄenÃ© nebo ne

ğŸ“ f1-skÃ³re: (2 * pÅ™esnost * Ãºplnost)/(pÅ™esnost + Ãºplnost) VÃ¡Å¾enÃ½ prÅ¯mÄ›r pÅ™esnosti a Ãºplnosti, pÅ™iÄemÅ¾ nejlepÅ¡Ã­ je 1 a nejhorÅ¡Ã­ 0

ğŸ“ Podpora: PoÄet vÃ½skytÅ¯ kaÅ¾dÃ©ho zÃ­skanÃ©ho Å¡tÃ­tku

ğŸ“ PÅ™esnost: (TP + TN)/(TP + TN + FP + FN) Procento Å¡tÃ­tkÅ¯ sprÃ¡vnÄ› pÅ™edpovÄ›zenÃ½ch pro vzorek.

ğŸ“ Makro prÅ¯mÄ›r: VÃ½poÄet nevÃ¡Å¾enÃ©ho prÅ¯mÄ›ru metrik pro kaÅ¾dÃ½ Å¡tÃ­tek, bez ohledu na nerovnovÃ¡hu Å¡tÃ­tkÅ¯.

ğŸ“ VÃ¡Å¾enÃ½ prÅ¯mÄ›r: VÃ½poÄet prÅ¯mÄ›ru metrik pro kaÅ¾dÃ½ Å¡tÃ­tek, pÅ™iÄemÅ¾ se bere v Ãºvahu nerovnovÃ¡ha Å¡tÃ­tkÅ¯ jejich vÃ¡Å¾enÃ­m podle podpory (poÄtu skuteÄnÃ½ch instancÃ­ pro kaÅ¾dÃ½ Å¡tÃ­tek).

âœ… DokÃ¡Å¾ete si pÅ™edstavit, kterou metriku byste mÄ›li sledovat, pokud chcete, aby vÃ¡Å¡ model snÃ­Å¾il poÄet faleÅ¡nÄ› negativnÃ­ch?

## Vizualizace ROC kÅ™ivky tohoto modelu

[![ML pro zaÄÃ¡teÄnÃ­ky - AnalÃ½za vÃ½konu logistickÃ© regrese pomocÃ­ ROC kÅ™ivek](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML pro zaÄÃ¡teÄnÃ­ky - AnalÃ½za vÃ½konu logistickÃ© regrese pomocÃ­ ROC kÅ™ivek")


> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled ROC kÅ™ivek

PojÄme udÄ›lat jeÅ¡tÄ› jednu vizualizaci, abychom vidÄ›li tzv. 'ROC' kÅ™ivku:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

PomocÃ­ Matplotlibu vykreslete [Receiver Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) nebo ROC modelu. ROC kÅ™ivky se Äasto pouÅ¾Ã­vajÃ­ k zÃ­skÃ¡nÃ­ pohledu na vÃ½stup klasifikÃ¡toru z hlediska jeho sprÃ¡vnÄ› vs. faleÅ¡nÄ› pozitivnÃ­ch. "ROC kÅ™ivky obvykle zobrazujÃ­ mÃ­ru sprÃ¡vnÄ› pozitivnÃ­ch na ose Y a mÃ­ru faleÅ¡nÄ› pozitivnÃ­ch na ose X." Proto zÃ¡leÅ¾Ã­ na strmosti kÅ™ivky a prostoru mezi stÅ™edovou Äarou a kÅ™ivkou: chcete kÅ™ivku, kterÃ¡ rychle stoupÃ¡ a pÅ™echÃ¡zÃ­ pÅ™es ÄÃ¡ru. V naÅ¡em pÅ™Ã­padÄ› jsou na zaÄÃ¡tku faleÅ¡nÄ› pozitivnÃ­, a potÃ© ÄÃ¡ra sprÃ¡vnÄ› stoupÃ¡ a pÅ™echÃ¡zÃ­ pÅ™es ÄÃ¡ru:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Nakonec pouÅ¾ijte Scikit-learn [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) k vÃ½poÄtu skuteÄnÃ© 'plochy pod kÅ™ivkou' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
VÃ½sledek je `0.9749908725812341`. Vzhledem k tomu, Å¾e AUC se pohybuje od 0 do 1, chcete vysokÃ© skÃ³re, protoÅ¾e model, kterÃ½ je ve svÃ½ch pÅ™edpovÄ›dÃ­ch 100% sprÃ¡vnÃ½, bude mÃ­t AUC 1; v tomto pÅ™Ã­padÄ› je model _docela dobrÃ½_. 

V budoucÃ­ch lekcÃ­ch o klasifikacÃ­ch se nauÄÃ­te, jak iterovat a zlepÅ¡ovat skÃ³re svÃ©ho modelu. Ale prozatÃ­m gratulujeme! DokonÄili jste tyto lekce o regresi!

---
## ğŸš€VÃ½zva

LogistickÃ¡ regrese nabÃ­zÃ­ mnoho dalÅ¡Ã­ch moÅ¾nostÃ­! NejlepÅ¡Ã­ zpÅ¯sob, jak se uÄit, je experimentovat. NajdÄ›te datovou sadu, kterÃ¡ se hodÃ­ k tomuto typu analÃ½zy, a vytvoÅ™te s nÃ­ model. Co jste se nauÄili? tip: zkuste [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) pro zajÃ­mavÃ© datovÃ© sady.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ml/)

## PÅ™ehled & Samostudium

PÅ™eÄtÄ›te si prvnÃ­ nÄ›kolik strÃ¡nek [tohoto ÄlÃ¡nku ze Stanfordu](https://web.stanford.edu/~jurafsky/slp3/5.pdf) o nÄ›kterÃ½ch praktickÃ½ch vyuÅ¾itÃ­ch logistickÃ© regrese. PÅ™emÃ½Å¡lejte o ÃºlohÃ¡ch, kterÃ© jsou lÃ©pe vhodnÃ© pro jeden nebo druhÃ½ typ regresnÃ­ch Ãºloh, kterÃ© jsme dosud studovali. Co by fungovalo nejlÃ©pe?

## Ãškol 

[Znovu vyzkouÅ¡ejte tuto regresi](assignment.md)

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace doporuÄujeme profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.