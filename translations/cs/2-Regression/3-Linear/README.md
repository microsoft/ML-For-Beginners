<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-04T23:20:43+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "cs"
}
-->
# VytvoÅ™enÃ­ regresnÃ­ho modelu pomocÃ­ Scikit-learn: ÄtyÅ™i zpÅ¯soby regresÃ­

![Infografika lineÃ¡rnÃ­ vs. polynomiÃ¡lnÃ­ regrese](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografika od [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [KvÃ­z pÅ™ed lekcÃ­](https://ff-quizzes.netlify.app/en/ml/)

> ### [Tato lekce je dostupnÃ¡ v R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Ãšvod 

Doposud jste prozkoumali, co je regrese, na vzorovÃ½ch datech zÃ­skanÃ½ch z datasetu cen dÃ½nÃ­, kterÃ½ budeme pouÅ¾Ã­vat v celÃ© tÃ©to lekci. TakÃ© jste ji vizualizovali pomocÃ­ Matplotlibu.

NynÃ­ jste pÅ™ipraveni ponoÅ™it se hloubÄ›ji do regresÃ­ pro strojovÃ© uÄenÃ­. ZatÃ­mco vizualizace vÃ¡m umoÅ¾Åˆuje pochopit data, skuteÄnÃ¡ sÃ­la strojovÃ©ho uÄenÃ­ spoÄÃ­vÃ¡ v _trÃ©novÃ¡nÃ­ modelÅ¯_. Modely jsou trÃ©novÃ¡ny na historickÃ½ch datech, aby automaticky zachytily zÃ¡vislosti mezi daty, a umoÅ¾ÅˆujÃ­ vÃ¡m pÅ™edpovÃ­dat vÃ½sledky pro novÃ¡ data, kterÃ¡ model dosud nevidÄ›l.

V tÃ©to lekci se dozvÃ­te vÃ­ce o dvou typech regresÃ­: _zÃ¡kladnÃ­ lineÃ¡rnÃ­ regrese_ a _polynomiÃ¡lnÃ­ regrese_, spolu s nÄ›kterÃ½mi matematickÃ½mi zÃ¡klady tÄ›chto technik. Tyto modely nÃ¡m umoÅ¾nÃ­ pÅ™edpovÃ­dat ceny dÃ½nÃ­ na zÃ¡kladÄ› rÅ¯znÃ½ch vstupnÃ­ch dat.

[![ML pro zaÄÃ¡teÄnÃ­ky - PorozumÄ›nÃ­ lineÃ¡rnÃ­ regresi](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pro zaÄÃ¡teÄnÃ­ky - PorozumÄ›nÃ­ lineÃ¡rnÃ­ regresi")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled o lineÃ¡rnÃ­ regresi.

> V celÃ©m tomto kurzu pÅ™edpoklÃ¡dÃ¡me minimÃ¡lnÃ­ znalosti matematiky a snaÅ¾Ã­me se ji zpÅ™Ã­stupnit studentÅ¯m z jinÃ½ch oborÅ¯, takÅ¾e sledujte poznÃ¡mky, ğŸ§® vÃ½poÄty, diagramy a dalÅ¡Ã­ uÄebnÃ­ nÃ¡stroje, kterÃ© vÃ¡m pomohou s pochopenÃ­m.

### PÅ™edpoklady

NynÃ­ byste mÄ›li bÃ½t obeznÃ¡meni se strukturou dat o dÃ½nÃ­ch, kterÃ¡ zkoumÃ¡me. Najdete je pÅ™edem naÄtenÃ¡ a pÅ™edem vyÄiÅ¡tÄ›nÃ¡ v souboru _notebook.ipynb_ tÃ©to lekce. V souboru je cena dÃ½nÃ­ zobrazena za buÅ¡l v novÃ©m datovÃ©m rÃ¡mci. UjistÄ›te se, Å¾e mÅ¯Å¾ete tyto notebooky spustit v jÃ¡drech ve Visual Studio Code.

### PÅ™Ã­prava

PÅ™ipomeÅˆme si, Å¾e tato data naÄÃ­tÃ¡te, abyste si mohli klÃ¡st otÃ¡zky:

- Kdy je nejlepÅ¡Ã­ Äas na nÃ¡kup dÃ½nÃ­? 
- Jakou cenu mohu oÄekÃ¡vat za balenÃ­ mini dÃ½nÃ­?
- MÃ¡m je koupit v poloviÄnÃ­ch buÅ¡lovÃ½ch koÅ¡Ã­ch nebo v krabici o velikosti 1 1/9 buÅ¡lu?
PokraÄujme v prozkoumÃ¡vÃ¡nÃ­ tÄ›chto dat.

V pÅ™edchozÃ­ lekci jste vytvoÅ™ili datovÃ½ rÃ¡mec Pandas a naplnili jej ÄÃ¡stÃ­ pÅ¯vodnÃ­ho datasetu, standardizovali ceny podle buÅ¡lu. TÃ­mto zpÅ¯sobem jste vÅ¡ak byli schopni shromÃ¡Å¾dit pouze asi 400 datovÃ½ch bodÅ¯ a pouze pro podzimnÃ­ mÄ›sÃ­ce.

PodÃ­vejte se na data, kterÃ¡ jsme pÅ™edem naÄetli v doprovodnÃ©m notebooku tÃ©to lekce. Data jsou pÅ™edem naÄtena a poÄÃ¡teÄnÃ­ bodovÃ½ graf je vytvoÅ™en, aby ukÃ¡zal data podle mÄ›sÃ­cÅ¯. MoÅ¾nÃ¡ mÅ¯Å¾eme zÃ­skat trochu vÃ­ce detailÅ¯ o povaze dat jejich dalÅ¡Ã­m ÄiÅ¡tÄ›nÃ­m.

## LineÃ¡rnÃ­ regresnÃ­ pÅ™Ã­mka

Jak jste se nauÄili v lekci 1, cÃ­lem cviÄenÃ­ lineÃ¡rnÃ­ regrese je bÃ½t schopen vykreslit pÅ™Ã­mku, kterÃ¡:

- **Ukazuje vztahy mezi promÄ›nnÃ½mi**. Ukazuje vztah mezi promÄ›nnÃ½mi
- **DÄ›lÃ¡ pÅ™edpovÄ›di**. UmoÅ¾Åˆuje pÅ™esnÄ› pÅ™edpovÄ›dÄ›t, kde by novÃ½ datovÃ½ bod spadal ve vztahu k tÃ©to pÅ™Ã­mce. 
 
Je typickÃ© pro **regresi metodou nejmenÅ¡Ã­ch ÄtvercÅ¯**, Å¾e se kreslÃ­ tento typ pÅ™Ã­mky. TermÃ­n 'nejmenÅ¡Ã­ Ätverce' znamenÃ¡, Å¾e vÅ¡echny datovÃ© body obklopujÃ­cÃ­ regresnÃ­ pÅ™Ã­mku jsou umocnÄ›ny na druhou a potÃ© seÄteny. IdeÃ¡lnÄ› je tento koneÄnÃ½ souÄet co nejmenÅ¡Ã­, protoÅ¾e chceme nÃ­zkÃ½ poÄet chyb, tedy `nejmenÅ¡Ã­ Ätverce`. 

DÄ›lÃ¡me to proto, Å¾e chceme modelovat pÅ™Ã­mku, kterÃ¡ mÃ¡ nejmenÅ¡Ã­ kumulativnÃ­ vzdÃ¡lenost od vÅ¡ech naÅ¡ich datovÃ½ch bodÅ¯. TakÃ© umocÅˆujeme hodnoty na druhou pÅ™ed jejich seÄtenÃ­m, protoÅ¾e nÃ¡s zajÃ­mÃ¡ jejich velikost, nikoli smÄ›r.

> **ğŸ§® UkaÅ¾te mi matematiku** 
> 
> Tato pÅ™Ã­mka, nazÃ½vanÃ¡ _pÅ™Ã­mka nejlepÅ¡Ã­ho pÅ™izpÅ¯sobenÃ­_, mÅ¯Å¾e bÃ½t vyjÃ¡dÅ™ena [rovnicÃ­](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` je 'vysvÄ›tlujÃ­cÃ­ promÄ›nnÃ¡'. `Y` je 'zÃ¡vislÃ¡ promÄ›nnÃ¡'. Sklon pÅ™Ã­mky je `b` a `a` je prÅ¯seÄÃ­k s osou Y, coÅ¾ odkazuje na hodnotu `Y`, kdyÅ¾ `X = 0`. 
>
>![vÃ½poÄet sklonu](../../../../2-Regression/3-Linear/images/slope.png)
>
> Nejprve vypoÄÃ­tejte sklon `b`. Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> JinÃ½mi slovy, a odkazujÃ­c na pÅ¯vodnÃ­ otÃ¡zku o datech dÃ½nÃ­: "pÅ™edpovÄ›zte cenu dÃ½nÄ› za buÅ¡l podle mÄ›sÃ­ce", `X` by odkazovalo na cenu a `Y` by odkazovalo na mÄ›sÃ­c prodeje. 
>
>![dokonÄenÃ­ rovnice](../../../../2-Regression/3-Linear/images/calculation.png)
>
> VypoÄÃ­tejte hodnotu Y. Pokud platÃ­te kolem $4, musÃ­ bÃ½t duben! Infografika od [Jen Looper](https://twitter.com/jenlooper)
>
> Matematika, kterÃ¡ vypoÄÃ­tÃ¡vÃ¡ pÅ™Ã­mku, musÃ­ ukÃ¡zat sklon pÅ™Ã­mky, kterÃ½ takÃ© zÃ¡visÃ­ na prÅ¯seÄÃ­ku, tedy na tom, kde se `Y` nachÃ¡zÃ­, kdyÅ¾ `X = 0`.
>
> Metodu vÃ½poÄtu tÄ›chto hodnot mÅ¯Å¾ete pozorovat na webu [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html). NavÅ¡tivte takÃ© [tento kalkulÃ¡tor nejmenÅ¡Ã­ch ÄtvercÅ¯](https://www.mathsisfun.com/data/least-squares-calculator.html), abyste vidÄ›li, jak hodnoty ÄÃ­sel ovlivÅˆujÃ­ pÅ™Ã­mku.

## Korelace

JeÅ¡tÄ› jeden termÃ­n, kterÃ½ je tÅ™eba pochopit, je **koeficient korelace** mezi danÃ½mi promÄ›nnÃ½mi X a Y. PomocÃ­ bodovÃ©ho grafu mÅ¯Å¾ete rychle vizualizovat tento koeficient. Graf s datovÃ½mi body rozptÃ½lenÃ½mi v ÃºhlednÃ© pÅ™Ã­mce mÃ¡ vysokou korelaci, ale graf s datovÃ½mi body rozptÃ½lenÃ½mi vÅ¡ude mezi X a Y mÃ¡ nÃ­zkou korelaci.

DobrÃ½ model lineÃ¡rnÃ­ regrese bude takovÃ½, kterÃ½ mÃ¡ vysokÃ½ (blÃ­Å¾e k 1 neÅ¾ k 0) koeficient korelace pomocÃ­ metody nejmenÅ¡Ã­ch ÄtvercÅ¯ s regresnÃ­ pÅ™Ã­mkou.

âœ… SpusÅ¥te notebook doprovÃ¡zejÃ­cÃ­ tuto lekci a podÃ­vejte se na bodovÃ½ graf MÄ›sÃ­c vs. Cena. ZdÃ¡ se, Å¾e data spojujÃ­cÃ­ MÄ›sÃ­c s Cenou za prodej dÃ½nÃ­ majÃ­ podle vaÅ¡eho vizuÃ¡lnÃ­ho hodnocenÃ­ bodovÃ©ho grafu vysokou nebo nÃ­zkou korelaci? ZmÄ›nÃ­ se to, pokud pouÅ¾ijete jemnÄ›jÅ¡Ã­ mÄ›Å™Ã­tko mÃ­sto `MÄ›sÃ­c`, napÅ™. *den v roce* (tj. poÄet dnÃ­ od zaÄÃ¡tku roku)?

V nÃ­Å¾e uvedenÃ©m kÃ³du pÅ™edpoklÃ¡dÃ¡me, Å¾e jsme data vyÄistili a zÃ­skali datovÃ½ rÃ¡mec nazvanÃ½ `new_pumpkins`, podobnÃ½ nÃ¡sledujÃ­cÃ­mu:

ID | MÄ›sÃ­c | DenVRoce | Druh | MÄ›sto | BalenÃ­ | NÃ­zkÃ¡ cena | VysokÃ¡ cena | Cena
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> KÃ³d pro ÄiÅ¡tÄ›nÃ­ dat je dostupnÃ½ v [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Provedli jsme stejnÃ© kroky ÄiÅ¡tÄ›nÃ­ jako v pÅ™edchozÃ­ lekci a vypoÄÃ­tali sloupec `DenVRoce` pomocÃ­ nÃ¡sledujÃ­cÃ­ho vÃ½razu: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

NynÃ­, kdyÅ¾ mÃ¡te pochopenÃ­ matematiky za lineÃ¡rnÃ­ regresÃ­, vytvoÅ™me regresnÃ­ model, abychom zjistili, zda mÅ¯Å¾eme pÅ™edpovÄ›dÄ›t, kterÃ© balenÃ­ dÃ½nÃ­ bude mÃ­t nejlepÅ¡Ã­ ceny dÃ½nÃ­. NÄ›kdo, kdo kupuje dÃ½nÄ› pro svÃ¡teÄnÃ­ dÃ½Åˆovou zahradu, by mohl chtÃ­t tyto informace, aby mohl optimalizovat svÃ© nÃ¡kupy balenÃ­ dÃ½nÃ­ pro zahradu.

## HledÃ¡nÃ­ korelace

[![ML pro zaÄÃ¡teÄnÃ­ky - HledÃ¡nÃ­ korelace: KlÃ­Ä k lineÃ¡rnÃ­ regresi](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pro zaÄÃ¡teÄnÃ­ky - HledÃ¡nÃ­ korelace: KlÃ­Ä k lineÃ¡rnÃ­ regresi")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled o korelaci.

Z pÅ™edchozÃ­ lekce jste pravdÄ›podobnÄ› vidÄ›li, Å¾e prÅ¯mÄ›rnÃ¡ cena pro rÅ¯znÃ© mÄ›sÃ­ce vypadÃ¡ takto:

<img alt="PrÅ¯mÄ›rnÃ¡ cena podle mÄ›sÃ­ce" src="../2-Data/images/barchart.png" width="50%"/>

To naznaÄuje, Å¾e by mÄ›la existovat nÄ›jakÃ¡ korelace, a mÅ¯Å¾eme zkusit trÃ©novat model lineÃ¡rnÃ­ regrese, abychom pÅ™edpovÄ›dÄ›li vztah mezi `MÄ›sÃ­c` a `Cena`, nebo mezi `DenVRoce` a `Cena`. Zde je bodovÃ½ graf, kterÃ½ ukazuje druhÃ½ vztah:

<img alt="BodovÃ½ graf Cena vs. Den v roce" src="images/scatter-dayofyear.png" width="50%" /> 

PodÃ­vejme se, zda existuje korelace pomocÃ­ funkce `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

ZdÃ¡ se, Å¾e korelace je pomÄ›rnÄ› malÃ¡, -0.15 podle `MÄ›sÃ­c` a -0.17 podle `DenVRoce`, ale mohlo by existovat jinÃ© dÅ¯leÅ¾itÃ© spojenÃ­. ZdÃ¡ se, Å¾e existujÃ­ rÅ¯znÃ© shluky cen odpovÃ­dajÃ­cÃ­ rÅ¯znÃ½m druhÅ¯m dÃ½nÃ­. Abychom tuto hypotÃ©zu potvrdili, vykresleme kaÅ¾dou kategorii dÃ½nÃ­ pomocÃ­ jinÃ© barvy. PÅ™edÃ¡nÃ­m parametru `ax` funkci pro vykreslenÃ­ bodovÃ©ho grafu mÅ¯Å¾eme vykreslit vÅ¡echny body na stejnÃ½ graf:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="BodovÃ½ graf Cena vs. Den v roce" src="images/scatter-dayofyear-color.png" width="50%" /> 

NaÅ¡e zkoumÃ¡nÃ­ naznaÄuje, Å¾e druh mÃ¡ vÄ›tÅ¡Ã­ vliv na celkovou cenu neÅ¾ skuteÄnÃ© datum prodeje. MÅ¯Å¾eme to vidÄ›t na sloupcovÃ©m grafu:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="SloupcovÃ½ graf cena vs. druh" src="images/price-by-variety.png" width="50%" /> 

ZamÄ›Å™me se nynÃ­ pouze na jeden druh dÃ½nÃ­, 'pie type', a podÃ­vejme se, jakÃ½ vliv mÃ¡ datum na cenu:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="BodovÃ½ graf Cena vs. Den v roce" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Pokud nynÃ­ vypoÄÃ­tÃ¡me korelaci mezi `Cena` a `DenVRoce` pomocÃ­ funkce `corr`, dostaneme nÄ›co jako `-0.27` - coÅ¾ znamenÃ¡, Å¾e trÃ©novÃ¡nÃ­ prediktivnÃ­ho modelu mÃ¡ smysl.

> PÅ™ed trÃ©novÃ¡nÃ­m modelu lineÃ¡rnÃ­ regrese je dÅ¯leÅ¾itÃ© zajistit, Å¾e naÅ¡e data jsou ÄistÃ¡. LineÃ¡rnÃ­ regrese nefunguje dobÅ™e s chybÄ›jÃ­cÃ­mi hodnotami, proto mÃ¡ smysl zbavit se vÅ¡ech prÃ¡zdnÃ½ch bunÄ›k:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

DalÅ¡Ã­m pÅ™Ã­stupem by bylo vyplnit tyto prÃ¡zdnÃ© hodnoty prÅ¯mÄ›rnÃ½mi hodnotami z odpovÃ­dajÃ­cÃ­ho sloupce.

## JednoduchÃ¡ lineÃ¡rnÃ­ regrese

[![ML pro zaÄÃ¡teÄnÃ­ky - LineÃ¡rnÃ­ a polynomiÃ¡lnÃ­ regrese pomocÃ­ Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pro zaÄÃ¡teÄnÃ­ky - LineÃ¡rnÃ­ a polynomiÃ¡lnÃ­ regrese pomocÃ­ Scikit-learn")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ video pÅ™ehled o lineÃ¡rnÃ­ a polynomiÃ¡lnÃ­ regresi.

Pro trÃ©novÃ¡nÃ­ naÅ¡eho modelu lineÃ¡rnÃ­ regrese pouÅ¾ijeme knihovnu **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

ZaÄneme oddÄ›lenÃ­m vstupnÃ­ch hodnot (features) a oÄekÃ¡vanÃ©ho vÃ½stupu (label) do samostatnÃ½ch numpy polÃ­:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> VÅ¡imnÄ›te si, Å¾e jsme museli provÃ©st `reshape` na vstupnÃ­ch datech, aby je balÃ­Äek lineÃ¡rnÃ­ regrese sprÃ¡vnÄ› pochopil. LineÃ¡rnÃ­ regrese oÄekÃ¡vÃ¡ 2D pole jako vstup, kde kaÅ¾dÃ½ Å™Ã¡dek pole odpovÃ­dÃ¡ vektoru vstupnÃ­ch vlastnostÃ­. V naÅ¡em pÅ™Ã­padÄ›, protoÅ¾e mÃ¡me pouze jeden vstup, potÅ™ebujeme pole s tvarem NÃ—1, kde N je velikost datasetu.

PotÃ© musÃ­me data rozdÄ›lit na trÃ©novacÃ­ a testovacÃ­ dataset, abychom mohli po trÃ©novÃ¡nÃ­ ovÄ›Å™it nÃ¡Å¡ model:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Nakonec samotnÃ© trÃ©novÃ¡nÃ­ modelu lineÃ¡rnÃ­ regrese zabere pouze dva Å™Ã¡dky kÃ³du. Definujeme objekt `LinearRegression` a pÅ™izpÅ¯sobÃ­me ho naÅ¡im datÅ¯m pomocÃ­ metody `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Objekt `LinearRegression` po pÅ™izpÅ¯sobenÃ­ obsahuje vÅ¡echny koeficienty regrese, kterÃ© lze zÃ­skat pomocÃ­ vlastnosti `.coef_`. V naÅ¡em pÅ™Ã­padÄ› je pouze jeden koeficient, kterÃ½ by mÄ›l bÃ½t kolem `-0.017`. To znamenÃ¡, Å¾e ceny se zdajÃ­ s Äasem mÃ­rnÄ› klesat, ale ne pÅ™Ã­liÅ¡, asi o 2 centy za den. PrÅ¯seÄÃ­k regrese s osou Y mÅ¯Å¾eme takÃ© zÃ­skat pomocÃ­ `lin_reg.intercept_` - bude kolem `21` v naÅ¡em pÅ™Ã­padÄ›, coÅ¾ naznaÄuje cenu na zaÄÃ¡tku roku.

Abychom vidÄ›li, jak pÅ™esnÃ½ je nÃ¡Å¡ model, mÅ¯Å¾eme pÅ™edpovÄ›dÄ›t ceny na testovacÃ­m datasetu a potÃ© zmÄ›Å™it, jak blÃ­zko jsou naÅ¡e pÅ™edpovÄ›di oÄekÃ¡vanÃ½m hodnotÃ¡m. To lze provÃ©st pomocÃ­ metriky stÅ™ednÃ­ kvadratickÃ© chyby (MSE), coÅ¾ je prÅ¯mÄ›r vÅ¡ech kvadratickÃ½ch rozdÃ­lÅ¯ mezi oÄekÃ¡vanou a pÅ™edpovÄ›zenou hodnotou.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
NaÅ¡e chyba se zdÃ¡ bÃ½t kolem 2 bodÅ¯, coÅ¾ je ~17 %. Nic moc. DalÅ¡Ã­m ukazatelem kvality modelu je **koeficient determinace**, kterÃ½ lze zÃ­skat takto:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Pokud je hodnota 0, znamenÃ¡ to, Å¾e model nebere v Ãºvahu vstupnÃ­ data a funguje jako *nejhorÅ¡Ã­ lineÃ¡rnÃ­ prediktor*, coÅ¾ je jednoduÅ¡e prÅ¯mÄ›rnÃ¡ hodnota vÃ½sledku. Hodnota 1 znamenÃ¡, Å¾e mÅ¯Å¾eme dokonale pÅ™edpovÄ›dÄ›t vÅ¡echny oÄekÃ¡vanÃ© vÃ½stupy. V naÅ¡em pÅ™Ã­padÄ› je koeficient kolem 0,06, coÅ¾ je pomÄ›rnÄ› nÃ­zkÃ©.

MÅ¯Å¾eme takÃ© vykreslit testovacÃ­ data spolu s regresnÃ­ pÅ™Ã­mkou, abychom lÃ©pe vidÄ›li, jak regresnÃ­ analÃ½za v naÅ¡em pÅ™Ã­padÄ› funguje:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="LineÃ¡rnÃ­ regrese" src="images/linear-results.png" width="50%" />

## PolynomickÃ¡ regrese

DalÅ¡Ã­m typem lineÃ¡rnÃ­ regrese je polynomickÃ¡ regrese. ZatÃ­mco nÄ›kdy existuje lineÃ¡rnÃ­ vztah mezi promÄ›nnÃ½mi â€“ ÄÃ­m vÄ›tÅ¡Ã­ je objem dÃ½nÄ›, tÃ­m vyÅ¡Å¡Ã­ je cena â€“ nÄ›kdy tyto vztahy nelze vykreslit jako rovinu nebo pÅ™Ã­mku.

âœ… Zde jsou [nÄ›kterÃ© dalÅ¡Ã­ pÅ™Ã­klady](https://online.stat.psu.edu/stat501/lesson/9/9.8) dat, kterÃ¡ by mohla vyuÅ¾Ã­t polynomickou regresi.

PodÃ­vejte se znovu na vztah mezi datem a cenou. ZdÃ¡ se, Å¾e tento bodovÃ½ graf by mÄ›l bÃ½t nutnÄ› analyzovÃ¡n pÅ™Ã­mkou? Nemohou ceny kolÃ­sat? V tomto pÅ™Ã­padÄ› mÅ¯Å¾ete zkusit polynomickou regresi.

âœ… Polynomy jsou matematickÃ© vÃ½razy, kterÃ© mohou obsahovat jednu nebo vÃ­ce promÄ›nnÃ½ch a koeficientÅ¯.

PolynomickÃ¡ regrese vytvÃ¡Å™Ã­ zakÅ™ivenou ÄÃ¡ru, kterÃ¡ lÃ©pe odpovÃ­dÃ¡ nelineÃ¡rnÃ­m datÅ¯m. V naÅ¡em pÅ™Ã­padÄ›, pokud do vstupnÃ­ch dat zahrneme kvadratickou promÄ›nnou `DayOfYear`, mÄ›li bychom bÃ½t schopni pÅ™izpÅ¯sobit naÅ¡e data parabolickÃ© kÅ™ivce, kterÃ¡ bude mÃ­t minimum v urÄitÃ©m bodÄ› bÄ›hem roku.

Scikit-learn obsahuje uÅ¾iteÄnÃ© [API pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) pro kombinaci rÅ¯znÃ½ch krokÅ¯ zpracovÃ¡nÃ­ dat dohromady. **Pipeline** je Å™etÄ›zec **odhadovaÄÅ¯**. V naÅ¡em pÅ™Ã­padÄ› vytvoÅ™Ã­me pipeline, kterÃ¡ nejprve pÅ™idÃ¡ polynomickÃ© prvky do naÅ¡eho modelu a potÃ© provede trÃ©nink regrese:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

PouÅ¾itÃ­ `PolynomialFeatures(2)` znamenÃ¡, Å¾e zahrneme vÅ¡echny polynomy druhÃ©ho stupnÄ› ze vstupnÃ­ch dat. V naÅ¡em pÅ™Ã­padÄ› to bude jednoduÅ¡e `DayOfYear`<sup>2</sup>, ale pokud mÃ¡me dvÄ› vstupnÃ­ promÄ›nnÃ© X a Y, pÅ™idÃ¡ to X<sup>2</sup>, XY a Y<sup>2</sup>. MÅ¯Å¾eme takÃ© pouÅ¾Ã­t polynomy vyÅ¡Å¡Ã­ho stupnÄ›, pokud chceme.

Pipeline lze pouÅ¾Ã­t stejnÃ½m zpÅ¯sobem jako pÅ¯vodnÃ­ objekt `LinearRegression`, tj. mÅ¯Å¾eme pipeline `fit` a potÃ© pouÅ¾Ã­t `predict` k zÃ­skÃ¡nÃ­ vÃ½sledkÅ¯ predikce. Zde je graf zobrazujÃ­cÃ­ testovacÃ­ data a aproximaÄnÃ­ kÅ™ivku:

<img alt="PolynomickÃ¡ regrese" src="images/poly-results.png" width="50%" />

PouÅ¾itÃ­m polynomickÃ© regrese mÅ¯Å¾eme dosÃ¡hnout mÃ­rnÄ› niÅ¾Å¡Ã­ MSE a vyÅ¡Å¡Ã­ determinace, ale ne vÃ½raznÄ›. MusÃ­me vzÃ­t v Ãºvahu dalÅ¡Ã­ prvky!

> VidÃ­te, Å¾e minimÃ¡lnÃ­ ceny dÃ½nÃ­ jsou pozorovÃ¡ny nÄ›kde kolem Halloweenu. Jak to mÅ¯Å¾ete vysvÄ›tlit?

ğŸƒ Gratulujeme, prÃ¡vÄ› jste vytvoÅ™ili model, kterÃ½ mÅ¯Å¾e pomoci pÅ™edpovÄ›dÄ›t cenu dÃ½nÃ­ na kolÃ¡Äe. PravdÄ›podobnÄ› mÅ¯Å¾ete stejnÃ½ postup zopakovat pro vÅ¡echny typy dÃ½nÃ­, ale to by bylo zdlouhavÃ©. NauÄme se nynÃ­, jak vzÃ­t do Ãºvahy odrÅ¯du dÃ½nÃ­ v naÅ¡em modelu!

## KategorickÃ© prvky

V ideÃ¡lnÃ­m svÄ›tÄ› bychom chtÄ›li bÃ½t schopni pÅ™edpovÄ›dÄ›t ceny pro rÅ¯znÃ© odrÅ¯dy dÃ½nÃ­ pomocÃ­ stejnÃ©ho modelu. Sloupec `Variety` je vÅ¡ak ponÄ›kud odliÅ¡nÃ½ od sloupcÅ¯ jako `Month`, protoÅ¾e obsahuje nenumerickÃ© hodnoty. TakovÃ© sloupce se nazÃ½vajÃ­ **kategorickÃ©**.

[![ML pro zaÄÃ¡teÄnÃ­ky - Predikce kategorickÃ½ch prvkÅ¯ pomocÃ­ lineÃ¡rnÃ­ regrese](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML pro zaÄÃ¡teÄnÃ­ky - Predikce kategorickÃ½ch prvkÅ¯ pomocÃ­ lineÃ¡rnÃ­ regrese")

> ğŸ¥ KliknÄ›te na obrÃ¡zek vÃ½Å¡e pro krÃ¡tkÃ½ pÅ™ehled o pouÅ¾itÃ­ kategorickÃ½ch prvkÅ¯.

Zde mÅ¯Å¾ete vidÄ›t, jak prÅ¯mÄ›rnÃ¡ cena zÃ¡visÃ­ na odrÅ¯dÄ›:

<img alt="PrÅ¯mÄ›rnÃ¡ cena podle odrÅ¯dy" src="images/price-by-variety.png" width="50%" />

Abychom vzali odrÅ¯du v Ãºvahu, musÃ­me ji nejprve pÅ™evÃ©st na ÄÃ­selnou formu, nebo ji **zakÃ³dovat**. Existuje nÄ›kolik zpÅ¯sobÅ¯, jak to udÄ›lat:

* JednoduchÃ© **ÄÃ­selnÃ© kÃ³dovÃ¡nÃ­** vytvoÅ™Ã­ tabulku rÅ¯znÃ½ch odrÅ¯d a potÃ© nahradÃ­ nÃ¡zev odrÅ¯dy indexem v tÃ©to tabulce. To nenÃ­ nejlepÅ¡Ã­ nÃ¡pad pro lineÃ¡rnÃ­ regresi, protoÅ¾e lineÃ¡rnÃ­ regrese bere skuteÄnou ÄÃ­selnou hodnotu indexu a pÅ™idÃ¡vÃ¡ ji k vÃ½sledku, nÃ¡sobÃ­ ji nÄ›jakÃ½m koeficientem. V naÅ¡em pÅ™Ã­padÄ› je vztah mezi ÄÃ­slem indexu a cenou zjevnÄ› nelineÃ¡rnÃ­, i kdyÅ¾ zajistÃ­me, Å¾e indexy budou seÅ™azeny nÄ›jakÃ½m specifickÃ½m zpÅ¯sobem.
* **One-hot kÃ³dovÃ¡nÃ­** nahradÃ­ sloupec `Variety` ÄtyÅ™mi rÅ¯znÃ½mi sloupci, jeden pro kaÅ¾dou odrÅ¯du. KaÅ¾dÃ½ sloupec bude obsahovat `1`, pokud odpovÃ­dajÃ­cÃ­ Å™Ã¡dek patÅ™Ã­ danÃ© odrÅ¯dÄ›, a `0` jinak. To znamenÃ¡, Å¾e v lineÃ¡rnÃ­ regresi budou ÄtyÅ™i koeficienty, jeden pro kaÅ¾dou odrÅ¯du dÃ½nÃ­, odpovÄ›dnÃ© za "vÃ½chozÃ­ cenu" (nebo spÃ­Å¡e "dodateÄnou cenu") pro danou odrÅ¯du.

NÃ­Å¾e uvedenÃ½ kÃ³d ukazuje, jak mÅ¯Å¾eme provÃ©st one-hot kÃ³dovÃ¡nÃ­ odrÅ¯dy:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Abychom provedli trÃ©nink lineÃ¡rnÃ­ regrese s pouÅ¾itÃ­m one-hot kÃ³dovanÃ© odrÅ¯dy jako vstupu, staÄÃ­ sprÃ¡vnÄ› inicializovat data `X` a `y`:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Zbytek kÃ³du je stejnÃ½ jako ten, kterÃ½ jsme pouÅ¾ili vÃ½Å¡e pro trÃ©nink lineÃ¡rnÃ­ regrese. Pokud to vyzkouÅ¡Ã­te, uvidÃ­te, Å¾e stÅ™ednÃ­ kvadratickÃ¡ chyba je pÅ™ibliÅ¾nÄ› stejnÃ¡, ale zÃ­skÃ¡me mnohem vyÅ¡Å¡Ã­ koeficient determinace (~77 %). Pro jeÅ¡tÄ› pÅ™esnÄ›jÅ¡Ã­ predikce mÅ¯Å¾eme vzÃ­t v Ãºvahu vÃ­ce kategorickÃ½ch prvkÅ¯, stejnÄ› jako numerickÃ© prvky, jako `Month` nebo `DayOfYear`. Abychom zÃ­skali jedno velkÃ© pole prvkÅ¯, mÅ¯Å¾eme pouÅ¾Ã­t `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Zde takÃ© bereme v Ãºvahu `City` a typ balenÃ­ `Package`, coÅ¾ nÃ¡m dÃ¡vÃ¡ MSE 2,84 (10 %) a determinaci 0,94!

## SpojenÃ­ vÅ¡eho dohromady

Abychom vytvoÅ™ili nejlepÅ¡Ã­ model, mÅ¯Å¾eme pouÅ¾Ã­t kombinovanÃ¡ data (one-hot kÃ³dovanÃ© kategorickÃ© + numerickÃ©) z vÃ½Å¡e uvedenÃ©ho pÅ™Ã­kladu spolu s polynomickou regresÃ­. Zde je kompletnÃ­ kÃ³d pro vaÅ¡e pohodlÃ­:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

To by nÃ¡m mÄ›lo dÃ¡t nejlepÅ¡Ã­ koeficient determinace tÃ©mÄ›Å™ 97 % a MSE=2,23 (~8 % chybovost predikce).

| Model | MSE | Determinace |
|-------|-----|-------------|
| `DayOfYear` LineÃ¡rnÃ­ | 2,77 (17,2 %) | 0,07 |
| `DayOfYear` PolynomickÃ¡ | 2,73 (17,0 %) | 0,08 |
| `Variety` LineÃ¡rnÃ­ | 5,24 (19,7 %) | 0,77 |
| VÅ¡echny prvky LineÃ¡rnÃ­ | 2,84 (10,5 %) | 0,94 |
| VÅ¡echny prvky PolynomickÃ¡ | 2,23 (8,25 %) | 0,97 |

ğŸ† SkvÄ›lÃ¡ prÃ¡ce! VytvoÅ™ili jste ÄtyÅ™i regresnÃ­ modely v jednÃ© lekci a zlepÅ¡ili kvalitu modelu na 97 %. V poslednÃ­ ÄÃ¡sti o regresi se nauÄÃ­te o logistickÃ© regresi pro urÄenÃ­ kategoriÃ­.

---
## ğŸš€VÃ½zva

Otestujte nÄ›kolik rÅ¯znÃ½ch promÄ›nnÃ½ch v tomto notebooku a zjistÄ›te, jak korelace odpovÃ­dÃ¡ pÅ™esnosti modelu.

## [KvÃ­z po pÅ™ednÃ¡Å¡ce](https://ff-quizzes.netlify.app/en/ml/)

## PÅ™ehled & Samostudium

V tÃ©to lekci jsme se nauÄili o lineÃ¡rnÃ­ regresi. ExistujÃ­ dalÅ¡Ã­ dÅ¯leÅ¾itÃ© typy regrese. PÅ™eÄtÄ›te si o technikÃ¡ch Stepwise, Ridge, Lasso a Elasticnet. Dobrou moÅ¾nostÃ­ pro dalÅ¡Ã­ studium je [kurz statistickÃ©ho uÄenÃ­ Stanfordu](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Ãškol 

[Postavte model](assignment.md)

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by pro automatickÃ½ pÅ™eklad [Co-op Translator](https://github.com/Azure/co-op-translator). AÄkoli se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho pÅ¯vodnÃ­m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ© nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.