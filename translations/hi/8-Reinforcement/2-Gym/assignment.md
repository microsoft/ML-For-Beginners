<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-04T00:32:02+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "hi"
}
-->
# माउंटेन कार को प्रशिक्षित करें

[OpenAI Gym](http://gym.openai.com) को इस तरह से डिज़ाइन किया गया है कि सभी वातावरण एक ही API प्रदान करते हैं - जैसे कि वही विधियां `reset`, `step` और `render`, और **क्रिया स्थान** और **अवलोकन स्थान** की समान अमूर्तताएं। इसलिए, यह संभव होना चाहिए कि एक ही सुदृढीकरण शिक्षण एल्गोरिदम को न्यूनतम कोड परिवर्तनों के साथ विभिन्न वातावरणों में अनुकूलित किया जा सके।

## माउंटेन कार का वातावरण

[माउंटेन कार वातावरण](https://gym.openai.com/envs/MountainCar-v0/) में एक कार एक घाटी में फंसी हुई है:

इसका लक्ष्य घाटी से बाहर निकलना और झंडे को पकड़ना है, प्रत्येक चरण में निम्नलिखित कार्यों में से एक को करके:

| मान | अर्थ |
|---|---|
| 0 | बाईं ओर तेज करें |
| 1 | तेज न करें |
| 2 | दाईं ओर तेज करें |

हालांकि, इस समस्या की मुख्य चाल यह है कि कार का इंजन एक ही प्रयास में पहाड़ पर चढ़ने के लिए पर्याप्त शक्तिशाली नहीं है। इसलिए, सफल होने का एकमात्र तरीका यह है कि गति प्राप्त करने के लिए आगे-पीछे ड्राइव करें।

अवलोकन स्थान में केवल दो मान होते हैं:

| संख्या | अवलोकन  | न्यूनतम | अधिकतम |
|-----|--------------|-----|-----|
|  0  | कार की स्थिति | -1.2| 0.6 |
|  1  | कार की गति | -0.07 | 0.07 |

माउंटेन कार के लिए पुरस्कार प्रणाली थोड़ी जटिल है:

 * यदि एजेंट ने पहाड़ के ऊपर झंडे (स्थिति = 0.5) तक पहुंच बना ली है, तो 0 का पुरस्कार दिया जाता है।
 * यदि एजेंट की स्थिति 0.5 से कम है, तो -1 का पुरस्कार दिया जाता है।

एपिसोड समाप्त हो जाता है यदि कार की स्थिति 0.5 से अधिक हो जाती है, या एपिसोड की लंबाई 200 से अधिक हो जाती है।

## निर्देश

हमारे सुदृढीकरण शिक्षण एल्गोरिदम को माउंटेन कार समस्या को हल करने के लिए अनुकूलित करें। मौजूदा [notebook.ipynb](notebook.ipynb) कोड से शुरू करें, नए वातावरण को प्रतिस्थापित करें, स्थिति विवेचन कार्यों को बदलें, और मौजूदा एल्गोरिदम को न्यूनतम कोड संशोधनों के साथ प्रशिक्षित करने का प्रयास करें। हाइपरपैरामीटर समायोजित करके परिणाम को अनुकूलित करें।

> **नोट**: एल्गोरिदम को अभिसरण कराने के लिए हाइपरपैरामीटर समायोजन की आवश्यकता हो सकती है।

## मूल्यांकन मानदंड

| मानदंड | उत्कृष्ट | पर्याप्त | सुधार की आवश्यकता |
| -------- | --------- | -------- | ----------------- |
|          | Q-लर्निंग एल्गोरिदम को सफलतापूर्वक CartPole उदाहरण से न्यूनतम कोड संशोधनों के साथ अनुकूलित किया गया है, जो 200 चरणों के भीतर झंडे को पकड़ने की समस्या को हल करने में सक्षम है। | एक नया Q-लर्निंग एल्गोरिदम इंटरनेट से अपनाया गया है, लेकिन अच्छी तरह से प्रलेखित है; या मौजूदा एल्गोरिदम अपनाया गया है, लेकिन वांछित परिणाम प्राप्त नहीं करता है। | छात्र किसी भी एल्गोरिदम को सफलतापूर्वक अपनाने में सक्षम नहीं था, लेकिन समाधान की दिशा में महत्वपूर्ण कदम उठाए हैं (स्थिति विवेचन, Q-टेबल डेटा संरचना, आदि को लागू किया)। |

---

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।  