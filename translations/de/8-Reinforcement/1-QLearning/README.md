<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-04T22:04:40+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "de"
}
-->
# Einf√ºhrung in Reinforcement Learning und Q-Learning

![Zusammenfassung von Reinforcement Learning in einer Sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote von [Tomomi Imura](https://www.twitter.com/girlie_mac)

Reinforcement Learning umfasst drei wichtige Konzepte: den Agenten, einige Zust√§nde und eine Menge von Aktionen pro Zustand. Indem der Agent in einem bestimmten Zustand eine Aktion ausf√ºhrt, erh√§lt er eine Belohnung. Stellen Sie sich erneut das Computerspiel Super Mario vor. Sie sind Mario, befinden sich in einem Level und stehen am Rand einer Klippe. √úber Ihnen schwebt eine M√ºnze. Sie, als Mario, in einem Level, an einer bestimmten Position ... das ist Ihr Zustand. Wenn Sie einen Schritt nach rechts machen (eine Aktion), fallen Sie √ºber die Klippe, was Ihnen eine niedrige Punktzahl einbringt. Dr√ºcken Sie jedoch die Sprungtaste, k√∂nnen Sie eine M√ºnze einsammeln und bleiben am Leben. Das ist ein positives Ergebnis und sollte mit einer positiven Punktzahl belohnt werden.

Mit Reinforcement Learning und einem Simulator (dem Spiel) k√∂nnen Sie lernen, wie Sie das Spiel spielen, um die Belohnung zu maximieren, also am Leben zu bleiben und so viele Punkte wie m√∂glich zu sammeln.

[![Einf√ºhrung in Reinforcement Learning](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Klicken Sie auf das Bild oben, um Dmitry √ºber Reinforcement Learning sprechen zu h√∂ren.

## [Quiz vor der Lektion](https://ff-quizzes.netlify.app/en/ml/)

## Voraussetzungen und Einrichtung

In dieser Lektion werden wir mit etwas Python-Code experimentieren. Sie sollten in der Lage sein, den Jupyter-Notebook-Code aus dieser Lektion entweder auf Ihrem Computer oder in der Cloud auszuf√ºhren.

Sie k√∂nnen [das Notebook zur Lektion](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) √∂ffnen und die Lektion Schritt f√ºr Schritt durchgehen.

> **Hinweis:** Wenn Sie diesen Code aus der Cloud √∂ffnen, m√ºssen Sie auch die Datei [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) abrufen, die im Notebook-Code verwendet wird. F√ºgen Sie sie in dasselbe Verzeichnis wie das Notebook ein.

## Einf√ºhrung

In dieser Lektion erkunden wir die Welt von **[Peter und der Wolf](https://de.wikipedia.org/wiki/Peter_und_der_Wolf)**, inspiriert von einem musikalischen M√§rchen des russischen Komponisten [Sergei Prokofjew](https://de.wikipedia.org/wiki/Sergei_Prokofjew). Wir verwenden **Reinforcement Learning**, um Peter seine Umgebung erkunden zu lassen, leckere √Ñpfel zu sammeln und dem Wolf auszuweichen.

**Reinforcement Learning** (RL) ist eine Lerntechnik, die es uns erm√∂glicht, das optimale Verhalten eines **Agenten** in einer bestimmten **Umgebung** durch viele Experimente zu erlernen. Ein Agent in dieser Umgebung sollte ein **Ziel** haben, das durch eine **Belohnungsfunktion** definiert ist.

## Die Umgebung

Der Einfachheit halber nehmen wir an, dass Peters Welt ein quadratisches Spielfeld der Gr√∂√üe `Breite` x `H√∂he` ist, wie dieses:

![Peters Umgebung](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Jede Zelle auf diesem Spielfeld kann entweder sein:

* **Boden**, auf dem Peter und andere Wesen laufen k√∂nnen.
* **Wasser**, auf dem man offensichtlich nicht laufen kann.
* ein **Baum** oder **Gras**, ein Ort, an dem man sich ausruhen kann.
* ein **Apfel**, den Peter gerne finden w√ºrde, um sich zu ern√§hren.
* ein **Wolf**, der gef√§hrlich ist und gemieden werden sollte.

Es gibt ein separates Python-Modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), das den Code f√ºr die Arbeit mit dieser Umgebung enth√§lt. Da dieser Code nicht entscheidend f√ºr das Verst√§ndnis unserer Konzepte ist, importieren wir das Modul und verwenden es, um das Beispielbrett zu erstellen (Codeblock 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Dieser Code sollte ein Bild der Umgebung ausgeben, das dem oben gezeigten √§hnelt.

## Aktionen und Strategie

In unserem Beispiel besteht Peters Ziel darin, einen Apfel zu finden, w√§hrend er dem Wolf und anderen Hindernissen ausweicht. Dazu kann er sich im Grunde umherbewegen, bis er einen Apfel findet.

An jeder Position kann er zwischen den folgenden Aktionen w√§hlen: nach oben, nach unten, nach links und nach rechts.

Wir definieren diese Aktionen als ein W√∂rterbuch und ordnen sie Paaren von entsprechenden Koordinaten√§nderungen zu. Zum Beispiel w√ºrde die Bewegung nach rechts (`R`) einem Paar `(1,0)` entsprechen. (Codeblock 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Zusammengefasst sind die Strategie und das Ziel dieses Szenarios wie folgt:

- **Die Strategie** unseres Agenten (Peter) wird durch eine sogenannte **Policy** definiert. Eine Policy ist eine Funktion, die die Aktion in einem bestimmten Zustand zur√ºckgibt. In unserem Fall wird der Zustand des Problems durch das Spielfeld einschlie√ülich der aktuellen Position des Spielers dargestellt.

- **Das Ziel** des Reinforcement Learning ist es, schlie√ülich eine gute Policy zu erlernen, die es uns erm√∂glicht, das Problem effizient zu l√∂sen. Als Ausgangspunkt betrachten wir jedoch die einfachste Policy, die als **Random Walk** bezeichnet wird.

## Random Walk

Lassen Sie uns zun√§chst unser Problem l√∂sen, indem wir eine Random-Walk-Strategie implementieren. Beim Random Walk w√§hlen wir zuf√§llig die n√§chste Aktion aus den erlaubten Aktionen, bis wir den Apfel erreichen (Codeblock 3).

1. Implementieren Sie den Random Walk mit dem folgenden Code:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Der Aufruf von `walk` sollte die L√§nge des entsprechenden Pfads zur√ºckgeben, die von einem Lauf zum anderen variieren kann.

1. F√ºhren Sie das Walk-Experiment mehrmals durch (z. B. 100 Mal) und geben Sie die resultierenden Statistiken aus (Codeblock 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Beachten Sie, dass die durchschnittliche L√§nge eines Pfads etwa 30-40 Schritte betr√§gt, was ziemlich viel ist, wenn man bedenkt, dass die durchschnittliche Entfernung zum n√§chsten Apfel etwa 5-6 Schritte betr√§gt.

    Sie k√∂nnen auch sehen, wie sich Peter w√§hrend des Random Walks bewegt:

    ![Peters Random Walk](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Belohnungsfunktion

Um unsere Policy intelligenter zu machen, m√ºssen wir verstehen, welche Z√ºge "besser" sind als andere. Dazu m√ºssen wir unser Ziel definieren.

Das Ziel kann in Form einer **Belohnungsfunktion** definiert werden, die f√ºr jeden Zustand einen Punktwert zur√ºckgibt. Je h√∂her die Zahl, desto besser die Belohnung. (Codeblock 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Eine interessante Eigenschaft von Belohnungsfunktionen ist, dass in den meisten F√§llen *eine wesentliche Belohnung erst am Ende des Spiels gegeben wird*. Das bedeutet, dass unser Algorithmus irgendwie "gute" Schritte, die zu einer positiven Belohnung am Ende f√ºhren, speichern und deren Bedeutung erh√∂hen sollte. Ebenso sollten alle Z√ºge, die zu schlechten Ergebnissen f√ºhren, entmutigt werden.

## Q-Learning

Ein Algorithmus, den wir hier besprechen werden, hei√üt **Q-Learning**. In diesem Algorithmus wird die Policy durch eine Funktion (oder eine Datenstruktur) definiert, die als **Q-Tabelle** bezeichnet wird. Sie zeichnet die "G√ºte" jeder Aktion in einem bestimmten Zustand auf.

Die Q-Tabelle wird so genannt, weil es oft praktisch ist, sie als Tabelle oder mehrdimensionales Array darzustellen. Da unser Spielfeld die Dimensionen `Breite` x `H√∂he` hat, k√∂nnen wir die Q-Tabelle mit einem numpy-Array der Form `Breite` x `H√∂he` x `len(actions)` darstellen: (Codeblock 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Beachten Sie, dass wir alle Werte der Q-Tabelle mit einem gleichen Wert initialisieren, in unserem Fall 0,25. Dies entspricht der "Random-Walk"-Policy, da alle Z√ºge in jedem Zustand gleich gut sind. Wir k√∂nnen die Q-Tabelle an die `plot`-Funktion √ºbergeben, um die Tabelle auf dem Spielfeld zu visualisieren: `m.plot(Q)`.

![Peters Umgebung](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

In der Mitte jeder Zelle befindet sich ein "Pfeil", der die bevorzugte Bewegungsrichtung anzeigt. Da alle Richtungen gleich sind, wird ein Punkt angezeigt.

Nun m√ºssen wir die Simulation ausf√ºhren, unsere Umgebung erkunden und eine bessere Verteilung der Q-Tabelle-Werte erlernen, die es uns erm√∂glicht, den Weg zum Apfel viel schneller zu finden.

## Essenz des Q-Learning: Bellman-Gleichung

Sobald wir uns bewegen, hat jede Aktion eine entsprechende Belohnung, d. h. wir k√∂nnten theoretisch die n√§chste Aktion basierend auf der h√∂chsten unmittelbaren Belohnung ausw√§hlen. In den meisten Zust√§nden wird der Zug jedoch nicht unser Ziel, den Apfel zu erreichen, erf√ºllen, und daher k√∂nnen wir nicht sofort entscheiden, welche Richtung besser ist.

> Denken Sie daran, dass nicht das unmittelbare Ergebnis z√§hlt, sondern das Endergebnis, das wir am Ende der Simulation erhalten.

Um diese verz√∂gerte Belohnung zu ber√ºcksichtigen, m√ºssen wir die Prinzipien der **[dynamischen Programmierung](https://de.wikipedia.org/wiki/Dynamische_Programmierung)** verwenden, die es uns erm√∂glichen, unser Problem rekursiv zu betrachten.

Angenommen, wir befinden uns jetzt im Zustand *s* und m√∂chten zum n√§chsten Zustand *s'* wechseln. Indem wir dies tun, erhalten wir die unmittelbare Belohnung *r(s,a)*, die durch die Belohnungsfunktion definiert ist, plus eine zuk√ºnftige Belohnung. Wenn wir annehmen, dass unsere Q-Tabelle die "Attraktivit√§t" jeder Aktion korrekt widerspiegelt, dann w√§hlen wir im Zustand *s'* eine Aktion *a*, die dem maximalen Wert von *Q(s',a')* entspricht. Somit wird die bestm√∂gliche zuk√ºnftige Belohnung, die wir im Zustand *s* erhalten k√∂nnten, durch `max`

## √úberpr√ºfung der Richtlinie

Da die Q-Tabelle die "Attraktivit√§t" jeder Aktion in jedem Zustand auflistet, ist es recht einfach, sie zu nutzen, um eine effiziente Navigation in unserer Welt zu definieren. Im einfachsten Fall k√∂nnen wir die Aktion ausw√§hlen, die dem h√∂chsten Wert in der Q-Tabelle entspricht: (Codeblock 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Wenn Sie den obigen Code mehrmals ausprobieren, werden Sie m√∂glicherweise feststellen, dass er manchmal "h√§ngt" und Sie die STOP-Taste im Notebook dr√ºcken m√ºssen, um ihn zu unterbrechen. Dies passiert, weil es Situationen geben kann, in denen zwei Zust√§nde sich gegenseitig in Bezug auf den optimalen Q-Wert "zeigen", wodurch der Agent zwischen diesen Zust√§nden endlos hin- und herwechselt.

## üöÄHerausforderung

> **Aufgabe 1:** √Ñndern Sie die Funktion `walk`, um die maximale Pfadl√§nge auf eine bestimmte Anzahl von Schritten (z. B. 100) zu begrenzen, und beobachten Sie, wie der obige Code diesen Wert von Zeit zu Zeit zur√ºckgibt.

> **Aufgabe 2:** √Ñndern Sie die Funktion `walk` so, dass sie nicht an Orte zur√ºckkehrt, an denen sie zuvor bereits war. Dies verhindert, dass `walk` in einer Schleife h√§ngen bleibt. Allerdings kann der Agent dennoch in einer Position "gefangen" sein, aus der er nicht entkommen kann.

## Navigation

Eine bessere Navigationsstrategie w√§re die, die wir w√§hrend des Trainings verwendet haben, die Ausnutzung und Erkundung kombiniert. In dieser Strategie w√§hlen wir jede Aktion mit einer bestimmten Wahrscheinlichkeit aus, die proportional zu den Werten in der Q-Tabelle ist. Diese Strategie kann dazu f√ºhren, dass der Agent zu einer Position zur√ºckkehrt, die er bereits erkundet hat. Wie Sie jedoch aus dem untenstehenden Code sehen k√∂nnen, f√ºhrt sie zu einem sehr kurzen durchschnittlichen Pfad zur gew√ºnschten Position (denken Sie daran, dass `print_statistics` die Simulation 100 Mal ausf√ºhrt): (Codeblock 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Nach Ausf√ºhrung dieses Codes sollten Sie eine deutlich k√ºrzere durchschnittliche Pfadl√§nge als zuvor erhalten, im Bereich von 3-6.

## Untersuchung des Lernprozesses

Wie bereits erw√§hnt, ist der Lernprozess ein Gleichgewicht zwischen der Erkundung und der Nutzung des gewonnenen Wissens √ºber die Struktur des Problemraums. Wir haben gesehen, dass die Ergebnisse des Lernens (die F√§higkeit, einem Agenten zu helfen, einen kurzen Weg zum Ziel zu finden) sich verbessert haben. Es ist jedoch auch interessant zu beobachten, wie sich die durchschnittliche Pfadl√§nge w√§hrend des Lernprozesses verh√§lt:

Die Erkenntnisse lassen sich wie folgt zusammenfassen:

- **Durchschnittliche Pfadl√§nge nimmt zu**. Was wir hier sehen, ist, dass die durchschnittliche Pfadl√§nge zun√§chst zunimmt. Dies liegt wahrscheinlich daran, dass wir, wenn wir nichts √ºber die Umgebung wissen, dazu neigen, in schlechten Zust√§nden, wie Wasser oder bei einem Wolf, gefangen zu werden. Wenn wir mehr lernen und dieses Wissen nutzen, k√∂nnen wir die Umgebung l√§nger erkunden, wissen aber immer noch nicht genau, wo sich die √Ñpfel befinden.

- **Pfadl√§nge nimmt ab, je mehr wir lernen**. Sobald wir genug gelernt haben, wird es f√ºr den Agenten einfacher, das Ziel zu erreichen, und die Pfadl√§nge beginnt abzunehmen. Wir sind jedoch weiterhin offen f√ºr Erkundungen, sodass wir oft vom besten Weg abweichen und neue Optionen erkunden, was den Pfad l√§nger als optimal macht.

- **L√§nge nimmt abrupt zu**. Was wir auf diesem Diagramm ebenfalls beobachten, ist, dass die L√§nge an einem Punkt abrupt zunimmt. Dies zeigt die stochastische Natur des Prozesses und dass wir die Q-Tabellen-Koeffizienten durch √úberschreiben mit neuen Werten "verderben" k√∂nnen. Dies sollte idealerweise minimiert werden, indem die Lernrate verringert wird (zum Beispiel passen wir gegen Ende des Trainings die Q-Tabellen-Werte nur noch geringf√ºgig an).

Insgesamt ist es wichtig zu bedenken, dass der Erfolg und die Qualit√§t des Lernprozesses stark von Parametern wie Lernrate, Abnahme der Lernrate und Diskontierungsfaktor abh√§ngen. Diese werden oft als **Hyperparameter** bezeichnet, um sie von **Parametern** zu unterscheiden, die wir w√§hrend des Trainings optimieren (zum Beispiel Q-Tabellen-Koeffizienten). Der Prozess, die besten Werte f√ºr die Hyperparameter zu finden, wird als **Hyperparameter-Optimierung** bezeichnet und verdient ein eigenes Thema.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

## Aufgabe 
[Eine realistischere Welt](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe des KI-√úbersetzungsdienstes [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.