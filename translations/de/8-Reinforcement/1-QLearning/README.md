<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0ffe994d1cc881bdeb49226a064116e5",
  "translation_date": "2025-09-03T21:58:59+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "de"
}
-->
# Einf√ºhrung in Reinforcement Learning und Q-Learning

![Zusammenfassung von Reinforcement Learning in maschinellem Lernen in einer Sketchnote](../../../../translated_images/ml-reinforcement.94024374d63348dbb3571c343ca7ddabef72adac0b8086d47164b769ba3a8a1d.de.png)
> Sketchnote von [Tomomi Imura](https://www.twitter.com/girlie_mac)

Reinforcement Learning umfasst drei wichtige Konzepte: den Agenten, einige Zust√§nde und eine Menge von Aktionen pro Zustand. Indem der Agent in einem bestimmten Zustand eine Aktion ausf√ºhrt, erh√§lt er eine Belohnung. Stellen Sie sich erneut das Computerspiel Super Mario vor. Sie sind Mario, befinden sich in einem Level und stehen am Rand einer Klippe. √úber Ihnen ist eine M√ºnze. Sie, als Mario, in einem Level, an einer bestimmten Position ... das ist Ihr Zustand. Wenn Sie einen Schritt nach rechts machen (eine Aktion), fallen Sie √ºber die Klippe, was Ihnen eine niedrige Punktzahl einbringt. Wenn Sie jedoch die Sprungtaste dr√ºcken, erzielen Sie einen Punkt und bleiben am Leben. Das ist ein positives Ergebnis und sollte Ihnen eine positive Punktzahl einbringen.

Mit Hilfe von Reinforcement Learning und einem Simulator (dem Spiel) k√∂nnen Sie lernen, wie Sie das Spiel spielen, um die Belohnung zu maximieren, also am Leben zu bleiben und so viele Punkte wie m√∂glich zu sammeln.

[![Einf√ºhrung in Reinforcement Learning](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Klicken Sie auf das Bild oben, um Dmitry √ºber Reinforcement Learning sprechen zu h√∂ren.

## [Quiz vor der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/)

## Voraussetzungen und Einrichtung

In dieser Lektion werden wir mit etwas Code in Python experimentieren. Sie sollten in der Lage sein, den Jupyter Notebook-Code aus dieser Lektion entweder auf Ihrem Computer oder in der Cloud auszuf√ºhren.

Sie k√∂nnen [das Notebook der Lektion](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) √∂ffnen und diese Lektion durchgehen, um es zu erstellen.

> **Hinweis:** Wenn Sie diesen Code aus der Cloud √∂ffnen, m√ºssen Sie auch die Datei [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) abrufen, die im Notebook-Code verwendet wird. F√ºgen Sie sie in dasselbe Verzeichnis wie das Notebook ein.

## Einf√ºhrung

In dieser Lektion erkunden wir die Welt von **[Peter und der Wolf](https://de.wikipedia.org/wiki/Peter_und_der_Wolf)**, inspiriert von einem musikalischen M√§rchen des russischen Komponisten [Sergei Prokofjew](https://de.wikipedia.org/wiki/Sergei_Prokofjew). Wir verwenden **Reinforcement Learning**, um Peter seine Umgebung erkunden zu lassen, leckere √Ñpfel zu sammeln und dem Wolf auszuweichen.

**Reinforcement Learning** (RL) ist eine Lerntechnik, die es uns erm√∂glicht, ein optimales Verhalten eines **Agenten** in einer **Umgebung** durch viele Experimente zu erlernen. Ein Agent in dieser Umgebung sollte ein **Ziel** haben, das durch eine **Belohnungsfunktion** definiert ist.

## Die Umgebung

Der Einfachheit halber betrachten wir Peters Welt als ein quadratisches Spielfeld der Gr√∂√üe `width` x `height`, wie dieses:

![Peters Umgebung](../../../../translated_images/environment.40ba3cb66256c93fa7e92f6f7214e1d1f588aafa97d266c11d108c5c5d101b6c.de.png)

Jede Zelle auf diesem Spielfeld kann entweder sein:

* **Boden**, auf dem Peter und andere Kreaturen laufen k√∂nnen.
* **Wasser**, auf dem man offensichtlich nicht laufen kann.
* ein **Baum** oder **Gras**, ein Ort, an dem man sich ausruhen kann.
* ein **Apfel**, den Peter gerne finden w√ºrde, um sich zu ern√§hren.
* ein **Wolf**, der gef√§hrlich ist und dem man ausweichen sollte.

Es gibt ein separates Python-Modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), das den Code enth√§lt, um mit dieser Umgebung zu arbeiten. Da dieser Code nicht wichtig f√ºr das Verst√§ndnis unserer Konzepte ist, importieren wir das Modul und verwenden es, um das Beispielbrett zu erstellen (Codeblock 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Dieser Code sollte ein Bild der Umgebung √§hnlich dem oben gezeigten ausgeben.

## Aktionen und Strategie

In unserem Beispiel besteht Peters Ziel darin, einen Apfel zu finden, w√§hrend er dem Wolf und anderen Hindernissen ausweicht. Dazu kann er im Wesentlichen umherlaufen, bis er einen Apfel findet.

An jeder Position kann er zwischen den folgenden Aktionen w√§hlen: hoch, runter, links und rechts.

Wir definieren diese Aktionen als ein Dictionary und ordnen sie Paaren von entsprechenden Koordinaten√§nderungen zu. Zum Beispiel w√ºrde die Bewegung nach rechts (`R`) einem Paar `(1,0)` entsprechen. (Codeblock 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Zusammengefasst sind die Strategie und das Ziel dieses Szenarios wie folgt:

- **Die Strategie** unseres Agenten (Peter) wird durch eine sogenannte **Policy** definiert. Eine Policy ist eine Funktion, die die Aktion in einem gegebenen Zustand zur√ºckgibt. In unserem Fall wird der Zustand des Problems durch das Spielfeld einschlie√ülich der aktuellen Position des Spielers dargestellt.

- **Das Ziel** des Reinforcement Learnings ist es, letztendlich eine gute Policy zu erlernen, die es uns erm√∂glicht, das Problem effizient zu l√∂sen. Als Ausgangspunkt betrachten wir jedoch die einfachste Policy, die als **Random Walk** bezeichnet wird.

## Random Walk

Lassen Sie uns zun√§chst unser Problem l√∂sen, indem wir eine Random-Walk-Strategie implementieren. Beim Random Walk w√§hlen wir zuf√§llig die n√§chste Aktion aus den erlaubten Aktionen, bis wir den Apfel erreichen (Codeblock 3).

1. Implementieren Sie den Random Walk mit dem folgenden Code:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Der Aufruf von `walk` sollte die L√§nge des entsprechenden Pfades zur√ºckgeben, die von einem Lauf zum anderen variieren kann.

1. F√ºhren Sie das Walk-Experiment mehrmals durch (z. B. 100 Mal) und geben Sie die resultierenden Statistiken aus (Codeblock 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Beachten Sie, dass die durchschnittliche L√§nge eines Pfades etwa 30-40 Schritte betr√§gt, was ziemlich viel ist, wenn man bedenkt, dass die durchschnittliche Entfernung zum n√§chsten Apfel etwa 5-6 Schritte betr√§gt.

    Sie k√∂nnen auch sehen, wie Peters Bewegung w√§hrend des Random Walk aussieht:

    ![Peters Random Walk](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Belohnungsfunktion

Um unsere Policy intelligenter zu machen, m√ºssen wir verstehen, welche Z√ºge "besser" sind als andere. Dazu m√ºssen wir unser Ziel definieren.

Das Ziel kann in Form einer **Belohnungsfunktion** definiert werden, die f√ºr jeden Zustand einen Punktwert zur√ºckgibt. Je h√∂her die Zahl, desto besser die Belohnungsfunktion. (Codeblock 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Eine interessante Sache an Belohnungsfunktionen ist, dass in den meisten F√§llen *wir nur am Ende des Spiels eine wesentliche Belohnung erhalten*. Das bedeutet, dass unser Algorithmus irgendwie "gute" Schritte, die zu einer positiven Belohnung am Ende f√ºhren, speichern und deren Bedeutung erh√∂hen sollte. Ebenso sollten alle Z√ºge, die zu schlechten Ergebnissen f√ºhren, entmutigt werden.

## Q-Learning

Ein Algorithmus, den wir hier besprechen werden, hei√üt **Q-Learning**. In diesem Algorithmus wird die Policy durch eine Funktion (oder eine Datenstruktur) namens **Q-Tabelle** definiert. Sie zeichnet die "Qualit√§t" jeder Aktion in einem gegebenen Zustand auf.

Es wird als Q-Tabelle bezeichnet, weil es oft praktisch ist, sie als Tabelle oder mehrdimensionales Array darzustellen. Da unser Spielfeld die Dimensionen `width` x `height` hat, k√∂nnen wir die Q-Tabelle mit einem numpy-Array der Form `width` x `height` x `len(actions)` darstellen: (Codeblock 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Beachten Sie, dass wir alle Werte der Q-Tabelle mit einem gleichen Wert initialisieren, in unserem Fall - 0.25. Dies entspricht der "Random Walk"-Policy, da alle Z√ºge in jedem Zustand gleich gut sind. Wir k√∂nnen die Q-Tabelle an die `plot`-Funktion √ºbergeben, um die Tabelle auf dem Spielfeld zu visualisieren: `m.plot(Q)`.

![Peters Umgebung](../../../../translated_images/env_init.04e8f26d2d60089e128f21d22e5fef57d580e559f0d5937b06c689e5e7cdd438.de.png)

In der Mitte jeder Zelle befindet sich ein "Pfeil", der die bevorzugte Bewegungsrichtung anzeigt. Da alle Richtungen gleich sind, wird ein Punkt angezeigt.

Jetzt m√ºssen wir die Simulation ausf√ºhren, unsere Umgebung erkunden und eine bessere Verteilung der Q-Tabelle-Werte erlernen, die es uns erm√∂glicht, den Weg zum Apfel viel schneller zu finden.

## Essenz des Q-Learnings: Bellman-Gleichung

Sobald wir uns bewegen, hat jede Aktion eine entsprechende Belohnung, d. h. wir k√∂nnten theoretisch die n√§chste Aktion basierend auf der h√∂chsten unmittelbaren Belohnung ausw√§hlen. In den meisten Zust√§nden wird der Zug jedoch nicht unser Ziel erreichen, den Apfel zu finden, und wir k√∂nnen daher nicht sofort entscheiden, welche Richtung besser ist.

> Denken Sie daran, dass nicht das unmittelbare Ergebnis z√§hlt, sondern das Endergebnis, das wir am Ende der Simulation erhalten.

Um diese verz√∂gerte Belohnung zu ber√ºcksichtigen, m√ºssen wir die Prinzipien der **[dynamischen Programmierung](https://de.wikipedia.org/wiki/Dynamische_Programmierung)** verwenden, die es uns erm√∂glichen, unser Problem rekursiv zu betrachten.

Angenommen, wir befinden uns jetzt im Zustand *s* und m√∂chten zum n√§chsten Zustand *s'* wechseln. Indem wir dies tun, erhalten wir die unmittelbare Belohnung *r(s,a)*, definiert durch die Belohnungsfunktion, plus eine zuk√ºnftige Belohnung. Wenn wir annehmen, dass unsere Q-Tabelle die "Attraktivit√§t" jeder Aktion korrekt widerspiegelt, w√§hlen wir im Zustand *s'* eine Aktion *a*, die dem maximalen Wert von *Q(s',a')* entspricht. Somit wird die bestm√∂gliche zuk√ºnftige Belohnung, die wir im Zustand *s* erhalten k√∂nnten, durch `max`

## √úberpr√ºfung der Richtlinie

Da die Q-Tabelle die "Attraktivit√§t" jeder Aktion in jedem Zustand auflistet, ist es recht einfach, sie zu nutzen, um eine effiziente Navigation in unserer Welt zu definieren. Im einfachsten Fall k√∂nnen wir die Aktion ausw√§hlen, die dem h√∂chsten Wert in der Q-Tabelle entspricht: (Codeblock 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Wenn Sie den obigen Code mehrmals ausf√ºhren, werden Sie m√∂glicherweise feststellen, dass er manchmal "h√§ngt" und Sie die STOP-Taste im Notebook dr√ºcken m√ºssen, um ihn zu unterbrechen. Dies geschieht, weil es Situationen geben kann, in denen zwei Zust√§nde sich gegenseitig in Bezug auf den optimalen Q-Wert "zeigen", wodurch der Agent zwischen diesen Zust√§nden endlos hin- und herwechselt.

## üöÄHerausforderung

> **Aufgabe 1:** √Ñndern Sie die `walk`-Funktion so, dass die maximale Pfadl√§nge auf eine bestimmte Anzahl von Schritten (z. B. 100) begrenzt wird, und beobachten Sie, wie der obige Code diesen Wert von Zeit zu Zeit zur√ºckgibt.

> **Aufgabe 2:** √Ñndern Sie die `walk`-Funktion so, dass sie nicht an Orte zur√ºckkehrt, an denen sie bereits zuvor war. Dies verhindert, dass `walk` in einer Schleife h√§ngen bleibt, allerdings kann der Agent dennoch in einer Position "gefangen" sein, aus der er nicht entkommen kann.

## Navigation

Eine bessere Navigationsstrategie w√§re diejenige, die wir w√§hrend des Trainings verwendet haben, die eine Kombination aus Ausnutzung und Erkundung darstellt. Bei dieser Strategie w√§hlen wir jede Aktion mit einer bestimmten Wahrscheinlichkeit aus, die proportional zu den Werten in der Q-Tabelle ist. Diese Strategie kann zwar immer noch dazu f√ºhren, dass der Agent zu einer bereits erkundeten Position zur√ºckkehrt, aber wie Sie im folgenden Code sehen k√∂nnen, f√ºhrt sie zu einem sehr kurzen durchschnittlichen Pfad zur gew√ºnschten Position (denken Sie daran, dass `print_statistics` die Simulation 100 Mal ausf√ºhrt): (Codeblock 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Nach dem Ausf√ºhren dieses Codes sollten Sie eine deutlich k√ºrzere durchschnittliche Pfadl√§nge als zuvor erhalten, im Bereich von 3-6.

## Untersuchung des Lernprozesses

Wie bereits erw√§hnt, ist der Lernprozess ein Gleichgewicht zwischen der Erkundung und der Nutzung des gewonnenen Wissens √ºber die Struktur des Problemraums. Wir haben gesehen, dass die Ergebnisse des Lernens (die F√§higkeit, einem Agenten zu helfen, einen kurzen Weg zum Ziel zu finden) sich verbessert haben, aber es ist auch interessant zu beobachten, wie sich die durchschnittliche Pfadl√§nge w√§hrend des Lernprozesses verh√§lt:

Die Erkenntnisse lassen sich wie folgt zusammenfassen:

- **Durchschnittliche Pfadl√§nge nimmt zu**. Was wir hier sehen, ist, dass die durchschnittliche Pfadl√§nge zun√§chst zunimmt. Dies liegt wahrscheinlich daran, dass wir, wenn wir nichts √ºber die Umgebung wissen, dazu neigen, in schlechten Zust√§nden, wie Wasser oder bei einem Wolf, stecken zu bleiben. Wenn wir mehr lernen und dieses Wissen nutzen, k√∂nnen wir die Umgebung l√§nger erkunden, wissen aber immer noch nicht genau, wo sich die √Ñpfel befinden.

- **Pfadl√§nge nimmt ab, je mehr wir lernen**. Sobald wir genug gelernt haben, wird es f√ºr den Agenten einfacher, das Ziel zu erreichen, und die Pfadl√§nge beginnt abzunehmen. Allerdings sind wir weiterhin offen f√ºr Erkundungen, sodass wir oft vom besten Weg abweichen und neue Optionen ausprobieren, was den Pfad l√§nger als optimal macht.

- **L√§nge nimmt abrupt zu**. Was wir auf diesem Diagramm ebenfalls beobachten, ist, dass die L√§nge an einem Punkt abrupt zunimmt. Dies deutet auf die stochastische Natur des Prozesses hin und darauf, dass wir die Q-Tabellen-Koeffizienten durch das √úberschreiben mit neuen Werten "verschlechtern" k√∂nnen. Dies sollte idealerweise minimiert werden, indem die Lernrate verringert wird (zum Beispiel passen wir gegen Ende des Trainings die Q-Tabellen-Werte nur noch geringf√ºgig an).

Insgesamt ist es wichtig zu beachten, dass der Erfolg und die Qualit√§t des Lernprozesses stark von Parametern wie der Lernrate, dem Abfall der Lernrate und dem Diskontierungsfaktor abh√§ngen. Diese werden oft als **Hyperparameter** bezeichnet, um sie von **Parametern** zu unterscheiden, die wir w√§hrend des Trainings optimieren (zum Beispiel die Q-Tabellen-Koeffizienten). Der Prozess, die besten Werte f√ºr die Hyperparameter zu finden, wird als **Hyperparameter-Optimierung** bezeichnet und verdient ein eigenes Thema.

## [Quiz nach der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Aufgabe 
[Eine realistischere Welt](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.