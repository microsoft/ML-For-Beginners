<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-03T21:58:11+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "de"
}
-->
# Einf√ºhrung in das Reinforcement Learning

Reinforcement Learning, RL, wird als eines der grundlegenden Paradigmen des maschinellen Lernens angesehen, neben dem √ºberwachten und un√ºberwachten Lernen. RL dreht sich um Entscheidungen: die richtigen Entscheidungen treffen oder zumindest aus ihnen lernen.

Stellen Sie sich vor, Sie haben eine simulierte Umgebung wie den Aktienmarkt. Was passiert, wenn Sie eine bestimmte Regulierung einf√ºhren? Hat dies eine positive oder negative Wirkung? Wenn etwas Negatives passiert, m√ºssen Sie diese _negative Verst√§rkung_ nutzen, daraus lernen und den Kurs √§ndern. Wenn es ein positives Ergebnis ist, m√ºssen Sie darauf aufbauen und die _positive Verst√§rkung_ nutzen.

![Peter und der Wolf](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.de.png)

> Peter und seine Freunde m√ºssen dem hungrigen Wolf entkommen! Bild von [Jen Looper](https://twitter.com/jenlooper)

## Regionales Thema: Peter und der Wolf (Russland)

[Peter und der Wolf](https://de.wikipedia.org/wiki/Peter_und_der_Wolf) ist ein musikalisches M√§rchen, geschrieben von dem russischen Komponisten [Sergei Prokofjew](https://de.wikipedia.org/wiki/Sergei_Prokofjew). Es ist die Geschichte des jungen Pioniers Peter, der mutig sein Haus verl√§sst, um auf der Waldlichtung den Wolf zu jagen. In diesem Abschnitt werden wir maschinelle Lernalgorithmen trainieren, die Peter helfen:

- **Die Umgebung erkunden** und eine optimale Navigationskarte erstellen.
- **Lernen**, wie man ein Skateboard benutzt und darauf balanciert, um sich schneller fortzubewegen.

[![Peter und der Wolf](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Klicken Sie auf das Bild oben, um Peter und der Wolf von Prokofjew zu h√∂ren.

## Reinforcement Learning

In den vorherigen Abschnitten haben Sie zwei Beispiele f√ºr maschinelle Lernprobleme gesehen:

- **√úberwacht**, bei dem wir Datens√§tze haben, die m√∂gliche L√∂sungen f√ºr das Problem vorschlagen, das wir l√∂sen m√∂chten. [Klassifikation](../4-Classification/README.md) und [Regression](../2-Regression/README.md) sind Aufgaben des √ºberwachten Lernens.
- **Un√ºberwacht**, bei dem wir keine gelabelten Trainingsdaten haben. Das Hauptbeispiel f√ºr un√ºberwachtes Lernen ist [Clustering](../5-Clustering/README.md).

In diesem Abschnitt f√ºhren wir Sie in eine neue Art von Lernproblem ein, das keine gelabelten Trainingsdaten erfordert. Es gibt mehrere Arten solcher Probleme:

- **[Semi-√ºberwachtes Lernen](https://de.wikipedia.org/wiki/Semi-√ºberwachtes_Lernen)**, bei dem wir viele ungelabelte Daten haben, die verwendet werden k√∂nnen, um das Modell vorzutrainieren.
- **[Reinforcement Learning](https://de.wikipedia.org/wiki/Reinforcement_Learning)**, bei dem ein Agent lernt, sich zu verhalten, indem er Experimente in einer simulierten Umgebung durchf√ºhrt.

### Beispiel - Computerspiel

Angenommen, Sie m√∂chten einem Computer beibringen, ein Spiel zu spielen, wie Schach oder [Super Mario](https://de.wikipedia.org/wiki/Super_Mario). Damit der Computer ein Spiel spielen kann, muss er vorhersagen, welchen Zug er in jedem Spielzustand machen soll. Obwohl dies wie ein Klassifikationsproblem erscheinen mag, ist es keines ‚Äì denn wir haben keinen Datensatz mit Zust√§nden und entsprechenden Aktionen. W√§hrend wir einige Daten wie bestehende Schachpartien oder Aufzeichnungen von Spielern, die Super Mario spielen, haben k√∂nnten, ist es wahrscheinlich, dass diese Daten nicht ausreichend viele m√∂gliche Zust√§nde abdecken.

Anstatt nach bestehenden Spieldaten zu suchen, basiert **Reinforcement Learning** (RL) auf der Idee, dass der Computer *das Spiel viele Male spielt* und das Ergebnis beobachtet. Um Reinforcement Learning anzuwenden, ben√∂tigen wir daher zwei Dinge:

- **Eine Umgebung** und **einen Simulator**, die es uns erm√∂glichen, ein Spiel viele Male zu spielen. Dieser Simulator w√ºrde alle Spielregeln sowie m√∂gliche Zust√§nde und Aktionen definieren.

- **Eine Belohnungsfunktion**, die uns sagt, wie gut wir bei jedem Zug oder Spiel abgeschnitten haben.

Der Hauptunterschied zwischen anderen Arten des maschinellen Lernens und RL besteht darin, dass wir bei RL normalerweise nicht wissen, ob wir gewinnen oder verlieren, bis wir das Spiel beendet haben. Daher k√∂nnen wir nicht sagen, ob ein bestimmter Zug allein gut oder schlecht ist ‚Äì wir erhalten die Belohnung erst am Ende des Spiels. Unser Ziel ist es, Algorithmen zu entwickeln, die es uns erm√∂glichen, ein Modell unter unsicheren Bedingungen zu trainieren. Wir werden einen RL-Algorithmus namens **Q-Learning** kennenlernen.

## Lektionen

1. [Einf√ºhrung in Reinforcement Learning und Q-Learning](1-QLearning/README.md)
2. [Verwendung einer Gym-Simulationsumgebung](2-Gym/README.md)

## Credits

"Einf√ºhrung in Reinforcement Learning" wurde mit ‚ô•Ô∏è geschrieben von [Dmitry Soshnikov](http://soshnikov.com)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.