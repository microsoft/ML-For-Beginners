<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8f819813b2ca08ec7b9f60a2c9336045",
  "translation_date": "2025-09-03T21:50:06+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "de"
}
-->
# Entwicklung von Machine-Learning-L√∂sungen mit verantwortungsbewusster KI

![Zusammenfassung der verantwortungsbewussten KI im Machine Learning in einer Sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.de.png)
> Sketchnote von [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz vor der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Einf√ºhrung

In diesem Lehrplan werden Sie beginnen zu entdecken, wie Machine Learning unser t√§gliches Leben beeinflussen kann und bereits beeinflusst. Schon jetzt sind Systeme und Modelle in allt√§gliche Entscheidungsprozesse eingebunden, wie etwa bei medizinischen Diagnosen, Kreditgenehmigungen oder der Betrugserkennung. Daher ist es wichtig, dass diese Modelle zuverl√§ssig arbeiten, um vertrauensw√ºrdige Ergebnisse zu liefern. Wie jede Softwareanwendung k√∂nnen auch KI-Systeme Erwartungen nicht erf√ºllen oder unerw√ºnschte Ergebnisse liefern. Deshalb ist es entscheidend, das Verhalten eines KI-Modells verstehen und erkl√§ren zu k√∂nnen.

Stellen Sie sich vor, was passieren kann, wenn die Daten, die Sie zur Erstellung dieser Modelle verwenden, bestimmte demografische Gruppen wie Rasse, Geschlecht, politische Ansichten oder Religion nicht ber√ºcksichtigen oder diese unverh√§ltnism√§√üig repr√§sentieren. Was passiert, wenn die Ergebnisse des Modells so interpretiert werden, dass sie eine bestimmte demografische Gruppe bevorzugen? Welche Konsequenzen hat das f√ºr die Anwendung? Und was passiert, wenn das Modell ein sch√§dliches Ergebnis liefert? Wer ist f√ºr das Verhalten des KI-Systems verantwortlich? Dies sind einige der Fragen, die wir in diesem Lehrplan untersuchen werden.

In dieser Lektion werden Sie:

- Ihr Bewusstsein f√ºr die Bedeutung von Fairness im Machine Learning und die damit verbundenen Sch√§den sch√§rfen.
- Sich mit der Praxis vertraut machen, Ausrei√üer und ungew√∂hnliche Szenarien zu untersuchen, um Zuverl√§ssigkeit und Sicherheit zu gew√§hrleisten.
- Ein Verst√§ndnis daf√ºr gewinnen, wie wichtig es ist, alle Menschen durch die Gestaltung inklusiver Systeme zu st√§rken.
- Erkunden, wie entscheidend es ist, die Privatsph√§re und Sicherheit von Daten und Menschen zu sch√ºtzen.
- Die Bedeutung eines transparenten Ansatzes erkennen, um das Verhalten von KI-Modellen zu erkl√§ren.
- Sich bewusst machen, wie essenziell Verantwortlichkeit ist, um Vertrauen in KI-Systeme aufzubauen.

## Voraussetzungen

Als Voraussetzung sollten Sie den "Responsible AI Principles"-Lernpfad absolvieren und das folgende Video zum Thema ansehen:

Erfahren Sie mehr √ºber verantwortungsbewusste KI, indem Sie diesem [Lernpfad](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott) folgen.

[![Microsofts Ansatz f√ºr verantwortungsbewusste KI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsofts Ansatz f√ºr verantwortungsbewusste KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Microsofts Ansatz f√ºr verantwortungsbewusste KI

## Fairness

KI-Systeme sollten alle Menschen fair behandeln und vermeiden, √§hnliche Gruppen unterschiedlich zu beeinflussen. Beispielsweise sollten KI-Systeme bei medizinischen Behandlungen, Kreditantr√§gen oder Besch√§ftigungsentscheidungen die gleichen Empfehlungen f√ºr alle mit √§hnlichen Symptomen, finanziellen Umst√§nden oder beruflichen Qualifikationen geben. Jeder von uns tr√§gt als Mensch ererbte Vorurteile mit sich, die unsere Entscheidungen und Handlungen beeinflussen. Diese Vorurteile k√∂nnen sich in den Daten widerspiegeln, die wir zur Schulung von KI-Systemen verwenden. Solche Manipulationen k√∂nnen manchmal unbeabsichtigt geschehen. Es ist oft schwierig, bewusst zu erkennen, wann man Vorurteile in Daten einf√ºhrt.

**‚ÄûUnfairness‚Äú** umfasst negative Auswirkungen oder ‚ÄûSch√§den‚Äú f√ºr eine Gruppe von Menschen, wie etwa solche, die durch Rasse, Geschlecht, Alter oder Behinderungsstatus definiert sind. Die Hauptsch√§den im Zusammenhang mit Fairness k√∂nnen wie folgt klassifiziert werden:

- **Zuweisung**, wenn beispielsweise ein Geschlecht oder eine Ethnie gegen√ºber einer anderen bevorzugt wird.
- **Qualit√§t des Dienstes**. Wenn die Daten f√ºr ein spezifisches Szenario trainiert werden, die Realit√§t jedoch viel komplexer ist, f√ºhrt dies zu einem schlecht funktionierenden Dienst. Zum Beispiel ein Seifenspender, der scheinbar keine Menschen mit dunkler Haut erkennen konnte. [Referenz](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Herabsetzung**. Unfaire Kritik oder Etikettierung von etwas oder jemandem. Ein Beispiel ist eine Bildkennzeichnungstechnologie, die Bilder von dunkelh√§utigen Menschen f√§lschlicherweise als Gorillas bezeichnete.
- **√úber- oder Unterrepr√§sentation**. Die Idee, dass eine bestimmte Gruppe in einem bestimmten Beruf nicht gesehen wird, und jede Funktion oder Dienstleistung, die dies weiter f√∂rdert, tr√§gt zu Schaden bei.
- **Stereotypisierung**. Die Zuordnung vorgefertigter Attribute zu einer bestimmten Gruppe. Zum Beispiel kann ein Sprach√ºbersetzungssystem zwischen Englisch und T√ºrkisch Ungenauigkeiten aufweisen, die auf stereotypische Geschlechtsassoziationen zur√ºckzuf√ºhren sind.

![√úbersetzung ins T√ºrkische](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.de.png)
> √úbersetzung ins T√ºrkische

![√úbersetzung zur√ºck ins Englische](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.de.png)
> √úbersetzung zur√ºck ins Englische

Beim Entwerfen und Testen von KI-Systemen m√ºssen wir sicherstellen, dass KI fair ist und nicht so programmiert wird, dass sie voreingenommene oder diskriminierende Entscheidungen trifft, die auch Menschen untersagt sind. Fairness in KI und Machine Learning zu garantieren bleibt eine komplexe soziotechnische Herausforderung.

### Zuverl√§ssigkeit und Sicherheit

Um Vertrauen aufzubauen, m√ºssen KI-Systeme zuverl√§ssig, sicher und konsistent unter normalen und unerwarteten Bedingungen sein. Es ist wichtig zu wissen, wie sich KI-Systeme in einer Vielzahl von Situationen verhalten, insbesondere bei Ausrei√üern. Beim Aufbau von KI-L√∂sungen muss ein erheblicher Fokus darauf gelegt werden, wie eine breite Palette von Umst√§nden gehandhabt werden kann, denen die KI-L√∂sungen begegnen k√∂nnten. Zum Beispiel muss ein selbstfahrendes Auto die Sicherheit der Menschen als oberste Priorit√§t betrachten. Folglich muss die KI, die das Auto antreibt, alle m√∂glichen Szenarien ber√ºcksichtigen, denen das Auto begegnen k√∂nnte, wie Nacht, Gewitter oder Schneest√ºrme, Kinder, die √ºber die Stra√üe laufen, Haustiere, Stra√üenbauarbeiten usw. Wie gut ein KI-System eine Vielzahl von Bedingungen zuverl√§ssig und sicher handhaben kann, spiegelt das Ma√ü an Antizipation wider, das der Datenwissenschaftler oder KI-Entwickler w√§hrend des Designs oder Tests des Systems ber√ºcksichtigt hat.

> [üé• Klicken Sie hier f√ºr ein Video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inklusivit√§t

KI-Systeme sollten so gestaltet sein, dass sie alle einbeziehen und st√§rken. Beim Entwerfen und Implementieren von KI-Systemen identifizieren und adressieren Datenwissenschaftler und KI-Entwickler potenzielle Barrieren im System, die Menschen unbeabsichtigt ausschlie√üen k√∂nnten. Zum Beispiel gibt es weltweit 1 Milliarde Menschen mit Behinderungen. Mit den Fortschritten in der KI k√∂nnen sie in ihrem t√§glichen Leben leichter auf eine Vielzahl von Informationen und M√∂glichkeiten zugreifen. Durch die Beseitigung von Barrieren entstehen Chancen, KI-Produkte mit besseren Erfahrungen zu entwickeln, die allen zugutekommen.

> [üé• Klicken Sie hier f√ºr ein Video: Inklusivit√§t in KI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sicherheit und Datenschutz

KI-Systeme sollten sicher sein und die Privatsph√§re der Menschen respektieren. Menschen haben weniger Vertrauen in Systeme, die ihre Privatsph√§re, Informationen oder ihr Leben gef√§hrden. Beim Training von Machine-Learning-Modellen verlassen wir uns auf Daten, um die besten Ergebnisse zu erzielen. Dabei muss die Herkunft und Integrit√§t der Daten ber√ºcksichtigt werden. Zum Beispiel: Wurden die Daten von Nutzern eingereicht oder waren sie √∂ffentlich verf√ºgbar? W√§hrend der Arbeit mit den Daten ist es entscheidend, KI-Systeme zu entwickeln, die vertrauliche Informationen sch√ºtzen und Angriffen widerstehen k√∂nnen. Da KI immer h√§ufiger eingesetzt wird, wird der Schutz der Privatsph√§re und die Sicherung wichtiger pers√∂nlicher und gesch√§ftlicher Informationen immer wichtiger und komplexer. Datenschutz- und Datensicherheitsfragen erfordern besonders gro√üe Aufmerksamkeit bei KI, da der Zugang zu Daten entscheidend ist, damit KI-Systeme genaue und fundierte Vorhersagen und Entscheidungen √ºber Menschen treffen k√∂nnen.

> [üé• Klicken Sie hier f√ºr ein Video: Sicherheit in KI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Als Branche haben wir bedeutende Fortschritte im Bereich Datenschutz und Sicherheit gemacht, die ma√ügeblich durch Vorschriften wie die DSGVO (Datenschutz-Grundverordnung) vorangetrieben wurden.
- Dennoch m√ºssen wir bei KI-Systemen die Spannung zwischen dem Bedarf an mehr pers√∂nlichen Daten, um Systeme pers√∂nlicher und effektiver zu machen, und dem Datenschutz anerkennen.
- √Ñhnlich wie bei der Geburt vernetzter Computer mit dem Internet sehen wir auch einen enormen Anstieg der Sicherheitsprobleme im Zusammenhang mit KI.
- Gleichzeitig wird KI genutzt, um die Sicherheit zu verbessern. Ein Beispiel: Die meisten modernen Antiviren-Scanner werden heute von KI-Heuristiken betrieben.
- Wir m√ºssen sicherstellen, dass unsere Datenwissenschaftsprozesse harmonisch mit den neuesten Datenschutz- und Sicherheitspraktiken zusammenarbeiten.

### Transparenz

KI-Systeme sollten verst√§ndlich sein. Ein wesentlicher Bestandteil der Transparenz ist die Erkl√§rung des Verhaltens von KI-Systemen und ihrer Komponenten. Die Verbesserung des Verst√§ndnisses von KI-Systemen erfordert, dass Interessengruppen verstehen, wie und warum sie funktionieren, damit sie potenzielle Leistungsprobleme, Sicherheits- und Datenschutzbedenken, Vorurteile, ausschlie√üende Praktiken oder unbeabsichtigte Ergebnisse identifizieren k√∂nnen. Wir glauben auch, dass diejenigen, die KI-Systeme nutzen, ehrlich und offen dar√ºber sein sollten, wann, warum und wie sie sich entscheiden, diese einzusetzen. Ebenso √ºber die Grenzen der Systeme, die sie verwenden. Zum Beispiel: Wenn eine Bank ein KI-System zur Unterst√ºtzung ihrer Kreditentscheidungen einsetzt, ist es wichtig, die Ergebnisse zu pr√ºfen und zu verstehen, welche Daten die Empfehlungen des Systems beeinflussen. Regierungen beginnen, KI branchen√ºbergreifend zu regulieren, daher m√ºssen Datenwissenschaftler und Organisationen erkl√§ren, ob ein KI-System die regulatorischen Anforderungen erf√ºllt, insbesondere wenn es zu einem unerw√ºnschten Ergebnis kommt.

> [üé• Klicken Sie hier f√ºr ein Video: Transparenz in KI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Da KI-Systeme so komplex sind, ist es schwierig zu verstehen, wie sie funktionieren und die Ergebnisse zu interpretieren.
- Dieses mangelnde Verst√§ndnis beeinflusst die Art und Weise, wie diese Systeme verwaltet, operationalisiert und dokumentiert werden.
- Noch wichtiger ist, dass dieses mangelnde Verst√§ndnis die Entscheidungen beeinflusst, die auf Grundlage der von diesen Systemen erzeugten Ergebnisse getroffen werden.

### Verantwortlichkeit

Die Menschen, die KI-Systeme entwerfen und einsetzen, m√ºssen f√ºr die Funktionsweise ihrer Systeme verantwortlich sein. Die Notwendigkeit von Verantwortlichkeit ist besonders wichtig bei sensiblen Technologien wie Gesichtserkennung. In letzter Zeit gibt es eine wachsende Nachfrage nach Gesichtserkennungstechnologie, insbesondere von Strafverfolgungsbeh√∂rden, die das Potenzial der Technologie in Anwendungen wie der Suche nach vermissten Kindern sehen. Diese Technologien k√∂nnten jedoch von einer Regierung genutzt werden, um die Grundfreiheiten ihrer B√ºrger zu gef√§hrden, indem sie beispielsweise eine kontinuierliche √úberwachung bestimmter Personen erm√∂glichen. Daher m√ºssen Datenwissenschaftler und Organisationen verantwortlich daf√ºr sein, wie ihr KI-System Einzelpersonen oder die Gesellschaft beeinflusst.

[![F√ºhrender KI-Forscher warnt vor Massen√ºberwachung durch Gesichtserkennung](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.de.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsofts Ansatz f√ºr verantwortungsbewusste KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Warnungen vor Massen√ºberwachung durch Gesichtserkennung

Letztendlich ist eine der gr√∂√üten Fragen f√ºr unsere Generation, als die erste Generation, die KI in die Gesellschaft bringt, wie wir sicherstellen k√∂nnen, dass Computer weiterhin den Menschen gegen√ºber verantwortlich bleiben und wie wir sicherstellen k√∂nnen, dass die Menschen, die Computer entwerfen, allen anderen gegen√ºber verantwortlich bleiben.

## Auswirkungen bewerten

Bevor ein Machine-Learning-Modell trainiert wird, ist es wichtig, eine Auswirkungsbewertung durchzuf√ºhren, um den Zweck des KI-Systems zu verstehen; wie es verwendet werden soll; wo es eingesetzt wird; und wer mit dem System interagieren wird. Diese Bewertungen sind hilfreich f√ºr Pr√ºfer oder Tester, die das System evaluieren, um zu wissen, welche Faktoren bei der Identifizierung potenzieller Risiken und erwarteter Konsequenzen ber√ºcksichtigt werden m√ºssen.

Die folgenden Bereiche sollten bei der Durchf√ºhrung einer Auswirkungsbewertung ber√ºcksichtigt werden:

* **Negative Auswirkungen auf Einzelpersonen**. Es ist wichtig, sich √ºber Einschr√§nkungen oder Anforderungen, nicht unterst√ºtzte Verwendungen oder bekannte Einschr√§nkungen, die die Leistung des Systems beeintr√§chtigen k√∂nnten, bewusst zu sein, um sicherzustellen, dass das System nicht auf eine Weise verwendet wird, die Einzelpersonen schaden k√∂nnte.
* **Datenanforderungen**. Ein Verst√§ndnis daf√ºr, wie und wo das System Daten verwendet, erm√∂glicht es Pr√ºfern, m√∂gliche Datenanforderungen zu untersuchen, die ber√ºcksichtigt werden m√ºssen (z. B. DSGVO- oder HIPPA-Datenvorschriften). Dar√ºber hinaus sollte gepr√ºft werden, ob die Quelle oder Menge der Daten f√ºr das Training ausreichend ist.
* **Zusammenfassung der Auswirkungen**. Eine Liste potenzieller Sch√§den erstellen, die durch die Nutzung des Systems entstehen k√∂nnten. W√§hrend des gesamten ML-Lebenszyklus √ºberpr√ºfen, ob die identifizierten Probleme gemindert oder adressiert wurden.
* **Anwendbare Ziele** f√ºr jedes der sechs Kernprinzipien. Bewerten, ob die Ziele jedes Prinzips erreicht wurden und ob es L√ºcken gibt.

## Debugging mit verantwortungsbewusster KI

√Ñhnlich wie beim Debugging einer Softwareanwendung ist das Debugging eines KI-Systems ein notwendiger Prozess zur Identifizierung und Behebung von Problemen im System. Es gibt viele Faktoren, die dazu f√ºhren k√∂nnen, dass ein Modell nicht wie erwartet oder verantwortungsvoll funktioniert. Die meisten traditionellen Leistungsmetriken f√ºr Modelle sind quantitative Zusammenfassungen der Leistung eines Modells, die nicht ausreichen, um zu analysieren, wie ein Modell gegen die Prinzipien der verantwortungsbewussten KI verst√∂√üt. Dar√ºber hinaus ist ein Machine-Learning-Modell eine Blackbox, die es schwierig macht, zu verstehen, was seine Ergebnisse antreibt oder eine Erkl√§rung zu liefern, wenn es einen Fehler macht. Sp√§ter in diesem Kurs werden wir lernen, wie man das Responsible AI-Dashboard verwendet, um KI-Systeme zu debuggen. Das Dashboard bietet ein ganzheitliches Werkzeug f√ºr Datenwissenschaftler und KI-Entwickler, um:

* **Fehleranalyse**. Um die Fehlerverteilung des Modells zu identifizieren, die die Fairness oder Zuverl√§ssigkeit des Systems beeinflussen kann.
* **Modell√ºbersicht**. Um herauszufinden, wo es Leistungsunterschiede des Modells √ºber verschiedene Datenkohorten gibt.
* **Datenanalyse**. Um die Datenverteilung zu verstehen und m√∂gliche Vorurteile in den Daten zu identifizieren, die zu Fairness-, Inklusivit√§ts- und Zuverl√§ssigkeitsproblemen f√ºhren k√∂nnten.
* **Modellinterpretierbarkeit**. Um zu verstehen, was die Vorhersagen des Modells beeinflusst. Dies hilft, das Verhalten des Modells zu erkl√§ren, was f√ºr Transparenz und Verantwortlichkeit wichtig ist.

## üöÄ Herausforderung

Um Sch√§den von Anfang an zu verhindern, sollten wir:

- eine Vielfalt an Hintergr√ºnden und Perspektiven unter den Menschen haben, die an den Systemen arbeiten
- in Datens√§tze investieren, die die Vielfalt unserer Gesellschaft widerspiegeln
- bessere Methoden im gesamten Machine-Learning-Lebenszyklus entwickeln, um verantwortungsbewusste KI zu erkennen und zu korrigieren, wenn sie auftritt

Denken Sie √ºber reale Szenarien nach, in denen die Unzuverl√§ssigkeit eines Modells beim Modellaufbau und -einsatz offensichtlich ist. Was sollten wir noch ber√ºcksichtigen?

## [Quiz nach der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## √úberpr√ºfung & Selbststudium
In dieser Lektion haben Sie einige Grundlagen der Konzepte von Fairness und Unfairness im maschinellen Lernen kennengelernt.  

Sehen Sie sich diesen Workshop an, um tiefer in die Themen einzutauchen: 

- Auf der Suche nach verantwortungsvoller KI: Prinzipien in die Praxis umsetzen von Besmira Nushi, Mehrnoosh Sameki und Amit Sharma

[![Responsible AI Toolbox: Ein Open-Source-Framework f√ºr verantwortungsvolle KI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Ein Open-Source-Framework f√ºr verantwortungsvolle KI")


> üé• Klicken Sie auf das Bild oben f√ºr ein Video: RAI Toolbox: Ein Open-Source-Framework f√ºr verantwortungsvolle KI von Besmira Nushi, Mehrnoosh Sameki und Amit Sharma

Lesen Sie au√üerdem: 

- Microsofts RAI-Ressourcenzentrum: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Microsofts FATE-Forschungsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox: 

- [Responsible AI Toolbox GitHub Repository](https://github.com/microsoft/responsible-ai-toolbox)

Lesen Sie √ºber die Tools von Azure Machine Learning, um Fairness sicherzustellen:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## Aufgabe

[Erkunden Sie die RAI Toolbox](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.