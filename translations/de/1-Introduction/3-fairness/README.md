<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-04T21:58:29+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "de"
}
-->
# Aufbau von Machine-Learning-L√∂sungen mit verantwortungsbewusster KI

![Zusammenfassung von verantwortungsbewusster KI im Machine Learning in einer Sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote von [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

## Einf√ºhrung

In diesem Lehrplan werden Sie entdecken, wie Machine Learning unser t√§gliches Leben beeinflusst. Schon jetzt sind Systeme und Modelle in allt√§gliche Entscheidungsprozesse eingebunden, wie z. B. bei medizinischen Diagnosen, Kreditgenehmigungen oder der Betrugserkennung. Daher ist es wichtig, dass diese Modelle zuverl√§ssig arbeiten und vertrauensw√ºrdige Ergebnisse liefern. Genau wie jede andere Softwareanwendung k√∂nnen KI-Systeme Erwartungen nicht erf√ºllen oder unerw√ºnschte Ergebnisse liefern. Deshalb ist es entscheidend, das Verhalten eines KI-Modells zu verstehen und erkl√§ren zu k√∂nnen.

Stellen Sie sich vor, was passieren kann, wenn die Daten, die Sie zur Erstellung dieser Modelle verwenden, bestimmte demografische Gruppen wie Ethnie, Geschlecht, politische Ansichten oder Religion nicht ber√ºcksichtigen oder unverh√§ltnism√§√üig repr√§sentieren. Was passiert, wenn die Ergebnisse des Modells so interpretiert werden, dass sie eine bestimmte demografische Gruppe bevorzugen? Welche Konsequenzen hat das f√ºr die Anwendung? Und was passiert, wenn das Modell ein sch√§dliches Ergebnis liefert? Wer ist f√ºr das Verhalten des KI-Systems verantwortlich? Diese Fragen werden wir in diesem Lehrplan untersuchen.

In dieser Lektion werden Sie:

- Ein Bewusstsein f√ºr die Bedeutung von Fairness im Machine Learning und fairnessbezogene Sch√§den entwickeln.
- Die Praxis des Erkundens von Ausrei√üern und ungew√∂hnlichen Szenarien kennenlernen, um Zuverl√§ssigkeit und Sicherheit zu gew√§hrleisten.
- Verstehen, warum es wichtig ist, inklusive Systeme zu entwerfen, die alle Menschen einbeziehen.
- Erforschen, wie entscheidend es ist, die Privatsph√§re und Sicherheit von Daten und Menschen zu sch√ºtzen.
- Die Bedeutung eines transparenten Ansatzes erkennen, um das Verhalten von KI-Modellen zu erkl√§ren.
- Verstehen, warum Verantwortlichkeit essenziell ist, um Vertrauen in KI-Systeme aufzubauen.

## Voraussetzungen

Als Voraussetzung sollten Sie den "Responsible AI Principles"-Lernpfad absolvieren und das folgende Video zum Thema ansehen:

Erfahren Sie mehr √ºber verantwortungsbewusste KI, indem Sie diesem [Lernpfad](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott) folgen.

[![Microsofts Ansatz f√ºr verantwortungsbewusste KI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsofts Ansatz f√ºr verantwortungsbewusste KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Microsofts Ansatz f√ºr verantwortungsbewusste KI

## Fairness

KI-Systeme sollten alle Menschen fair behandeln und vermeiden, √§hnliche Gruppen unterschiedlich zu beeinflussen. Beispielsweise sollten KI-Systeme bei medizinischen Behandlungen, Kreditantr√§gen oder Besch√§ftigungsentscheidungen dieselben Empfehlungen f√ºr Menschen mit √§hnlichen Symptomen, finanziellen Verh√§ltnissen oder beruflichen Qualifikationen geben. Jeder von uns tr√§gt unbewusste Vorurteile mit sich, die unsere Entscheidungen und Handlungen beeinflussen. Diese Vorurteile k√∂nnen sich in den Daten widerspiegeln, die wir zur Schulung von KI-Systemen verwenden. Solche Verzerrungen k√∂nnen manchmal unbeabsichtigt auftreten. Es ist oft schwierig, bewusst zu erkennen, wann man Vorurteile in Daten einf√ºhrt.

**‚ÄûUnfairness‚Äú** umfasst negative Auswirkungen oder ‚ÄûSch√§den‚Äú f√ºr eine Gruppe von Menschen, z. B. definiert durch Ethnie, Geschlecht, Alter oder Behinderungsstatus. Die Hauptarten von fairnessbezogenen Sch√§den lassen sich wie folgt klassifizieren:

- **Zuweisung**: Wenn z. B. ein Geschlecht oder eine Ethnie gegen√ºber einer anderen bevorzugt wird.
- **Qualit√§t des Dienstes**: Wenn die Daten f√ºr ein spezifisches Szenario trainiert wurden, die Realit√§t jedoch viel komplexer ist, f√ºhrt dies zu einer schlechten Leistung des Dienstes. Ein Beispiel ist ein Seifenspender, der Menschen mit dunkler Hautfarbe nicht erkennen konnte. [Referenz](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Herabw√ºrdigung**: Unfaire Kritik oder Etikettierung von etwas oder jemandem. Ein Beispiel ist eine Bildkennzeichnungstechnologie, die dunkelh√§utige Menschen f√§lschlicherweise als Gorillas bezeichnete.
- **√úber- oder Unterrepr√§sentation**: Die Idee, dass eine bestimmte Gruppe in einem bestimmten Beruf nicht sichtbar ist, und jede Funktion, die dies weiter f√∂rdert, tr√§gt zu Schaden bei.
- **Stereotypisierung**: Die Zuordnung vorgefertigter Eigenschaften zu einer bestimmten Gruppe. Ein Beispiel ist ein Sprach√ºbersetzungssystem zwischen Englisch und T√ºrkisch, das aufgrund von stereotypischen Geschlechterassoziationen Ungenauigkeiten aufweist.

![√úbersetzung ins T√ºrkische](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> √úbersetzung ins T√ºrkische

![√úbersetzung zur√ºck ins Englische](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> √úbersetzung zur√ºck ins Englische

Beim Entwerfen und Testen von KI-Systemen m√ºssen wir sicherstellen, dass KI fair ist und nicht so programmiert wird, dass sie voreingenommene oder diskriminierende Entscheidungen trifft, die auch Menschen nicht treffen d√ºrfen. Fairness in KI und Machine Learning zu gew√§hrleisten, bleibt eine komplexe soziotechnische Herausforderung.

### Zuverl√§ssigkeit und Sicherheit

Um Vertrauen aufzubauen, m√ºssen KI-Systeme zuverl√§ssig, sicher und konsistent unter normalen und unerwarteten Bedingungen sein. Es ist wichtig zu wissen, wie sich KI-Systeme in verschiedenen Situationen verhalten, insbesondere bei Ausrei√üern. Beim Aufbau von KI-L√∂sungen sollte ein erheblicher Fokus darauf gelegt werden, wie eine Vielzahl von Umst√§nden gehandhabt werden kann, denen die KI-L√∂sungen begegnen k√∂nnten. Zum Beispiel muss ein selbstfahrendes Auto die Sicherheit der Menschen als oberste Priorit√§t betrachten. Daher muss die KI, die das Auto antreibt, alle m√∂glichen Szenarien ber√ºcksichtigen, denen das Auto begegnen k√∂nnte, wie z. B. Nacht, Gewitter, Schneest√ºrme, Kinder, die √ºber die Stra√üe rennen, Haustiere, Stra√üenbauarbeiten usw. Wie gut ein KI-System eine Vielzahl von Bedingungen zuverl√§ssig und sicher bew√§ltigen kann, spiegelt das Ma√ü an Voraussicht wider, das der Datenwissenschaftler oder KI-Entwickler w√§hrend des Designs oder Tests des Systems ber√ºcksichtigt hat.

> [üé• Klicken Sie hier f√ºr ein Video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inklusivit√§t

KI-Systeme sollten so gestaltet sein, dass sie alle Menschen einbeziehen und bef√§higen. Beim Entwerfen und Implementieren von KI-Systemen identifizieren und adressieren Datenwissenschaftler und KI-Entwickler potenzielle Barrieren im System, die Menschen unbeabsichtigt ausschlie√üen k√∂nnten. Zum Beispiel gibt es weltweit 1 Milliarde Menschen mit Behinderungen. Mit den Fortschritten in der KI k√∂nnen sie leichter auf eine Vielzahl von Informationen und M√∂glichkeiten in ihrem t√§glichen Leben zugreifen. Durch die Beseitigung von Barrieren entstehen Chancen, KI-Produkte mit besseren Erfahrungen zu entwickeln, die allen zugutekommen.

> [üé• Klicken Sie hier f√ºr ein Video: Inklusivit√§t in der KI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sicherheit und Datenschutz

KI-Systeme sollten sicher sein und die Privatsph√§re der Menschen respektieren. Menschen vertrauen Systemen weniger, die ihre Privatsph√§re, Informationen oder ihr Leben gef√§hrden. Beim Training von Machine-Learning-Modellen verlassen wir uns auf Daten, um die besten Ergebnisse zu erzielen. Dabei muss die Herkunft und Integrit√§t der Daten ber√ºcksichtigt werden. Zum Beispiel: Wurden die Daten von Nutzern bereitgestellt oder waren sie √∂ffentlich zug√§nglich? W√§hrend der Arbeit mit den Daten ist es entscheidend, KI-Systeme zu entwickeln, die vertrauliche Informationen sch√ºtzen und Angriffen widerstehen k√∂nnen. Da KI immer weiter verbreitet wird, wird der Schutz der Privatsph√§re und die Sicherung wichtiger pers√∂nlicher und gesch√§ftlicher Informationen immer kritischer und komplexer. Datenschutz- und Datensicherheitsfragen erfordern besondere Aufmerksamkeit, da der Zugang zu Daten f√ºr KI-Systeme essenziell ist, um genaue und fundierte Vorhersagen und Entscheidungen √ºber Menschen zu treffen.

> [üé• Klicken Sie hier f√ºr ein Video: Sicherheit in der KI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Als Branche haben wir bedeutende Fortschritte im Bereich Datenschutz und Sicherheit gemacht, die ma√ügeblich durch Vorschriften wie die DSGVO (Datenschutz-Grundverordnung) vorangetrieben wurden.
- Dennoch m√ºssen wir bei KI-Systemen die Spannung zwischen dem Bedarf an mehr pers√∂nlichen Daten, um Systeme pers√∂nlicher und effektiver zu machen, und dem Datenschutz anerkennen.
- Genau wie bei der Einf√ºhrung vernetzter Computer mit dem Internet sehen wir auch einen starken Anstieg der Sicherheitsprobleme im Zusammenhang mit KI.
- Gleichzeitig wird KI genutzt, um die Sicherheit zu verbessern. Zum Beispiel werden die meisten modernen Antiviren-Scanner heute von KI-Heuristiken angetrieben.
- Wir m√ºssen sicherstellen, dass unsere Datenwissenschaftsprozesse harmonisch mit den neuesten Datenschutz- und Sicherheitspraktiken zusammenarbeiten.

### Transparenz

KI-Systeme sollten verst√§ndlich sein. Ein wesentlicher Bestandteil der Transparenz ist die Erkl√§rung des Verhaltens von KI-Systemen und ihrer Komponenten. Die Verbesserung des Verst√§ndnisses von KI-Systemen erfordert, dass Interessengruppen verstehen, wie und warum sie funktionieren, damit sie potenzielle Leistungsprobleme, Sicherheits- und Datenschutzbedenken, Vorurteile, ausschlie√üende Praktiken oder unbeabsichtigte Ergebnisse identifizieren k√∂nnen. Wir glauben auch, dass diejenigen, die KI-Systeme nutzen, ehrlich und offen dar√ºber sein sollten, wann, warum und wie sie diese einsetzen. Ebenso √ºber die Grenzen der Systeme, die sie verwenden. Zum Beispiel: Wenn eine Bank ein KI-System zur Unterst√ºtzung ihrer Kreditentscheidungen einsetzt, ist es wichtig, die Ergebnisse zu √ºberpr√ºfen und zu verstehen, welche Daten die Empfehlungen des Systems beeinflussen. Regierungen beginnen, KI branchen√ºbergreifend zu regulieren, daher m√ºssen Datenwissenschaftler und Organisationen erkl√§ren, ob ein KI-System die regulatorischen Anforderungen erf√ºllt, insbesondere wenn es zu einem unerw√ºnschten Ergebnis kommt.

> [üé• Klicken Sie hier f√ºr ein Video: Transparenz in der KI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Da KI-Systeme so komplex sind, ist es schwierig zu verstehen, wie sie funktionieren und ihre Ergebnisse zu interpretieren.
- Dieses mangelnde Verst√§ndnis beeinflusst, wie diese Systeme verwaltet, operationalisiert und dokumentiert werden.
- Noch wichtiger ist, dass dieses mangelnde Verst√§ndnis die Entscheidungen beeinflusst, die auf Basis der Ergebnisse dieser Systeme getroffen werden.

### Verantwortlichkeit

Die Menschen, die KI-Systeme entwerfen und einsetzen, m√ºssen f√ºr deren Betrieb verantwortlich sein. Die Notwendigkeit der Verantwortlichkeit ist besonders wichtig bei sensiblen Technologien wie Gesichtserkennung. In letzter Zeit gibt es eine wachsende Nachfrage nach Gesichtserkennungstechnologie, insbesondere von Strafverfolgungsbeh√∂rden, die das Potenzial der Technologie beispielsweise bei der Suche nach vermissten Kindern sehen. Diese Technologien k√∂nnten jedoch von einer Regierung genutzt werden, um die Grundfreiheiten ihrer B√ºrger zu gef√§hrden, indem sie beispielsweise eine kontinuierliche √úberwachung bestimmter Personen erm√∂glichen. Daher m√ºssen Datenwissenschaftler und Organisationen verantwortlich daf√ºr sein, wie ihr KI-System Einzelpersonen oder die Gesellschaft beeinflusst.

[![F√ºhrender KI-Forscher warnt vor Massen√ºberwachung durch Gesichtserkennung](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsofts Ansatz f√ºr verantwortungsbewusste KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Warnungen vor Massen√ºberwachung durch Gesichtserkennung

Letztendlich ist eine der gr√∂√üten Fragen f√ºr unsere Generation, als die erste Generation, die KI in die Gesellschaft bringt, wie wir sicherstellen k√∂nnen, dass Computer den Menschen gegen√ºber rechenschaftspflichtig bleiben und dass die Menschen, die Computer entwerfen, allen anderen gegen√ºber rechenschaftspflichtig bleiben.

## Auswirkungen bewerten

Bevor ein Machine-Learning-Modell trainiert wird, ist es wichtig, eine Auswirkungsbewertung durchzuf√ºhren, um den Zweck des KI-Systems zu verstehen: Was ist der beabsichtigte Nutzen? Wo wird es eingesetzt? Und wer wird mit dem System interagieren? Diese Bewertungen helfen Pr√ºfern oder Testern, die das System bewerten, zu wissen, welche Faktoren bei der Identifizierung potenzieller Risiken und erwarteter Konsequenzen zu ber√ºcksichtigen sind.

Die folgenden Bereiche stehen bei einer Auswirkungsbewertung im Fokus:

* **Negative Auswirkungen auf Einzelpersonen**: Es ist wichtig, sich √ºber Einschr√§nkungen, Anforderungen, nicht unterst√ºtzte Verwendungen oder bekannte Begrenzungen bewusst zu sein, die die Leistung des Systems beeintr√§chtigen k√∂nnten, um sicherzustellen, dass das System nicht auf eine Weise verwendet wird, die Einzelpersonen schaden k√∂nnte.
* **Datenanforderungen**: Ein Verst√§ndnis daf√ºr zu gewinnen, wie und wo das System Daten verwendet, erm√∂glicht es Pr√ºfern, Datenanforderungen zu identifizieren, die ber√ºcksichtigt werden m√ºssen (z. B. DSGVO- oder HIPAA-Datenvorschriften). Au√üerdem sollte gepr√ºft werden, ob die Quelle oder Menge der Daten f√ºr das Training ausreicht.
* **Zusammenfassung der Auswirkungen**: Eine Liste potenzieller Sch√§den erstellen, die durch die Nutzung des Systems entstehen k√∂nnten. W√§hrend des gesamten ML-Lebenszyklus √ºberpr√ºfen, ob die identifizierten Probleme behoben oder adressiert wurden.
* **Ziele f√ºr die sechs Kernprinzipien**: Bewerten, ob die Ziele jedes Prinzips erreicht wurden und ob es L√ºcken gibt.

## Debugging mit verantwortungsbewusster KI

√Ñhnlich wie beim Debugging einer Softwareanwendung ist das Debugging eines KI-Systems ein notwendiger Prozess, um Probleme im System zu identifizieren und zu l√∂sen. Es gibt viele Faktoren, die dazu f√ºhren k√∂nnen, dass ein Modell nicht wie erwartet oder verantwortungsvoll funktioniert. Die meisten traditionellen Leistungsmetriken f√ºr Modelle sind quantitative Zusammenfassungen der Modellleistung, die nicht ausreichen, um zu analysieren, wie ein Modell gegen die Prinzipien der verantwortungsbewussten KI verst√∂√üt. Dar√ºber hinaus ist ein Machine-Learning-Modell eine Blackbox, die es schwierig macht, die Gr√ºnde f√ºr seine Ergebnisse zu verstehen oder Erkl√§rungen zu liefern, wenn es Fehler macht. Sp√§ter in diesem Kurs lernen wir, wie man das Responsible AI Dashboard verwendet, um KI-Systeme zu debuggen. Das Dashboard bietet ein umfassendes Werkzeug f√ºr Datenwissenschaftler und KI-Entwickler, um:

* **Fehleranalyse**: Die Fehlerverteilung des Modells zu identifizieren, die die Fairness oder Zuverl√§ssigkeit des Systems beeintr√§chtigen k√∂nnte.
* **Modell√ºbersicht**: Zu entdecken, wo es Leistungsunterschiede des Modells √ºber verschiedene Datenkohorten hinweg gibt.
* **Datenanalyse**: Die Datenverteilung zu verstehen und potenzielle Verzerrungen in den Daten zu identifizieren, die zu Fairness-, Inklusivit√§ts- und Zuverl√§ssigkeitsproblemen f√ºhren k√∂nnten.
* **Modellinterpretierbarkeit**: Zu verstehen, was die Vorhersagen des Modells beeinflusst. Dies hilft, das Verhalten des Modells zu erkl√§ren, was f√ºr Transparenz und Verantwortlichkeit wichtig ist.

## üöÄ Herausforderung

Um Sch√§den von vornherein zu vermeiden, sollten wir:

- eine Vielfalt an Hintergr√ºnden und Perspektiven unter den Menschen haben, die an den Systemen arbeiten
- in Datens√§tze investieren, die die Vielfalt unserer Gesellschaft widerspiegeln
- bessere Methoden im gesamten Machine-Learning-Lebenszyklus entwickeln, um verantwortungsbewusste KI zu erkennen und zu korrigieren, wenn sie auftritt

Denken Sie an reale Szenarien, in denen die Unzuverl√§ssigkeit eines Modells beim Erstellen und Verwenden offensichtlich wird. Was sollten wir noch ber√ºcksichtigen?

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

## R√ºckblick & Selbststudium

In dieser Lektion haben Sie einige Grundlagen zu den Konzepten von Fairness und Unfairness im Machine Learning gelernt.
Schauen Sie sich diesen Workshop an, um tiefer in die Themen einzutauchen:

- Auf der Suche nach verantwortungsvoller KI: Prinzipien in die Praxis umsetzen von Besmira Nushi, Mehrnoosh Sameki und Amit Sharma

[![Responsible AI Toolbox: Ein Open-Source-Framework f√ºr den Aufbau verantwortungsvoller KI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Ein Open-Source-Framework f√ºr den Aufbau verantwortungsvoller KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: RAI Toolbox: Ein Open-Source-Framework f√ºr den Aufbau verantwortungsvoller KI von Besmira Nushi, Mehrnoosh Sameki und Amit Sharma

Lesen Sie au√üerdem:

- Microsofts RAI-Ressourcenzentrum: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE-Forschungsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Responsible AI Toolbox GitHub-Repository](https://github.com/microsoft/responsible-ai-toolbox)

Lesen Sie √ºber die Tools von Azure Machine Learning, um Fairness sicherzustellen:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Aufgabe

[Erkunden Sie die RAI Toolbox](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe des KI-√úbersetzungsdienstes [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.