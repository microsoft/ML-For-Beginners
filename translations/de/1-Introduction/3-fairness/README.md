# Aufbau von Machine Learning-L√∂sungen mit verantwortungsbewusster KI

![Zusammenfassung von verantwortungsbewusster KI im Machine Learning in einer Sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.de.png)
> Sketchnote von [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Vorlesungsquiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Einf√ºhrung

In diesem Lehrplan werden Sie entdecken, wie Machine Learning unser t√§gliches Leben beeinflussen kann und bereits beeinflusst. Schon jetzt sind Systeme und Modelle in t√§glichen Entscheidungsprozessen involviert, wie z.B. bei medizinischen Diagnosen, Kreditgenehmigungen oder der Betrugserkennung. Daher ist es wichtig, dass diese Modelle gut funktionieren, um vertrauensw√ºrdige Ergebnisse zu liefern. Wie jede Softwareanwendung werden auch KI-Systeme Erwartungen nicht erf√ºllen oder unerw√ºnschte Ergebnisse liefern. Deshalb ist es entscheidend, das Verhalten eines KI-Modells zu verstehen und erkl√§ren zu k√∂nnen.

Stellen Sie sich vor, was passieren kann, wenn die Daten, die Sie verwenden, um diese Modelle zu erstellen, bestimmte demografische Merkmale wie Rasse, Geschlecht, politische Ansichten oder Religion nicht ber√ºcksichtigen oder diese demografischen Merkmale unverh√§ltnism√§√üig repr√§sentieren. Was passiert, wenn die Ausgabe des Modells so interpretiert wird, dass sie eine bestimmte demografische Gruppe beg√ºnstigt? Was sind die Konsequenzen f√ºr die Anwendung? Und was geschieht, wenn das Modell ein nachteilhaftes Ergebnis hat und Menschen schadet? Wer ist verantwortlich f√ºr das Verhalten der KI-Systeme? Dies sind einige Fragen, die wir in diesem Lehrplan untersuchen werden.

In dieser Lektion werden Sie:

- Ihr Bewusstsein f√ºr die Bedeutung von Fairness im Machine Learning und damit verbundenen Sch√§den sch√§rfen.
- Sich mit der Praxis vertrautmachen, Ausrei√üer und ungew√∂hnliche Szenarien zu erkunden, um Zuverl√§ssigkeit und Sicherheit zu gew√§hrleisten.
- Verst√§ndnis daf√ºr gewinnen, wie wichtig es ist, alle zu erm√§chtigen, indem inklusive Systeme entworfen werden.
- Erkunden, wie entscheidend es ist, die Privatsph√§re und Sicherheit von Daten und Personen zu sch√ºtzen.
- Die Bedeutung eines ‚ÄûGlasbox‚Äú-Ansatzes erkennen, um das Verhalten von KI-Modellen zu erkl√§ren.
- Achtsam sein, wie wichtig Verantwortung ist, um Vertrauen in KI-Systeme aufzubauen.

## Voraussetzungen

Als Voraussetzung sollten Sie den Lernpfad "Verantwortungsbewusste KI-Prinzipien" absolvieren und das folgende Video zu diesem Thema ansehen:

Erfahren Sie mehr √ºber verantwortungsbewusste KI, indem Sie diesem [Lernpfad](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott) folgen.

[![Microsofts Ansatz zur verantwortungsbewussten KI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsofts Ansatz zur verantwortungsbewussten KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Microsofts Ansatz zur verantwortungsbewussten KI

## Fairness

KI-Systeme sollten alle fair behandeln und vermeiden, √§hnliche Gruppen von Menschen unterschiedlich zu beeinflussen. Zum Beispiel sollten KI-Systeme, die Empfehlungen zu medizinischen Behandlungen, Kreditantr√§gen oder Besch√§ftigung abgeben, allen mit √§hnlichen Symptomen, finanziellen Umst√§nden oder beruflichen Qualifikationen dieselben Empfehlungen geben. Jeder von uns tr√§gt ererbte Vorurteile in sich, die unsere Entscheidungen und Handlungen beeinflussen. Diese Vorurteile k√∂nnen in den Daten, die wir zur Schulung von KI-Systemen verwenden, offensichtlich werden. Solche Manipulation kann manchmal unbeabsichtigt geschehen. Es ist oft schwierig, sich bewusst zu sein, wenn man Vorurteile in Daten einf√ºhrt.

**‚ÄûUnfairness‚Äú** umfasst negative Auswirkungen oder ‚ÄûSch√§den‚Äú f√ºr eine Gruppe von Menschen, wie z.B. solche, die in Bezug auf Rasse, Geschlecht, Alter oder Behinderungsstatus definiert sind. Die Hauptsch√§den, die mit Fairness verbunden sind, k√∂nnen klassifiziert werden als:

- **Zuteilung**, wenn beispielsweise ein Geschlecht oder eine Ethnie bevorzugt wird.
- **Qualit√§t des Services**. Wenn Sie die Daten f√ºr ein bestimmtes Szenario trainieren, die Realit√§t jedoch viel komplexer ist, f√ºhrt dies zu einem schlecht funktionierenden Service. Zum Beispiel ein Handseifenspender, der anscheinend nicht in der Lage ist, Personen mit dunkler Haut zu erkennen. [Referenz](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Herabw√ºrdigung**. Etwas oder jemanden unfair zu kritisieren und zu kennzeichnen. Zum Beispiel wurde eine Bildkennzeichnungstechnologie ber√ºchtigt daf√ºr, Bilder von dunkelh√§utigen Menschen als Gorillas zu kennzeichnen.
- **√úber- oder Unterrepr√§sentation**. Die Idee ist, dass eine bestimmte Gruppe in einem bestimmten Beruf nicht gesehen wird, und jeder Service oder jede Funktion, die dies weiterhin f√∂rdert, tr√§gt zu Sch√§den bei.
- **Stereotypisierung**. Eine bestimmte Gruppe mit vorab zugewiesenen Eigenschaften zu assoziieren. Zum Beispiel kann ein Sprach√ºbersetzungssystem zwischen Englisch und T√ºrkisch Ungenauigkeiten aufweisen, aufgrund von W√∂rtern mit stereotypischen Assoziationen zum Geschlecht.

![√úbersetzung ins T√ºrkische](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.de.png)
> √úbersetzung ins T√ºrkische

![√úbersetzung zur√ºck ins Englische](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.de.png)
> √úbersetzung zur√ºck ins Englische

Beim Entwerfen und Testen von KI-Systemen m√ºssen wir sicherstellen, dass KI fair ist und nicht darauf programmiert ist, voreingenommene oder diskriminierende Entscheidungen zu treffen, die auch Menschen verboten sind. Die Gew√§hrleistung von Fairness in KI und Machine Learning bleibt eine komplexe soziotechnische Herausforderung.

### Zuverl√§ssigkeit und Sicherheit

Um Vertrauen aufzubauen, m√ºssen KI-Systeme zuverl√§ssig, sicher und konsistent unter normalen und unerwarteten Bedingungen sein. Es ist wichtig zu wissen, wie KI-Systeme in verschiedenen Situationen reagieren, insbesondere wenn sie Ausrei√üer sind. Beim Aufbau von KI-L√∂sungen muss ein erheblicher Fokus darauf gelegt werden, wie eine Vielzahl von Umst√§nden, mit denen die KI-L√∂sungen konfrontiert werden k√∂nnten, zu bew√§ltigen ist. Zum Beispiel muss ein selbstfahrendes Auto die Sicherheit der Menschen an oberste Stelle setzen. Daher muss die KI, die das Auto antreibt, alle m√∂glichen Szenarien ber√ºcksichtigen, mit denen das Auto konfrontiert werden k√∂nnte, wie Nacht, Gewitter oder Schneest√ºrme, Kinder, die √ºber die Stra√üe laufen, Haustiere, Stra√üenbau usw. Wie gut ein KI-System eine breite Palette von Bedingungen zuverl√§ssig und sicher bew√§ltigen kann, spiegelt das Ma√ü an Voraussicht wider, das der Datenwissenschaftler oder KI-Entwickler w√§hrend des Designs oder der Tests des Systems ber√ºcksichtigt hat.

> [üé• Klicken Sie hier f√ºr ein Video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inklusivit√§t

KI-Systeme sollten so gestaltet sein, dass sie alle einbeziehen und erm√§chtigen. Bei der Gestaltung und Implementierung von KI-Systemen identifizieren und beheben Datenwissenschaftler und KI-Entwickler potenzielle Barrieren im System, die unbeabsichtigt Menschen ausschlie√üen k√∂nnten. Zum Beispiel gibt es weltweit 1 Milliarde Menschen mit Behinderungen. Mit dem Fortschritt der KI k√∂nnen sie in ihrem t√§glichen Leben leichter auf eine Vielzahl von Informationen und M√∂glichkeiten zugreifen. Indem Barrieren angesprochen werden, entstehen Chancen f√ºr Innovation und Entwicklung von KI-Produkten mit besseren Erfahrungen, die allen zugutekommen.

> [üé• Klicken Sie hier f√ºr ein Video: Inklusivit√§t in KI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Sicherheit und Datenschutz

KI-Systeme sollten sicher sein und die Privatsph√§re der Menschen respektieren. Menschen haben weniger Vertrauen in Systeme, die ihre Privatsph√§re, Informationen oder Leben gef√§hrden. Bei der Schulung von Machine Learning-Modellen verlassen wir uns auf Daten, um die besten Ergebnisse zu erzielen. Dabei m√ºssen die Herkunft der Daten und die Integrit√§t ber√ºcksichtigt werden. Zum Beispiel, wurden die Daten vom Benutzer eingereicht oder sind sie √∂ffentlich verf√ºgbar? Dar√ºber hinaus ist es beim Arbeiten mit Daten entscheidend, KI-Systeme zu entwickeln, die vertrauliche Informationen sch√ºtzen und Angriffen widerstehen k√∂nnen. Da KI immer verbreiteter wird, wird der Schutz der Privatsph√§re und die Sicherung wichtiger pers√∂nlicher und gesch√§ftlicher Informationen zunehmend kritischer und komplexer. Datenschutz- und Datensicherheitsprobleme erfordern besonders viel Aufmerksamkeit f√ºr KI, da der Zugang zu Daten f√ºr KI-Systeme entscheidend ist, um genaue und informierte Vorhersagen und Entscheidungen √ºber Menschen zu treffen.

> [üé• Klicken Sie hier f√ºr ein Video: Sicherheit in KI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Als Branche haben wir bedeutende Fortschritte im Bereich Datenschutz und Sicherheit gemacht, die ma√ügeblich durch Vorschriften wie die DSGVO (Datenschutz-Grundverordnung) gef√∂rdert wurden.
- Dennoch m√ºssen wir bei KI-Systemen die Spannung zwischen dem Bedarf an mehr pers√∂nlichen Daten, um Systeme pers√∂nlicher und effektiver zu machen, und dem Datenschutz anerkennen.
- √Ñhnlich wie bei der Geburt vernetzter Computer mit dem Internet sehen wir auch einen enormen Anstieg der Anzahl von Sicherheitsproblemen im Zusammenhang mit KI.
- Gleichzeitig haben wir gesehen, dass KI zur Verbesserung der Sicherheit eingesetzt wird. Ein Beispiel sind die meisten modernen Antiviren-Scanner, die heute von KI-Heuristiken gesteuert werden.
- Wir m√ºssen sicherstellen, dass unsere Data-Science-Prozesse harmonisch mit den neuesten Datenschutz- und Sicherheitspraktiken kombiniert werden.

### Transparenz

KI-Systeme sollten verst√§ndlich sein. Ein entscheidender Teil der Transparenz besteht darin, das Verhalten von KI-Systemen und ihren Komponenten zu erkl√§ren. Das Verst√§ndnis von KI-Systemen zu verbessern, erfordert, dass die Stakeholder nachvollziehen, wie und warum sie funktionieren, damit sie potenzielle Leistungsprobleme, Sicherheits- und Datenschutzbedenken, Vorurteile, ausschlie√üende Praktiken oder unbeabsichtigte Ergebnisse identifizieren k√∂nnen. Wir glauben auch, dass diejenigen, die KI-Systeme nutzen, ehrlich und offen dar√ºber sein sollten, wann, warum und wie sie diese einsetzen, sowie √ºber die Einschr√§nkungen der Systeme, die sie verwenden. Zum Beispiel, wenn eine Bank ein KI-System zur Unterst√ºtzung ihrer Verbraucherentscheidungen verwendet, ist es wichtig, die Ergebnisse zu √ºberpr√ºfen und zu verstehen, welche Daten die Empfehlungen des Systems beeinflussen. Regierungen beginnen, KI in verschiedenen Branchen zu regulieren, sodass Datenwissenschaftler und Organisationen erkl√§ren m√ºssen, ob ein KI-System die regulatorischen Anforderungen erf√ºllt, insbesondere wenn es zu einem unerw√ºnschten Ergebnis kommt.

> [üé• Klicken Sie hier f√ºr ein Video: Transparenz in KI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Da KI-Systeme so komplex sind, ist es schwer zu verstehen, wie sie funktionieren und die Ergebnisse zu interpretieren.
- Dieser Mangel an Verst√§ndnis beeinflusst, wie diese Systeme verwaltet, operationalisiert und dokumentiert werden.
- Dieser Mangel an Verst√§ndnis beeinflusst insbesondere die Entscheidungen, die auf der Grundlage der Ergebnisse getroffen werden, die diese Systeme produzieren.

### Verantwortung

Die Personen, die KI-Systeme entwerfen und implementieren, m√ºssen f√ºr das Verhalten ihrer Systeme verantwortlich sein. Die Notwendigkeit von Verantwortung ist besonders wichtig bei sensiblen Technologien wie Gesichtserkennung. K√ºrzlich gab es eine wachsende Nachfrage nach Gesichtserkennungstechnologie, insbesondere von Strafverfolgungsbeh√∂rden, die das Potenzial dieser Technologie zur Auffindung vermisster Kinder sehen. Diese Technologien k√∂nnten jedoch von einer Regierung genutzt werden, um die grundlegenden Freiheiten ihrer B√ºrger zu gef√§hrden, indem sie beispielsweise die kontinuierliche √úberwachung bestimmter Personen erm√∂glichen. Daher m√ºssen Datenwissenschaftler und Organisationen verantwortlich daf√ºr sein, wie ihr KI-System Individuen oder die Gesellschaft beeinflusst.

[![F√ºhrender KI-Forscher warnt vor Massen√ºberwachung durch Gesichtserkennung](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.de.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsofts Ansatz zur verantwortungsbewussten KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Warnungen vor Massen√ºberwachung durch Gesichtserkennung

Letztendlich ist eine der gr√∂√üten Fragen f√ºr unsere Generation, die erste Generation, die KI in die Gesellschaft bringt, wie sichergestellt werden kann, dass Computer den Menschen gegen√ºber verantwortlich bleiben und wie sichergestellt werden kann, dass die Menschen, die Computer entwerfen, allen anderen gegen√ºber verantwortlich bleiben.

## Auswirkungen bewerten

Vor der Schulung eines Machine Learning-Modells ist es wichtig, eine Auswirkungenbewertung durchzuf√ºhren, um den Zweck des KI-Systems zu verstehen; was die beabsichtigte Nutzung ist; wo es eingesetzt wird; und wer mit dem System interagiert. Diese Informationen sind hilfreich f√ºr Gutachter oder Tester, die das System bewerten, um zu wissen, welche Faktoren bei der Identifizierung potenzieller Risiken und erwarteter Konsequenzen zu ber√ºcksichtigen sind.

Die folgenden Bereiche sind bei der Durchf√ºhrung einer Auswirkungenbewertung zu beachten:

* **Negative Auswirkungen auf Einzelpersonen**. Es ist wichtig, sich √ºber Einschr√§nkungen oder Anforderungen, nicht unterst√ºtzte Nutzungen oder bekannte Einschr√§nkungen, die die Leistung des Systems behindern, bewusst zu sein, um sicherzustellen, dass das System nicht in einer Weise verwendet wird, die Einzelpersonen schaden k√∂nnte.
* **Datenanforderungen**. Ein Verst√§ndnis dar√ºber, wie und wo das System Daten verwenden wird, erm√∂glicht es Gutachtern, etwaige Datenanforderungen zu erkunden, die Sie beachten sollten (z.B. DSGVO oder HIPAA-Datenvorschriften). Dar√ºber hinaus sollte gepr√ºft werden, ob die Quelle oder Menge der Daten ausreichend f√ºr das Training ist.
* **Zusammenfassung der Auswirkungen**. Erstellen Sie eine Liste potenzieller Sch√§den, die durch die Nutzung des Systems entstehen k√∂nnten. √úberpr√ºfen Sie im Verlauf des ML-Lebenszyklus, ob die identifizierten Probleme gemildert oder angesprochen werden.
* **Anwendbare Ziele** f√ºr jedes der sechs Kernprinzipien. Bewerten Sie, ob die Ziele jedes der Prinzipien erf√ºllt werden und ob es L√ºcken gibt.

## Debugging mit verantwortungsbewusster KI

√Ñhnlich wie beim Debugging einer Softwareanwendung ist das Debugging eines KI-Systems ein notwendiger Prozess, um Probleme im System zu identifizieren und zu beheben. Es gibt viele Faktoren, die dazu f√ºhren k√∂nnen, dass ein Modell nicht wie erwartet oder verantwortungsvoll funktioniert. Die meisten traditionellen Leistungsmetriken f√ºr Modelle sind quantitative Aggregationen der Leistung eines Modells, die nicht ausreichen, um zu analysieren, wie ein Modell gegen die Prinzipien verantwortungsbewusster KI verst√∂√üt. Dar√ºber hinaus ist ein Machine Learning-Modell eine Black Box, die es schwierig macht zu verstehen, was seine Ergebnisse beeinflusst oder eine Erkl√§rung zu liefern, wenn es einen Fehler macht. Sp√§ter in diesem Kurs werden wir lernen, wie wir das Responsible AI Dashboard verwenden k√∂nnen, um KI-Systeme zu debuggen. Das Dashboard bietet ein ganzheitliches Werkzeug f√ºr Datenwissenschaftler und KI-Entwickler, um Folgendes durchzuf√ºhren:

* **Fehleranalyse**. Um die Fehlerverteilung des Modells zu identifizieren, die die Fairness oder Zuverl√§ssigkeit des Systems beeintr√§chtigen kann.
* **Modell√ºbersicht**. Um herauszufinden, wo es Ungleichheiten in der Leistung des Modells √ºber Datenkohorten hinweg gibt.
* **Datenanalyse**. Um die Datenverteilung zu verstehen und potenzielle Vorurteile in den Daten zu identifizieren, die zu Fairness-, Inklusivit√§ts- und Zuverl√§ssigkeitsproblemen f√ºhren k√∂nnten.
* **Modellinterpretierbarkeit**. Um zu verstehen, was die Vorhersagen des Modells beeinflusst oder beeinflusst. Dies hilft, das Verhalten des Modells zu erkl√§ren, was wichtig f√ºr Transparenz und Verantwortung ist.

## üöÄ Herausforderung

Um zu verhindern, dass Sch√§den von vornherein entstehen, sollten wir:

- eine Vielfalt von Hintergr√ºnden und Perspektiven unter den Menschen haben, die an den Systemen arbeiten
- in Datens√§tze investieren, die die Vielfalt unserer Gesellschaft widerspiegeln
- bessere Methoden im gesamten Lebenszyklus des Machine Learning entwickeln, um verantwortungsbewusste KI zu erkennen und zu korrigieren, wenn sie auftritt

Denken Sie an reale Szenarien, in denen das Misstrauen gegen√ºber einem Modell offensichtlich ist, sowohl beim Modellaufbau als auch bei der Nutzung. Was sollten wir noch ber√ºcksichtigen?

## [Nachlesungsquiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)

## √úberpr√ºfung & Selbststudium

In dieser Lektion haben Sie einige Grundlagen der Konzepte von Fairness und Unfairness im Machine Learning gelernt.

Sehen Sie sich diesen Workshop an, um tiefer in die Themen einzutauchen:

- Auf der Suche nach verantwortungsbewusster KI: Prinzipien in die Praxis umsetzen von Besmira Nushi, Mehrnoosh Sameki und Amit Sharma

[![Responsible AI Toolbox: Ein Open-Source-Rahmenwerk f√ºr den Aufbau verantwortungsbewusster KI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Ein Open-Source-Rahmenwerk f√ºr den Aufbau verantwortungsbewusster KI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: RAI Toolbox: Ein Open-Source-Rahmenwerk f√ºr den Aufbau verantwortungsbewusster KI von Besmira Nushi, Mehrnoosh Sameki und Amit Sharma

Lesen Sie auch:

- Microsofts RAI-Ressourcenzentrum: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE-Forschungsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Responsible AI Toolbox GitHub-Repository](https://github.com/microsoft/responsible-ai-toolbox)

Lesen Sie √ºber die Tools von Azure Machine Learning, um Fairness sicherzustellen:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Aufgabe

[RAI Toolbox erkunden](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, sollten Sie sich bewusst sein, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als autoritative Quelle betrachtet werden. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.