<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-04T21:49:11+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "de"
}
-->
# Erstellen eines Regressionsmodells mit Scikit-learn: Regression auf vier Arten

![Infografik zu linearer vs. polynomialer Regression](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografik von [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

> ### [Diese Lektion ist auch in R verf√ºgbar!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Einf√ºhrung 

Bisher haben Sie untersucht, was Regression ist, anhand von Beispieldaten aus dem K√ºrbispreisdataset, das wir in dieser Lektion verwenden werden. Sie haben es auch mit Matplotlib visualisiert.

Jetzt sind Sie bereit, tiefer in die Regression f√ºr maschinelles Lernen einzutauchen. W√§hrend die Visualisierung Ihnen hilft, Daten zu verstehen, liegt die wahre St√§rke des maschinellen Lernens im _Trainieren von Modellen_. Modelle werden mit historischen Daten trainiert, um automatisch Datenabh√§ngigkeiten zu erfassen, und sie erm√∂glichen es Ihnen, Ergebnisse f√ºr neue Daten vorherzusagen, die das Modell zuvor nicht gesehen hat.

In dieser Lektion lernen Sie mehr √ºber zwei Arten von Regression: _einfache lineare Regression_ und _polynomiale Regression_, zusammen mit einigen mathematischen Grundlagen dieser Techniken. Diese Modelle erm√∂glichen es uns, K√ºrbispreise basierend auf verschiedenen Eingabedaten vorherzusagen.

[![ML f√ºr Anf√§nger - Verst√§ndnis der linearen Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML f√ºr Anf√§nger - Verst√§ndnis der linearen Regression")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur linearen Regression.

> Im gesamten Lehrplan gehen wir von minimalen mathematischen Kenntnissen aus und versuchen, das Thema f√ºr Studierende aus anderen Fachbereichen zug√§nglich zu machen. Achten Sie auf Notizen, üßÆ Hinweise, Diagramme und andere Lernhilfen, die das Verst√§ndnis erleichtern.

### Voraussetzungen

Sie sollten inzwischen mit der Struktur der K√ºrbisdaten vertraut sein, die wir untersuchen. Sie finden sie vorab geladen und bereinigt in der Datei _notebook.ipynb_ dieser Lektion. In der Datei wird der K√ºrbispreis pro Scheffel in einem neuen DataFrame angezeigt. Stellen Sie sicher, dass Sie diese Notebooks in Visual Studio Code-Kernels ausf√ºhren k√∂nnen.

### Vorbereitung

Zur Erinnerung: Sie laden diese Daten, um Fragen dazu zu stellen.

- Wann ist die beste Zeit, K√ºrbisse zu kaufen? 
- Welchen Preis kann ich f√ºr eine Kiste Miniaturk√ºrbisse erwarten?
- Sollte ich sie in halben Scheffelk√∂rben oder in 1 1/9 Scheffelkisten kaufen?
Lassen Sie uns weiter in diese Daten eintauchen.

In der vorherigen Lektion haben Sie einen Pandas-DataFrame erstellt und ihn mit einem Teil des urspr√ºnglichen Datasets gef√ºllt, indem Sie die Preise pro Scheffel standardisiert haben. Dadurch konnten Sie jedoch nur etwa 400 Datenpunkte sammeln, und das nur f√ºr die Herbstmonate.

Werfen Sie einen Blick auf die Daten, die wir in dem begleitenden Notebook dieser Lektion vorab geladen haben. Die Daten sind vorab geladen, und ein erster Streudiagramm wurde erstellt, um Monatsdaten darzustellen. Vielleicht k√∂nnen wir durch eine gr√ºndlichere Bereinigung der Daten noch mehr Details √ºber die Natur der Daten erhalten.

## Eine lineare Regressionslinie

Wie Sie in Lektion 1 gelernt haben, besteht das Ziel einer linearen Regression darin, eine Linie zu zeichnen, um:

- **Beziehungen zwischen Variablen zu zeigen**. Die Beziehung zwischen Variablen darzustellen.
- **Vorhersagen zu treffen**. Genaue Vorhersagen dar√ºber zu machen, wo ein neuer Datenpunkt im Verh√§ltnis zu dieser Linie liegen w√ºrde.

Typisch f√ºr die **Methode der kleinsten Quadrate** ist es, diese Art von Linie zu zeichnen. Der Begriff "kleinste Quadrate" bedeutet, dass alle Datenpunkte um die Regressionslinie quadriert und dann addiert werden. Idealerweise ist diese endg√ºltige Summe so klein wie m√∂glich, da wir eine geringe Anzahl von Fehlern oder `kleinste Quadrate` w√ºnschen.

Wir tun dies, da wir eine Linie modellieren m√∂chten, die die geringste kumulative Entfernung von allen unseren Datenpunkten hat. Wir quadrieren die Terme vor dem Addieren, da uns die Gr√∂√üe der Abweichung wichtiger ist als ihre Richtung.

> **üßÆ Zeig mir die Mathematik** 
> 
> Diese Linie, die als _Linie der besten Anpassung_ bezeichnet wird, kann durch [eine Gleichung](https://en.wikipedia.org/wiki/Simple_linear_regression) ausgedr√ºckt werden: 
> 
> ```
> Y = a + bX
> ```
>
> `X` ist die 'erkl√§rende Variable'. `Y` ist die 'abh√§ngige Variable'. Die Steigung der Linie ist `b`, und `a` ist der y-Achsenabschnitt, der den Wert von `Y` angibt, wenn `X = 0`. 
>
>![Berechnung der Steigung](../../../../2-Regression/3-Linear/images/slope.png)
>
> Zuerst berechnen Sie die Steigung `b`. Infografik von [Jen Looper](https://twitter.com/jenlooper)
>
> Mit anderen Worten, und bezogen auf die urspr√ºngliche Frage zu den K√ºrbisdaten: "Vorhersage des Preises eines K√ºrbisses pro Scheffel nach Monat", w√ºrde sich `X` auf den Preis und `Y` auf den Verkaufsmonat beziehen. 
>
>![Vervollst√§ndigung der Gleichung](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Berechnen Sie den Wert von Y. Wenn Sie etwa 4 $ zahlen, muss es April sein! Infografik von [Jen Looper](https://twitter.com/jenlooper)
>
> Die Mathematik, die die Linie berechnet, muss die Steigung der Linie demonstrieren, die auch vom Achsenabschnitt abh√§ngt, oder wo `Y` liegt, wenn `X = 0`.
>
> Sie k√∂nnen die Methode zur Berechnung dieser Werte auf der Website [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) beobachten. Besuchen Sie auch [diesen Rechner f√ºr die Methode der kleinsten Quadrate](https://www.mathsisfun.com/data/least-squares-calculator.html), um zu sehen, wie die Werte der Zahlen die Linie beeinflussen.

## Korrelation

Ein weiterer Begriff, den Sie verstehen sollten, ist der **Korrelationskoeffizient** zwischen den gegebenen X- und Y-Variablen. Mit einem Streudiagramm k√∂nnen Sie diesen Koeffizienten schnell visualisieren. Ein Diagramm mit Datenpunkten, die in einer ordentlichen Linie verstreut sind, hat eine hohe Korrelation, aber ein Diagramm mit Datenpunkten, die √ºberall zwischen X und Y verstreut sind, hat eine niedrige Korrelation.

Ein gutes lineares Regressionsmodell ist eines, das einen hohen (n√§her an 1 als an 0) Korrelationskoeffizienten mit der Methode der kleinsten Quadrate und einer Regressionslinie hat.

‚úÖ F√ºhren Sie das begleitende Notebook dieser Lektion aus und betrachten Sie das Streudiagramm von Monat zu Preis. Scheint die Datenassoziation zwischen Monat und Preis f√ºr K√ºrbisverk√§ufe Ihrer visuellen Interpretation des Streudiagramms zufolge eine hohe oder niedrige Korrelation zu haben? √Ñndert sich das, wenn Sie eine feinere Messung anstelle von `Monat` verwenden, z. B. *Tag des Jahres* (d. h. Anzahl der Tage seit Jahresbeginn)?

Im folgenden Code nehmen wir an, dass wir die Daten bereinigt und einen DataFrame namens `new_pumpkins` erhalten haben, √§hnlich dem folgenden:

ID | Monat | TagDesJahres | Sorte | Stadt | Verpackung | Niedriger Preis | Hoher Preis | Preis
---|-------|--------------|-------|-------|------------|-----------------|-------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 Scheffelkisten | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 Scheffelkisten | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 Scheffelkisten | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 Scheffelkisten | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 Scheffelkisten | 15.0 | 15.0 | 13.636364

> Der Code zur Bereinigung der Daten ist verf√ºgbar in [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Wir haben die gleichen Bereinigungsschritte wie in der vorherigen Lektion durchgef√ºhrt und die Spalte `TagDesJahres` mit dem folgenden Ausdruck berechnet: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Jetzt, da Sie die Mathematik hinter der linearen Regression verstanden haben, lassen Sie uns ein Regressionsmodell erstellen, um zu sehen, ob wir vorhersagen k√∂nnen, welches K√ºrbispaket die besten K√ºrbispreise haben wird. Jemand, der K√ºrbisse f√ºr einen Feiertags-K√ºrbisstand kauft, k√∂nnte diese Informationen ben√∂tigen, um seine Eink√§ufe von K√ºrbispaketen f√ºr den Stand zu optimieren.

## Auf der Suche nach Korrelation

[![ML f√ºr Anf√§nger - Auf der Suche nach Korrelation: Der Schl√ºssel zur linearen Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML f√ºr Anf√§nger - Auf der Suche nach Korrelation: Der Schl√ºssel zur linearen Regression")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur Korrelation.

Aus der vorherigen Lektion haben Sie wahrscheinlich gesehen, dass der Durchschnittspreis f√ºr verschiedene Monate wie folgt aussieht:

<img alt="Durchschnittspreis nach Monat" src="../2-Data/images/barchart.png" width="50%"/>

Dies deutet darauf hin, dass es eine gewisse Korrelation geben sollte, und wir k√∂nnen versuchen, ein lineares Regressionsmodell zu trainieren, um die Beziehung zwischen `Monat` und `Preis` oder zwischen `TagDesJahres` und `Preis` vorherzusagen. Hier ist das Streudiagramm, das die letztere Beziehung zeigt:

<img alt="Streudiagramm von Preis vs. Tag des Jahres" src="images/scatter-dayofyear.png" width="50%" /> 

Lassen Sie uns sehen, ob es eine Korrelation gibt, indem wir die Funktion `corr` verwenden:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Es sieht so aus, als ob die Korrelation ziemlich gering ist, -0.15 nach `Monat` und -0.17 nach `TagDesMonats`, aber es k√∂nnte eine andere wichtige Beziehung geben. Es scheint, dass es verschiedene Preiscluster gibt, die mit verschiedenen K√ºrbissorten korrespondieren. Um diese Hypothese zu best√§tigen, lassen Sie uns jede K√ºrbiskategorie mit einer anderen Farbe darstellen. Indem wir einen `ax`-Parameter an die `scatter`-Plot-Funktion √ºbergeben, k√∂nnen wir alle Punkte im selben Diagramm darstellen:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Streudiagramm von Preis vs. Tag des Jahres" src="images/scatter-dayofyear-color.png" width="50%" /> 

Unsere Untersuchung legt nahe, dass die Sorte einen gr√∂√üeren Einfluss auf den Gesamtpreis hat als das tats√§chliche Verkaufsdatum. Wir k√∂nnen dies mit einem Balkendiagramm sehen:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Balkendiagramm von Preis vs. Sorte" src="images/price-by-variety.png" width="50%" /> 

Lassen Sie uns uns vorerst nur auf eine K√ºrbissorte, den 'Pie Type', konzentrieren und sehen, welchen Einfluss das Datum auf den Preis hat:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Streudiagramm von Preis vs. Tag des Jahres" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Wenn wir jetzt die Korrelation zwischen `Preis` und `TagDesJahres` mit der Funktion `corr` berechnen, erhalten wir etwa `-0.27` - was bedeutet, dass das Trainieren eines Vorhersagemodells sinnvoll ist.

> Bevor Sie ein lineares Regressionsmodell trainieren, ist es wichtig sicherzustellen, dass Ihre Daten sauber sind. Lineare Regression funktioniert nicht gut mit fehlenden Werten, daher ist es sinnvoll, alle leeren Zellen zu entfernen:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Eine andere Herangehensweise w√§re, diese leeren Werte mit den Mittelwerten der entsprechenden Spalte zu f√ºllen.

## Einfache lineare Regression

[![ML f√ºr Anf√§nger - Lineare und polynomiale Regression mit Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML f√ºr Anf√§nger - Lineare und polynomiale Regression mit Scikit-learn")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur linearen und polynomialen Regression.

Um unser lineares Regressionsmodell zu trainieren, verwenden wir die **Scikit-learn**-Bibliothek.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Wir beginnen damit, Eingabewerte (Features) und die erwartete Ausgabe (Label) in separate numpy-Arrays zu trennen:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Beachten Sie, dass wir `reshape` auf die Eingabedaten anwenden mussten, damit das Paket f√ºr lineare Regression sie korrekt versteht. Lineare Regression erwartet ein 2D-Array als Eingabe, wobei jede Zeile des Arrays einem Vektor von Eingabefeatures entspricht. In unserem Fall, da wir nur eine Eingabe haben, ben√∂tigen wir ein Array mit der Form N√ó1, wobei N die Gr√∂√üe des Datasets ist.

Dann m√ºssen wir die Daten in Trainings- und Testdatens√§tze aufteilen, damit wir unser Modell nach dem Training validieren k√∂nnen:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Schlie√ülich dauert das Training des eigentlichen linearen Regressionsmodells nur zwei Codezeilen. Wir definieren das `LinearRegression`-Objekt und passen es mit der Methode `fit` an unsere Daten an:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Das `LinearRegression`-Objekt enth√§lt nach dem `fit`-Vorgang alle Koeffizienten der Regression, die √ºber die Eigenschaft `.coef_` abgerufen werden k√∂nnen. In unserem Fall gibt es nur einen Koeffizienten, der etwa `-0.017` sein sollte. Das bedeutet, dass die Preise mit der Zeit etwas sinken, aber nicht zu stark, etwa um 2 Cent pro Tag. Wir k√∂nnen auch den Schnittpunkt der Regression mit der Y-Achse √ºber `lin_reg.intercept_` abrufen - er wird in unserem Fall etwa `21` betragen, was den Preis zu Jahresbeginn angibt.

Um zu sehen, wie genau unser Modell ist, k√∂nnen wir die Preise auf einem Testdatensatz vorhersagen und dann messen, wie nah unsere Vorhersagen an den erwarteten Werten liegen. Dies kann mit der Mean-Square-Error (MSE)-Metrik erfolgen, die den Mittelwert aller quadrierten Unterschiede zwischen erwartetem und vorhergesagtem Wert darstellt.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Unser Fehler scheint sich auf 2 Punkte zu konzentrieren, was etwa 17 % entspricht. Nicht besonders gut. Ein weiterer Indikator f√ºr die Modellqualit√§t ist der **Bestimmtheitskoeffizient**, der wie folgt berechnet werden kann:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```  
Wenn der Wert 0 ist, bedeutet das, dass das Modell die Eingabedaten nicht ber√ºcksichtigt und als *schlechtester linearer Pr√§diktor* agiert, was einfach der Mittelwert des Ergebnisses ist. Ein Wert von 1 bedeutet, dass wir alle erwarteten Ausgaben perfekt vorhersagen k√∂nnen. In unserem Fall liegt der Koeffizient bei etwa 0,06, was ziemlich niedrig ist.

Wir k√∂nnen auch die Testdaten zusammen mit der Regressionslinie plotten, um besser zu sehen, wie die Regression in unserem Fall funktioniert:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```  

<img alt="Lineare Regression" src="images/linear-results.png" width="50%" />

## Polynomiale Regression

Eine andere Art der linearen Regression ist die polynomiale Regression. W√§hrend es manchmal eine lineare Beziehung zwischen Variablen gibt ‚Äì je gr√∂√üer der K√ºrbis im Volumen, desto h√∂her der Preis ‚Äì k√∂nnen diese Beziehungen manchmal nicht als Ebene oder gerade Linie dargestellt werden.

‚úÖ Hier sind [einige weitere Beispiele](https://online.stat.psu.edu/stat501/lesson/9/9.8) f√ºr Daten, die polynomiale Regression verwenden k√∂nnten.

Schauen Sie sich die Beziehung zwischen Datum und Preis noch einmal an. Sieht dieses Streudiagramm so aus, als sollte es unbedingt durch eine gerade Linie analysiert werden? K√∂nnen Preise nicht schwanken? In diesem Fall k√∂nnen Sie polynomiale Regression ausprobieren.

‚úÖ Polynome sind mathematische Ausdr√ºcke, die aus einer oder mehreren Variablen und Koeffizienten bestehen k√∂nnen.

Die polynomiale Regression erstellt eine gekr√ºmmte Linie, um nichtlineare Daten besser anzupassen. In unserem Fall sollten wir, wenn wir eine quadrierte `DayOfYear`-Variable in die Eingabedaten aufnehmen, unsere Daten mit einer parabolischen Kurve anpassen k√∂nnen, die an einem bestimmten Punkt im Jahr ein Minimum hat.

Scikit-learn enth√§lt eine hilfreiche [Pipeline-API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), um verschiedene Schritte der Datenverarbeitung zu kombinieren. Eine **Pipeline** ist eine Kette von **Sch√§tzern**. In unserem Fall erstellen wir eine Pipeline, die zuerst polynomiale Merkmale zu unserem Modell hinzuf√ºgt und dann die Regression trainiert:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```  

Die Verwendung von `PolynomialFeatures(2)` bedeutet, dass wir alle Polynome zweiten Grades aus den Eingabedaten einbeziehen. In unserem Fall bedeutet das einfach `DayOfYear`<sup>2</sup>, aber bei zwei Eingabevariablen X und Y w√ºrde dies X<sup>2</sup>, XY und Y<sup>2</sup> hinzuf√ºgen. Wir k√∂nnen auch Polynome h√∂heren Grades verwenden, wenn wir m√∂chten.

Pipelines k√∂nnen genauso verwendet werden wie das urspr√ºngliche `LinearRegression`-Objekt, d. h. wir k√∂nnen die Pipeline `fit`ten und dann `predict` verwenden, um die Vorhersageergebnisse zu erhalten. Hier ist das Diagramm, das die Testdaten und die Ann√§herungskurve zeigt:

<img alt="Polynomiale Regression" src="images/poly-results.png" width="50%" />

Mit polynomialer Regression k√∂nnen wir einen etwas niedrigeren MSE und eine h√∂here Bestimmtheit erreichen, aber nicht signifikant. Wir m√ºssen andere Merkmale ber√ºcksichtigen!

> Sie k√∂nnen sehen, dass die minimalen K√ºrbispreise irgendwo um Halloween herum beobachtet werden. Wie k√∂nnen Sie das erkl√§ren?

üéÉ Herzlichen Gl√ºckwunsch, Sie haben gerade ein Modell erstellt, das helfen kann, den Preis von K√ºrbissen f√ºr Kuchen vorherzusagen. Sie k√∂nnten wahrscheinlich das gleiche Verfahren f√ºr alle K√ºrbissorten wiederholen, aber das w√§re m√ºhsam. Lernen wir jetzt, wie man K√ºrbissorten in unser Modell einbezieht!

## Kategorische Merkmale

In der idealen Welt m√∂chten wir in der Lage sein, Preise f√ºr verschiedene K√ºrbissorten mit demselben Modell vorherzusagen. Die Spalte `Variety` unterscheidet sich jedoch etwas von Spalten wie `Month`, da sie nicht-numerische Werte enth√§lt. Solche Spalten werden als **kategorisch** bezeichnet.

[![ML f√ºr Anf√§nger ‚Äì Kategorische Merkmalsvorhersagen mit linearer Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML f√ºr Anf√§nger ‚Äì Kategorische Merkmalsvorhersagen mit linearer Regression")

> üé• Klicken Sie auf das Bild oben f√ºr eine kurze Video√ºbersicht zur Verwendung kategorischer Merkmale.

Hier k√∂nnen Sie sehen, wie der Durchschnittspreis von der Sorte abh√§ngt:

<img alt="Durchschnittspreis nach Sorte" src="images/price-by-variety.png" width="50%" />

Um die Sorte zu ber√ºcksichtigen, m√ºssen wir sie zuerst in numerische Form umwandeln, also **kodieren**. Es gibt mehrere M√∂glichkeiten, dies zu tun:

* Eine einfache **numerische Kodierung** erstellt eine Tabelle mit verschiedenen Sorten und ersetzt dann den Sortennamen durch einen Index in dieser Tabelle. Dies ist keine gute Idee f√ºr die lineare Regression, da die lineare Regression den tats√§chlichen numerischen Wert des Indexes nimmt und ihn mit einem Koeffizienten multipliziert, um ihn zum Ergebnis hinzuzuf√ºgen. In unserem Fall ist die Beziehung zwischen der Indexnummer und dem Preis eindeutig nicht linear, selbst wenn wir sicherstellen, dass die Indizes in einer bestimmten Reihenfolge angeordnet sind.
* **One-Hot-Encoding** ersetzt die Spalte `Variety` durch 4 verschiedene Spalten, eine f√ºr jede Sorte. Jede Spalte enth√§lt `1`, wenn die entsprechende Zeile einer bestimmten Sorte entspricht, und `0` andernfalls. Das bedeutet, dass es in der linearen Regression vier Koeffizienten gibt, einen f√ºr jede K√ºrbissorte, die f√ºr den "Startpreis" (oder eher den "zus√§tzlichen Preis") f√ºr diese bestimmte Sorte verantwortlich sind.

Der folgende Code zeigt, wie wir eine Sorte mit One-Hot-Encoding kodieren k√∂nnen:

```python
pd.get_dummies(new_pumpkins['Variety'])
```  

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE  
----|-----------|-----------|--------------------------|----------  
70 | 0 | 0 | 0 | 1  
71 | 0 | 0 | 0 | 1  
... | ... | ... | ... | ...  
1738 | 0 | 1 | 0 | 0  
1739 | 0 | 1 | 0 | 0  
1740 | 0 | 1 | 0 | 0  
1741 | 0 | 1 | 0 | 0  
1742 | 0 | 1 | 0 | 0  

Um die lineare Regression mit One-Hot-kodierter Sorte als Eingabe zu trainieren, m√ºssen wir nur die `X`- und `y`-Daten korrekt initialisieren:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```  

Der Rest des Codes ist derselbe wie der, den wir oben verwendet haben, um die lineare Regression zu trainieren. Wenn Sie es ausprobieren, werden Sie sehen, dass der mittlere quadratische Fehler ungef√§hr gleich bleibt, aber wir erhalten einen viel h√∂heren Bestimmtheitskoeffizienten (~77 %). Um noch genauere Vorhersagen zu erhalten, k√∂nnen wir mehr kategorische Merkmale sowie numerische Merkmale wie `Month` oder `DayOfYear` ber√ºcksichtigen. Um ein gro√ües Array von Merkmalen zu erhalten, k√∂nnen wir `join` verwenden:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```  

Hier ber√ºcksichtigen wir auch `City` und `Package`-Typ, was uns einen MSE von 2,84 (10 %) und eine Bestimmtheit von 0,94 gibt!

## Alles zusammenf√ºgen

Um das beste Modell zu erstellen, k√∂nnen wir kombinierte (One-Hot-kodierte kategorische + numerische) Daten aus dem obigen Beispiel zusammen mit polynomialer Regression verwenden. Hier ist der vollst√§ndige Code f√ºr Ihre Bequemlichkeit:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```  

Dies sollte uns den besten Bestimmtheitskoeffizienten von fast 97 % und MSE=2,23 (~8 % Vorhersagefehler) geben.

| Modell | MSE | Bestimmtheit |  
|--------|-----|--------------|  
| `DayOfYear` Linear | 2,77 (17,2 %) | 0,07 |  
| `DayOfYear` Polynomial | 2,73 (17,0 %) | 0,08 |  
| `Variety` Linear | 5,24 (19,7 %) | 0,77 |  
| Alle Merkmale Linear | 2,84 (10,5 %) | 0,94 |  
| Alle Merkmale Polynomial | 2,23 (8,25 %) | 0,97 |  

üèÜ Gut gemacht! Sie haben in einer Lektion vier Regressionsmodelle erstellt und die Modellqualit√§t auf 97 % verbessert. Im letzten Abschnitt zur Regression lernen Sie die logistische Regression kennen, um Kategorien zu bestimmen.

---

## üöÄ Herausforderung

Testen Sie in diesem Notebook verschiedene Variablen, um zu sehen, wie die Korrelation mit der Modellgenauigkeit zusammenh√§ngt.

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

## R√ºckblick & Selbststudium

In dieser Lektion haben wir √ºber lineare Regression gelernt. Es gibt andere wichtige Arten der Regression. Lesen Sie √ºber Stepwise-, Ridge-, Lasso- und Elasticnet-Techniken. Ein guter Kurs, um mehr zu lernen, ist der [Stanford Statistical Learning Course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Aufgabe

[Erstellen Sie ein Modell](assignment.md)  

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.