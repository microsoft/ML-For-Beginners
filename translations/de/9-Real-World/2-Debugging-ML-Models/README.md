<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ba0f6e1019351351c8ee4c92867b6a0b",
  "translation_date": "2025-09-03T21:49:04+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "de"
}
-->
# Postskriptum: Modell-Debugging im maschinellen Lernen mit Komponenten des Responsible AI Dashboards

## [Quiz vor der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Einf√ºhrung

Maschinelles Lernen beeinflusst unser t√§gliches Leben. KI findet ihren Weg in einige der wichtigsten Systeme, die uns als Individuen und unsere Gesellschaft betreffen, wie Gesundheitswesen, Finanzen, Bildung und Besch√§ftigung. Beispielsweise sind Systeme und Modelle an allt√§glichen Entscheidungsprozessen beteiligt, wie etwa bei medizinischen Diagnosen oder der Betrugserkennung. Folglich werden die Fortschritte in der KI und deren beschleunigte Einf√ºhrung von sich entwickelnden gesellschaftlichen Erwartungen und wachsender Regulierung begleitet. Wir sehen st√§ndig Bereiche, in denen KI-Systeme weiterhin Erwartungen verfehlen; sie bringen neue Herausforderungen mit sich, und Regierungen beginnen, KI-L√∂sungen zu regulieren. Daher ist es wichtig, dass diese Modelle analysiert werden, um faire, zuverl√§ssige, inklusive, transparente und verantwortungsvolle Ergebnisse f√ºr alle zu gew√§hrleisten.

In diesem Lehrplan werden wir praktische Werkzeuge betrachten, die verwendet werden k√∂nnen, um zu beurteilen, ob ein Modell Probleme mit verantwortungsvoller KI hat. Traditionelle Debugging-Techniken im maschinellen Lernen basieren oft auf quantitativen Berechnungen wie aggregierter Genauigkeit oder durchschnittlichem Fehlerverlust. Stellen Sie sich vor, was passieren kann, wenn die Daten, die Sie zur Erstellung dieser Modelle verwenden, bestimmte demografische Merkmale wie Rasse, Geschlecht, politische Ansichten oder Religion nicht enthalten oder diese √ºberproportional repr√§sentieren. Was passiert, wenn die Ausgabe des Modells so interpretiert wird, dass sie eine bestimmte demografische Gruppe bevorzugt? Dies kann zu einer √úber- oder Unterrepr√§sentation dieser sensiblen Merkmale f√ºhren, was zu Problemen in Bezug auf Fairness, Inklusivit√§t oder Zuverl√§ssigkeit des Modells f√ºhrt. Ein weiterer Faktor ist, dass maschinelle Lernmodelle oft als Black Boxes betrachtet werden, was es schwierig macht, zu verstehen und zu erkl√§ren, was die Vorhersagen eines Modells antreibt. All dies sind Herausforderungen, denen sich Datenwissenschaftler und KI-Entwickler gegen√ºbersehen, wenn sie nicht √ºber geeignete Werkzeuge verf√ºgen, um die Fairness oder Vertrauensw√ºrdigkeit eines Modells zu debuggen und zu bewerten.

In dieser Lektion lernen Sie, wie Sie Ihre Modelle debuggen k√∂nnen, indem Sie:

- **Fehleranalyse**: Identifizieren, wo in Ihrer Datenverteilung das Modell hohe Fehlerraten aufweist.
- **Modell√ºbersicht**: Vergleichende Analysen √ºber verschiedene Datenkohorten durchf√ºhren, um Diskrepanzen in den Leistungsmetriken Ihres Modells zu entdecken.
- **Datenanalyse**: Untersuchen, wo es eine √úber- oder Unterrepr√§sentation Ihrer Daten geben k√∂nnte, die Ihr Modell dazu bringen kann, eine Daten-Demografie gegen√ºber einer anderen zu bevorzugen.
- **Feature-Wichtigkeit**: Verstehen, welche Merkmale die Vorhersagen Ihres Modells auf globaler oder lokaler Ebene beeinflussen.

## Voraussetzung

Als Voraussetzung lesen Sie bitte die √úbersicht [Responsible AI tools for developers](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![Gif zu Responsible AI Tools](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Fehleranalyse

Traditionelle Leistungsmetriken von Modellen zur Messung der Genauigkeit basieren meist auf Berechnungen von korrekten vs. falschen Vorhersagen. Beispielsweise kann die Feststellung, dass ein Modell zu 89 % genau ist und einen Fehlerverlust von 0,001 aufweist, als gute Leistung angesehen werden. Fehler sind jedoch oft nicht gleichm√§√üig in Ihrem zugrunde liegenden Datensatz verteilt. Sie k√∂nnten eine Modellgenauigkeit von 89 % erzielen, aber feststellen, dass es in verschiedenen Bereichen Ihrer Daten Gruppen gibt, bei denen das Modell zu 42 % fehlerhaft ist. Die Konsequenzen dieser Fehlermuster bei bestimmten Datengruppen k√∂nnen zu Problemen in Bezug auf Fairness oder Zuverl√§ssigkeit f√ºhren. Es ist entscheidend, Bereiche zu verstehen, in denen das Modell gut oder schlecht abschneidet. Die Datenbereiche, in denen Ihr Modell viele Ungenauigkeiten aufweist, k√∂nnten sich als wichtige demografische Daten herausstellen.

![Analyse und Debugging von Modellfehlern](../../../../translated_images/ea-error-distribution.117452e1177c1dd84fab2369967a68bcde787c76c6ea7fdb92fcf15d1fce8206.de.png)

Die Fehleranalyse-Komponente des RAI Dashboards zeigt, wie Modellfehler √ºber verschiedene Kohorten mit einer Baumvisualisierung verteilt sind. Dies ist n√ºtzlich, um Merkmale oder Bereiche zu identifizieren, in denen Ihre Daten eine hohe Fehlerrate aufweisen. Indem Sie sehen, wo die meisten Ungenauigkeiten des Modells auftreten, k√∂nnen Sie beginnen, die Ursache zu untersuchen. Sie k√∂nnen auch Datenkohorten erstellen, um Analysen durchzuf√ºhren. Diese Datenkohorten helfen im Debugging-Prozess, um festzustellen, warum die Modellleistung in einer Kohorte gut, aber in einer anderen fehlerhaft ist.

![Fehleranalyse](../../../../translated_images/ea-error-cohort.6886209ea5d438c4daa8bfbf5ce3a7042586364dd3eccda4a4e3d05623ac702a.de.png)

Die visuellen Indikatoren auf der Baumkarte helfen, Problemstellen schneller zu lokalisieren. Beispielsweise zeigt ein dunklerer Rotton eines Baumknotens eine h√∂here Fehlerrate an.

Eine Heatmap ist eine weitere Visualisierungsfunktion, die Benutzer verwenden k√∂nnen, um die Fehlerrate anhand eines oder zweier Merkmale zu untersuchen und so einen Beitrag zu den Modellfehlern √ºber den gesamten Datensatz oder Kohorten hinweg zu finden.

![Fehleranalyse Heatmap](../../../../translated_images/ea-heatmap.8d27185e28cee3830c85e1b2e9df9d2d5e5c8c940f41678efdb68753f2f7e56c.de.png)

Verwenden Sie die Fehleranalyse, wenn Sie:

* Ein tiefes Verst√§ndnis daf√ºr gewinnen m√∂chten, wie Modellfehler √ºber einen Datensatz und mehrere Eingabe- und Merkmalsdimensionen verteilt sind.
* Die aggregierten Leistungsmetriken aufschl√ºsseln m√∂chten, um fehlerhafte Kohorten automatisch zu entdecken und gezielte Ma√ünahmen zur Fehlerbehebung zu ergreifen.

## Modell√ºbersicht

Die Bewertung der Leistung eines maschinellen Lernmodells erfordert ein ganzheitliches Verst√§ndnis seines Verhaltens. Dies kann erreicht werden, indem mehr als eine Metrik wie Fehlerrate, Genauigkeit, Recall, Pr√§zision oder MAE (Mean Absolute Error) √ºberpr√ºft wird, um Diskrepanzen zwischen den Leistungsmetriken zu finden. Eine Leistungsmetrik mag gro√üartig aussehen, aber Ungenauigkeiten k√∂nnen in einer anderen Metrik aufgedeckt werden. Dar√ºber hinaus hilft der Vergleich der Metriken √ºber den gesamten Datensatz oder Kohorten hinweg, Licht darauf zu werfen, wo das Modell gut oder schlecht abschneidet. Dies ist besonders wichtig, um die Leistung des Modells zwischen sensiblen und unsensiblen Merkmalen (z. B. Rasse, Geschlecht oder Alter von Patienten) zu sehen, um potenzielle Unfairness des Modells aufzudecken. Beispielsweise kann die Entdeckung, dass das Modell in einer Kohorte mit sensiblen Merkmalen fehlerhafter ist, potenzielle Unfairness des Modells offenbaren.

Die Modell√ºbersicht-Komponente des RAI Dashboards hilft nicht nur bei der Analyse der Leistungsmetriken der Datenrepr√§sentation in einer Kohorte, sondern gibt den Benutzern auch die M√∂glichkeit, das Verhalten des Modells √ºber verschiedene Kohorten hinweg zu vergleichen.

![Datensatzkohorten - Modell√ºbersicht im RAI Dashboard](../../../../translated_images/model-overview-dataset-cohorts.dfa463fb527a35a0afc01b7b012fc87bf2cad756763f3652bbd810cac5d6cf33.de.png)

Die Funktionalit√§t der merkmalsbasierten Analyse der Komponente erm√∂glicht es Benutzern, Datenuntergruppen innerhalb eines bestimmten Merkmals einzugrenzen, um Anomalien auf granularer Ebene zu identifizieren. Beispielsweise verf√ºgt das Dashboard √ºber eine eingebaute Intelligenz, um automatisch Kohorten f√ºr ein vom Benutzer ausgew√§hltes Merkmal zu generieren (z. B. *"time_in_hospital < 3"* oder *"time_in_hospital >= 7"*). Dies erm√∂glicht es einem Benutzer, ein bestimmtes Merkmal aus einer gr√∂√üeren Datengruppe zu isolieren, um zu sehen, ob es ein Schl√ºsselbeeinflusser der fehlerhaften Ergebnisse des Modells ist.

![Merkmalskohorten - Modell√ºbersicht im RAI Dashboard](../../../../translated_images/model-overview-feature-cohorts.c5104d575ffd0c80b7ad8ede7703fab6166bfc6f9125dd395dcc4ace2f522f70.de.png)

Die Modell√ºbersicht-Komponente unterst√ºtzt zwei Klassen von Diskrepanzmetriken:

**Diskrepanz in der Modellleistung**: Diese Metriken berechnen die Diskrepanz (Unterschied) in den Werten der ausgew√§hlten Leistungsmetrik √ºber Untergruppen von Daten. Hier einige Beispiele:

* Diskrepanz in der Genauigkeitsrate
* Diskrepanz in der Fehlerrate
* Diskrepanz in der Pr√§zision
* Diskrepanz im Recall
* Diskrepanz im mittleren absoluten Fehler (MAE)

**Diskrepanz in der Auswahlrate**: Diese Metrik enth√§lt den Unterschied in der Auswahlrate (g√ºnstige Vorhersage) zwischen Untergruppen. Ein Beispiel hierf√ºr ist die Diskrepanz in den Kreditgenehmigungsraten. Auswahlrate bedeutet den Anteil der Datenpunkte in jeder Klasse, die als 1 klassifiziert werden (bei bin√§rer Klassifikation) oder die Verteilung der Vorhersagewerte (bei Regression).

## Datenanalyse

> "Wenn Sie die Daten lange genug foltern, werden sie alles gestehen" - Ronald Coase

Diese Aussage klingt extrem, aber es stimmt, dass Daten manipuliert werden k√∂nnen, um jede Schlussfolgerung zu unterst√ºtzen. Solche Manipulationen k√∂nnen manchmal unbeabsichtigt geschehen. Als Menschen haben wir alle Vorurteile, und es ist oft schwierig, bewusst zu erkennen, wann man Vorurteile in Daten einf√ºhrt. Fairness in KI und maschinellem Lernen zu garantieren bleibt eine komplexe Herausforderung.

Daten sind ein gro√ües blinder Fleck f√ºr traditionelle Leistungsmetriken von Modellen. Sie k√∂nnten hohe Genauigkeitswerte haben, aber dies spiegelt nicht immer die zugrunde liegende Datenverzerrung wider, die in Ihrem Datensatz vorhanden sein k√∂nnte. Beispielsweise k√∂nnte ein Datensatz von Mitarbeitern, der 27 % Frauen in F√ºhrungspositionen und 73 % M√§nner auf derselben Ebene enth√§lt, dazu f√ºhren, dass ein KI-Modell f√ºr Stellenanzeigen, das auf diesen Daten trainiert wurde, haupts√§chlich eine m√§nnliche Zielgruppe f√ºr F√ºhrungspositionen anspricht. Diese Ungleichheit in den Daten hat die Vorhersage des Modells verzerrt, sodass eine Geschlechterbevorzugung vorliegt. Dies zeigt ein Fairness-Problem, bei dem ein Geschlechterbias im KI-Modell vorhanden ist.

Die Datenanalyse-Komponente des RAI Dashboards hilft, Bereiche zu identifizieren, in denen es eine √úber- und Unterrepr√§sentation im Datensatz gibt. Sie hilft Benutzern, die Ursache von Fehlern und Fairness-Problemen zu diagnostizieren, die durch Datenungleichgewichte oder mangelnde Repr√§sentation einer bestimmten Datengruppe eingef√ºhrt wurden. Dies gibt Benutzern die M√∂glichkeit, Datens√§tze basierend auf vorhergesagten und tats√§chlichen Ergebnissen, Fehlergruppen und spezifischen Merkmalen zu visualisieren. Manchmal kann die Entdeckung einer unterrepr√§sentierten Datengruppe auch aufdecken, dass das Modell nicht gut lernt, was zu hohen Ungenauigkeiten f√ºhrt. Ein Modell mit Datenverzerrung ist nicht nur ein Fairness-Problem, sondern zeigt auch, dass das Modell nicht inklusiv oder zuverl√§ssig ist.

![Datenanalyse-Komponente im RAI Dashboard](../../../../translated_images/dataanalysis-cover.8d6d0683a70a5c1e274e5a94b27a71137e3d0a3b707761d7170eb340dd07f11d.de.png)

Verwenden Sie die Datenanalyse, wenn Sie:

* Die Statistiken Ihres Datensatzes erkunden m√∂chten, indem Sie verschiedene Filter ausw√§hlen, um Ihre Daten in verschiedene Dimensionen (auch Kohorten genannt) aufzuteilen.
* Die Verteilung Ihres Datensatzes √ºber verschiedene Kohorten und Merkmalsgruppen verstehen m√∂chten.
* Feststellen m√∂chten, ob Ihre Erkenntnisse zu Fairness, Fehleranalyse und Kausalit√§t (abgeleitet aus anderen Dashboard-Komponenten) auf die Verteilung Ihres Datensatzes zur√ºckzuf√ºhren sind.
* Entscheiden m√∂chten, in welchen Bereichen Sie mehr Daten sammeln sollten, um Fehler zu beheben, die durch Repr√§sentationsprobleme, Label-Rauschen, Merkmalsrauschen, Label-Bias und √§hnliche Faktoren entstehen.

## Modellinterpretierbarkeit

Maschinelle Lernmodelle neigen dazu, Black Boxes zu sein. Es kann schwierig sein zu verstehen, welche Schl√ºsselmerkmale die Vorhersagen eines Modells antreiben. Es ist wichtig, Transparenz dar√ºber zu schaffen, warum ein Modell eine bestimmte Vorhersage trifft. Beispielsweise sollte ein KI-System, das vorhersagt, dass ein Diabetespatient Gefahr l√§uft, innerhalb von weniger als 30 Tagen wieder ins Krankenhaus eingeliefert zu werden, unterst√ºtzende Daten liefern k√∂nnen, die zu seiner Vorhersage gef√ºhrt haben. Das Vorhandensein unterst√ºtzender Datenindikatoren bringt Transparenz, die Kliniken oder Krankenh√§usern hilft, fundierte Entscheidungen zu treffen. Dar√ºber hinaus erm√∂glicht die F√§higkeit, zu erkl√§ren, warum ein Modell eine Vorhersage f√ºr einen einzelnen Patienten getroffen hat, Verantwortlichkeit im Hinblick auf Gesundheitsvorschriften. Wenn Sie maschinelle Lernmodelle auf eine Weise verwenden, die das Leben von Menschen beeinflusst, ist es entscheidend zu verstehen und zu erkl√§ren, was das Verhalten eines Modells beeinflusst. Modell-Erkl√§rbarkeit und Interpretierbarkeit helfen, Fragen in Szenarien wie diesen zu beantworten:

* Modell-Debugging: Warum hat mein Modell diesen Fehler gemacht? Wie kann ich mein Modell verbessern?
* Mensch-KI-Zusammenarbeit: Wie kann ich die Entscheidungen des Modells verstehen und ihm vertrauen?
* Gesetzliche Einhaltung: Erf√ºllt mein Modell die gesetzlichen Anforderungen?

Die Feature-Wichtigkeit-Komponente des RAI Dashboards hilft Ihnen, Ihr Modell zu debuggen und ein umfassendes Verst√§ndnis daf√ºr zu gewinnen, wie ein Modell Vorhersagen trifft. Es ist auch ein n√ºtzliches Werkzeug f√ºr Fachleute im maschinellen Lernen und Entscheidungstr√§ger, um zu erkl√§ren und Beweise f√ºr Merkmale zu liefern, die das Verhalten eines Modells beeinflussen, um gesetzliche Anforderungen zu erf√ºllen. Benutzer k√∂nnen sowohl globale als auch lokale Erkl√§rungen untersuchen, um zu validieren, welche Merkmale die Vorhersagen eines Modells beeinflussen. Globale Erkl√§rungen listen die wichtigsten Merkmale auf, die die Gesamtvorhersage eines Modells beeinflusst haben. Lokale Erkl√§rungen zeigen, welche Merkmale zu einer Vorhersage des Modells f√ºr einen einzelnen Fall gef√ºhrt haben. Die F√§higkeit, lokale Erkl√§rungen zu bewerten, ist auch hilfreich beim Debugging oder bei der Pr√ºfung eines bestimmten Falls, um besser zu verstehen und zu interpretieren, warum ein Modell eine korrekte oder fehlerhafte Vorhersage getroffen hat.

![Feature-Wichtigkeit-Komponente des RAI Dashboards](../../../../translated_images/9-feature-importance.cd3193b4bba3fd4bccd415f566c2437fb3298c4824a3dabbcab15270d783606e.de.png)

* Globale Erkl√§rungen: Zum Beispiel, welche Merkmale beeinflussen das Gesamtverhalten eines Modells zur Krankenhauswiederaufnahme von Diabetespatienten?
* Lokale Erkl√§rungen: Zum Beispiel, warum wurde ein Diabetespatient √ºber 60 Jahre mit vorherigen Krankenhausaufenthalten vorhergesagt, innerhalb von 30 Tagen wieder eingeliefert oder nicht eingeliefert zu werden?

Im Debugging-Prozess der Untersuchung der Modellleistung √ºber verschiedene Kohorten zeigt die Feature-Wichtigkeit, welchen Einfluss ein Merkmal auf die Kohorten hat. Sie hilft, Anomalien aufzudecken, wenn man den Einflussgrad des Merkmals vergleicht, das die fehlerhaften Vorhersagen des Modells antreibt. Die Feature-Wichtigkeit-Komponente kann zeigen, welche Werte in einem Merkmal die Ergebnisse des Modells positiv oder negativ beeinflusst haben. Wenn ein Modell beispielsweise eine fehlerhafte Vorhersage gemacht hat, gibt die Komponente Ihnen die M√∂glichkeit, ins Detail zu gehen und zu bestimmen, welche Merkmale oder Merkmalswerte die Vorhersage beeinflusst haben. Diese Detailgenauigkeit hilft nicht nur beim Debugging, sondern bietet auch Transparenz und Verantwortlichkeit in Pr√ºfungssituationen. Schlie√ülich kann die Komponente helfen, Fairness-Probleme zu identifizieren. Wenn ein sensibles Merkmal wie Ethnizit√§t oder Geschlecht einen hohen Einfluss auf die Vorhersage eines Modells hat, k√∂nnte dies ein Hinweis auf Rassen- oder Geschlechterbias im Modell sein.

![Feature-Wichtigkeit](../../../../translated_images/9-features-influence.3ead3d3f68a84029f1e40d3eba82107445d3d3b6975d4682b23d8acc905da6d0.de.png)

Verwenden Sie Interpretierbarkeit, wenn Sie:

* Bestimmen m√∂chten, wie vertrauensw√ºrdig die Vorhersagen Ihres KI-Systems sind, indem Sie verstehen, welche Merkmale f√ºr die Vorhersagen am wichtigsten sind.
* Das Debugging Ihres Modells angehen m√∂chten, indem Sie es zuerst verstehen und feststellen, ob das Modell gesunde Merkmale verwendet oder lediglich falsche Korrelationen.
* Potenzielle Quellen von Unfairness aufdecken m√∂chten, indem Sie verstehen, ob das Modell Vorhersagen auf sensiblen Merkmalen oder auf Merkmalen, die stark mit ihnen korreliert sind, basiert.
* Das Vertrauen der Benutzer in die Entscheidungen Ihres Modells aufbauen m√∂chten, indem Sie lokale Erkl√§rungen generieren, um deren Ergebnisse zu veranschaulichen.
* Eine gesetzliche Pr√ºfung eines KI-Systems abschlie√üen m√∂chten, um Modelle zu validieren und die Auswirkungen von Modellentscheidungen auf Menschen zu √ºberwachen.

## Fazit

Alle Komponenten des RAI Dashboards sind praktische Werkzeuge, die Ihnen helfen, maschinelle Lernmodelle zu erstellen, die weniger sch√§dlich und vertrauensw√ºrdiger f√ºr die Gesellschaft sind. Sie verbessern die Pr√§vention von Bedrohungen f√ºr Menschenrechte; Diskriminierung oder Ausschluss bestimmter Gruppen von Lebensm√∂glichkeiten; und das Risiko physischer oder psychologischer Sch√§den. Sie helfen auch, Vertrauen in die Entscheidungen Ihres Modells aufzubauen, indem sie lokale Erkl√§rungen generieren, um deren Ergebnisse zu veranschaulichen. Einige der potenziellen Sch√§den k√∂nnen wie folgt klassifiziert werden:

- **Zuweisung**, wenn beispielsweise ein Geschlecht oder eine Ethnizit√§t gegen√ºber einer anderen bevorzugt wird.
- **Qualit√§t des Dienstes**. Wenn Sie die Daten f√ºr ein spezifisches Szenario trainieren, aber die Realit√§t viel komplexer ist, f√ºhrt dies zu einem schlecht funktionierenden Dienst.
- **Stereotypisierung**. Die Zuordnung einer bestimmten Gruppe zu vorgegebenen Eigenschaften.
- **Herabsetzung**. Unfaire Kritik und Etikettierung von etwas oder jemandem.
- **√úber- oder Unterrepr√§sentation**. Die Idee dahinter ist, dass eine bestimmte Gruppe in einem bestimmten Beruf nicht vertreten ist, und jede Dienstleistung oder Funktion, die dies weiterhin f√∂rdert, tr√§gt zu Schaden bei.

### Azure RAI-Dashboard

Das [Azure RAI-Dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu), das auf Open-Source-Tools f√ºhrender akademischer Institutionen und Organisationen, einschlie√ülich Microsoft, basiert, ist entscheidend f√ºr Datenwissenschaftler und KI-Entwickler, um das Verhalten von Modellen besser zu verstehen, unerw√ºnschte Probleme in KI-Modellen zu erkennen und zu beheben.

- Erfahren Sie, wie Sie die verschiedenen Komponenten nutzen k√∂nnen, indem Sie die [Dokumentation des RAI-Dashboards](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) lesen.

- Schauen Sie sich einige [Beispiel-Notebooks des RAI-Dashboards](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) an, um verantwortungsvollere KI-Szenarien in Azure Machine Learning zu debuggen.

---
## üöÄ Herausforderung

Um statistische oder datenbezogene Verzerrungen von Anfang an zu vermeiden, sollten wir:

- eine Vielfalt an Hintergr√ºnden und Perspektiven unter den Menschen haben, die an den Systemen arbeiten
- in Datens√§tze investieren, die die Vielfalt unserer Gesellschaft widerspiegeln
- bessere Methoden entwickeln, um Verzerrungen zu erkennen und zu korrigieren, wenn sie auftreten

Denken Sie √ºber reale Szenarien nach, in denen Ungerechtigkeit beim Modellaufbau und der Nutzung offensichtlich ist. Was sollten wir noch ber√ºcksichtigen?

## [Quiz nach der Vorlesung](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## √úberpr√ºfung & Selbststudium

In dieser Lektion haben Sie einige praktische Werkzeuge kennengelernt, um verantwortungsvolle KI in maschinelles Lernen zu integrieren.

Sehen Sie sich diesen Workshop an, um tiefer in die Themen einzutauchen:

- Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die praktische Umsetzung von RAI von Besmira Nushi und Mehrnoosh Sameki

[![Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die praktische Umsetzung von RAI](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die praktische Umsetzung von RAI")

> üé• Klicken Sie auf das Bild oben f√ºr ein Video: Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die praktische Umsetzung von RAI von Besmira Nushi und Mehrnoosh Sameki

Nutzen Sie die folgenden Materialien, um mehr √ºber verantwortungsvolle KI zu erfahren und vertrauensw√ºrdigere Modelle zu entwickeln:

- Microsofts RAI-Dashboard-Tools zur Fehlerbehebung bei ML-Modellen: [Ressourcen f√ºr Responsible AI-Tools](https://aka.ms/rai-dashboard)

- Erkunden Sie das Responsible AI Toolkit: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsofts RAI-Ressourcenzentrum: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE-Forschungsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Aufgabe

[Erkunden Sie das RAI-Dashboard](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.