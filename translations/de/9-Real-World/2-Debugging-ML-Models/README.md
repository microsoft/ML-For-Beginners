<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-04T21:57:35+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "de"
}
-->
# Postskriptum: Modell-Debugging im maschinellen Lernen mit Komponenten des Responsible AI Dashboards

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)

## Einf√ºhrung

Maschinelles Lernen beeinflusst unser t√§gliches Leben. KI findet ihren Weg in einige der wichtigsten Systeme, die uns als Individuen und unsere Gesellschaft betreffen, wie Gesundheitswesen, Finanzen, Bildung und Besch√§ftigung. Beispielsweise sind Systeme und Modelle an allt√§glichen Entscheidungsprozessen beteiligt, wie Diagnosen im Gesundheitswesen oder der Betrugserkennung. Folglich werden die Fortschritte in der KI und ihre beschleunigte Einf√ºhrung von sich entwickelnden gesellschaftlichen Erwartungen und wachsender Regulierung begleitet. Immer wieder sehen wir Bereiche, in denen KI-Systeme Erwartungen nicht erf√ºllen, neue Herausforderungen aufzeigen und Regierungen beginnen, KI-L√∂sungen zu regulieren. Daher ist es wichtig, diese Modelle zu analysieren, um faire, zuverl√§ssige, inklusive, transparente und verantwortungsvolle Ergebnisse f√ºr alle zu gew√§hrleisten.

In diesem Lehrplan werden wir uns praktische Werkzeuge ansehen, die verwendet werden k√∂nnen, um zu beurteilen, ob ein Modell Probleme im Bereich der verantwortungsvollen KI aufweist. Traditionelle Debugging-Techniken im maschinellen Lernen basieren oft auf quantitativen Berechnungen wie aggregierter Genauigkeit oder durchschnittlichem Fehlerverlust. Stellen Sie sich vor, was passieren kann, wenn die Daten, die Sie zur Erstellung dieser Modelle verwenden, bestimmte demografische Gruppen wie Rasse, Geschlecht, politische Ansichten oder Religion nicht enthalten oder diese unverh√§ltnism√§√üig stark repr√§sentieren. Was ist, wenn die Ausgabe des Modells so interpretiert wird, dass sie eine bestimmte demografische Gruppe bevorzugt? Dies kann zu einer √úber- oder Unterrepr√§sentation dieser sensiblen Merkmale f√ºhren, was zu Problemen in Bezug auf Fairness, Inklusivit√§t oder Zuverl√§ssigkeit des Modells f√ºhrt. Ein weiterer Faktor ist, dass maschinelle Lernmodelle oft als Blackboxen betrachtet werden, was es schwierig macht, die treibenden Faktoren hinter den Vorhersagen eines Modells zu verstehen und zu erkl√§ren. All dies sind Herausforderungen, denen sich Datenwissenschaftler und KI-Entwickler stellen m√ºssen, wenn sie nicht √ºber geeignete Werkzeuge verf√ºgen, um die Fairness oder Vertrauensw√ºrdigkeit eines Modells zu debuggen und zu bewerten.

In dieser Lektion lernen Sie, wie Sie Ihre Modelle debuggen k√∂nnen, indem Sie:

- **Fehleranalyse**: Identifizieren, in welchen Bereichen Ihrer Datenverteilung das Modell hohe Fehlerraten aufweist.
- **Modell√ºbersicht**: Vergleichende Analysen √ºber verschiedene Datenkohorten durchf√ºhren, um Diskrepanzen in den Leistungsmetriken Ihres Modells zu entdecken.
- **Datenanalyse**: Untersuchen, wo es eine √úber- oder Unterrepr√§sentation Ihrer Daten geben k√∂nnte, die Ihr Modell dazu verleiten kann, eine Daten-Demografie gegen√ºber einer anderen zu bevorzugen.
- **Feature-Wichtigkeit**: Verstehen, welche Merkmale die Vorhersagen Ihres Modells auf globaler oder lokaler Ebene beeinflussen.

## Voraussetzungen

Als Voraussetzung lesen Sie bitte die √úbersicht [Responsible AI tools for developers](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard).

> ![Gif zu Responsible AI Tools](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## Fehleranalyse

Traditionelle Leistungsmetriken von Modellen zur Messung der Genauigkeit basieren meist auf Berechnungen von korrekten vs. falschen Vorhersagen. Zum Beispiel kann ein Modell, das zu 89 % genau ist und einen Fehlerverlust von 0,001 aufweist, als leistungsstark angesehen werden. Fehler sind jedoch oft nicht gleichm√§√üig in Ihrem zugrunde liegenden Datensatz verteilt. Sie k√∂nnten eine Modellgenauigkeit von 89 % erzielen, aber feststellen, dass es in bestimmten Bereichen Ihrer Daten Regionen gibt, in denen das Modell zu 42 % fehlerhaft ist. Die Konsequenzen dieser Fehlermuster bei bestimmten Datengruppen k√∂nnen zu Problemen in Bezug auf Fairness oder Zuverl√§ssigkeit f√ºhren. Es ist entscheidend, die Bereiche zu verstehen, in denen das Modell gut oder schlecht abschneidet. Die Datenregionen, in denen Ihr Modell viele Ungenauigkeiten aufweist, k√∂nnten sich als wichtige demografische Daten herausstellen.

![Analyse und Debugging von Modellfehlern](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

Die Fehleranalyse-Komponente des RAI Dashboards zeigt, wie Modellfehler √ºber verschiedene Kohorten hinweg mit einer Baumvisualisierung verteilt sind. Dies ist n√ºtzlich, um Merkmale oder Bereiche zu identifizieren, in denen Ihre Daten eine hohe Fehlerrate aufweisen. Indem Sie sehen, woher die meisten Ungenauigkeiten des Modells stammen, k√∂nnen Sie beginnen, die Ursache zu untersuchen. Sie k√∂nnen auch Datenkohorten erstellen, um Analysen durchzuf√ºhren. Diese Datenkohorten helfen im Debugging-Prozess, um festzustellen, warum die Modellleistung in einer Kohorte gut, in einer anderen jedoch fehlerhaft ist.

![Fehleranalyse](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

Die visuellen Indikatoren in der Baumkarte helfen, Problemstellen schneller zu lokalisieren. Zum Beispiel zeigt ein dunklerer Rotton eines Baumknotens eine h√∂here Fehlerrate an.

Eine weitere Visualisierungsfunktion ist die Heatmap, mit der Benutzer die Fehlerrate anhand eines oder zweier Merkmale untersuchen k√∂nnen, um einen Beitrag zu den Modellfehlern im gesamten Datensatz oder in Kohorten zu finden.

![Fehleranalyse Heatmap](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

Verwenden Sie die Fehleranalyse, wenn Sie:

* Ein tiefes Verst√§ndnis daf√ºr gewinnen m√∂chten, wie Modellfehler √ºber einen Datensatz und mehrere Eingabe- und Merkmalsdimensionen verteilt sind.
* Die aggregierten Leistungsmetriken aufschl√ºsseln m√∂chten, um fehlerhafte Kohorten automatisch zu entdecken und gezielte Ma√ünahmen zur Behebung zu ergreifen.

## Modell√ºbersicht

Die Bewertung der Leistung eines maschinellen Lernmodells erfordert ein ganzheitliches Verst√§ndnis seines Verhaltens. Dies kann erreicht werden, indem mehr als eine Metrik wie Fehlerrate, Genauigkeit, Recall, Pr√§zision oder MAE (Mean Absolute Error) √ºberpr√ºft wird, um Diskrepanzen zwischen den Leistungsmetriken zu finden. Eine Leistungsmetrik mag gro√üartig aussehen, aber Ungenauigkeiten k√∂nnen in einer anderen Metrik aufgedeckt werden. Dar√ºber hinaus hilft der Vergleich der Metriken √ºber den gesamten Datensatz oder Kohorten hinweg, Licht darauf zu werfen, wo das Modell gut oder schlecht abschneidet. Dies ist besonders wichtig, um die Leistung des Modells bei sensiblen vs. unsensiblen Merkmalen (z. B. ethnische Zugeh√∂rigkeit, Geschlecht oder Alter von Patienten) zu sehen, um potenzielle Unfairness des Modells aufzudecken. Zum Beispiel kann die Entdeckung, dass das Modell in einer Kohorte mit sensiblen Merkmalen fehlerhafter ist, potenzielle Unfairness aufzeigen.

Die Modell√ºbersicht-Komponente des RAI Dashboards hilft nicht nur bei der Analyse der Leistungsmetriken der Datenrepr√§sentation in einer Kohorte, sondern gibt Benutzern auch die M√∂glichkeit, das Verhalten des Modells √ºber verschiedene Kohorten hinweg zu vergleichen.

![Datensatzkohorten - Modell√ºbersicht im RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

Die funktionsbasierte Analysefunktion der Komponente erm√∂glicht es Benutzern, Datensubgruppen innerhalb eines bestimmten Merkmals einzugrenzen, um Anomalien auf granularer Ebene zu identifizieren. Beispielsweise verf√ºgt das Dashboard √ºber eine eingebaute Intelligenz, um Kohorten f√ºr ein vom Benutzer ausgew√§hltes Merkmal automatisch zu generieren (z. B. *"time_in_hospital < 3"* oder *"time_in_hospital >= 7"*). Dies erm√∂glicht es einem Benutzer, ein bestimmtes Merkmal aus einer gr√∂√üeren Datengruppe zu isolieren, um zu sehen, ob es ein Schl√ºsselfaktor f√ºr die fehlerhaften Ergebnisse des Modells ist.

![Merkmalskohorten - Modell√ºbersicht im RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

Die Modell√ºbersicht-Komponente unterst√ºtzt zwei Klassen von Diskrepanzmetriken:

**Diskrepanz in der Modellleistung**: Diese Metriken berechnen die Diskrepanz (Differenz) in den Werten der ausgew√§hlten Leistungsmetrik √ºber Untergruppen von Daten. Hier einige Beispiele:

* Diskrepanz in der Genauigkeitsrate
* Diskrepanz in der Fehlerrate
* Diskrepanz in der Pr√§zision
* Diskrepanz im Recall
* Diskrepanz im mittleren absoluten Fehler (MAE)

**Diskrepanz in der Auswahlrate**: Diese Metrik enth√§lt die Differenz in der Auswahlrate (g√ºnstige Vorhersage) zwischen Untergruppen. Ein Beispiel hierf√ºr ist die Diskrepanz in den Kreditgenehmigungsraten. Die Auswahlrate bezeichnet den Anteil der Datenpunkte in jeder Klasse, die als 1 klassifiziert werden (bei bin√§rer Klassifikation) oder die Verteilung der Vorhersagewerte (bei Regression).

## Datenanalyse

> "Wenn man Daten lange genug foltert, gestehen sie alles" - Ronald Coase

Diese Aussage klingt extrem, aber es stimmt, dass Daten manipuliert werden k√∂nnen, um jede Schlussfolgerung zu unterst√ºtzen. Eine solche Manipulation kann manchmal unbeabsichtigt geschehen. Als Menschen haben wir alle Vorurteile, und es ist oft schwierig, bewusst zu erkennen, wann man Vorurteile in Daten einf√ºhrt. Fairness in KI und maschinellem Lernen zu gew√§hrleisten, bleibt eine komplexe Herausforderung.

Daten sind ein gro√üer blinder Fleck f√ºr traditionelle Modellleistungsmetriken. Sie k√∂nnen hohe Genauigkeitswerte haben, aber das spiegelt nicht immer die zugrunde liegenden Datenverzerrungen wider, die in Ihrem Datensatz vorhanden sein k√∂nnten. Zum Beispiel, wenn ein Datensatz von Mitarbeitern 27 % Frauen in F√ºhrungspositionen und 73 % M√§nner auf derselben Ebene enth√§lt, k√∂nnte ein auf diesen Daten trainiertes Stellenanzeigen-KI-Modell haupts√§chlich ein m√§nnliches Publikum f√ºr F√ºhrungspositionen ansprechen. Dieses Ungleichgewicht in den Daten hat die Vorhersage des Modells verzerrt, sodass eine Geschlechterpr√§ferenz entsteht. Dies zeigt ein Fairness-Problem, bei dem ein Geschlechterbias im KI-Modell vorliegt.

Die Datenanalyse-Komponente des RAI Dashboards hilft, Bereiche zu identifizieren, in denen es eine √úber- oder Unterrepr√§sentation im Datensatz gibt. Sie hilft Benutzern, die Ursache von Fehlern und Fairness-Problemen zu diagnostizieren, die durch Datenungleichgewichte oder mangelnde Repr√§sentation einer bestimmten Datengruppe entstehen. Dies gibt Benutzern die M√∂glichkeit, Datens√§tze basierend auf vorhergesagten und tats√§chlichen Ergebnissen, Fehlergruppen und spezifischen Merkmalen zu visualisieren. Manchmal kann die Entdeckung einer unterrepr√§sentierten Datengruppe auch aufzeigen, dass das Modell nicht gut lernt, was zu hohen Ungenauigkeiten f√ºhrt. Ein Modell mit Datenbias ist nicht nur ein Fairness-Problem, sondern zeigt auch, dass das Modell nicht inklusiv oder zuverl√§ssig ist.

![Datenanalyse-Komponente im RAI Dashboard](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

Verwenden Sie die Datenanalyse, wenn Sie:

* Statistiken Ihres Datensatzes erkunden m√∂chten, indem Sie verschiedene Filter ausw√§hlen, um Ihre Daten in verschiedene Dimensionen (auch Kohorten genannt) aufzuteilen.
* Die Verteilung Ihres Datensatzes √ºber verschiedene Kohorten und Merkmalsgruppen hinweg verstehen m√∂chten.
* Feststellen m√∂chten, ob Ihre Erkenntnisse zu Fairness, Fehleranalyse und Kausalit√§t (abgeleitet aus anderen Dashboard-Komponenten) auf der Verteilung Ihres Datensatzes basieren.
* Entscheiden m√∂chten, in welchen Bereichen Sie mehr Daten sammeln sollten, um Fehler zu mindern, die durch Repr√§sentationsprobleme, Label-Rauschen, Merkmalsrauschen, Label-Bias und √§hnliche Faktoren entstehen.

## Modellinterpretierbarkeit

Maschinelle Lernmodelle werden oft als Blackboxen betrachtet. Es kann schwierig sein zu verstehen, welche Schl√ºsseldatenmerkmale die Vorhersagen eines Modells antreiben. Es ist wichtig, Transparenz dar√ºber zu schaffen, warum ein Modell eine bestimmte Vorhersage trifft. Zum Beispiel, wenn ein KI-System vorhersagt, dass ein Diabetespatient ein Risiko hat, innerhalb von weniger als 30 Tagen wieder ins Krankenhaus eingeliefert zu werden, sollte es unterst√ºtzende Daten liefern k√∂nnen, die zu seiner Vorhersage gef√ºhrt haben. Solche unterst√ºtzenden Datenindikatoren schaffen Transparenz, um Kliniken oder Krankenh√§usern zu helfen, fundierte Entscheidungen zu treffen. Dar√ºber hinaus erm√∂glicht die F√§higkeit, zu erkl√§ren, warum ein Modell eine Vorhersage f√ºr einen einzelnen Patienten getroffen hat, die Einhaltung von Gesundheitsvorschriften. Wenn Sie maschinelle Lernmodelle in Bereichen einsetzen, die das Leben von Menschen betreffen, ist es entscheidend, das Verhalten eines Modells zu verstehen und zu erkl√§ren. Modell-Erkl√§rbarkeit und -Interpretierbarkeit hilft, Fragen in Szenarien wie diesen zu beantworten:

* Modell-Debugging: Warum hat mein Modell diesen Fehler gemacht? Wie kann ich mein Modell verbessern?
* Mensch-KI-Zusammenarbeit: Wie kann ich die Entscheidungen des Modells verstehen und ihm vertrauen?
* Gesetzliche Anforderungen: Erf√ºllt mein Modell die rechtlichen Vorgaben?

Die Feature-Wichtigkeit-Komponente des RAI Dashboards hilft Ihnen, Ihr Modell zu debuggen und ein umfassendes Verst√§ndnis daf√ºr zu gewinnen, wie ein Modell Vorhersagen trifft. Sie ist auch ein n√ºtzliches Werkzeug f√ºr Fachleute im maschinellen Lernen und Entscheidungstr√§ger, um zu erkl√§ren und nachzuweisen, welche Merkmale das Verhalten eines Modells beeinflussen, um gesetzliche Anforderungen zu erf√ºllen. Benutzer k√∂nnen sowohl globale als auch lokale Erkl√§rungen untersuchen, um zu validieren, welche Merkmale die Vorhersagen eines Modells antreiben. Globale Erkl√§rungen listen die wichtigsten Merkmale auf, die die Gesamtvorhersage eines Modells beeinflusst haben. Lokale Erkl√§rungen zeigen, welche Merkmale zu einer Vorhersage des Modells f√ºr einen einzelnen Fall gef√ºhrt haben. Die M√∂glichkeit, lokale Erkl√§rungen zu bewerten, ist auch hilfreich beim Debugging oder bei der Pr√ºfung eines bestimmten Falls, um besser zu verstehen und zu interpretieren, warum ein Modell eine korrekte oder fehlerhafte Vorhersage getroffen hat.

![Feature-Wichtigkeit-Komponente des RAI Dashboards](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* Globale Erkl√§rungen: Zum Beispiel, welche Merkmale beeinflussen das Gesamtverhalten eines Modells zur Vorhersage von Krankenhauswiedereinweisungen bei Diabetes?
* Lokale Erkl√§rungen: Zum Beispiel, warum wurde ein Diabetespatient √ºber 60 Jahre mit vorherigen Krankenhausaufenthalten vorhergesagt, innerhalb von 30 Tagen wieder oder nicht wieder ins Krankenhaus eingeliefert zu werden?

Im Debugging-Prozess, bei dem die Leistung eines Modells √ºber verschiedene Kohorten untersucht wird, zeigt die Feature-Wichtigkeit, wie stark ein Merkmal die Kohorten beeinflusst. Sie hilft, Anomalien aufzudecken, wenn man den Einfluss eines Merkmals auf die fehlerhaften Vorhersagen eines Modells vergleicht. Die Feature-Wichtigkeit-Komponente kann zeigen, welche Werte in einem Merkmal die Ergebnisse des Modells positiv oder negativ beeinflusst haben. Wenn ein Modell beispielsweise eine fehlerhafte Vorhersage gemacht hat, gibt die Komponente Ihnen die M√∂glichkeit, ins Detail zu gehen und herauszufinden, welche Merkmale oder Merkmalswerte die Vorhersage beeinflusst haben. Dieses Detailniveau hilft nicht nur beim Debugging, sondern bietet auch Transparenz und Verantwortlichkeit in Pr√ºfungssituationen. Schlie√ülich kann die Komponente helfen, Fairness-Probleme zu identifizieren. Wenn beispielsweise ein sensibles Merkmal wie ethnische Zugeh√∂rigkeit oder Geschlecht einen hohen Einfluss auf die Vorhersage eines Modells hat, k√∂nnte dies ein Hinweis auf Rassen- oder Geschlechterbias im Modell sein.

![Feature-Wichtigkeit](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

Verwenden Sie Interpretierbarkeit, wenn Sie:

* Bestimmen m√∂chten, wie vertrauensw√ºrdig die Vorhersagen Ihres KI-Systems sind, indem Sie verstehen, welche Merkmale f√ºr die Vorhersagen am wichtigsten sind.
* Den Debugging-Prozess Ihres Modells angehen m√∂chten, indem Sie es zuerst verstehen und feststellen, ob das Modell gesunde Merkmale oder lediglich falsche Korrelationen verwendet.
* Potenzielle Quellen von Unfairness aufdecken m√∂chten, indem Sie verstehen, ob das Modell Vorhersagen auf sensiblen Merkmalen oder auf Merkmalen, die stark mit ihnen korreliert sind, basiert.
* Das Vertrauen der Benutzer in die Entscheidungen Ihres Modells aufbauen m√∂chten, indem Sie lokale Erkl√§rungen generieren, um deren Ergebnisse zu veranschaulichen.
* Eine gesetzliche Pr√ºfung eines KI-Systems abschlie√üen m√∂chten, um Modelle zu validieren und die Auswirkungen von Modellentscheidungen auf Menschen zu √ºberwachen.

## Fazit

Alle Komponenten des RAI Dashboards sind praktische Werkzeuge, die Ihnen helfen, maschinelle Lernmodelle zu entwickeln, die weniger sch√§dlich und vertrauensw√ºrdiger f√ºr die Gesellschaft sind. Sie tragen dazu bei, Bedrohungen der Menschenrechte zu verhindern, wie die Diskriminierung oder den Ausschluss bestimmter Gruppen von Lebenschancen, sowie das Risiko physischer oder psychischer Sch√§den. Sie helfen auch, Vertrauen in die Entscheidungen Ihres Modells aufzubauen, indem sie lokale Erkl√§rungen generieren, um deren Ergebnisse zu veranschaulichen. Einige der potenziellen Sch√§den k√∂nnen wie folgt klassifiziert werden:

- **Zuweisung**: Wenn beispielsweise ein Geschlecht oder eine ethnische Zugeh√∂rigkeit gegen√ºber einer anderen bevorzugt wird.
- **Qualit√§t des Dienstes**: Wenn Sie die Daten f√ºr ein spezifisches Szenario trainieren, die Realit√§t jedoch viel komplexer ist, f√ºhrt dies zu einem schlecht funktionierenden Dienst.
- **Stereotypisierung**: Die Zuordnung einer bestimmten Gruppe zu vorgegebenen Eigenschaften.
- **Herabw√ºrdigung**: Eine unfaire Kritik oder Etikettierung von etwas oder jemandem.
- **√úber- oder Unterrepr√§sentation**. Die Idee dahinter ist, dass eine bestimmte Gruppe in einem bestimmten Berufsfeld nicht vertreten ist, und jede Dienstleistung oder Funktion, die dies weiter f√∂rdert, tr√§gt zu Schaden bei.

### Azure RAI-Dashboard

Das [Azure RAI-Dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) basiert auf Open-Source-Tools, die von f√ºhrenden akademischen Institutionen und Organisationen, einschlie√ülich Microsoft, entwickelt wurden. Diese Tools sind f√ºr Datenwissenschaftler und KI-Entwickler von entscheidender Bedeutung, um das Verhalten von Modellen besser zu verstehen, unerw√ºnschte Probleme in KI-Modellen zu erkennen und zu beheben.

- Erfahren Sie, wie Sie die verschiedenen Komponenten nutzen k√∂nnen, indem Sie die [Dokumentation zum RAI-Dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) lesen.

- Schauen Sie sich einige [Beispiel-Notebooks des RAI-Dashboards](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) an, um verantwortungsvollere KI-Szenarien in Azure Machine Learning zu debuggen.

---
## üöÄ Herausforderung

Um statistische oder datenbezogene Verzerrungen von Anfang an zu vermeiden, sollten wir:

- eine Vielfalt an Hintergr√ºnden und Perspektiven unter den Personen haben, die an den Systemen arbeiten
- in Datens√§tze investieren, die die Vielfalt unserer Gesellschaft widerspiegeln
- bessere Methoden entwickeln, um Verzerrungen zu erkennen und zu korrigieren, wenn sie auftreten

Denken Sie √ºber reale Szenarien nach, in denen Unfairness beim Erstellen und Verwenden von Modellen offensichtlich ist. Was sollten wir noch ber√ºcksichtigen?

## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ml/)
## R√ºckblick & Selbststudium

In dieser Lektion haben Sie einige praktische Werkzeuge kennengelernt, um verantwortungsvolle KI in maschinelles Lernen zu integrieren.

Sehen Sie sich diesen Workshop an, um tiefer in die Themen einzutauchen:

- Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die Operationalisierung von RAI in der Praxis von Besmira Nushi und Mehrnoosh Sameki

[![Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die Operationalisierung von RAI in der Praxis](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die Operationalisierung von RAI in der Praxis")

> üé• Klicken Sie auf das Bild oben, um das Video anzusehen: Responsible AI Dashboard: Eine zentrale Anlaufstelle f√ºr die Operationalisierung von RAI in der Praxis von Besmira Nushi und Mehrnoosh Sameki

Nutzen Sie die folgenden Materialien, um mehr √ºber verantwortungsvolle KI zu erfahren und vertrauensw√ºrdigere Modelle zu entwickeln:

- Microsofts RAI-Dashboard-Tools zur Fehlerbehebung bei ML-Modellen: [Ressourcen f√ºr Responsible AI-Tools](https://aka.ms/rai-dashboard)

- Erkunden Sie das Responsible AI Toolkit: [Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsofts RAI-Ressourcenzentrum: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsofts FATE-Forschungsgruppe: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## Aufgabe

[Erkunden Sie das RAI-Dashboard](assignment.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.