<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "df2b538e8fbb3e91cf0419ae2f858675",
  "translation_date": "2025-09-05T09:51:39+00:00",
  "source_file": "9-Real-World/2-Debugging-ML-Models/README.md",
  "language_code": "tw"
}
-->
# 後記：使用負責任 AI 儀表板元件進行機器學習模型調試

## [課前測驗](https://ff-quizzes.netlify.app/en/ml/)

## 簡介

機器學習已深刻影響我們的日常生活。AI 正逐漸融入一些對個人及社會至關重要的系統，例如醫療保健、金融、教育和就業。例如，系統和模型參與了日常決策任務，如醫療診斷或欺詐檢測。因此，AI 的進步及其加速採用也伴隨著不斷演變的社會期望和日益增長的監管要求。我們經常看到 AI 系統未能達到期望的領域；它們暴露出新的挑戰；而各國政府也開始對 AI 解決方案進行監管。因此，分析這些模型以提供公平、可靠、包容、透明和負責任的結果對每個人都至關重要。

在本課程中，我們將探討一些實用工具，用於評估模型是否存在負責任 AI 的問題。傳統的機器學習調試技術通常基於定量計算，例如整體準確率或平均錯誤損失。想像一下，如果您用來構建這些模型的數據缺乏某些人口統計信息，例如種族、性別、政治觀點、宗教，或者這些人口統計信息的比例不均衡，會發生什麼情況？如果模型的輸出被解釋為偏向某些人口統計信息，又會如何？這可能導致這些敏感特徵群體的過度或不足表現，從而引發模型的公平性、包容性或可靠性問題。另一個因素是，機器學習模型通常被視為黑盒子，這使得理解和解釋模型的預測驅動因素變得困難。當數據科學家和 AI 開發者缺乏足夠的工具來調試和評估模型的公平性或可信度時，這些都是他們面臨的挑戰。

在本課程中，您將學習如何使用以下方法調試模型：

- **錯誤分析**：識別模型在數據分佈中錯誤率較高的區域。
- **模型概覽**：對不同數據群體進行比較分析，以發現模型性能指標中的差異。
- **數據分析**：調查數據是否存在過度或不足表現，可能導致模型偏向某些數據群體。
- **特徵重要性**：了解哪些特徵在全局或局部層面上驅動模型的預測。

## 前置條件

作為前置條件，請先查看 [開發者的負責任 AI 工具](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)

> ![負責任 AI 工具的動態圖](../../../../9-Real-World/2-Debugging-ML-Models/images/rai-overview.gif)

## 錯誤分析

傳統的模型性能指標用於衡量準確性，通常基於正確與錯誤預測的計算。例如，判斷模型 89% 的時間是準確的，錯誤損失為 0.001，可以被認為是良好的性能。然而，錯誤通常在底層數據集中並非均勻分佈。您可能獲得 89% 的模型準確率，但發現模型在某些數據區域的錯誤率高達 42%。這些特定數據群體的失敗模式可能導致公平性或可靠性問題。因此，了解模型表現良好或不佳的區域至關重要。模型在某些數據區域的高錯誤率可能揭示出重要的數據人口統計信息。

![分析和調試模型錯誤](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-distribution.png)

RAI 儀表板上的錯誤分析元件通過樹狀可視化展示模型失敗在各群體中的分佈情況。這有助於識別數據集中錯誤率較高的特徵或區域。通過查看模型大部分錯誤的來源，您可以開始調查根本原因。您還可以創建數據群體進行分析。這些數據群體有助於調試過程，確定為什麼模型在某些群體中表現良好，而在其他群體中出現錯誤。

![錯誤分析](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-error-cohort.png)

樹狀圖上的可視化指標有助於更快定位問題區域。例如，樹節的紅色陰影越深，錯誤率越高。

熱圖是另一種可視化功能，用於通過一個或兩個特徵調查錯誤率，找出整個數據集或群體中模型錯誤的貢獻因素。

![錯誤分析熱圖](../../../../9-Real-World/2-Debugging-ML-Models/images/ea-heatmap.png)

使用錯誤分析時，您可以：

* 深入了解模型失敗如何在數據集和多個輸入及特徵維度中分佈。
* 分解整體性能指標，並自動發現錯誤群體，以指導有針對性的緩解措施。

## 模型概覽

評估機器學習模型的性能需要全面了解其行為。這可以通過查看多個指標（例如錯誤率、準確率、召回率、精確度或 MAE）來實現，以發現性能指標中的差異。一個性能指標可能看起來很好，但另一個指標可能暴露出不準確性。此外，比較整個數據集或群體中的指標差異有助於揭示模型表現良好或不佳的地方。這在查看模型對敏感特徵（例如患者的種族、性別或年齡）與非敏感特徵的表現時尤為重要，以揭示模型可能存在的潛在不公平性。例如，發現模型在具有敏感特徵的群體中錯誤率更高，可能揭示模型存在潛在的不公平性。

RAI 儀表板的模型概覽元件不僅有助於分析數據群體中的性能指標，還使用戶能夠比較模型在不同群體中的行為。

![數據集群體 - RAI 儀表板中的模型概覽](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-dataset-cohorts.png)

該元件的基於特徵的分析功能允許用戶縮小特定特徵中的數據子群體，以更細緻地識別異常。例如，儀表板具有內置智能，可以自動生成用戶選擇特徵的群體（例如 *"time_in_hospital < 3"* 或 *"time_in_hospital >= 7"*）。這使用戶能夠從更大的數據群體中隔離特定特徵，以查看它是否是模型錯誤結果的關鍵影響因素。

![特徵群體 - RAI 儀表板中的模型概覽](../../../../9-Real-World/2-Debugging-ML-Models/images/model-overview-feature-cohorts.png)

模型概覽元件支持兩類差異指標：

**模型性能差異**：這些指標計算所選性能指標在數據子群體之間的差異（差距）。以下是一些示例：

* 準確率差異
* 錯誤率差異
* 精確度差異
* 召回率差異
* 平均絕對誤差（MAE）差異

**選擇率差異**：此指標包含子群體之間選擇率（有利預測）的差異。例如，貸款批准率的差異。選擇率指的是每個類別中被分類為 1 的數據點比例（在二元分類中）或預測值的分佈（在回歸中）。

## 數據分析

> 「如果你對數據施加足夠的壓力，它會承認任何事情」——羅納德·科斯

這句話聽起來極端，但事實上數據確實可以被操縱以支持任何結論。有時這種操縱可能是無意的。作為人類，我們都有偏見，並且往往難以有意識地知道何時在數據中引入了偏見。保證 AI 和機器學習的公平性仍然是一個複雜的挑戰。

數據是傳統模型性能指標的一個巨大盲點。您可能擁有高準確率，但這並不總是反映出數據集中可能存在的底層數據偏差。例如，如果某公司高管職位的員工數據集中有 27% 的女性和 73% 的男性，基於此數據訓練的招聘 AI 模型可能會主要針對男性群體進行高級職位的招聘。數據中的這種不平衡使模型的預測偏向於某一性別，揭示了模型存在性別偏見的公平性問題。

RAI 儀表板上的數據分析元件有助於識別數據集中過度和不足表現的區域。它幫助用戶診斷由數據不平衡或缺乏特定數據群體代表性引入的錯誤和公平性問題的根本原因。這使用戶能夠根據預測和實際結果、錯誤群體以及特定特徵來可視化數據集。有時發現一個代表性不足的數據群體也可能揭示模型未能很好地學習，因此錯誤率較高。具有數據偏差的模型不僅是一個公平性問題，還表明模型不具包容性或可靠性。

![RAI 儀表板上的數據分析元件](../../../../9-Real-World/2-Debugging-ML-Models/images/dataanalysis-cover.png)

使用數據分析時，您可以：

* 通過選擇不同的篩選器探索數據集統計，將數據切片為不同的維度（也稱為群體）。
* 了解數據集在不同群體和特徵群體中的分佈。
* 確定與公平性、錯誤分析和因果關係相關的發現（來自其他儀表板元件）是否是數據集分佈的結果。
* 決定在哪些區域收集更多數據，以減少由代表性問題、標籤噪音、特徵噪音、標籤偏差等因素引起的錯誤。

## 模型可解釋性

機器學習模型通常是黑盒子。理解哪些關鍵數據特徵驅動模型的預測可能具有挑戰性。提供模型做出某種預測的透明性至關重要。例如，如果 AI 系統預測某糖尿病患者有可能在 30 天內再次入院，它應該能夠提供支持其預測的數據。提供支持數據指標可以幫助醫生或醫院做出明智的決策。此外，能夠解釋模型為個別患者做出某種預測的原因，有助於符合健康法規的問責要求。當您使用機器學習模型影響人們的生活時，了解和解釋模型行為的驅動因素至關重要。模型可解釋性和可理解性有助於回答以下場景中的問題：

* 模型調試：為什麼我的模型犯了這個錯誤？我如何改進我的模型？
* 人類與 AI 協作：我如何理解並信任模型的決策？
* 法規合規性：我的模型是否符合法律要求？

RAI 儀表板的特徵重要性元件幫助您調試並全面了解模型如何做出預測。它也是機器學習專業人士和決策者解釋和展示影響模型行為的特徵證據的有用工具，以符合法規要求。接下來，用戶可以探索全局和局部解釋，驗證哪些特徵驅動模型的預測。全局解釋列出影響模型整體預測的主要特徵。局部解釋顯示哪些特徵導致模型對個別案例的預測。評估局部解釋的能力在調試或審核特定案例時也很有幫助，以更好地理解和解釋模型為什麼做出準確或不準確的預測。

![RAI 儀表板的特徵重要性元件](../../../../9-Real-World/2-Debugging-ML-Models/images/9-feature-importance.png)

* 全局解釋：例如，哪些特徵影響糖尿病患者再次入院模型的整體行為？
* 局部解釋：例如，為什麼一位超過 60 歲且有過住院史的糖尿病患者被預測為會或不會在 30 天內再次入院？

在調試模型性能的過程中，特徵重要性顯示特徵在不同群體中的影響程度。它有助於揭示異常情況，尤其是在比較特徵對模型錯誤預測的影響程度時。特徵重要性元件可以顯示特徵中的哪些值對模型結果產生了正面或負面影響。例如，如果模型做出了不準確的預測，該元件使您能夠深入分析並確定哪些特徵或特徵值驅動了預測。這種細節不僅有助於調試，還在審核情況下提供透明性和問責性。最後，該元件可以幫助您識別公平性問題。例如，如果某敏感特徵（如種族或性別）在驅動模型預測中具有高度影響力，這可能表明模型存在種族或性別偏見。

![特徵重要性](../../../../9-Real-World/2-Debugging-ML-Models/images/9-features-influence.png)

使用可解釋性時，您可以：

* 通過了解哪些特徵對預測最重要，判斷 AI 系統的預測是否值得信任。
* 通過首先理解模型並確定模型是否使用健康特徵或僅僅是錯誤相關性，來進行模型調試。
* 通過了解模型是否基於敏感特徵或與敏感特徵高度相關的特徵進行預測，揭示潛在的不公平性來源。
* 通過生成局部解釋來展示模型結果，建立用戶對模型決策的信任。
* 完成 AI 系統的法規審核，以驗證模型並監控模型決策對人類的影響。

## 結論

RAI 儀表板的所有元件都是幫助您構建對社會更少傷害、更值得信賴的機器學習模型的實用工具。它有助於防止對人權的威脅；避免歧視或排除某些群體的生活機會；以及減少身體或心理傷害的風險。它還通過生成局部解釋來展示模型結果，幫助建立對模型決策的信任。一些潛在的傷害可以分類為：

- **分配**：例如，某性別或種族被偏向於另一性別或種族。
- **服務質量**：如果您僅針對一個特定場景訓練數據，但現實情況更為複雜，可能導致服務性能不佳。
- **刻板印象**：將某一群體與預先分配的屬性聯繫起來。
- **貶低**：不公平地批評或標籤某事或某人。
- **過度或不足的代表性**。這個概念指的是某些群體在某些職業中未被看到，而任何持續推動這種情況的服務或功能都會造成傷害。

### Azure RAI 儀表板

[Azure RAI 儀表板](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) 基於由領先的學術機構和組織（包括 Microsoft）開發的開源工具構建，這些工具對於數據科學家和 AI 開發者更好地理解模型行為、發現並減輕 AI 模型中的不良問題至關重要。

- 通過查看 RAI 儀表板的[文檔](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)來學習如何使用不同的組件。

- 查看一些 RAI 儀表板的[範例筆記本](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks)，以便在 Azure 機器學習中調試更負責任的 AI 場景。

---
## 🚀 挑戰

為了從一開始就防止統計或數據偏差的引入，我們應該：

- 確保參與系統開發的人員具有多樣化的背景和觀點  
- 投資於能夠反映我們社會多樣性的數據集  
- 開發更好的方法來檢測和糾正偏差  

思考一些現實生活中模型構建和使用中明顯不公平的情境。我們還應該考慮什麼？

## [課後測驗](https://ff-quizzes.netlify.app/en/ml/)
## 回顧與自學

在本課中，您學習了一些將負責任 AI 融入機器學習的實用工具。

觀看以下工作坊以更深入了解這些主題：

- 負責任 AI 儀表板：實踐中實現 RAI 的一站式工具，由 Besmira Nushi 和 Mehrnoosh Sameki 主講

[![負責任 AI 儀表板：實踐中實現 RAI 的一站式工具](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg "負責任 AI 儀表板：實踐中實現 RAI 的一站式工具")

> 🎥 點擊上方圖片觀看影片：負責任 AI 儀表板：實踐中實現 RAI 的一站式工具，由 Besmira Nushi 和 Mehrnoosh Sameki 主講

參考以下材料以進一步了解負責任 AI 以及如何構建更值得信賴的模型：

- Microsoft 的 RAI 儀表板工具，用於調試 ML 模型：[負責任 AI 工具資源](https://aka.ms/rai-dashboard)

- 探索負責任 AI 工具包：[Github](https://github.com/microsoft/responsible-ai-toolbox)

- Microsoft 的 RAI 資源中心：[負責任 AI 資源 – Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Microsoft 的 FATE 研究小組：[FATE：AI 中的公平性、問責性、透明性和倫理 - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

## 作業

[探索 RAI 儀表板](assignment.md)

---

**免責聲明**：  
本文件使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原始語言的文件作為權威來源。對於關鍵資訊，建議尋求專業人工翻譯。我們對因使用此翻譯而產生的任何誤解或錯誤解讀概不負責。