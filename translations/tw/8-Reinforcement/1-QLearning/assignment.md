<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "68394b2102d3503882e5e914bd0ff5c1",
  "translation_date": "2025-09-03T18:37:58+00:00",
  "source_file": "8-Reinforcement/1-QLearning/assignment.md",
  "language_code": "tw"
}
-->
# 一個更真實的世界

在我們的情境中，彼得幾乎可以毫不疲倦或飢餓地四處移動。在一個更真實的世界裡，他需要不時地坐下來休息，還需要進食。我們來讓這個世界更貼近現實，通過實現以下規則：

1. 每次從一個地方移動到另一個地方，彼得會損失一些**能量**並增加一些**疲勞**。
2. 彼得可以通過吃蘋果來獲得更多能量。
3. 彼得可以通過在樹下或草地上休息來消除疲勞（例如，走到有樹或草地的棋盤位置——綠色區域）。
4. 彼得需要找到並擊敗狼。
5. 為了擊敗狼，彼得需要達到一定的能量和疲勞水平，否則他會輸掉戰鬥。

## 指導說明

使用原始的 [notebook.ipynb](notebook.ipynb) 筆記本作為解決方案的起點。

根據遊戲規則修改上述的獎勵函數，運行強化學習算法以學習贏得遊戲的最佳策略，並將隨機漫步的結果與你的算法進行比較，從而分析贏得和輸掉的遊戲數量。

> **Note**: 在你的新世界中，狀態會更加複雜，除了人類的位置，還包括疲勞和能量水平。你可以選擇將狀態表示為一個元組 (Board, energy, fatigue)，或者為狀態定義一個類（你也可以考慮從 `Board` 繼承），甚至修改原始的 `Board` 類（位於 [rlboard.py](../../../../8-Reinforcement/1-QLearning/rlboard.py) 中）。

在你的解決方案中，請保留負責隨機漫步策略的代碼，並在最後將你的算法結果與隨機漫步進行比較。

> **Note**: 你可能需要調整超參數以使其正常運行，特別是訓練的迭代次數。由於遊戲的成功（例如擊敗狼）是一個罕見事件，你可以預期訓練時間會更長。

## 評分標準

| 評分標準 | 優秀                                                                                                                                                                                                 | 合格                                                                                                                                                                                    | 需要改進                                                                                                                                |
| -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
|          | 筆記本展示了新世界規則的定義、Q-Learning 算法以及一些文字說明。Q-Learning 能夠顯著改善與隨機漫步相比的結果。                                                                                     | 筆記本展示了 Q-Learning 的實現，並且結果相比隨機漫步有所改善，但不顯著；或者筆記本文檔不完整，代碼結構不佳。                                                                                 | 嘗試重新定義世界規則，但 Q-Learning 算法無法正常運行，或者獎勵函數未完全定義。                                                                                             |

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議使用專業人工翻譯。我們對因使用此翻譯而產生的任何誤解或錯誤解釋不承擔責任。