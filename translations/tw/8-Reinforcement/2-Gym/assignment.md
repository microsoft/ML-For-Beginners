<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-09-03T18:44:32+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "tw"
}
-->
# 訓練山地車

[OpenAI Gym](http://gym.openai.com) 的設計使得所有環境都提供相同的 API —— 即相同的方法 `reset`、`step` 和 `render`，以及相同的 **動作空間** 和 **觀察空間** 抽象。因此，應該可以將相同的強化學習算法適配到不同的環境，並且只需進行最少的代碼修改。

## 山地車環境

[山地車環境](https://gym.openai.com/envs/MountainCar-v0/) 包含一輛被困在山谷中的車：

目標是通過每一步執行以下動作之一，成功駛出山谷並捕獲旗幟：

| 值   | 意義             |
|------|------------------|
| 0    | 向左加速         |
| 1    | 不加速           |
| 2    | 向右加速         |

然而，這個問題的主要難點在於，車子的引擎不夠強大，無法一次性爬上山頂。因此，唯一的成功方法是來回駕駛以積累動能。

觀察空間僅包含兩個值：

| 編號 | 觀察項目         | 最小值 | 最大值 |
|------|------------------|--------|--------|
|  0   | 車的位置         | -1.2   | 0.6    |
|  1   | 車的速度         | -0.07  | 0.07   |

山地車的獎勵系統相當棘手：

 * 如果代理成功到達山頂的旗幟位置（位置 = 0.5），則獲得 0 的獎勵。
 * 如果代理的位置小於 0.5，則獲得 -1 的獎勵。

當車的位置超過 0.5 或劇集長度超過 200 時，劇集終止。

## 指導說明

將我們的強化學習算法適配到山地車問題。從現有的 [notebook.ipynb](notebook.ipynb) 代碼開始，替換新的環境，修改狀態離散化函數，並嘗試以最少的代碼修改使現有算法進行訓練。通過調整超參數來優化結果。

> **注意**: 可能需要調整超參數以使算法收斂。

## 評分標準

| 評分標準 | 優秀表現 | 合格表現 | 需要改進 |
|----------|----------|----------|----------|
|          | 成功將 Q-Learning 算法從 CartPole 示例中適配，並進行最少的代碼修改，能夠在 200 步內解決捕獲旗幟的問題。 | 從網絡上採用了新的 Q-Learning 算法，但有良好的文檔記錄；或者採用了現有算法，但未達到預期結果。 | 學生未能成功採用任何算法，但在解決方案上有實質性進展（實現了狀態離散化、Q-Table 數據結構等）。 |

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原始語言的文件作為權威來源。對於關鍵資訊，建議尋求專業人工翻譯。我們對因使用此翻譯而產生的任何誤解或錯誤解讀概不負責。