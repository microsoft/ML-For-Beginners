<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-08-29T22:15:55+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "bn"
}
-->
# ট্রেন মাউন্টেন কার

[OpenAI Gym](http://gym.openai.com) এমনভাবে ডিজাইন করা হয়েছে যাতে সব পরিবেশ একই API প্রদান করে - অর্থাৎ একই `reset`, `step` এবং `render` মেথড, এবং **action space** এবং **observation space** এর একই বিমূর্ততা। তাই একই রিইনফোর্সমেন্ট লার্নিং অ্যালগরিদমকে বিভিন্ন পরিবেশে সামান্য কোড পরিবর্তনের মাধ্যমে মানিয়ে নেওয়া সম্ভব।

## একটি মাউন্টেন কার পরিবেশ

[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) এ একটি গাড়ি একটি উপত্যকায় আটকে আছে:

উদ্দেশ্য হলো উপত্যকা থেকে বেরিয়ে পতাকা দখল করা, প্রতিটি ধাপে নিম্নলিখিত কাজগুলোর মধ্যে একটি করে:

| মান | অর্থ |
|---|---|
| 0 | বামে ত্বরান্বিত করুন |
| 1 | ত্বরান্বিত করবেন না |
| 2 | ডানে ত্বরান্বিত করুন |

তবে, এই সমস্যার প্রধান কৌশল হলো গাড়ির ইঞ্জিন একবারে পাহাড়ে উঠার জন্য যথেষ্ট শক্তিশালী নয়। তাই সফল হওয়ার একমাত্র উপায় হলো গাড়িটিকে বারবার সামনে-পেছনে চালিয়ে গতি তৈরি করা।

Observation space শুধুমাত্র দুটি মান নিয়ে গঠিত:

| নম্বর | পর্যবেক্ষণ | সর্বনিম্ন | সর্বাধিক |
|-----|--------------|-----|-----|
|  0  | গাড়ির অবস্থান | -1.2 | 0.6 |
|  1  | গাড়ির গতি | -0.07 | 0.07 |

মাউন্টেন কারের জন্য পুরস্কার ব্যবস্থা বেশ জটিল:

 * যদি এজেন্ট পতাকা (অবস্থান = 0.5) পাহাড়ের চূড়ায় পৌঁছায়, তাহলে 0 পুরস্কার দেওয়া হয়।
 * যদি এজেন্টের অবস্থান 0.5 এর কম হয়, তাহলে -1 পুরস্কার দেওয়া হয়।

পর্ব শেষ হয় যদি গাড়ির অবস্থান 0.5 এর বেশি হয়, অথবা পর্বের দৈর্ঘ্য 200 এর বেশি হয়।
## নির্দেশনা

আমাদের রিইনফোর্সমেন্ট লার্নিং অ্যালগরিদমকে মাউন্টেন কার সমস্যার সমাধানের জন্য মানিয়ে নিন। বিদ্যমান [notebook.ipynb](notebook.ipynb) কোড দিয়ে শুরু করুন, নতুন পরিবেশ প্রতিস্থাপন করুন, state discretization ফাংশন পরিবর্তন করুন, এবং বিদ্যমান অ্যালগরিদমকে সামান্য কোড পরিবর্তনের মাধ্যমে প্রশিক্ষণ দেওয়ার চেষ্টা করুন। হাইপারপ্যারামিটার সমন্বয় করে ফলাফল অপ্টিমাইজ করুন।

> **Note**: অ্যালগরিদমকে সঠিকভাবে কাজ করানোর জন্য হাইপারপ্যারামিটার সমন্বয় প্রয়োজন হতে পারে। 
## মূল্যায়ন

| মানদণ্ড | চমৎকার | পর্যাপ্ত | উন্নতির প্রয়োজন |
| -------- | --------- | -------- | ----------------- |
|          | Q-Learning অ্যালগরিদম সফলভাবে CartPole উদাহরণ থেকে মানিয়ে নেওয়া হয়েছে, সামান্য কোড পরিবর্তনের মাধ্যমে, যা 200 ধাপের মধ্যে পতাকা দখল করার সমস্যার সমাধান করতে সক্ষম। | একটি নতুন Q-Learning অ্যালগরিদম ইন্টারনেট থেকে গ্রহণ করা হয়েছে, তবে এটি ভালোভাবে ডকুমেন্টেড; অথবা বিদ্যমান অ্যালগরিদম গ্রহণ করা হয়েছে, কিন্তু কাঙ্ক্ষিত ফলাফল অর্জন করতে পারেনি। | শিক্ষার্থী কোনো অ্যালগরিদম সফলভাবে গ্রহণ করতে পারেনি, তবে সমাধানের দিকে উল্লেখযোগ্য পদক্ষেপ নিয়েছে (state discretization, Q-Table ডেটা স্ট্রাকচার ইত্যাদি বাস্তবায়ন করেছে)। |

---

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা তার জন্য দায়ী থাকব না।