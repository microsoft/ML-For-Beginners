<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1f2b7441745eb52e25745423b247016b",
  "translation_date": "2025-08-29T22:16:07+00:00",
  "source_file": "8-Reinforcement/2-Gym/assignment.md",
  "language_code": "ru"
}
-->
# Обучение на примере Mountain Car

[OpenAI Gym](http://gym.openai.com) разработан таким образом, что все среды предоставляют одинаковый API - то есть одни и те же методы `reset`, `step` и `render`, а также одинаковые абстракции **пространства действий** и **пространства наблюдений**. Таким образом, должно быть возможно адаптировать одни и те же алгоритмы обучения с подкреплением к различным средам с минимальными изменениями кода.

## Среда Mountain Car

[Среда Mountain Car](https://gym.openai.com/envs/MountainCar-v0/) представляет собой автомобиль, застрявший в долине:

Цель состоит в том, чтобы выбраться из долины и захватить флаг, выполняя на каждом шаге одно из следующих действий:

| Значение | Значение действия |
|---|---|
| 0 | Ускорение влево |
| 1 | Не ускоряться |
| 2 | Ускорение вправо |

Основная сложность этой задачи заключается в том, что двигатель автомобиля недостаточно мощный, чтобы подняться на гору за один раз. Поэтому единственный способ добиться успеха — это разгоняться, двигаясь вперед-назад, чтобы набрать инерцию.

Пространство наблюдений состоит всего из двух значений:

| № | Наблюдение  | Мин | Макс |
|---|-------------|-----|-----|
|  0 | Позиция автомобиля | -1.2 | 0.6 |
|  1 | Скорость автомобиля | -0.07 | 0.07 |

Система вознаграждений для Mountain Car довольно сложная:

 * Вознаграждение 0 присуждается, если агент достиг флага (позиция = 0.5) на вершине горы.
 * Вознаграждение -1 присуждается, если позиция агента меньше 0.5.

Эпизод завершается, если позиция автомобиля превышает 0.5 или длина эпизода превышает 200 шагов.

## Инструкции

Адаптируйте наш алгоритм обучения с подкреплением для решения задачи Mountain Car. Начните с существующего кода в [notebook.ipynb](notebook.ipynb), замените среду, измените функции дискретизации состояний и попытайтесь заставить существующий алгоритм обучаться с минимальными изменениями кода. Оптимизируйте результат, корректируя гиперпараметры.

> **Примечание**: Для сходимости алгоритма, скорее всего, потребуется корректировка гиперпараметров.

## Критерии оценки

| Критерий | Превосходно | Удовлетворительно | Требует улучшений |
| -------- | ----------- | ----------------- | ----------------- |
|          | Алгоритм Q-Learning успешно адаптирован из примера CartPole с минимальными изменениями кода и способен решить задачу захвата флага менее чем за 200 шагов. | Новый алгоритм Q-Learning был взят из Интернета, но хорошо задокументирован; либо существующий алгоритм адаптирован, но не достигает желаемых результатов. | Студент не смог успешно адаптировать алгоритм, но предпринял значительные шаги к решению (реализовал дискретизацию состояний, структуру данных Q-таблицы и т.д.). |

---

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.