{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "lesson_12-R.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "ir",
   "display_name": "R"
  },
  "language_info": {
   "name": "R"
  },
  "coopTranslator": {
   "original_hash": "fab50046ca413a38939d579f8432274f",
   "translation_date": "2025-09-06T14:46:56+00:00",
   "source_file": "4-Classification/3-Classifiers-2/solution/R/lesson_12-R.ipynb",
   "language_code": "sv"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsFutf_ygqSx"
   },
   "source": [
    "# Bygg en klassificeringsmodell: Uts√∂kta asiatiska och indiska r√§tter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD54bEefgtNO"
   },
   "source": [
    "## Klassificerare f√∂r k√∂k 2\n",
    "\n",
    "I denna andra lektion om klassificering kommer vi att utforska `fler s√§tt` att klassificera kategoriska data. Vi kommer ocks√• att l√§ra oss om konsekvenserna av att v√§lja en klassificerare framf√∂r en annan.\n",
    "\n",
    "### [**Quiz f√∂re f√∂rel√§sningen**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/23/)\n",
    "\n",
    "### **F√∂rkunskaper**\n",
    "\n",
    "Vi antar att du har slutf√∂rt de tidigare lektionerna eftersom vi kommer att bygga vidare p√• n√•gra koncept vi l√§rde oss tidigare.\n",
    "\n",
    "F√∂r denna lektion beh√∂ver vi f√∂ljande paket:\n",
    "\n",
    "-   `tidyverse`: [tidyverse](https://www.tidyverse.org/) √§r en [samling av R-paket](https://www.tidyverse.org/packages) som √§r utformade f√∂r att g√∂ra datavetenskap snabbare, enklare och roligare!\n",
    "\n",
    "-   `tidymodels`: [tidymodels](https://www.tidymodels.org/) √§r ett [ramverk av paket](https://www.tidymodels.org/packages/) f√∂r modellering och maskininl√§rning.\n",
    "\n",
    "-   `themis`: [themis-paketet](https://themis.tidymodels.org/) tillhandah√•ller extra receptsteg f√∂r att hantera obalanserad data.\n",
    "\n",
    "Du kan installera dem med:\n",
    "\n",
    "`install.packages(c(\"tidyverse\", \"tidymodels\", \"kernlab\", \"themis\", \"ranger\", \"xgboost\", \"kknn\"))`\n",
    "\n",
    "Alternativt kan skriptet nedan kontrollera om du har de paket som kr√§vs f√∂r att slutf√∂ra denna modul och installera dem √•t dig om de saknas.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vZ57IuUxgyQt"
   },
   "source": [
    "suppressWarnings(if (!require(\"pacman\"))install.packages(\"pacman\"))\n",
    "\n",
    "pacman::p_load(tidyverse, tidymodels, themis, kernlab, ranger, xgboost, kknn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z22M-pj4g07x"
   },
   "source": [
    "## **1. En klassificeringskarta**\n",
    "\n",
    "I v√•r [f√∂reg√•ende lektion](https://github.com/microsoft/ML-For-Beginners/tree/main/4-Classification/2-Classifiers-1) f√∂rs√∂kte vi besvara fr√•gan: hur v√§ljer vi mellan flera modeller? Till stor del beror det p√• egenskaperna hos datan och typen av problem vi vill l√∂sa (till exempel klassificering eller regression).\n",
    "\n",
    "Tidigare l√§rde vi oss om de olika alternativen du har n√§r du klassificerar data med hj√§lp av Microsofts fusklapp. Python's Machine Learning-ramverk, Scikit-learn, erbjuder en liknande men mer detaljerad fusklapp som kan hj√§lpa dig att ytterligare begr√§nsa dina estimatorer (ett annat ord f√∂r klassificerare):\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/map.png\"\n",
    "   width=\"700\"/>\n",
    "   <figcaption></figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1i3xRIVg7vG"
   },
   "source": [
    "> Tips: [bes√∂k den h√§r kartan online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) och klicka l√§ngs v√§gen f√∂r att l√§sa dokumentationen.\n",
    ">\n",
    "> [Tidymodels referenssida](https://www.tidymodels.org/find/parsnip/#models) erbjuder ocks√• utm√§rkt dokumentation om olika typer av modeller.\n",
    "\n",
    "### **Planen** üó∫Ô∏è\n",
    "\n",
    "Den h√§r kartan √§r v√§ldigt anv√§ndbar n√§r du har en tydlig f√∂rst√•else f√∂r din data, eftersom du kan \"vandra\" l√§ngs dess v√§gar mot ett beslut:\n",
    "\n",
    "-   Vi har \\>50 prover\n",
    "\n",
    "-   Vi vill f√∂ruts√§ga en kategori\n",
    "\n",
    "-   Vi har m√§rkt data\n",
    "\n",
    "-   Vi har f√§rre √§n 100K prover\n",
    "\n",
    "-   ‚ú® Vi kan v√§lja en Linear SVC\n",
    "\n",
    "-   Om det inte fungerar, eftersom vi har numerisk data\n",
    "\n",
    "    -   Kan vi prova en ‚ú® KNeighbors Classifier\n",
    "\n",
    "        -   Om det inte fungerar, prova ‚ú® SVC och ‚ú® Ensemble Classifiers\n",
    "\n",
    "Det h√§r √§r en v√§ldigt anv√§ndbar v√§g att f√∂lja. Nu ska vi dyka rakt in i det med [tidymodels](https://www.tidymodels.org/) modelleringsramverket: en konsekvent och flexibel samling av R-paket utvecklade f√∂r att fr√§mja god statistisk praxis üòä.\n",
    "\n",
    "## 2. Dela upp data och hantera obalanserade dataset.\n",
    "\n",
    "Fr√•n v√•ra tidigare lektioner l√§rde vi oss att det fanns en upps√§ttning vanliga ingredienser √∂ver v√•ra k√∂k. Dessutom fanns det en ganska oj√§mn f√∂rdelning i antalet k√∂k.\n",
    "\n",
    "Vi kommer att hantera detta genom att\n",
    "\n",
    "-   Ta bort de vanligaste ingredienserna som skapar f√∂rvirring mellan olika k√∂k, med hj√§lp av `dplyr::select()`.\n",
    "\n",
    "-   Anv√§nda ett `recipe` som f√∂rbehandlar data f√∂r att g√∂ra den redo f√∂r modellering genom att till√§mpa en `over-sampling`-algoritm.\n",
    "\n",
    "Vi tittade redan p√• detta i den tidigare lektionen, s√• det h√§r borde g√• som en dans ü•≥!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6tj_rN00hClA"
   },
   "source": [
    "# Load the core Tidyverse and Tidymodels packages\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "\n",
    "# Load the original cuisines data\n",
    "df <- read_csv(file = \"https://raw.githubusercontent.com/microsoft/ML-For-Beginners/main/4-Classification/data/cuisines.csv\")\n",
    "\n",
    "# Drop id column, rice, garlic and ginger from our original data set\n",
    "df_select <- df %>% \n",
    "  select(-c(1, rice, garlic, ginger)) %>%\n",
    "  # Encode cuisine column as categorical\n",
    "  mutate(cuisine = factor(cuisine))\n",
    "\n",
    "\n",
    "# Create data split specification\n",
    "set.seed(2056)\n",
    "cuisines_split <- initial_split(data = df_select,\n",
    "                                strata = cuisine,\n",
    "                                prop = 0.7)\n",
    "\n",
    "# Extract the data in each split\n",
    "cuisines_train <- training(cuisines_split)\n",
    "cuisines_test <- testing(cuisines_split)\n",
    "\n",
    "# Display distribution of cuisines in the training set\n",
    "cuisines_train %>% \n",
    "  count(cuisine) %>% \n",
    "  arrange(desc(n))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFin5yw3hHb1"
   },
   "source": [
    "### Hantera obalanserad data\n",
    "\n",
    "Obalanserad data p√•verkar ofta modellens prestanda negativt. M√•nga modeller presterar b√§st n√§r antalet observationer √§r lika, och har d√§rf√∂r en tendens att ha sv√•rt med obalanserad data.\n",
    "\n",
    "Det finns huvudsakligen tv√• s√§tt att hantera obalanserade datas√§tt:\n",
    "\n",
    "-   l√§gga till observationer till minoritetsklassen: `√ñver-sampling`, t.ex. med hj√§lp av en SMOTE-algoritm som syntetiskt genererar nya exempel av minoritetsklassen genom att anv√§nda n√§rmaste grannar till dessa fall.\n",
    "\n",
    "-   ta bort observationer fr√•n majoritetsklassen: `Under-sampling`\n",
    "\n",
    "I v√•r tidigare lektion visade vi hur man hanterar obalanserade datas√§tt med hj√§lp av ett `recept`. Ett recept kan ses som en ritning som beskriver vilka steg som ska till√§mpas p√• ett datas√§tt f√∂r att g√∂ra det redo f√∂r dataanalys. I v√•rt fall vill vi ha en j√§mn f√∂rdelning av antalet k√∂k i v√•r `tr√§ningsupps√§ttning`. L√•t oss s√§tta ig√•ng!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cRzTnHolhLWd"
   },
   "source": [
    "# Load themis package for dealing with imbalanced data\n",
    "library(themis)\n",
    "\n",
    "# Create a recipe for preprocessing training data\n",
    "cuisines_recipe <- recipe(cuisine ~ ., data = cuisines_train) %>%\n",
    "  step_smote(cuisine) \n",
    "\n",
    "# Print recipe\n",
    "cuisines_recipe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxOQ2ORhhO81"
   },
   "source": [
    "Nu √§r vi redo att tr√§na modeller üë©‚Äçüíªüë®‚Äçüíª!\n",
    "\n",
    "## 3. Ut√∂ver multinomiala regressionsmodeller\n",
    "\n",
    "I v√•r tidigare lektion tittade vi p√• multinomiala regressionsmodeller. L√•t oss utforska n√•gra mer flexibla modeller f√∂r klassificering.\n",
    "\n",
    "### Support Vector Machines\n",
    "\n",
    "I klassificeringssammanhang √§r `Support Vector Machines` en maskininl√§rningsteknik som f√∂rs√∂ker hitta ett *hyperplan* som \"b√§st\" separerar klasserna. L√•t oss titta p√• ett enkelt exempel:\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/svm.png\"\n",
    "   width=\"300\"/>\n",
    "   <figcaption>https://commons.wikimedia.org/w/index.php?curid=22877598</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4Wsd0vZhXYu"
   },
   "source": [
    "H1~ separerar inte klasserna. H2~ g√∂r det, men endast med en liten marginal. H3~ separerar dem med maximal marginal.\n",
    "\n",
    "#### Linj√§r Support Vector Classifier\n",
    "\n",
    "Support-Vector clustering (SVC) √§r en del av Support-Vector-maskinerna inom ML-tekniker. I SVC v√§ljs hyperplanet f√∂r att korrekt separera `de flesta` av tr√§ningsobservationerna, men `kan felklassificera` n√•gra observationer. Genom att till√•ta vissa punkter att vara p√• fel sida blir SVM mer robust mot avvikelser och d√§rmed b√§ttre p√• att generalisera till ny data. Parametern som reglerar denna √∂vertr√§delse kallas `cost` och har ett standardv√§rde p√• 1 (se `help(\"svm_poly\")`).\n",
    "\n",
    "L√•t oss skapa en linj√§r SVC genom att s√§tta `degree = 1` i en polynomisk SVM-modell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vJpp6nuChlBz"
   },
   "source": [
    "# Make a linear SVC specification\n",
    "svc_linear_spec <- svm_poly(degree = 1) %>% \n",
    "  set_engine(\"kernlab\") %>% \n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# Bundle specification and recipe into a worklow\n",
    "svc_linear_wf <- workflow() %>% \n",
    "  add_recipe(cuisines_recipe) %>% \n",
    "  add_model(svc_linear_spec)\n",
    "\n",
    "# Print out workflow\n",
    "svc_linear_wf"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDs8cWNkhoqu"
   },
   "source": [
    "Nu n√§r vi har sammanst√§llt f√∂rbehandlingsstegen och modellspecifikationen i ett *arbetsfl√∂de*, kan vi g√• vidare och tr√§na den linj√§ra SVC och samtidigt utv√§rdera resultaten. F√∂r prestandam√•tt, l√•t oss skapa en upps√§ttning m√•tt som kommer att utv√§rdera: `accuracy`, `sensitivity`, `Positive Predicted Value` och `F Measure`.\n",
    "\n",
    "> `augment()` kommer att l√§gga till kolumn(er) f√∂r f√∂ruts√§gelser till den angivna datan.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "81wiqcwuhrnq"
   },
   "source": [
    "# Train a linear SVC model\n",
    "svc_linear_fit <- svc_linear_wf %>% \n",
    "  fit(data = cuisines_train)\n",
    "\n",
    "# Create a metric set\n",
    "eval_metrics <- metric_set(ppv, sens, accuracy, f_meas)\n",
    "\n",
    "\n",
    "# Make predictions and Evaluate model performance\n",
    "svc_linear_fit %>% \n",
    "  augment(new_data = cuisines_test) %>% \n",
    "  eval_metrics(truth = cuisine, estimate = .pred_class)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UFQvHf-huo3"
   },
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "Support Vector Machine (SVM) √§r en vidareutveckling av support vector classifier f√∂r att hantera en icke-linj√§r gr√§ns mellan klasserna. I grund och botten anv√§nder SVMs *kernel-tricket* f√∂r att ut√∂ka funktionsutrymmet och anpassa sig till icke-linj√§ra relationer mellan klasser. En popul√§r och mycket flexibel kernel-funktion som anv√§nds av SVMs √§r *Radial basis function.* L√•t oss se hur den presterar p√• v√•r data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-KX4S8mzhzmp"
   },
   "source": [
    "set.seed(2056)\n",
    "\n",
    "# Make an RBF SVM specification\n",
    "svm_rbf_spec <- svm_rbf() %>% \n",
    "  set_engine(\"kernlab\") %>% \n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# Bundle specification and recipe into a worklow\n",
    "svm_rbf_wf <- workflow() %>% \n",
    "  add_recipe(cuisines_recipe) %>% \n",
    "  add_model(svm_rbf_spec)\n",
    "\n",
    "\n",
    "# Train an RBF model\n",
    "svm_rbf_fit <- svm_rbf_wf %>% \n",
    "  fit(data = cuisines_train)\n",
    "\n",
    "\n",
    "# Make predictions and Evaluate model performance\n",
    "svm_rbf_fit %>% \n",
    "  augment(new_data = cuisines_test) %>% \n",
    "  eval_metrics(truth = cuisine, estimate = .pred_class)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBFSa7WSh4HQ"
   },
   "source": [
    "Mycket b√§ttre ü§©!\n",
    "\n",
    "> ‚úÖ V√§nligen se:\n",
    ">\n",
    "> -   [*Support Vector Machines*](https://bradleyboehmke.github.io/HOML/svm.html), Hands-on Machine Learning with R\n",
    ">\n",
    "> -   [*Support Vector Machines*](https://www.statlearning.com/), An Introduction to Statistical Learning with Applications in R\n",
    ">\n",
    "> f√∂r vidare l√§sning.\n",
    "\n",
    "### N√§rmaste granne-klassificerare\n",
    "\n",
    "*K*-n√§rmsta granne (KNN) √§r en algoritm d√§r varje observation f√∂ruts√§gs baserat p√• dess *likhet* med andra observationer.\n",
    "\n",
    "L√•t oss anpassa en till v√•r data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k4BxxBcdh9Ka"
   },
   "source": [
    "# Make a KNN specification\n",
    "knn_spec <- nearest_neighbor() %>% \n",
    "  set_engine(\"kknn\") %>% \n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# Bundle recipe and model specification into a workflow\n",
    "knn_wf <- workflow() %>% \n",
    "  add_recipe(cuisines_recipe) %>% \n",
    "  add_model(knn_spec)\n",
    "\n",
    "# Train a boosted tree model\n",
    "knn_wf_fit <- knn_wf %>% \n",
    "  fit(data = cuisines_train)\n",
    "\n",
    "\n",
    "# Make predictions and Evaluate model performance\n",
    "knn_wf_fit %>% \n",
    "  augment(new_data = cuisines_test) %>% \n",
    "  eval_metrics(truth = cuisine, estimate = .pred_class)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaegQseriAcj"
   },
   "source": [
    "Det verkar som att den h√§r modellen inte presterar s√§rskilt bra. F√∂rmodligen kan modellens prestanda f√∂rb√§ttras genom att √§ndra argumenten (se `help(\"nearest_neighbor\")`). Se till att testa detta.\n",
    "\n",
    "> ‚úÖ V√§nligen se:\n",
    ">\n",
    "> -   [Hands-on Machine Learning with R](https://bradleyboehmke.github.io/HOML/)\n",
    ">\n",
    "> -   [An Introduction to Statistical Learning with Applications in R](https://www.statlearning.com/)\n",
    ">\n",
    "> f√∂r att l√§ra dig mer om *K*-N√§rmaste Grannar-klassificerare.\n",
    "\n",
    "### Ensembleklassificerare\n",
    "\n",
    "Ensemblealgoritmer fungerar genom att kombinera flera basmodeller f√∂r att skapa en optimal modell antingen genom:\n",
    "\n",
    "`bagging`: att anv√§nda en *medelv√§rdesfunktion* p√• en samling av basmodeller\n",
    "\n",
    "`boosting`: att bygga en sekvens av modeller som bygger p√• varandra f√∂r att f√∂rb√§ttra den prediktiva prestandan.\n",
    "\n",
    "L√•t oss b√∂rja med att testa en Random Forest-modell, som bygger en stor samling beslutstr√§d och sedan anv√§nder en medelv√§rdesfunktion f√∂r att skapa en b√§ttre √∂vergripande modell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "49DPoVs6iK1M"
   },
   "source": [
    "# Make a random forest specification\n",
    "rf_spec <- rand_forest() %>% \n",
    "  set_engine(\"ranger\") %>% \n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# Bundle recipe and model specification into a workflow\n",
    "rf_wf <- workflow() %>% \n",
    "  add_recipe(cuisines_recipe) %>% \n",
    "  add_model(rf_spec)\n",
    "\n",
    "# Train a random forest model\n",
    "rf_wf_fit <- rf_wf %>% \n",
    "  fit(data = cuisines_train)\n",
    "\n",
    "\n",
    "# Make predictions and Evaluate model performance\n",
    "rf_wf_fit %>% \n",
    "  augment(new_data = cuisines_test) %>% \n",
    "  eval_metrics(truth = cuisine, estimate = .pred_class)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGVYwC_aiUWc"
   },
   "source": [
    "Bra jobbat üëè!\n",
    "\n",
    "L√•t oss ocks√• experimentera med en Boosted Tree-modell.\n",
    "\n",
    "Boosted Tree definierar en ensemblemetod som skapar en serie sekventiella beslutstr√§d d√§r varje tr√§d beror p√• resultaten fr√•n tidigare tr√§d i ett f√∂rs√∂k att gradvis minska felet. Den fokuserar p√• vikterna f√∂r felklassificerade objekt och justerar passformen f√∂r n√§sta klassificerare f√∂r att korrigera.\n",
    "\n",
    "Det finns olika s√§tt att passa denna modell (se `help(\"boost_tree\")`). I detta exempel kommer vi att passa Boosted trees via `xgboost`-motorn.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Py1YWo-micWs"
   },
   "source": [
    "# Make a boosted tree specification\n",
    "boost_spec <- boost_tree(trees = 200) %>% \n",
    "  set_engine(\"xgboost\") %>% \n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# Bundle recipe and model specification into a workflow\n",
    "boost_wf <- workflow() %>% \n",
    "  add_recipe(cuisines_recipe) %>% \n",
    "  add_model(boost_spec)\n",
    "\n",
    "# Train a boosted tree model\n",
    "boost_wf_fit <- boost_wf %>% \n",
    "  fit(data = cuisines_train)\n",
    "\n",
    "\n",
    "# Make predictions and Evaluate model performance\n",
    "boost_wf_fit %>% \n",
    "  augment(new_data = cuisines_test) %>% \n",
    "  eval_metrics(truth = cuisine, estimate = .pred_class)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNQnbuejigZM"
   },
   "source": [
    "> ‚úÖ V√§nligen se:\n",
    ">\n",
    "> -   [Machine Learning f√∂r samh√§llsvetare](https://cimentadaj.github.io/ml_socsci/tree-based-methods.html#random-forests)\n",
    ">\n",
    "> -   [Hands-on Machine Learning med R](https://bradleyboehmke.github.io/HOML/)\n",
    ">\n",
    "> -   [En introduktion till statistisk inl√§rning med applikationer i R](https://www.statlearning.com/)\n",
    ">\n",
    "> -   <https://algotech.netlify.app/blog/xgboost/> - Utforskar AdaBoost-modellen som √§r ett bra alternativ till xgboost.\n",
    ">\n",
    "> f√∂r att l√§ra dig mer om ensembleklassificerare.\n",
    "\n",
    "## 4. Extra - j√§mf√∂ra flera modeller\n",
    "\n",
    "Vi har anpassat ganska m√•nga modeller i denna labb üôå. Det kan bli tr√∂ttsamt eller jobbigt att skapa m√•nga arbetsfl√∂den fr√•n olika upps√§ttningar av f√∂rbehandlingsmetoder och/eller modellspecifikationer och sedan ber√§kna prestandam√•tten en efter en.\n",
    "\n",
    "L√•t oss se om vi kan l√∂sa detta genom att skapa en funktion som anpassar en lista med arbetsfl√∂den p√• tr√§ningsupps√§ttningen och sedan returnerar prestandam√•tten baserat p√• testupps√§ttningen. Vi kommer att anv√§nda `map()` och `map_dfr()` fr√•n paketet [purrr](https://purrr.tidyverse.org/) f√∂r att till√§mpa funktioner p√• varje element i en lista.\n",
    "\n",
    "> [`map()`](https://purrr.tidyverse.org/reference/map.html)-funktioner l√•ter dig ers√§tta m√•nga for-loopar med kod som b√•de √§r mer kortfattad och l√§ttare att l√§sa. Det b√§sta st√§llet att l√§ra sig om [`map()`](https://purrr.tidyverse.org/reference/map.html)-funktionerna √§r [kapitlet om iteration](http://r4ds.had.co.nz/iteration.html) i R f√∂r data science.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qzb7LyZnimd2"
   },
   "source": [
    "set.seed(2056)\n",
    "\n",
    "# Create a metric set\n",
    "eval_metrics <- metric_set(ppv, sens, accuracy, f_meas)\n",
    "\n",
    "# Define a function that returns performance metrics\n",
    "compare_models <- function(workflow_list, train_set, test_set){\n",
    "  \n",
    "  suppressWarnings(\n",
    "    # Fit each model to the train_set\n",
    "    map(workflow_list, fit, data = train_set) %>% \n",
    "    # Make predictions on the test set\n",
    "      map_dfr(augment, new_data = test_set, .id = \"model\") %>%\n",
    "    # Select desired columns\n",
    "      select(model, cuisine, .pred_class) %>% \n",
    "    # Evaluate model performance\n",
    "      group_by(model) %>% \n",
    "      eval_metrics(truth = cuisine, estimate = .pred_class) %>% \n",
    "      ungroup()\n",
    "  )\n",
    "  \n",
    "} # End of function"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwa712sNisDA"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3i4VJOi2iu-a"
   },
   "source": [
    "# Make a list of workflows\n",
    "workflow_list <- list(\n",
    "  \"svc\" = svc_linear_wf,\n",
    "  \"svm\" = svm_rbf_wf,\n",
    "  \"knn\" = knn_wf,\n",
    "  \"random_forest\" = rf_wf,\n",
    "  \"xgboost\" = boost_wf)\n",
    "\n",
    "# Call the function\n",
    "set.seed(2056)\n",
    "perf_metrics <- compare_models(workflow_list = workflow_list, train_set = cuisines_train, test_set = cuisines_test)\n",
    "\n",
    "# Print out performance metrics\n",
    "perf_metrics %>% \n",
    "  group_by(.metric) %>% \n",
    "  arrange(desc(.estimate)) %>% \n",
    "  slice_head(n=7)\n",
    "\n",
    "# Compare accuracy\n",
    "perf_metrics %>% \n",
    "  filter(.metric == \"accuracy\") %>% \n",
    "  arrange(desc(.estimate))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuWK_lEli4nW"
   },
   "source": [
    "[**workflowset**](https://workflowsets.tidymodels.org/) paketet g√∂r det m√∂jligt f√∂r anv√§ndare att skapa och enkelt anpassa ett stort antal modeller, men √§r fr√§mst utformat f√∂r att fungera med omprovningstekniker som `cross-validation`, en metod vi √§nnu inte har t√§ckt.\n",
    "\n",
    "## **üöÄUtmaning**\n",
    "\n",
    "Var och en av dessa tekniker har ett stort antal parametrar som du kan justera, till exempel `cost` i SVMs, `neighbors` i KNN, `mtry` (Slumpm√§ssigt Valda Prediktorer) i Random Forest.\n",
    "\n",
    "Unders√∂k standardparametrarna f√∂r var och en och fundera p√• vad justering av dessa parametrar skulle inneb√§ra f√∂r modellens kvalitet.\n",
    "\n",
    "F√∂r att ta reda p√• mer om en specifik modell och dess parametrar, anv√§nd: `help(\"model\")` t.ex. `help(\"rand_forest\")`\n",
    "\n",
    "> I praktiken brukar vi *estimera* de *b√§sta v√§rdena* f√∂r dessa genom att tr√§na m√•nga modeller p√• en `simulerad datam√§ngd` och m√§ta hur bra alla dessa modeller presterar. Denna process kallas **tuning**.\n",
    "\n",
    "### [**Quiz efter f√∂rel√§sningen**](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/24/)\n",
    "\n",
    "### **Granskning & Sj√§lvstudier**\n",
    "\n",
    "Det finns mycket facktermer i dessa lektioner, s√• ta en stund att g√• igenom [denna lista](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) med anv√§ndbar terminologi!\n",
    "\n",
    "#### TACK TILL:\n",
    "\n",
    "[`Allison Horst`](https://twitter.com/allison_horst/) f√∂r att ha skapat de fantastiska illustrationerna som g√∂r R mer v√§lkomnande och engagerande. Hitta fler illustrationer i hennes [galleri](https://www.google.com/url?q=https://github.com/allisonhorst/stats-illustrations&sa=D&source=editors&ust=1626380772530000&usg=AOvVaw3zcfyCizFQZpkSLzxiiQEM).\n",
    "\n",
    "[Cassie Breviu](https://www.twitter.com/cassieview) och [Jen Looper](https://www.twitter.com/jenlooper) f√∂r att ha skapat den ursprungliga Python-versionen av denna modul ‚ô•Ô∏è\n",
    "\n",
    "Lycka till med l√§randet,\n",
    "\n",
    "[Eric](https://twitter.com/ericntay), Gold Microsoft Learn Student Ambassador.\n",
    "\n",
    "<p >\n",
    "   <img src=\"../../images/r_learners_sm.jpeg\"\n",
    "   width=\"569\"/>\n",
    "   <figcaption>Konstverk av @allison_horst</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som kan uppst√• vid anv√§ndning av denna √∂vers√§ttning.\n"
   ]
  }
 ]
}