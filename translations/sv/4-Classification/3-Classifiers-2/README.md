<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49047911108adc49d605cddfb455749c",
  "translation_date": "2025-09-05T21:53:40+00:00",
  "source_file": "4-Classification/3-Classifiers-2/README.md",
  "language_code": "sv"
}
-->
# Klassificerare f√∂r matlagning 2

I den h√§r andra lektionen om klassificering kommer du att utforska fler s√§tt att klassificera numerisk data. Du kommer ocks√• att l√§ra dig om konsekvenserna av att v√§lja en klassificerare framf√∂r en annan.

## [F√∂rtest-quiz](https://ff-quizzes.netlify.app/en/ml/)

### F√∂rkunskaper

Vi antar att du har slutf√∂rt de tidigare lektionerna och har en st√§dad dataset i din `data`-mapp som heter _cleaned_cuisines.csv_ i roten av denna 4-lektionsmapp.

### F√∂rberedelse

Vi har laddat din _notebook.ipynb_-fil med den st√§dade datasetet och har delat upp den i X- och y-dataframes, redo f√∂r modellbyggnadsprocessen.

## En klassificeringskarta

Tidigare l√§rde du dig om de olika alternativen du har n√§r du klassificerar data med hj√§lp av Microsofts fusklapp. Scikit-learn erbjuder en liknande, men mer detaljerad fusklapp som kan hj√§lpa dig att ytterligare begr√§nsa dina estimatorer (en annan term f√∂r klassificerare):

![ML-karta fr√•n Scikit-learn](../../../../4-Classification/3-Classifiers-2/images/map.png)
> Tips: [bes√∂k denna karta online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) och klicka l√§ngs v√§gen f√∂r att l√§sa dokumentationen.

### Planen

Den h√§r kartan √§r mycket anv√§ndbar n√§r du har en tydlig f√∂rst√•else f√∂r din data, eftersom du kan "vandra" l√§ngs dess v√§gar till ett beslut:

- Vi har >50 prover
- Vi vill f√∂ruts√§ga en kategori
- Vi har m√§rkt data
- Vi har f√§rre √§n 100K prover
- ‚ú® Vi kan v√§lja en Linear SVC
- Om det inte fungerar, eftersom vi har numerisk data
    - Kan vi prova en ‚ú® KNeighbors Classifier 
      - Om det inte fungerar, prova ‚ú® SVC och ‚ú® Ensemble Classifiers

Detta √§r en mycket anv√§ndbar v√§g att f√∂lja.

## √ñvning - dela upp datan

F√∂ljande denna v√§g b√∂r vi b√∂rja med att importera n√•gra bibliotek att anv√§nda.

1. Importera de n√∂dv√§ndiga biblioteken:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Dela upp din tr√§nings- och testdata:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC-klassificerare

Support-Vector Clustering (SVC) √§r en del av Support-Vector Machines-familjen av ML-tekniker (l√§r dig mer om dessa nedan). I denna metod kan du v√§lja en 'kernel' f√∂r att best√§mma hur etiketterna ska klustras. Parametern 'C' h√§nvisar till 'regularisering' som reglerar p√•verkan av parametrar. Kerneln kan vara en av [flera](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); h√§r st√§ller vi in den p√• 'linear' f√∂r att s√§kerst√§lla att vi anv√§nder Linear SVC. Sannolikhet √§r som standard inst√§lld p√• 'false'; h√§r st√§ller vi in den p√• 'true' f√∂r att samla sannolikhetsuppskattningar. Vi st√§ller in random state p√• '0' f√∂r att blanda datan och f√• sannolikheter.

### √ñvning - till√§mpa en Linear SVC

B√∂rja med att skapa en array av klassificerare. Du kommer att l√§gga till successivt i denna array n√§r vi testar. 

1. B√∂rja med en Linear SVC:

    ```python
    C = 10
    # Create different classifiers.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Tr√§na din modell med Linear SVC och skriv ut en rapport:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Resultatet √§r ganska bra:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors-klassificerare

K-Neighbors √§r en del av "neighbors"-familjen av ML-metoder, som kan anv√§ndas f√∂r b√•de √∂vervakad och o√∂vervakad inl√§rning. I denna metod skapas ett f√∂rdefinierat antal punkter och data samlas runt dessa punkter s√• att generaliserade etiketter kan f√∂ruts√§gas f√∂r datan.

### √ñvning - till√§mpa K-Neighbors-klassificeraren

Den tidigare klassificeraren var bra och fungerade v√§l med datan, men kanske kan vi f√• b√§ttre noggrannhet. Prova en K-Neighbors-klassificerare.

1. L√§gg till en rad i din klassificerar-array (l√§gg till ett kommatecken efter Linear SVC-posten):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Resultatet √§r lite s√§mre:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ L√§s mer om [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector Classifier

Support-Vector-klassificerare √§r en del av [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine)-familjen av ML-metoder som anv√§nds f√∂r klassificerings- och regressionsuppgifter. SVMs "kartl√§gger tr√§ningsprover till punkter i rymden" f√∂r att maximera avst√•ndet mellan tv√• kategorier. Efterf√∂ljande data kartl√§ggs in i detta utrymme s√• att deras kategori kan f√∂ruts√§gas.

### √ñvning - till√§mpa en Support Vector Classifier

L√•t oss f√∂rs√∂ka f√• lite b√§ttre noggrannhet med en Support Vector Classifier.

1. L√§gg till ett kommatecken efter K-Neighbors-posten och l√§gg sedan till denna rad:

    ```python
    'SVC': SVC(),
    ```

    Resultatet √§r riktigt bra!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ L√§s mer om [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble-klassificerare

L√•t oss f√∂lja v√§gen till slutet, √§ven om det f√∂reg√•ende testet var riktigt bra. L√•t oss prova n√•gra 'Ensemble Classifiers', specifikt Random Forest och AdaBoost:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Resultatet √§r mycket bra, s√§rskilt f√∂r Random Forest:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ L√§s mer om [Ensemble Classifiers](https://scikit-learn.org/stable/modules/ensemble.html)

Denna metod f√∂r maskininl√§rning "kombinerar f√∂ruts√§gelser fr√•n flera basestimatorer" f√∂r att f√∂rb√§ttra modellens kvalitet. I v√•rt exempel anv√§nde vi Random Trees och AdaBoost. 

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), en genomsnittsmetod, bygger en 'skog' av 'beslutstr√§d' med inslag av slumpm√§ssighet f√∂r att undvika √∂veranpassning. Parametern n_estimators √§r inst√§lld p√• antalet tr√§d.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) anpassar en klassificerare till en dataset och anpassar sedan kopior av den klassificeraren till samma dataset. Den fokuserar p√• vikterna av felklassificerade objekt och justerar anpassningen f√∂r n√§sta klassificerare f√∂r att korrigera.

---

## üöÄUtmaning

Var och en av dessa tekniker har ett stort antal parametrar som du kan justera. Unders√∂k standardparametrarna f√∂r var och en och fundera p√• vad justering av dessa parametrar skulle inneb√§ra f√∂r modellens kvalitet.

## [Eftertest-quiz](https://ff-quizzes.netlify.app/en/ml/)

## Granskning & Sj√§lvstudier

Det finns mycket fackspr√•k i dessa lektioner, s√• ta en minut att granska [denna lista](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) med anv√§ndbar terminologi!

## Uppgift 

[Parameterlek](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.