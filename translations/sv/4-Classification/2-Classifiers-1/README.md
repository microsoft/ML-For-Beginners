<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1a6e9e46b34a2e559fbbfc1f95397c7b",
  "translation_date": "2025-09-05T21:50:01+00:00",
  "source_file": "4-Classification/2-Classifiers-1/README.md",
  "language_code": "sv"
}
-->
# Klassificering av k√∂k 1

I den h√§r lektionen kommer du att anv√§nda datasetet som du sparade fr√•n den senaste lektionen, fullt av balanserad och ren data om olika k√∂k.

Du kommer att anv√§nda detta dataset med en m√§ngd olika klassificerare f√∂r att _f√∂ruts√§ga ett visst nationellt k√∂k baserat p√• en grupp ingredienser_. Under tiden kommer du att l√§ra dig mer om hur algoritmer kan anv√§ndas f√∂r klassificeringsuppgifter.

## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)
# F√∂rberedelse

Om du har slutf√∂rt [Lektionen 1](../1-Introduction/README.md), se till att en _cleaned_cuisines.csv_-fil finns i rotmappen `/data` f√∂r dessa fyra lektioner.

## √ñvning - f√∂ruts√§g ett nationellt k√∂k

1. Arbeta i den h√§r lektionens _notebook.ipynb_-mapp och importera filen tillsammans med Pandas-biblioteket:

    ```python
    import pandas as pd
    cuisines_df = pd.read_csv("../data/cleaned_cuisines.csv")
    cuisines_df.head()
    ```

    Datat ser ut s√• h√§r:

|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |
| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |
| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |
| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |
  

1. Importera nu flera andra bibliotek:

    ```python
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    from sklearn.svm import SVC
    import numpy as np
    ```

1. Dela upp X- och y-koordinaterna i tv√• dataframes f√∂r tr√§ning. `cuisine` kan vara etikett-databasen:

    ```python
    cuisines_label_df = cuisines_df['cuisine']
    cuisines_label_df.head()
    ```

    Det kommer att se ut s√• h√§r:

    ```output
    0    indian
    1    indian
    2    indian
    3    indian
    4    indian
    Name: cuisine, dtype: object
    ```

1. Ta bort kolumnen `Unnamed: 0` och kolumnen `cuisine` genom att anv√§nda `drop()`. Spara resten av datan som tr√§ningsbara funktioner:

    ```python
    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
    cuisines_feature_df.head()
    ```

    Dina funktioner ser ut s√• h√§r:

|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |
| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |
|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |
|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |

Nu √§r du redo att tr√§na din modell!

## V√§lja klassificerare

Nu n√§r din data √§r ren och redo f√∂r tr√§ning m√•ste du best√§mma vilken algoritm du ska anv√§nda f√∂r uppgiften.

Scikit-learn grupperar klassificering under Supervised Learning, och inom den kategorin hittar du m√•nga s√§tt att klassificera. [Variationen](https://scikit-learn.org/stable/supervised_learning.html) kan verka √∂verv√§ldigande vid f√∂rsta anblicken. F√∂ljande metoder inkluderar alla klassificeringstekniker:

- Linj√§ra modeller
- Support Vector Machines
- Stokastisk gradientnedstigning
- N√§rmaste grannar
- Gaussiska processer
- Beslutstr√§d
- Ensemblemetoder (r√∂stningsklassificerare)
- Multiklass- och multioutput-algoritmer (multiklass- och multilabel-klassificering, multiklass-multioutput-klassificering)

> Du kan ocks√• anv√§nda [neurala n√§tverk f√∂r att klassificera data](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), men det ligger utanf√∂r denna lektions omfattning.

### Vilken klassificerare ska man v√§lja?

S√•, vilken klassificerare ska du v√§lja? Ofta kan man testa flera och leta efter ett bra resultat. Scikit-learn erbjuder en [j√§mf√∂relse sida vid sida](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) p√• ett skapat dataset, d√§r KNeighbors, SVC p√• tv√• s√§tt, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB och QuadraticDiscriminationAnalysis j√§mf√∂rs och resultaten visualiseras:

![j√§mf√∂relse av klassificerare](../../../../4-Classification/2-Classifiers-1/images/comparison.png)
> Diagram genererade fr√•n Scikit-learns dokumentation

> AutoML l√∂ser detta problem smidigt genom att k√∂ra dessa j√§mf√∂relser i molnet, vilket g√∂r att du kan v√§lja den b√§sta algoritmen f√∂r din data. Prova det [h√§r](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)

### En b√§ttre metod

En b√§ttre metod √§n att gissa vilt √§r att f√∂lja id√©erna i detta nedladdningsbara [ML Cheat Sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). H√§r uppt√§cker vi att vi f√∂r v√•rt multiklassproblem har n√•gra alternativ:

![fusklapp f√∂r multiklassproblem](../../../../4-Classification/2-Classifiers-1/images/cheatsheet.png)
> En del av Microsofts Algorithm Cheat Sheet, som beskriver alternativ f√∂r multiklassklassificering

‚úÖ Ladda ner denna fusklapp, skriv ut den och h√§ng upp den p√• v√§ggen!

### Resonemang

L√•t oss se om vi kan resonera oss fram till olika metoder med tanke p√• de begr√§nsningar vi har:

- **Neurala n√§tverk √§r f√∂r tunga**. Med tanke p√• v√•rt rena men minimala dataset och det faktum att vi k√∂r tr√§ning lokalt via notebooks, √§r neurala n√§tverk f√∂r resurskr√§vande f√∂r denna uppgift.
- **Ingen tv√•klassklassificerare**. Vi anv√§nder inte en tv√•klassklassificerare, s√• det utesluter one-vs-all.
- **Beslutstr√§d eller logistisk regression kan fungera**. Ett beslutstr√§d kan fungera, eller logistisk regression f√∂r multiklassdata.
- **Multiklass Boosted Decision Trees l√∂ser ett annat problem**. Multiklass Boosted Decision Tree √§r mest l√§mplig f√∂r icke-parametriska uppgifter, t.ex. uppgifter som √§r utformade f√∂r att skapa rankningar, s√• det √§r inte anv√§ndbart f√∂r oss.

### Anv√§nda Scikit-learn 

Vi kommer att anv√§nda Scikit-learn f√∂r att analysera v√•r data. Det finns dock m√•nga s√§tt att anv√§nda logistisk regression i Scikit-learn. Ta en titt p√• [parametrarna att skicka](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  

I huvudsak finns det tv√• viktiga parametrar - `multi_class` och `solver` - som vi beh√∂ver specificera n√§r vi ber Scikit-learn att utf√∂ra en logistisk regression. V√§rdet p√• `multi_class` till√§mpar ett visst beteende. V√§rdet p√• solver √§r vilken algoritm som ska anv√§ndas. Inte alla solvers kan kombineras med alla `multi_class`-v√§rden.

Enligt dokumentationen, i multiklassfallet, tr√§ningsalgoritmen:

- **Anv√§nder one-vs-rest (OvR)-schemat**, om `multi_class`-alternativet √§r inst√§llt p√• `ovr`
- **Anv√§nder korsentropif√∂rlust**, om `multi_class`-alternativet √§r inst√§llt p√• `multinomial`. (F√∂r n√§rvarande st√∂ds `multinomial`-alternativet endast av solvers ‚Äòlbfgs‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô och ‚Äònewton-cg‚Äô.)

> üéì 'Schemat' h√§r kan antingen vara 'ovr' (one-vs-rest) eller 'multinomial'. Eftersom logistisk regression egentligen √§r utformad f√∂r att st√∂dja bin√§r klassificering, till√•ter dessa scheman den att b√§ttre hantera multiklassklassificeringsuppgifter. [k√§lla](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)

> üéì 'Solver' definieras som "algoritmen som ska anv√§ndas i optimeringsproblemet". [k√§lla](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).

Scikit-learn erbjuder denna tabell f√∂r att f√∂rklara hur solvers hanterar olika utmaningar som presenteras av olika typer av datastrukturer:

![solvers](../../../../4-Classification/2-Classifiers-1/images/solvers.png)

## √ñvning - dela upp datan

Vi kan fokusera p√• logistisk regression f√∂r v√•r f√∂rsta tr√§ningsf√∂rs√∂k eftersom du nyligen l√§rde dig om den i en tidigare lektion.
Dela upp din data i tr√§nings- och testgrupper genom att kalla p√• `train_test_split()`:

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```

## √ñvning - till√§mpa logistisk regression

Eftersom du anv√§nder multiklassfallet beh√∂ver du v√§lja vilket _schema_ du ska anv√§nda och vilken _solver_ du ska st√§lla in. Anv√§nd LogisticRegression med en multiklassinst√§llning och **liblinear**-solver f√∂r att tr√§na.

1. Skapa en logistisk regression med multi_class inst√§lld p√• `ovr` och solver inst√§lld p√• `liblinear`:

    ```python
    lr = LogisticRegression(multi_class='ovr',solver='liblinear')
    model = lr.fit(X_train, np.ravel(y_train))
    
    accuracy = model.score(X_test, y_test)
    print ("Accuracy is {}".format(accuracy))
    ```

    ‚úÖ Prova en annan solver som `lbfgs`, som ofta √§r inst√§lld som standard
> Observera, anv√§nd Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html)-funktionen f√∂r att platta ut dina data vid behov.
Noggrannheten √§r bra p√• √∂ver **80%**!

1. Du kan se denna modell i aktion genom att testa en rad data (#50):

    ```python
    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
    print(f'cuisine: {y_test.iloc[50]}')
    ```

    Resultatet skrivs ut:

   ```output
   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')
   cuisine: indian
   ```

   ‚úÖ Prova ett annat radnummer och kontrollera resultaten

1. F√∂r att g√• djupare kan du kontrollera noggrannheten f√∂r denna f√∂ruts√§gelse:

    ```python
    test= X_test.iloc[50].values.reshape(-1, 1).T
    proba = model.predict_proba(test)
    classes = model.classes_
    resultdf = pd.DataFrame(data=proba, columns=classes)
    
    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
    topPrediction.head()
    ```

    Resultatet skrivs ut - indisk mat √§r dess b√§sta gissning, med h√∂g sannolikhet:

    |          |        0 |
    | -------: | -------: |
    |   indian | 0.715851 |
    |  chinese | 0.229475 |
    | japanese | 0.029763 |
    |   korean | 0.017277 |
    |     thai | 0.007634 |

    ‚úÖ Kan du f√∂rklara varf√∂r modellen √§r ganska s√§ker p√• att detta √§r indisk mat?

1. F√• mer detaljer genom att skriva ut en klassificeringsrapport, precis som du gjorde i regression-lektionerna:

    ```python
    y_pred = model.predict(X_test)
    print(classification_report(y_test,y_pred))
    ```

    |              | precision | recall | f1-score | support |
    | ------------ | --------- | ------ | -------- | ------- |
    | chinese      | 0.73      | 0.71   | 0.72     | 229     |
    | indian       | 0.91      | 0.93   | 0.92     | 254     |
    | japanese     | 0.70      | 0.75   | 0.72     | 220     |
    | korean       | 0.86      | 0.76   | 0.81     | 242     |
    | thai         | 0.79      | 0.85   | 0.82     | 254     |
    | accuracy     | 0.80      | 1199   |          |         |
    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |
    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |

## üöÄUtmaning

I denna lektion anv√§nde du dina rensade data f√∂r att bygga en maskininl√§rningsmodell som kan f√∂ruts√§ga ett nationellt k√∂k baserat p√• en serie ingredienser. Ta lite tid att l√§sa igenom de m√•nga alternativ som Scikit-learn erbjuder f√∂r att klassificera data. G√• djupare in i konceptet 'solver' f√∂r att f√∂rst√• vad som h√§nder bakom kulisserna.

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

## Granskning & Sj√§lvstudier

Gr√§v lite djupare i matematiken bakom logistisk regression i [denna lektion](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)  
## Uppgift 

[Studera l√∂sningsmetoderna](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r du vara medveten om att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller inexaktheter. Det ursprungliga dokumentet p√• dess ursprungliga spr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.