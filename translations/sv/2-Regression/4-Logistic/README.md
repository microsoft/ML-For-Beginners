<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-05T21:10:30+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "sv"
}
-->
# Logistisk regression f√∂r att f√∂ruts√§ga kategorier

![Infografik om logistisk vs. linj√§r regression](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Quiz f√∂re f√∂rel√§sning](https://ff-quizzes.netlify.app/en/ml/)

> ### [Denna lektion finns tillg√§nglig i R!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Introduktion

I denna sista lektion om regression, en av de grundl√§ggande _klassiska_ ML-teknikerna, ska vi titta p√• logistisk regression. Du kan anv√§nda denna teknik f√∂r att uppt√§cka m√∂nster och f√∂ruts√§ga bin√§ra kategorier. √Ñr detta godis choklad eller inte? √Ñr denna sjukdom smittsam eller inte? Kommer denna kund att v√§lja denna produkt eller inte?

I denna lektion kommer du att l√§ra dig:

- Ett nytt bibliotek f√∂r datavisualisering
- Tekniker f√∂r logistisk regression

‚úÖ F√∂rdjupa din f√∂rst√•else f√∂r att arbeta med denna typ av regression i detta [Learn-modul](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## F√∂rkunskaper

Efter att ha arbetat med pumpadatan √§r vi nu tillr√§ckligt bekanta med den f√∂r att inse att det finns en bin√§r kategori vi kan arbeta med: `Color`.

L√•t oss bygga en modell f√∂r logistisk regression f√∂r att f√∂ruts√§ga vilken f√§rg en given pumpa sannolikt har (orange üéÉ eller vit üëª), baserat p√• vissa variabler.

> Varf√∂r pratar vi om bin√§r klassificering i en lektion om regression? Endast av spr√•klig bekv√§mlighet, eftersom logistisk regression [egentligen √§r en klassificeringsmetod](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), √§ven om den √§r linj√§rbaserad. L√§r dig om andra s√§tt att klassificera data i n√§sta lektionsgrupp.

## Definiera fr√•gan

F√∂r v√•ra √§ndam√•l kommer vi att uttrycka detta som en bin√§r: 'Vit' eller 'Inte vit'. Det finns ocks√• en 'randig' kategori i v√•r dataset, men det finns f√• instanser av den, s√• vi kommer inte att anv√§nda den. Den f√∂rsvinner √§nd√• n√§r vi tar bort nullv√§rden fr√•n datasetet.

> üéÉ Rolig fakta: vi kallar ibland vita pumpor f√∂r 'sp√∂kpumpor'. De √§r inte s√§rskilt l√§tta att sk√§ra, s√• de √§r inte lika popul√§ra som de orangea, men de ser coola ut! S√• vi kan ocks√• formulera om v√•r fr√•ga som: 'Sp√∂ke' eller 'Inte sp√∂ke'. üëª

## Om logistisk regression

Logistisk regression skiljer sig fr√•n linj√§r regression, som du l√§rde dig om tidigare, p√• n√•gra viktiga s√§tt.

[![ML f√∂r nyb√∂rjare - F√∂rst√• logistisk regression f√∂r maskininl√§rningsklassificering](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML f√∂r nyb√∂rjare - F√∂rst√• logistisk regression f√∂r maskininl√§rningsklassificering")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt av logistisk regression.

### Bin√§r klassificering

Logistisk regression erbjuder inte samma funktioner som linj√§r regression. Den f√∂rstn√§mnda ger en f√∂ruts√§gelse om en bin√§r kategori ("vit eller inte vit"), medan den senare kan f√∂ruts√§ga kontinuerliga v√§rden, till exempel givet ursprunget av en pumpa och sk√∂rdetiden, _hur mycket dess pris kommer att stiga_.

![Pumpaklassificeringsmodell](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografik av [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Andra klassificeringar

Det finns andra typer av logistisk regression, inklusive multinomial och ordinal:

- **Multinomial**, som inneb√§r att ha mer √§n en kategori - "Orange, Vit och Randig".
- **Ordinal**, som inneb√§r ordnade kategorier, anv√§ndbart om vi vill ordna v√•ra resultat logiskt, som v√•ra pumpor som √§r ordnade efter ett begr√§nsat antal storlekar (mini, sm, med, lg, xl, xxl).

![Multinomial vs ordinal regression](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Variabler BEH√ñVER INTE korrelera

Kommer du ih√•g hur linj√§r regression fungerade b√§ttre med mer korrelerade variabler? Logistisk regression √§r motsatsen - variablerna beh√∂ver inte st√§mma √∂verens. Det fungerar f√∂r denna data som har n√•got svaga korrelationer.

### Du beh√∂ver mycket ren data

Logistisk regression ger mer exakta resultat om du anv√§nder mer data; v√•rt lilla dataset √§r inte optimalt f√∂r denna uppgift, s√• ha det i √•tanke.

[![ML f√∂r nyb√∂rjare - Dataanalys och f√∂rberedelse f√∂r logistisk regression](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML f√∂r nyb√∂rjare - Dataanalys och f√∂rberedelse f√∂r logistisk regression")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt av att f√∂rbereda data f√∂r linj√§r regression

‚úÖ Fundera p√• vilka typer av data som skulle passa bra f√∂r logistisk regression

## √ñvning - st√§da datan

F√∂rst, st√§da datan lite, ta bort nullv√§rden och v√§lj endast n√•gra av kolumnerna:

1. L√§gg till f√∂ljande kod:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Du kan alltid ta en titt p√• din nya dataframe:

    ```python
    pumpkins.info
    ```

### Visualisering - kategoriskt diagram

Vid det h√§r laget har du laddat upp [startnotebooken](../../../../2-Regression/4-Logistic/notebook.ipynb) med pumpadatan igen och st√§dat den f√∂r att bevara ett dataset som inneh√•ller n√•gra variabler, inklusive `Color`. L√•t oss visualisera dataframe i notebooken med ett annat bibliotek: [Seaborn](https://seaborn.pydata.org/index.html), som √§r byggt p√• Matplotlib som vi anv√§nde tidigare.

Seaborn erbjuder n√•gra smarta s√§tt att visualisera din data. Till exempel kan du j√§mf√∂ra distributioner av datan f√∂r varje `Variety` och `Color` i ett kategoriskt diagram.

1. Skapa ett s√•dant diagram genom att anv√§nda funktionen `catplot`, med v√•r pumpadata `pumpkins`, och specificera en f√§rgkartl√§ggning f√∂r varje pumpakategori (orange eller vit):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Ett rutn√§t av visualiserad data](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Genom att observera datan kan du se hur f√§rgdata relaterar till sort.

    ‚úÖ Givet detta kategoriska diagram, vilka intressanta unders√∂kningar kan du f√∂rest√§lla dig?

### Datapreparation: kodning av funktioner och etiketter
V√•rt pumpadataset inneh√•ller str√§ngv√§rden f√∂r alla dess kolumner. Att arbeta med kategorisk data √§r intuitivt f√∂r m√§nniskor men inte f√∂r maskiner. Maskininl√§rningsalgoritmer fungerar bra med siffror. D√§rf√∂r √§r kodning ett mycket viktigt steg i datapreparationsfasen, eftersom det g√∂r det m√∂jligt f√∂r oss att omvandla kategorisk data till numerisk data utan att f√∂rlora n√•gon information. Bra kodning leder till att bygga en bra modell.

F√∂r kodning av funktioner finns det tv√• huvudtyper av kodare:

1. Ordinal kodare: passar bra f√∂r ordnade variabler, som √§r kategoriska variabler d√§r deras data f√∂ljer en logisk ordning, som kolumnen `Item Size` i v√•rt dataset. Den skapar en kartl√§ggning s√• att varje kategori representeras av ett nummer, vilket √§r ordningen p√• kategorin i kolumnen.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorisk kodare: passar bra f√∂r nominella variabler, som √§r kategoriska variabler d√§r deras data inte f√∂ljer en logisk ordning, som alla funktioner utom `Item Size` i v√•rt dataset. Det √§r en one-hot-kodning, vilket inneb√§r att varje kategori representeras av en bin√§r kolumn: den kodade variabeln √§r lika med 1 om pumpan tillh√∂r den sorten och 0 annars.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```
Sedan anv√§nds `ColumnTransformer` f√∂r att kombinera flera kodare i ett enda steg och till√§mpa dem p√• de l√§mpliga kolumnerna.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```
√Ö andra sidan, f√∂r att koda etiketten, anv√§nder vi scikit-learn-klassen `LabelEncoder`, som √§r en hj√§lpsklass f√∂r att normalisera etiketter s√• att de endast inneh√•ller v√§rden mellan 0 och n_classes-1 (h√§r, 0 och 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```
N√§r vi har kodat funktionerna och etiketten kan vi sl√• samman dem till en ny dataframe `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```
‚úÖ Vilka √§r f√∂rdelarna med att anv√§nda en ordinal kodare f√∂r kolumnen `Item Size`?

### Analysera relationer mellan variabler

Nu n√§r vi har f√∂rberett v√•r data kan vi analysera relationerna mellan funktionerna och etiketten f√∂r att f√• en uppfattning om hur v√§l modellen kommer att kunna f√∂ruts√§ga etiketten baserat p√• funktionerna.
Det b√§sta s√§ttet att utf√∂ra denna typ av analys √§r att plotta datan. Vi kommer att anv√§nda Seaborns funktion `catplot` igen f√∂r att visualisera relationerna mellan `Item Size`, `Variety` och `Color` i ett kategoriskt diagram. F√∂r att b√§ttre plotta datan kommer vi att anv√§nda den kodade kolumnen `Item Size` och den okodade kolumnen `Variety`.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```
![Ett kategoriskt diagram av visualiserad data](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### Anv√§nd ett swarm-diagram

Eftersom Color √§r en bin√§r kategori (Vit eller Inte), beh√∂ver den 'en [specialiserad metod](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) f√∂r visualisering'. Det finns andra s√§tt att visualisera relationen mellan denna kategori och andra variabler.

Du kan visualisera variabler sida vid sida med Seaborn-diagram.

1. Prova ett 'swarm'-diagram f√∂r att visa distributionen av v√§rden:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Ett swarm-diagram av visualiserad data](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Observera**: koden ovan kan generera en varning, eftersom Seaborn misslyckas med att representera en s√•dan m√§ngd datapunkter i ett swarm-diagram. En m√∂jlig l√∂sning √§r att minska storleken p√• mark√∂ren genom att anv√§nda parametern 'size'. Men var medveten om att detta p√•verkar l√§sbarheten av diagrammet.

> **üßÆ Visa mig matematiken**
>
> Logistisk regression bygger p√• konceptet 'maximum likelihood' med hj√§lp av [sigmoidfunktioner](https://wikipedia.org/wiki/Sigmoid_function). En 'Sigmoidfunktion' p√• ett diagram ser ut som en 'S'-form. Den tar ett v√§rde och kartl√§gger det till n√•gonstans mellan 0 och 1. Dess kurva kallas ocks√• f√∂r en 'logistisk kurva'. Dess formel ser ut s√• h√§r:
>
> ![logistisk funktion](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> d√§r sigmoids mittpunkt befinner sig vid x:s 0-punkt, L √§r kurvans maximala v√§rde, och k √§r kurvans branthet. Om resultatet av funktionen √§r mer √§n 0.5, kommer etiketten i fr√•ga att ges klassen '1' av det bin√§ra valet. Om inte, kommer den att klassificeras som '0'.

## Bygg din modell

Att bygga en modell f√∂r att hitta dessa bin√§ra klassificeringar √§r f√∂rv√•nansv√§rt enkelt i Scikit-learn.

[![ML f√∂r nyb√∂rjare - Logistisk regression f√∂r klassificering av data](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML f√∂r nyb√∂rjare - Logistisk regression f√∂r klassificering av data")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt av att bygga en linj√§r regressionsmodell

1. V√§lj de variabler du vill anv√§nda i din klassificeringsmodell och dela upp tr√§nings- och testupps√§ttningarna genom att kalla p√• `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Nu kan du tr√§na din modell genom att kalla p√• `fit()` med din tr√§ningsdata och skriva ut dess resultat:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Ta en titt p√• din modells po√§ngrapport. Den √§r inte d√•lig, med tanke p√• att du bara har cirka 1000 rader data:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## B√§ttre f√∂rst√•else via en f√∂rvirringsmatris

√Ñven om du kan f√• en po√§ngrapport [termer](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) genom att skriva ut ovanst√•ende, kan du kanske f√∂rst√• din modell l√§ttare genom att anv√§nda en [f√∂rvirringsmatris](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) f√∂r att hj√§lpa oss att f√∂rst√• hur modellen presterar.

> üéì En '[f√∂rvirringsmatris](https://wikipedia.org/wiki/Confusion_matrix)' (eller 'felmatris') √§r en tabell som uttrycker din modells verkliga vs. falska positiva och negativa, och d√§rmed m√§ter noggrannheten i f√∂ruts√§gelserna.

1. F√∂r att anv√§nda en f√∂rvirringsmatris, kalla p√• `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Ta en titt p√• din modells f√∂rvirringsmatris:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

I Scikit-learn √§r rader (axel 0) faktiska etiketter och kolumner (axel 1) f√∂rutsagda etiketter.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Vad h√§nder h√§r? L√•t oss s√§ga att v√•r modell ombeds att klassificera pumpor mellan tv√• bin√§ra kategorier, kategori 'vit' och kategori 'inte-vit'.

- Om din modell f√∂rutsp√•r en pumpa som inte vit och den tillh√∂r kategorin 'inte-vit' i verkligheten kallar vi det en sann negativ, visat av det √∂vre v√§nstra numret.
- Om din modell f√∂rutsp√•r en pumpa som vit och den tillh√∂r kategorin 'inte-vit' i verkligheten kallar vi det en falsk negativ, visat av det nedre v√§nstra numret. 
- Om din modell f√∂rutsp√•r en pumpa som inte vit och den tillh√∂r kategorin 'vit' i verkligheten kallar vi det en falsk positiv, visat av det √∂vre h√∂gra numret. 
- Om din modell f√∂rutsp√•r en pumpa som vit och den tillh√∂r kategorin 'vit' i verkligheten kallar vi det en sann positiv, visat av det nedre h√∂gra numret.

Som du kanske har gissat √§r det att f√∂redra att ha ett st√∂rre antal sanna positiva och sanna negativa och ett l√§gre antal falska positiva och falska negativa, vilket inneb√§r att modellen presterar b√§ttre.
Hur relaterar f√∂rvirringsmatrisen till precision och √•terkallning? Kom ih√•g att klassificeringsrapporten som skrivits ut ovan visade precision (0,85) och √•terkallning (0,67).

Precision = tp / (tp + fp) = 22 / (22 + 4) = 0,8461538461538461

√Öterkallning = tp / (tp + fn) = 22 / (22 + 11) = 0,6666666666666666

‚úÖ F: Enligt f√∂rvirringsmatrisen, hur presterade modellen? S: Inte d√•ligt; det finns ett bra antal sanna negativa men ocks√• n√•gra falska negativa.

L√•t oss √•terbes√∂ka de termer vi s√•g tidigare med hj√§lp av f√∂rvirringsmatrisens kartl√§ggning av TP/TN och FP/FN:

üéì Precision: TP/(TP + FP) Andelen relevanta instanser bland de h√§mtade instanserna (t.ex. vilka etiketter som var korrekt m√§rkta)

üéì √Öterkallning: TP/(TP + FN) Andelen relevanta instanser som h√§mtades, oavsett om de var korrekt m√§rkta eller inte

üéì f1-score: (2 * precision * √•terkallning)/(precision + √•terkallning) Ett viktat genomsnitt av precision och √•terkallning, d√§r b√§sta √§r 1 och s√§msta √§r 0

üéì Support: Antalet f√∂rekomster av varje etikett som h√§mtats

üéì Noggrannhet: (TP + TN)/(TP + TN + FP + FN) Procentandelen etiketter som f√∂rutsades korrekt f√∂r ett urval.

üéì Makro-genomsnitt: Ber√§kningen av de oviktade genomsnittliga m√•tten f√∂r varje etikett, utan att ta h√§nsyn till obalans mellan etiketter.

üéì Viktat genomsnitt: Ber√§kningen av de genomsnittliga m√•tten f√∂r varje etikett, med h√§nsyn till obalans mellan etiketter genom att v√§ga dem efter deras support (antalet sanna instanser f√∂r varje etikett).

‚úÖ Kan du t√§nka dig vilken metrisk du b√∂r fokusera p√• om du vill att din modell ska minska antalet falska negativa?

## Visualisera ROC-kurvan f√∂r denna modell

[![ML f√∂r nyb√∂rjare - Analysera logistisk regressionsprestanda med ROC-kurvor](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML f√∂r nyb√∂rjare - Analysera logistisk regressionsprestanda med ROC-kurvor")

> üé• Klicka p√• bilden ovan f√∂r en kort videogenomg√•ng av ROC-kurvor

L√•t oss g√∂ra en visualisering till f√∂r att se den s√• kallade 'ROC'-kurvan:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

Anv√§nd Matplotlib f√∂r att plotta modellens [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) eller ROC. ROC-kurvor anv√§nds ofta f√∂r att f√• en √∂verblick √∂ver en klassificerares resultat i termer av dess sanna respektive falska positiva. "ROC-kurvor har vanligtvis den sanna positiva frekvensen p√• Y-axeln och den falska positiva frekvensen p√• X-axeln." D√§rf√∂r spelar kurvans branthet och utrymmet mellan mittlinjen och kurvan roll: du vill ha en kurva som snabbt g√•r upp och √∂ver linjen. I v√•rt fall finns det falska positiva i b√∂rjan, och sedan g√•r linjen upp och √∂ver korrekt:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Anv√§nd slutligen Scikit-learns [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) f√∂r att ber√§kna det faktiska 'Area Under the Curve' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Resultatet √§r `0.9749908725812341`. Eftersom AUC str√§cker sig fr√•n 0 till 1 vill du ha ett h√∂gt v√§rde, eftersom en modell som √§r 100 % korrekt i sina f√∂ruts√§gelser kommer att ha en AUC p√• 1; i detta fall √§r modellen _ganska bra_.

I framtida lektioner om klassificeringar kommer du att l√§ra dig hur du itererar f√∂r att f√∂rb√§ttra modellens resultat. Men f√∂r nu, grattis! Du har avslutat dessa regressionslektioner!

---
## üöÄUtmaning

Det finns mycket mer att utforska kring logistisk regression! Men det b√§sta s√§ttet att l√§ra sig √§r att experimentera. Hitta en dataset som l√§mpar sig f√∂r denna typ av analys och bygg en modell med den. Vad l√§r du dig? tips: prova [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) f√∂r intressanta dataset.

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

## Granskning & Sj√§lvstudier

L√§s de f√∂rsta sidorna av [denna artikel fr√•n Stanford](https://web.stanford.edu/~jurafsky/slp3/5.pdf) om n√•gra praktiska anv√§ndningar f√∂r logistisk regression. Fundera p√• uppgifter som √§r b√§ttre l√§mpade f√∂r den ena eller andra typen av regressionsuppgifter som vi har studerat hittills. Vad skulle fungera b√§st?

## Uppgift 

[F√∂rs√∂k denna regression igen](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen notera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• sitt ursprungliga spr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.