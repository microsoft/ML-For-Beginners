<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-05T21:06:20+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "sv"
}
-->
# Bygg en regressionsmodell med Scikit-learn: regression p√• fyra s√§tt

![Infografik om linj√§r vs polynomisk regression](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografik av [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

> ### [Den h√§r lektionen finns tillg√§nglig i R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introduktion 

Hittills har du utforskat vad regression √§r med exempeldata fr√•n pumpaprisdatam√§ngden som vi kommer att anv√§nda genom hela denna lektion. Du har ocks√• visualiserat den med hj√§lp av Matplotlib.

Nu √§r du redo att f√∂rdjupa dig i regression f√∂r maskininl√§rning. Medan visualisering hj√§lper dig att f√∂rst√• data, ligger den verkliga styrkan i maskininl√§rning i _att tr√§na modeller_. Modeller tr√§nas p√• historisk data f√∂r att automatiskt f√•nga datadependenser, och de g√∂r det m√∂jligt att f√∂ruts√§ga resultat f√∂r ny data som modellen inte har sett tidigare.

I denna lektion kommer du att l√§ra dig mer om tv√• typer av regression: _grundl√§ggande linj√§r regression_ och _polynomisk regression_, tillsammans med lite av matematiken bakom dessa tekniker. Dessa modeller kommer att hj√§lpa oss att f√∂ruts√§ga pumpapriser baserat p√• olika indata.

[![ML f√∂r nyb√∂rjare - F√∂rst√• linj√§r regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML f√∂r nyb√∂rjare - F√∂rst√• linj√§r regression")

> üé• Klicka p√• bilden ovan f√∂r en kort videogenomg√•ng av linj√§r regression.

> Genom hela detta l√§roplan antar vi minimal kunskap om matematik och str√§var efter att g√∂ra det tillg√§ngligt f√∂r studenter fr√•n andra omr√•den, s√• h√•ll utkik efter anteckningar, üßÆ matematiska inslag, diagram och andra l√§rverktyg f√∂r att underl√§tta f√∂rst√•elsen.

### F√∂rkunskaper

Du b√∂r nu vara bekant med strukturen f√∂r pumpadatan som vi unders√∂ker. Du hittar den f√∂rladdad och f√∂rberedd i denna lektions _notebook.ipynb_-fil. I filen visas pumpapriset per sk√§ppa i en ny dataram. Se till att du kan k√∂ra dessa notebooks i k√§rnor i Visual Studio Code.

### F√∂rberedelse

Som en p√•minnelse laddar du denna data f√∂r att kunna st√§lla fr√•gor om den. 

- N√§r √§r det b√§sta tillf√§llet att k√∂pa pumpor? 
- Vilket pris kan jag f√∂rv√§nta mig f√∂r en l√•da med miniatyrpumpor?
- Ska jag k√∂pa dem i halvsk√§ppakorgar eller i l√•dor med 1 1/9 sk√§ppa?
L√•t oss forts√§tta att gr√§va i denna data.

I den f√∂reg√•ende lektionen skapade du en Pandas-dataram och fyllde den med en del av den ursprungliga datam√§ngden, standardiserade priss√§ttningen per sk√§ppa. Genom att g√∂ra det kunde du dock bara samla in cirka 400 datapunkter och endast f√∂r h√∂stm√•naderna.

Ta en titt p√• datan som vi har f√∂rladdat i denna lektions medf√∂ljande notebook. Datan √§r f√∂rladdad och ett initialt spridningsdiagram √§r skapat f√∂r att visa m√•natlig data. Kanske kan vi f√• lite mer detaljer om datans natur genom att reng√∂ra den ytterligare.

## En linj√§r regressionslinje

Som du l√§rde dig i Lektion 1 √§r m√•let med en linj√§r regressions√∂vning att kunna rita en linje f√∂r att:

- **Visa variabelrelationer**. Visa relationen mellan variabler
- **G√∂ra f√∂ruts√§gelser**. G√∂ra korrekta f√∂ruts√§gelser om var en ny datapunkt skulle hamna i f√∂rh√•llande till den linjen. 
 
Det √§r typiskt f√∂r **Least-Squares Regression** att dra denna typ av linje. Termen 'least-squares' betyder att alla datapunkter runt regressionslinjen kvadreras och sedan adderas. Idealiskt sett √§r den slutliga summan s√• liten som m√∂jligt, eftersom vi vill ha ett l√•gt antal fel, eller `least-squares`. 

Vi g√∂r detta eftersom vi vill modellera en linje som har den minsta kumulativa avst√•ndet fr√•n alla v√•ra datapunkter. Vi kvadrerar ocks√• termerna innan vi adderar dem eftersom vi √§r intresserade av dess storlek snarare √§n dess riktning.

> **üßÆ Visa mig matematiken** 
> 
> Denna linje, kallad _linjen f√∂r b√§sta passform_, kan uttryckas med [en ekvation](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` √§r den 'f√∂rklarande variabeln'. `Y` √§r den 'beroende variabeln'. Lutningen p√• linjen √§r `b` och `a` √§r y-interceptet, vilket h√§nvisar till v√§rdet p√• `Y` n√§r `X = 0`. 
>
>![ber√§kna lutningen](../../../../2-Regression/3-Linear/images/slope.png)
>
> F√∂rst, ber√§kna lutningen `b`. Infografik av [Jen Looper](https://twitter.com/jenlooper)
>
> Med andra ord, och med h√§nvisning till v√•r ursprungliga fr√•ga om pumpadata: "f√∂ruts√§g priset p√• en pumpa per sk√§ppa efter m√•nad", skulle `X` h√§nvisa till priset och `Y` h√§nvisa till f√∂rs√§ljningsm√•naden. 
>
>![slutf√∂r ekvationen](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Ber√§kna v√§rdet p√• Y. Om du betalar runt $4, m√•ste det vara april! Infografik av [Jen Looper](https://twitter.com/jenlooper)
>
> Matematiken som ber√§knar linjen m√•ste visa lutningen p√• linjen, vilket ocks√• beror p√• interceptet, eller var `Y` √§r placerad n√§r `X = 0`.
>
> Du kan observera metoden f√∂r ber√§kning av dessa v√§rden p√• [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) webbplatsen. Bes√∂k ocks√• [denna Least-squares kalkylator](https://www.mathsisfun.com/data/least-squares-calculator.html) f√∂r att se hur v√§rdena p√•verkar linjen.

## Korrelation

Ett annat begrepp att f√∂rst√• √§r **korrelationskoefficienten** mellan givna X- och Y-variabler. Med ett spridningsdiagram kan du snabbt visualisera denna koefficient. Ett diagram med datapunkter som √§r snyggt uppradade har h√∂g korrelation, men ett diagram med datapunkter spridda √∂verallt mellan X och Y har l√•g korrelation.

En bra linj√§r regressionsmodell √§r en som har en h√∂g (n√§rmare 1 √§n 0) korrelationskoefficient med hj√§lp av Least-Squares Regression-metoden med en regressionslinje.

‚úÖ K√∂r notebooken som f√∂ljer med denna lektion och titta p√• spridningsdiagrammet f√∂r m√•nad till pris. Verkar datan som associerar m√•nad till pris f√∂r pumpaf√∂rs√§ljning ha h√∂g eller l√•g korrelation, enligt din visuella tolkning av spridningsdiagrammet? F√∂r√§ndras det om du anv√§nder en mer detaljerad m√§tning ist√§llet f√∂r `M√•nad`, t.ex. *dag p√• √•ret* (dvs. antal dagar sedan √•rets b√∂rjan)?

I koden nedan antar vi att vi har rengjort datan och f√•tt en dataram kallad `new_pumpkins`, liknande f√∂ljande:

ID | M√•nad | DagP√•√Öret | Sort | Stad | F√∂rpackning | L√§gsta Pris | H√∂gsta Pris | Pris
---|-------|-----------|------|------|-------------|-------------|-------------|-----
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 sk√§ppakartonger | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 sk√§ppakartonger | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 sk√§ppakartonger | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 sk√§ppakartonger | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 sk√§ppakartonger | 15.0 | 15.0 | 13.636364

> Koden f√∂r att reng√∂ra datan finns i [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Vi har utf√∂rt samma reng√∂ringssteg som i f√∂reg√•ende lektion och har ber√§knat kolumnen `DagP√•√Öret` med hj√§lp av f√∂ljande uttryck: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Nu n√§r du har en f√∂rst√•else f√∂r matematiken bakom linj√§r regression, l√•t oss skapa en regressionsmodell f√∂r att se om vi kan f√∂ruts√§ga vilken pumpaf√∂rpackning som kommer att ha de b√§sta pumpapriserna. N√•gon som k√∂per pumpor f√∂r en h√∂gtidspumpaplats kanske vill ha denna information f√∂r att optimera sina ink√∂p av pumpaf√∂rpackningar f√∂r platsen.

## S√∂ka efter korrelation

[![ML f√∂r nyb√∂rjare - S√∂ka efter korrelation: Nyckeln till linj√§r regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML f√∂r nyb√∂rjare - S√∂ka efter korrelation: Nyckeln till linj√§r regression")

> üé• Klicka p√• bilden ovan f√∂r en kort videogenomg√•ng av korrelation.

Fr√•n f√∂reg√•ende lektion har du f√∂rmodligen sett att det genomsnittliga priset f√∂r olika m√•nader ser ut s√• h√§r:

<img alt="Genomsnittligt pris per m√•nad" src="../2-Data/images/barchart.png" width="50%"/>

Detta antyder att det borde finnas n√•gon korrelation, och vi kan f√∂rs√∂ka tr√§na en linj√§r regressionsmodell f√∂r att f√∂ruts√§ga relationen mellan `M√•nad` och `Pris`, eller mellan `DagP√•√Öret` och `Pris`. H√§r √§r spridningsdiagrammet som visar den senare relationen:

<img alt="Spridningsdiagram av Pris vs. Dag p√• √•ret" src="images/scatter-dayofyear.png" width="50%" /> 

L√•t oss se om det finns en korrelation med hj√§lp av funktionen `corr`:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Det verkar som att korrelationen √§r ganska liten, -0.15 f√∂r `M√•nad` och -0.17 f√∂r `DagP√•√Öret`, men det kan finnas en annan viktig relation. Det verkar som att det finns olika kluster av priser som motsvarar olika pumpasorter. F√∂r att bekr√§fta denna hypotes, l√•t oss plotta varje pumpakategori med en annan f√§rg. Genom att skicka en `ax`-parameter till spridningsplotfunktionen kan vi plotta alla punkter p√• samma diagram:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Spridningsdiagram av Pris vs. Dag p√• √•ret" src="images/scatter-dayofyear-color.png" width="50%" /> 

V√•r unders√∂kning antyder att sorten har st√∂rre effekt p√• det √∂vergripande priset √§n det faktiska f√∂rs√§ljningsdatumet. Vi kan se detta med ett stapeldiagram:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Stapeldiagram av pris vs sort" src="images/price-by-variety.png" width="50%" /> 

L√•t oss f√∂r tillf√§llet fokusera endast p√• en pumpasort, 'pie type', och se vilken effekt datumet har p√• priset:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Spridningsdiagram av Pris vs. Dag p√• √•ret" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Om vi nu ber√§knar korrelationen mellan `Pris` och `DagP√•√Öret` med hj√§lp av funktionen `corr`, kommer vi att f√• n√•got som `-0.27` - vilket betyder att det √§r meningsfullt att tr√§na en prediktiv modell.

> Innan du tr√§nar en linj√§r regressionsmodell √§r det viktigt att se till att v√•r data √§r ren. Linj√§r regression fungerar inte bra med saknade v√§rden, s√• det √§r vettigt att ta bort alla tomma celler:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Ett annat tillv√§gag√•ngss√§tt skulle vara att fylla dessa tomma v√§rden med medelv√§rden fr√•n motsvarande kolumn.

## Enkel linj√§r regression

[![ML f√∂r nyb√∂rjare - Linj√§r och polynomisk regression med Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML f√∂r nyb√∂rjare - Linj√§r och polynomisk regression med Scikit-learn")

> üé• Klicka p√• bilden ovan f√∂r en kort videogenomg√•ng av linj√§r och polynomisk regression.

F√∂r att tr√§na v√•r linj√§ra regressionsmodell kommer vi att anv√§nda biblioteket **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Vi b√∂rjar med att separera indata (funktioner) och det f√∂rv√§ntade resultatet (etikett) i separata numpy-arrayer:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Observera att vi var tvungna att utf√∂ra `reshape` p√• indata f√∂r att linj√§r regressionspaketet skulle f√∂rst√• det korrekt. Linj√§r regression f√∂rv√§ntar sig en 2D-array som indata, d√§r varje rad i arrayen motsvarar en vektor av indatafunktioner. I v√•rt fall, eftersom vi bara har en indata - beh√∂ver vi en array med formen N√ó1, d√§r N √§r datam√§ngdens storlek.

Sedan beh√∂ver vi dela upp datan i tr√§nings- och testdatam√§ngder, s√• att vi kan validera v√•r modell efter tr√§ning:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Slutligen tar det bara tv√• rader kod att tr√§na den faktiska linj√§ra regressionsmodellen. Vi definierar `LinearRegression`-objektet och anpassar det till v√•r data med hj√§lp av metoden `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objektet efter att ha anpassats inneh√•ller alla koefficienter f√∂r regressionen, som kan n√•s med egenskapen `.coef_`. I v√•rt fall finns det bara en koefficient, som b√∂r vara runt `-0.017`. Det betyder att priser verkar sjunka lite med tiden, men inte mycket, runt 2 cent per dag. Vi kan ocks√• n√• sk√§rningspunkten f√∂r regressionen med Y-axeln med `lin_reg.intercept_` - det kommer att vara runt `21` i v√•rt fall, vilket indikerar priset i b√∂rjan av √•ret.

F√∂r att se hur exakt v√•r modell √§r kan vi f√∂ruts√§ga priser p√• en testdatam√§ngd och sedan m√§ta hur n√§ra v√•ra f√∂ruts√§gelser √§r de f√∂rv√§ntade v√§rdena. Detta kan g√∂ras med hj√§lp av metrik f√∂r medelkvadratfel (MSE), vilket √§r medelv√§rdet av alla kvadrerade skillnader mellan f√∂rv√§ntat och f√∂rutsp√•tt v√§rde.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
V√•rt fel verkar ligga runt 2 punkter, vilket √§r ~17 %. Inte s√• bra. En annan indikator p√• modellkvalitet √§r **determinationskoefficienten**, som kan erh√•llas s√• h√§r:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Om v√§rdet √§r 0 betyder det att modellen inte tar h√§nsyn till indata och fungerar som den *s√§msta linj√§ra prediktorn*, vilket helt enkelt √§r medelv√§rdet av resultatet. V√§rdet 1 betyder att vi kan f√∂ruts√§ga alla f√∂rv√§ntade utdata perfekt. I v√•rt fall √§r koefficienten runt 0,06, vilket √§r ganska l√•gt.

Vi kan ocks√• plotta testdata tillsammans med regressionslinjen f√∂r att b√§ttre se hur regression fungerar i v√•rt fall:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Linj√§r regression" src="images/linear-results.png" width="50%" />

## Polynomregression

En annan typ av linj√§r regression √§r polynomregression. √Ñven om det ibland finns ett linj√§rt samband mellan variabler - ju st√∂rre pumpan √§r i volym, desto h√∂gre pris - kan dessa samband ibland inte plottas som ett plan eller en rak linje.

‚úÖ H√§r √§r [n√•gra fler exempel](https://online.stat.psu.edu/stat501/lesson/9/9.8) p√• data som kan anv√§nda polynomregression.

Titta en g√•ng till p√• sambandet mellan datum och pris. Ser detta spridningsdiagram ut som om det n√∂dv√§ndigtvis borde analyseras med en rak linje? Kan inte priser fluktuera? I detta fall kan du prova polynomregression.

‚úÖ Polynom √§r matematiska uttryck som kan best√• av en eller flera variabler och koefficienter.

Polynomregression skapar en kurvad linje f√∂r att b√§ttre passa icke-linj√§r data. I v√•rt fall, om vi inkluderar en kvadrerad `DayOfYear`-variabel i indata, b√∂r vi kunna passa v√•r data med en parabolisk kurva, som kommer att ha ett minimum vid en viss punkt under √•ret.

Scikit-learn inkluderar ett anv√§ndbart [pipeline-API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) f√∂r att kombinera olika steg i databehandlingen. En **pipeline** √§r en kedja av **estimators**. I v√•rt fall kommer vi att skapa en pipeline som f√∂rst l√§gger till polynomfunktioner till v√•r modell och sedan tr√§nar regressionen:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Att anv√§nda `PolynomialFeatures(2)` betyder att vi kommer att inkludera alla andragradspolynom fr√•n indata. I v√•rt fall inneb√§r det bara `DayOfYear`<sup>2</sup>, men med tv√• indata X och Y kommer detta att l√§gga till X<sup>2</sup>, XY och Y<sup>2</sup>. Vi kan ocks√• anv√§nda polynom av h√∂gre grad om vi vill.

Pipelines kan anv√§ndas p√• samma s√§tt som det ursprungliga `LinearRegression`-objektet, dvs. vi kan `fit`-a pipelinen och sedan anv√§nda `predict` f√∂r att f√• prediktionsresultaten. H√§r √§r grafen som visar testdata och approximationskurvan:

<img alt="Polynomregression" src="images/poly-results.png" width="50%" />

Med polynomregression kan vi f√• n√•got l√§gre MSE och h√∂gre determination, men inte signifikant. Vi beh√∂ver ta h√§nsyn till andra funktioner!

> Du kan se att de l√§gsta pumpapriserna observeras n√•gonstans runt Halloween. Hur kan du f√∂rklara detta?

üéÉ Grattis, du har just skapat en modell som kan hj√§lpa till att f√∂ruts√§ga priset p√• pajpumpor. Du kan f√∂rmodligen upprepa samma procedur f√∂r alla pumpatyper, men det skulle vara tidskr√§vande. L√•t oss nu l√§ra oss hur man tar pumpasort i beaktande i v√•r modell!

## Kategoriska funktioner

I den ideala v√§rlden vill vi kunna f√∂ruts√§ga priser f√∂r olika pumpasorter med samma modell. Kolumnen `Variety` √§r dock n√•got annorlunda √§n kolumner som `Month`, eftersom den inneh√•ller icke-numeriska v√§rden. S√•dana kolumner kallas **kategoriska**.

[![ML f√∂r nyb√∂rjare - Kategoriska funktioner med linj√§r regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML f√∂r nyb√∂rjare - Kategoriska funktioner med linj√§r regression")

> üé• Klicka p√• bilden ovan f√∂r en kort video√∂versikt om att anv√§nda kategoriska funktioner.

H√§r kan du se hur medelpriset beror p√• sort:

<img alt="Medelpris per sort" src="images/price-by-variety.png" width="50%" />

F√∂r att ta sort i beaktande m√•ste vi f√∂rst konvertera den till numerisk form, eller **koda** den. Det finns flera s√§tt att g√∂ra detta:

* Enkel **numerisk kodning** bygger en tabell √∂ver olika sorter och ers√§tter sedan sortnamnet med ett index i den tabellen. Detta √§r inte den b√§sta id√©n f√∂r linj√§r regression, eftersom linj√§r regression tar det faktiska numeriska v√§rdet av indexet och l√§gger till det i resultatet, multiplicerat med n√•gon koefficient. I v√•rt fall √§r sambandet mellan indexnummer och pris klart icke-linj√§rt, √§ven om vi ser till att indexen √§r ordnade p√• n√•got specifikt s√§tt.
* **One-hot-kodning** ers√§tter kolumnen `Variety` med 4 olika kolumner, en f√∂r varje sort. Varje kolumn inneh√•ller `1` om den motsvarande raden √§r av en given sort och `0` annars. Detta inneb√§r att det kommer att finnas fyra koefficienter i linj√§r regression, en f√∂r varje pumpasort, som ansvarar f√∂r "startpris" (eller snarare "till√§ggspris") f√∂r den specifika sorten.

Koden nedan visar hur vi kan one-hot-koda en sort:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

F√∂r att tr√§na linj√§r regression med one-hot-kodad sort som indata beh√∂ver vi bara initiera `X` och `y`-data korrekt:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Resten av koden √§r densamma som den vi anv√§nde ovan f√∂r att tr√§na linj√§r regression. Om du provar det kommer du att se att medelkvadratfelet √§r ungef√§r detsamma, men vi f√•r mycket h√∂gre determinationskoefficient (~77 %). F√∂r att f√• √§nnu mer exakta f√∂ruts√§gelser kan vi ta fler kategoriska funktioner i beaktande, samt numeriska funktioner, s√•som `Month` eller `DayOfYear`. F√∂r att f√• en stor array av funktioner kan vi anv√§nda `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

H√§r tar vi ocks√• h√§nsyn till `City` och `Package`-typ, vilket ger oss MSE 2,84 (10 %) och determination 0,94!

## S√§tta ihop allt

F√∂r att skapa den b√§sta modellen kan vi anv√§nda kombinerad (one-hot-kodad kategorisk + numerisk) data fr√•n exemplet ovan tillsammans med polynomregression. H√§r √§r den kompletta koden f√∂r din bekv√§mlighet:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Detta b√∂r ge oss den b√§sta determinationskoefficienten p√• n√§stan 97 % och MSE=2,23 (~8 % prediktionsfel).

| Modell | MSE | Determination |
|--------|-----|---------------|
| `DayOfYear` Linj√§r | 2,77 (17,2 %) | 0,07 |
| `DayOfYear` Polynom | 2,73 (17,0 %) | 0,08 |
| `Variety` Linj√§r | 5,24 (19,7 %) | 0,77 |
| Alla funktioner Linj√§r | 2,84 (10,5 %) | 0,94 |
| Alla funktioner Polynom | 2,23 (8,25 %) | 0,97 |

üèÜ Bra jobbat! Du skapade fyra regressionsmodeller i en lektion och f√∂rb√§ttrade modellkvaliteten till 97 %. I det sista avsnittet om regression kommer du att l√§ra dig om logistisk regression f√∂r att best√§mma kategorier.

---
## üöÄUtmaning

Testa flera olika variabler i denna notebook f√∂r att se hur korrelationen motsvarar modellens noggrannhet.

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

## Granskning & Sj√§lvstudier

I denna lektion l√§rde vi oss om linj√§r regression. Det finns andra viktiga typer av regression. L√§s om Stepwise, Ridge, Lasso och Elasticnet-tekniker. En bra kurs att studera f√∂r att l√§ra sig mer √§r [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Uppgift

[Bygg en modell](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess ursprungliga spr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.