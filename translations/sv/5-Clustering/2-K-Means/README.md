<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7cdd17338d9bbd7e2171c2cd462eb081",
  "translation_date": "2025-09-05T21:29:18+00:00",
  "source_file": "5-Clustering/2-K-Means/README.md",
  "language_code": "sv"
}
-->
# K-Means klustring

## [Pre-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

I den h√§r lektionen kommer du att l√§ra dig hur man skapar kluster med hj√§lp av Scikit-learn och den nigerianska musikdatabasen som du importerade tidigare. Vi kommer att g√• igenom grunderna i K-Means f√∂r klustring. Kom ih√•g att, som du l√§rde dig i den tidigare lektionen, finns det m√•nga s√§tt att arbeta med kluster och metoden du anv√§nder beror p√• din data. Vi kommer att testa K-Means eftersom det √§r den vanligaste klustringstekniken. L√•t oss s√§tta ig√•ng!

Begrepp du kommer att l√§ra dig om:

- Silhouettescore
- Elbow-metoden
- Inertia
- Varians

## Introduktion

[K-Means klustring](https://wikipedia.org/wiki/K-means_clustering) √§r en metod som h√§rstammar fr√•n signalbehandlingsomr√•det. Den anv√§nds f√∂r att dela och partitionera grupper av data i 'k' kluster med hj√§lp av en serie observationer. Varje observation arbetar f√∂r att gruppera en given datapunkt n√§rmast dess n√§rmaste 'medelv√§rde', eller mittpunkten av ett kluster.

Klustrerna kan visualiseras som [Voronoi-diagram](https://wikipedia.org/wiki/Voronoi_diagram), som inkluderar en punkt (eller 'fr√∂') och dess motsvarande omr√•de.

![voronoi diagram](../../../../5-Clustering/2-K-Means/images/voronoi.png)

> Infografik av [Jen Looper](https://twitter.com/jenlooper)

K-Means klustringsprocessen [utf√∂rs i tre steg](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. Algoritmen v√§ljer k antal mittpunkter genom att ta ett urval fr√•n datasetet. D√§refter loopar den:
    1. Den tilldelar varje prov till den n√§rmaste mittpunkten.
    2. Den skapar nya mittpunkter genom att ta medelv√§rdet av alla prover som tilldelats de tidigare mittpunkterna.
    3. Sedan ber√§knar den skillnaden mellan de nya och gamla mittpunkterna och upprepar tills mittpunkterna stabiliseras.

En nackdel med att anv√§nda K-Means √§r att du m√•ste fastst√§lla 'k', det vill s√§ga antalet mittpunkter. Lyckligtvis hj√§lper 'elbow-metoden' till att uppskatta ett bra startv√§rde f√∂r 'k'. Du kommer att testa detta snart.

## F√∂ruts√§ttningar

Du kommer att arbeta i den h√§r lektionens [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb)-fil som inkluderar dataimporten och den prelimin√§ra datarensningen du gjorde i den f√∂rra lektionen.

## √ñvning - f√∂rberedelse

B√∂rja med att ta en ny titt p√• l√•tdatabasen.

1. Skapa ett boxplot genom att kalla p√• `boxplot()` f√∂r varje kolumn:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Den h√§r datan √§r lite brusig: genom att observera varje kolumn som ett boxplot kan du se avvikare.

    ![outliers](../../../../5-Clustering/2-K-Means/images/boxplots.png)

Du skulle kunna g√• igenom datasetet och ta bort dessa avvikare, men det skulle g√∂ra datan ganska minimal.

1. F√∂r tillf√§llet, v√§lj vilka kolumner du ska anv√§nda f√∂r din klustrings√∂vning. V√§lj s√•dana med liknande intervall och koda kolumnen `artist_top_genre` som numerisk data:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Nu beh√∂ver du v√§lja hur m√•nga kluster du ska rikta in dig p√•. Du vet att det finns 3 l√•tgenrer som vi har tagit fram ur datasetet, s√• l√•t oss testa med 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Du ser en array som skrivs ut med f√∂rutsagda kluster (0, 1 eller 2) f√∂r varje rad i dataramen.

1. Anv√§nd denna array f√∂r att ber√§kna en 'silhouettescore':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Silhouettescore

S√∂k efter en silhouettescore n√§ra 1. Denna score varierar fr√•n -1 till 1, och om scoren √§r 1 √§r klustret t√§tt och v√§l separerat fr√•n andra kluster. Ett v√§rde n√§ra 0 representerar √∂verlappande kluster med prover som ligger mycket n√§ra beslutsgr√§nsen f√∂r de n√§rliggande klustren. [(K√§lla)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

V√•r score √§r **.53**, allts√• mitt i mellan. Detta indikerar att v√•r data inte √§r s√§rskilt v√§l l√§mpad f√∂r denna typ av klustring, men l√•t oss forts√§tta.

### √ñvning - bygg en modell

1. Importera `KMeans` och starta klustringsprocessen.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Det finns n√•gra delar h√§r som f√∂rtj√§nar en f√∂rklaring.

    > üéì range: Detta √§r iterationerna av klustringsprocessen.

    > üéì random_state: "Best√§mmer slumpm√§ssig nummergenerering f√∂r initialisering av mittpunkter." [K√§lla](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "within-cluster sums of squares" m√§ter det kvadrerade genomsnittliga avst√•ndet f√∂r alla punkter inom ett kluster till klustrets mittpunkt. [K√§lla](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì Inertia: K-Means-algoritmer f√∂rs√∂ker v√§lja mittpunkter f√∂r att minimera 'inertia', "ett m√•tt p√• hur internt sammanh√§ngande kluster √§r." [K√§lla](https://scikit-learn.org/stable/modules/clustering.html). V√§rdet l√§ggs till i wcss-variabeln vid varje iteration.

    > üéì k-means++: I [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) kan du anv√§nda 'k-means++'-optimering, som "initialiserar mittpunkterna s√• att de (generellt sett) √§r l√•ngt ifr√•n varandra, vilket leder till troligen b√§ttre resultat √§n slumpm√§ssig initialisering."

### Elbow-metoden

Tidigare antog du att eftersom du har riktat in dig p√• 3 l√•tgenrer, b√∂r du v√§lja 3 kluster. Men √§r det verkligen s√•?

1. Anv√§nd 'elbow-metoden' f√∂r att vara s√§ker.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Anv√§nd variabeln `wcss` som du byggde i f√∂reg√•ende steg f√∂r att skapa ett diagram som visar var 'b√∂jen' i armb√•gen √§r, vilket indikerar det optimala antalet kluster. Kanske √§r det **verkligen** 3!

    ![elbow method](../../../../5-Clustering/2-K-Means/images/elbow.png)

## √ñvning - visa klustren

1. Testa processen igen, den h√§r g√•ngen med tre kluster, och visa klustren som ett scatterplot:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Kontrollera modellens noggrannhet:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    Den h√§r modellens noggrannhet √§r inte s√§rskilt bra, och formen p√• klustren ger dig en ledtr√•d om varf√∂r.

    ![clusters](../../../../5-Clustering/2-K-Means/images/clusters.png)

    Den h√§r datan √§r f√∂r obalanserad, f√∂r lite korrelerad och det finns f√∂r mycket varians mellan kolumnv√§rdena f√∂r att klustras v√§l. Faktum √§r att klustren som bildas f√∂rmodligen √§r starkt p√•verkade eller snedvridna av de tre genrekategorier vi definierade ovan. Det var en l√§randeprocess!

    I Scikit-learns dokumentation kan du se att en modell som denna, med kluster som inte √§r s√§rskilt v√§l avgr√§nsade, har ett 'varians'-problem:

    ![problem models](../../../../5-Clustering/2-K-Means/images/problems.png)
    > Infografik fr√•n Scikit-learn

## Varians

Varians definieras som "genomsnittet av de kvadrerade skillnaderna fr√•n medelv√§rdet" [(K√§lla)](https://www.mathsisfun.com/data/standard-deviation.html). I kontexten av detta klustringsproblem h√§nvisar det till data d√§r siffrorna i v√•rt dataset tenderar att avvika lite f√∂r mycket fr√•n medelv√§rdet.

‚úÖ Detta √§r ett bra tillf√§lle att fundera p√• alla s√§tt du kan korrigera detta problem. Justera datan lite mer? Anv√§nd andra kolumner? Anv√§nd en annan algoritm? Tips: Testa att [skala din data](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) f√∂r att normalisera den och testa andra kolumner.

> Testa denna '[varianskalkylator](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' f√∂r att f√∂rst√• konceptet lite b√§ttre.

---

## üöÄUtmaning

Tillbringa lite tid med denna notebook och justera parametrar. Kan du f√∂rb√§ttra modellens noggrannhet genom att rensa datan mer (till exempel ta bort avvikare)? Du kan anv√§nda vikter f√∂r att ge mer vikt √•t vissa dataprov. Vad mer kan du g√∂ra f√∂r att skapa b√§ttre kluster?

Tips: Testa att skala din data. Det finns kommenterad kod i notebooken som l√§gger till standardisering f√∂r att f√• datakolumnerna att likna varandra mer i termer av intervall. Du kommer att m√§rka att √§ven om silhouettescoren g√•r ner, s√• j√§mnas 'b√∂jen' i armb√•gsdiagrammet ut. Detta beror p√• att om datan l√§mnas oskalad till√•ts data med mindre varians att v√§ga tyngre. L√§s mer om detta problem [h√§r](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)

## Granskning & Sj√§lvstudier

Ta en titt p√• en K-Means Simulator [som denna](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Du kan anv√§nda detta verktyg f√∂r att visualisera exempeldata och best√§mma dess mittpunkter. Du kan redigera datans slumpm√§ssighet, antal kluster och antal mittpunkter. Hj√§lper detta dig att f√• en id√© om hur datan kan grupperas?

Ta ocks√• en titt p√• [detta handout om K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) fr√•n Stanford.

## Uppgift

[Testa olika klustringsmetoder](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen notera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.