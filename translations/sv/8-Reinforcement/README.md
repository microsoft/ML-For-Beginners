<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T22:01:26+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "sv"
}
-->
# Introduktion till f√∂rst√§rkningsinl√§rning

F√∂rst√§rkningsinl√§rning, RL, ses som ett av de grundl√§ggande paradigmen inom maskininl√§rning, vid sidan av √∂vervakad inl√§rning och o√∂vervakad inl√§rning. RL handlar om beslut: att fatta r√§tt beslut eller √•tminstone l√§ra sig av dem.

F√∂rest√§ll dig att du har en simulerad milj√∂, som aktiemarknaden. Vad h√§nder om du inf√∂r en viss reglering? Har det en positiv eller negativ effekt? Om n√•got negativt intr√§ffar m√•ste du ta detta _negativa f√∂rst√§rkning_, l√§ra dig av det och √§ndra kurs. Om det √§r ett positivt resultat m√•ste du bygga vidare p√• den _positiva f√∂rst√§rkningen_.

![peter och vargen](../../../8-Reinforcement/images/peter.png)

> Peter och hans v√§nner m√•ste fly fr√•n den hungriga vargen! Bild av [Jen Looper](https://twitter.com/jenlooper)

## Regionalt √§mne: Peter och Vargen (Ryssland)

[Peter och Vargen](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) √§r en musikalisk saga skriven av den ryske komposit√∂ren [Sergej Prokofjev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Det √§r en ber√§ttelse om den unge pionj√§ren Peter, som modigt l√§mnar sitt hus och g√•r ut p√• skogsgl√§ntan f√∂r att jaga vargen. I detta avsnitt kommer vi att tr√§na maskininl√§rningsalgoritmer som hj√§lper Peter att:

- **Utforska** omr√•det runt omkring och bygga en optimal navigeringskarta.
- **L√§ra sig** att anv√§nda en skateboard och balansera p√• den f√∂r att kunna r√∂ra sig snabbare.

[![Peter och Vargen](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Klicka p√• bilden ovan f√∂r att lyssna p√• Peter och Vargen av Prokofjev

## F√∂rst√§rkningsinl√§rning

I tidigare avsnitt har du sett tv√• exempel p√• maskininl√§rningsproblem:

- **√ñvervakad**, d√§r vi har dataset som f√∂resl√•r exempel p√• l√∂sningar till problemet vi vill l√∂sa. [Klassificering](../4-Classification/README.md) och [regression](../2-Regression/README.md) √§r uppgifter inom √∂vervakad inl√§rning.
- **O√∂vervakad**, d√§r vi inte har m√§rkta tr√§ningsdata. Det fr√§msta exemplet p√• o√∂vervakad inl√§rning √§r [klustring](../5-Clustering/README.md).

I detta avsnitt kommer vi att introducera dig till en ny typ av inl√§rningsproblem som inte kr√§ver m√§rkta tr√§ningsdata. Det finns flera typer av s√•dana problem:

- **[Semi√∂vervakad inl√§rning](https://wikipedia.org/wiki/Semi-supervised_learning)**, d√§r vi har mycket om√§rkta data som kan anv√§ndas f√∂r att f√∂rtr√§na modellen.
- **[F√∂rst√§rkningsinl√§rning](https://wikipedia.org/wiki/Reinforcement_learning)**, d√§r en agent l√§r sig hur den ska bete sig genom att utf√∂ra experiment i en simulerad milj√∂.

### Exempel - datorspel

Anta att du vill l√§ra en dator att spela ett spel, som schack eller [Super Mario](https://wikipedia.org/wiki/Super_Mario). F√∂r att datorn ska kunna spela ett spel beh√∂ver vi att den f√∂rutsp√•r vilket drag den ska g√∂ra i varje spelstadium. √Ñven om detta kan verka som ett klassificeringsproblem √§r det inte det ‚Äì eftersom vi inte har ett dataset med tillst√•nd och motsvarande √•tg√§rder. √Ñven om vi kanske har viss data, som befintliga schackmatcher eller inspelningar av spelare som spelar Super Mario, √§r det troligt att dessa data inte t√§cker ett tillr√§ckligt stort antal m√∂jliga tillst√•nd.

Ist√§llet f√∂r att leta efter befintlig speldata bygger **F√∂rst√§rkningsinl√§rning** (RL) p√• id√©n att *l√•ta datorn spela* m√•nga g√•nger och observera resultatet. F√∂r att till√§mpa f√∂rst√§rkningsinl√§rning beh√∂ver vi tv√• saker:

- **En milj√∂** och **en simulator** som l√•ter oss spela spelet m√•nga g√•nger. Denna simulator skulle definiera alla spelregler samt m√∂jliga tillst√•nd och √•tg√§rder.

- **En bel√∂ningsfunktion**, som ber√§ttar hur bra vi presterade under varje drag eller spel.

Den st√∂rsta skillnaden mellan andra typer av maskininl√§rning och RL √§r att vi i RL vanligtvis inte vet om vi vinner eller f√∂rlorar f√∂rr√§n vi avslutar spelet. D√§rf√∂r kan vi inte s√§ga om ett visst drag i sig √§r bra eller inte ‚Äì vi f√•r bara en bel√∂ning i slutet av spelet. V√•rt m√•l √§r att designa algoritmer som g√∂r det m√∂jligt f√∂r oss att tr√§na en modell under os√§kra f√∂rh√•llanden. Vi kommer att l√§ra oss om en RL-algoritm som kallas **Q-learning**.

## Lektioner

1. [Introduktion till f√∂rst√§rkningsinl√§rning och Q-Learning](1-QLearning/README.md)
2. [Anv√§nda en gym-simuleringsmilj√∂](2-Gym/README.md)

## Krediter

"Introduktion till F√∂rst√§rkningsinl√§rning" skrevs med ‚ô•Ô∏è av [Dmitry Soshnikov](http://soshnikov.com)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r du vara medveten om att automatiska √∂vers√§ttningar kan inneh√•lla fel eller inexaktheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.