<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T22:04:16+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "sv"
}
-->
# Introduktion till f칬rst칛rkningsinl칛rning och Q-Learning

![Sammanfattning av f칬rst칛rkning inom maskininl칛rning i en sketchnote](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote av [Tomomi Imura](https://www.twitter.com/girlie_mac)

F칬rst칛rkningsinl칛rning involverar tre viktiga koncept: agenten, n친gra tillst친nd och en upps칛ttning av handlingar per tillst친nd. Genom att utf칬ra en handling i ett specifikt tillst친nd f친r agenten en bel칬ning. T칛nk dig datorspelet Super Mario. Du 칛r Mario, du befinner dig p친 en niv친 i spelet, st친ende vid kanten av en klippa. Ovanf칬r dig finns ett mynt. Du som Mario, p친 en niv친 i spelet, p친 en specifik position ... det 칛r ditt tillst친nd. Om du tar ett steg 친t h칬ger (en handling) faller du 칬ver kanten, vilket ger dig en l친g numerisk po칛ng. Men om du trycker p친 hoppknappen f친r du en po칛ng och f칬rblir vid liv. Det 칛r ett positivt resultat och b칬r ge dig en positiv numerisk po칛ng.

Genom att anv칛nda f칬rst칛rkningsinl칛rning och en simulator (spelet) kan du l칛ra dig att spela spelet f칬r att maximera bel칬ningen, vilket inneb칛r att h친lla dig vid liv och samla s친 m친nga po칛ng som m칬jligt.

[![Introduktion till f칬rst칛rkningsinl칛rning](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> 游꿘 Klicka p친 bilden ovan f칬r att h칬ra Dmitry diskutera f칬rst칛rkningsinl칛rning

## [Quiz f칬re f칬rel칛sningen](https://ff-quizzes.netlify.app/en/ml/)

## F칬ruts칛ttningar och inst칛llning

I denna lektion kommer vi att experimentera med kod i Python. Du b칬r kunna k칬ra Jupyter Notebook-koden fr친n denna lektion, antingen p친 din dator eller i molnet.

Du kan 칬ppna [lektionens notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) och f칬lja med i lektionen f칬r att bygga.

> **Obs:** Om du 칬ppnar denna kod fr친n molnet beh칬ver du ocks친 h칛mta filen [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), som anv칛nds i notebook-koden. L칛gg den i samma katalog som notebook-filen.

## Introduktion

I denna lektion kommer vi att utforska v칛rlden av **[Peter och vargen](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspirerad av en musikalisk saga av den ryske komposit칬ren [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). Vi kommer att anv칛nda **f칬rst칛rkningsinl칛rning** f칬r att l친ta Peter utforska sin milj칬, samla goda 칛pplen och undvika att m칬ta vargen.

**F칬rst칛rkningsinl칛rning** (RL) 칛r en inl칛rningsteknik som g칬r det m칬jligt f칬r oss att l칛ra oss ett optimalt beteende hos en **agent** i en viss **milj칬** genom att k칬ra m친nga experiment. En agent i denna milj칬 b칬r ha ett **m친l**, definierat av en **bel칬ningsfunktion**.

## Milj칬n

F칬r enkelhetens skull kan vi t칛nka oss Peters v칛rld som en kvadratisk spelbr칛da med storleken `width` x `height`, som denna:

![Peters milj칬](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Varje cell p친 denna br칛da kan vara:

* **mark**, d칛r Peter och andra varelser kan g친.
* **vatten**, d칛r man uppenbarligen inte kan g친.
* ett **tr칛d** eller **gr칛s**, en plats d칛r man kan vila.
* ett **칛pple**, som representerar n친got Peter g칛rna vill hitta f칬r att 칛ta.
* en **varg**, som 칛r farlig och b칬r undvikas.

Det finns en separat Python-modul, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), som inneh친ller koden f칬r att arbeta med denna milj칬. Eftersom denna kod inte 칛r viktig f칬r att f칬rst친 v친ra koncept, kommer vi att importera modulen och anv칛nda den f칬r att skapa spelbr칛dan (kodblock 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Denna kod b칬r skriva ut en bild av milj칬n som liknar den ovan.

## Handlingar och strategi

I v친rt exempel skulle Peters m친l vara att hitta ett 칛pple, samtidigt som han undviker vargen och andra hinder. F칬r att g칬ra detta kan han i princip g친 runt tills han hittar ett 칛pple.

D칛rf칬r kan han vid varje position v칛lja mellan f칬ljande handlingar: upp, ner, v칛nster och h칬ger.

Vi kommer att definiera dessa handlingar som en ordbok och koppla dem till par av motsvarande koordinatf칬r칛ndringar. Till exempel skulle r칬relse 친t h칬ger (`R`) motsvara paret `(1,0)`. (kodblock 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Sammanfattningsvis 칛r strategin och m친let f칬r detta scenario f칬ljande:

- **Strategin**, f칬r v친r agent (Peter) definieras av en s친 kallad **policy**. En policy 칛r en funktion som returnerar handlingen vid ett givet tillst친nd. I v친rt fall representeras problemet av spelbr칛dan, inklusive spelarens aktuella position.

- **M친let**, med f칬rst칛rkningsinl칛rning 칛r att s친 sm친ningom l칛ra sig en bra policy som g칬r det m칬jligt f칬r oss att l칬sa problemet effektivt. Men som en grundl칛ggande utg친ngspunkt kan vi 칬verv칛ga den enklaste policyn som kallas **slumpvandring**.

## Slumpvandring

L친t oss f칬rst l칬sa v친rt problem genom att implementera en slumpvandring-strategi. Med slumpvandring kommer vi slumpm칛ssigt att v칛lja n칛sta handling fr친n de till친tna handlingarna tills vi n친r 칛pplet (kodblock 3).

1. Implementera slumpvandringen med koden nedan:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    Anropet till `walk` b칬r returnera l칛ngden p친 den motsvarande v칛gen, som kan variera fr친n en k칬rning till en annan.

1. K칬r experimentet med slumpvandring ett antal g친nger (s칛g 100) och skriv ut de resulterande statistiken (kodblock 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Observera att den genomsnittliga l칛ngden p친 en v칛g 칛r cirka 30-40 steg, vilket 칛r ganska mycket, med tanke p친 att det genomsnittliga avst친ndet till n칛rmaste 칛pple 칛r cirka 5-6 steg.

    Du kan ocks친 se hur Peters r칬relse ser ut under slumpvandringen:

    ![Peters slumpvandring](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Bel칬ningsfunktion

F칬r att g칬ra v친r policy mer intelligent beh칬ver vi f칬rst친 vilka r칬relser som 칛r "b칛ttre" 칛n andra. F칬r att g칬ra detta m친ste vi definiera v친rt m친l.

M친let kan definieras i termer av en **bel칬ningsfunktion**, som returnerar ett po칛ngv칛rde f칬r varje tillst친nd. Ju h칬gre nummer, desto b칛ttre bel칬ningsfunktion. (kodblock 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

En intressant sak med bel칬ningsfunktioner 칛r att i de flesta fall *f친r vi bara en betydande bel칬ning i slutet av spelet*. Detta inneb칛r att v친r algoritm p친 n친got s칛tt m친ste komma ih친g "bra" steg som leder till en positiv bel칬ning i slutet och 칬ka deras betydelse. P친 samma s칛tt b칬r alla r칬relser som leder till d친liga resultat avskr칛ckas.

## Q-Learning

En algoritm som vi kommer att diskutera h칛r kallas **Q-Learning**. I denna algoritm definieras policyn av en funktion (eller en datastruktur) som kallas en **Q-Tabell**. Den registrerar "kvaliteten" p친 varje handling i ett givet tillst친nd.

Den kallas en Q-Tabell eftersom det ofta 칛r bekv칛mt att representera den som en tabell eller en multidimensionell array. Eftersom v친r br칛da har dimensionerna `width` x `height`, kan vi representera Q-Tabellen med en numpy-array med formen `width` x `height` x `len(actions)`: (kodblock 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Observera att vi initialiserar alla v칛rden i Q-Tabellen med ett lika v칛rde, i v친rt fall - 0.25. Detta motsvarar policyn "slumpvandring", eftersom alla r칬relser i varje tillst친nd 칛r lika bra. Vi kan skicka Q-Tabellen till funktionen `plot` f칬r att visualisera tabellen p친 br칛dan: `m.plot(Q)`.

![Peters milj칬](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

I mitten av varje cell finns en "pil" som indikerar den f칬redragna r칬relseriktningen. Eftersom alla riktningar 칛r lika visas en punkt.

Nu beh칬ver vi k칬ra simuleringen, utforska v친r milj칬 och l칛ra oss en b칛ttre f칬rdelning av Q-Tabellv칛rden, vilket g칬r att vi kan hitta v칛gen till 칛pplet mycket snabbare.

## K칛rnan i Q-Learning: Bellman-ekvationen

N칛r vi b칬rjar r칬ra oss kommer varje handling att ha en motsvarande bel칬ning, dvs. vi kan teoretiskt v칛lja n칛sta handling baserat p친 den h칬gsta omedelbara bel칬ningen. Men i de flesta tillst친nd kommer r칬relsen inte att uppn친 v친rt m친l att n친 칛pplet, och vi kan d칛rf칬r inte omedelbart avg칬ra vilken riktning som 칛r b칛ttre.

> Kom ih친g att det inte 칛r det omedelbara resultatet som spelar roll, utan snarare det slutliga resultatet, som vi kommer att f친 i slutet av simuleringen.

F칬r att ta h칛nsyn till denna f칬rdr칬jda bel칬ning m친ste vi anv칛nda principerna f칬r **[dynamisk programmering](https://en.wikipedia.org/wiki/Dynamic_programming)**, som g칬r det m칬jligt f칬r oss att t칛nka p친 v친rt problem rekursivt.

Anta att vi nu 칛r i tillst친ndet *s*, och vi vill g친 vidare till n칛sta tillst친nd *s'*. Genom att g칬ra det kommer vi att f친 den omedelbara bel칬ningen *r(s,a)*, definierad av bel칬ningsfunktionen, plus n친gon framtida bel칬ning. Om vi antar att v친r Q-Tabell korrekt 친terspeglar "attraktiviteten" hos varje handling, kommer vi vid tillst친ndet *s'* att v칛lja en handling *a* som motsvarar det maximala v칛rdet av *Q(s',a')*. S친ledes definieras den b칛sta m칬jliga framtida bel칬ningen vi kan f친 vid tillst친ndet *s* som `max`

## Kontrollera policyn

Eftersom Q-Tabellen listar "attraktiviteten" f칬r varje handling i varje tillst친nd, 칛r det ganska enkelt att anv칛nda den f칬r att definiera effektiv navigering i v친r v칛rld. I det enklaste fallet kan vi v칛lja den handling som motsvarar det h칬gsta v칛rdet i Q-Tabellen: (kodblock 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Om du testar koden ovan flera g친nger kan du m칛rka att den ibland "fastnar", och du m친ste trycka p친 STOP-knappen i notebooken f칬r att avbryta den. Detta h칛nder eftersom det kan finnas situationer d칛r tv친 tillst친nd "pekar" p친 varandra i termer av optimalt Q-v칛rde, vilket g칬r att agenten hamnar i en o칛ndlig r칬relse mellan dessa tillst친nd.

## 游Utmaning

> **Uppgift 1:** Modifiera funktionen `walk` f칬r att begr칛nsa den maximala l칛ngden p친 v칛gen till ett visst antal steg (t.ex. 100), och se hur koden ovan returnerar detta v칛rde d친 och d친.

> **Uppgift 2:** Modifiera funktionen `walk` s친 att den inte g친r tillbaka till platser d칛r den redan har varit tidigare. Detta kommer att f칬rhindra att `walk` hamnar i en loop, men agenten kan fortfarande bli "fast" p친 en plats som den inte kan ta sig ifr친n.

## Navigering

En b칛ttre navigeringspolicy skulle vara den vi anv칛nde under tr칛ningen, som kombinerar exploatering och utforskning. I denna policy kommer vi att v칛lja varje handling med en viss sannolikhet, proportionell mot v칛rdena i Q-Tabellen. Denna strategi kan fortfarande leda till att agenten 친terv칛nder till en position den redan har utforskat, men som du kan se fr친n koden nedan resulterar den i en mycket kortare genomsnittlig v칛g till den 칬nskade platsen (kom ih친g att `print_statistics` k칬r simuleringen 100 g친nger): (kodblock 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Efter att ha k칬rt denna kod b칬r du f친 en mycket mindre genomsnittlig v칛g 칛n tidigare, i intervallet 3-6.

## Unders칬ka inl칛rningsprocessen

Som vi har n칛mnt 칛r inl칛rningsprocessen en balans mellan utforskning och utnyttjande av den kunskap vi har f친tt om problemomr친dets struktur. Vi har sett att resultaten av inl칛rningen (f칬rm친gan att hj칛lpa en agent att hitta en kort v칛g till m친let) har f칬rb칛ttrats, men det 칛r ocks친 intressant att observera hur den genomsnittliga v칛gl칛ngden beter sig under inl칛rningsprocessen:

## Sammanfattning av l칛rdomarna:

- **Genomsnittlig v칛gl칛ngd 칬kar**. Det vi ser h칛r 칛r att i b칬rjan 칬kar den genomsnittliga v칛gl칛ngden. Detta beror troligen p친 att n칛r vi inte vet n친got om milj칬n 칛r vi mer ben칛gna att fastna i d친liga tillst친nd, som vatten eller varg. N칛r vi l칛r oss mer och b칬rjar anv칛nda denna kunskap kan vi utforska milj칬n l칛ngre, men vi vet fortfarande inte var 칛pplena finns s칛rskilt v칛l.

- **V칛gl칛ngden minskar n칛r vi l칛r oss mer**. N칛r vi har l칛rt oss tillr칛ckligt blir det l칛ttare f칬r agenten att n친 m친let, och v칛gl칛ngden b칬rjar minska. Vi 칛r dock fortfarande 칬ppna f칬r utforskning, s친 vi avviker ofta fr친n den b칛sta v칛gen och utforskar nya alternativ, vilket g칬r v칛gen l칛ngre 칛n optimal.

- **L칛ngden 칬kar pl칬tsligt**. Det vi ocks친 observerar p친 denna graf 칛r att vid n친gon punkt 칬kade l칛ngden pl칬tsligt. Detta indikerar den stokastiska naturen hos processen, och att vi vid n친got tillf칛lle kan "f칬rst칬ra" Q-Tabellens koefficienter genom att skriva 칬ver dem med nya v칛rden. Detta b칬r idealt minimeras genom att minska inl칛rningshastigheten (till exempel mot slutet av tr칛ningen justerar vi endast Q-Tabellens v칛rden med ett litet v칛rde).

Sammanfattningsvis 칛r det viktigt att komma ih친g att framg친ngen och kvaliteten p친 inl칛rningsprocessen beror avsev칛rt p친 parametrar som inl칛rningshastighet, inl칛rningshastighetsminskning och diskonteringsfaktor. Dessa kallas ofta **hyperparametrar**, f칬r att skilja dem fr친n **parametrar**, som vi optimerar under tr칛ningen (till exempel Q-Tabellens koefficienter). Processen att hitta de b칛sta v칛rdena f칬r hyperparametrar kallas **hyperparameteroptimering**, och det f칬rtj칛nar ett eget 칛mne.

## [Quiz efter f칬rel칛sningen](https://ff-quizzes.netlify.app/en/ml/)

## Uppgift 
[En mer realistisk v칛rld](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har 칬versatts med hj칛lp av AI-칬vers칛ttningstj칛nsten [Co-op Translator](https://github.com/Azure/co-op-translator). 츿ven om vi str칛var efter noggrannhet, v칛nligen notera att automatiska 칬vers칛ttningar kan inneh친lla fel eller felaktigheter. Det ursprungliga dokumentet p친 dess originalspr친k b칬r betraktas som den auktoritativa k칛llan. F칬r kritisk information rekommenderas professionell m칛nsklig 칬vers칛ttning. Vi ansvarar inte f칬r eventuella missf칬rst친nd eller feltolkningar som uppst친r vid anv칛ndning av denna 칬vers칛ttning.