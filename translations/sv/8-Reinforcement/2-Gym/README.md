<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T22:08:49+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "sv"
}
-->
## F√∂rkunskaper

I denna lektion kommer vi att anv√§nda ett bibliotek som heter **OpenAI Gym** f√∂r att simulera olika **milj√∂er**. Du kan k√∂ra kod fr√•n denna lektion lokalt (t.ex. fr√•n Visual Studio Code), i vilket fall simuleringen √∂ppnas i ett nytt f√∂nster. Om du k√∂r koden online kan du beh√∂va g√∂ra vissa justeringar, som beskrivs [h√§r](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

I den f√∂reg√•ende lektionen definierades spelets regler och tillst√•nd av klassen `Board` som vi skapade sj√§lva. H√§r kommer vi att anv√§nda en speciell **simuleringsmilj√∂** som simulerar fysiken bakom den balanserande st√•ngen. En av de mest popul√§ra simuleringsmilj√∂erna f√∂r att tr√§na f√∂rst√§rkningsinl√§rningsalgoritmer kallas [Gym](https://gym.openai.com/), som underh√•lls av [OpenAI](https://openai.com/). Med hj√§lp av Gym kan vi skapa olika **milj√∂er**, fr√•n cartpole-simuleringar till Atari-spel.

> **Note**: Du kan se andra milj√∂er som finns tillg√§ngliga fr√•n OpenAI Gym [h√§r](https://gym.openai.com/envs/#classic_control).

F√∂rst installerar vi Gym och importerar n√∂dv√§ndiga bibliotek (kodblock 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## √ñvning - initiera en cartpole-milj√∂

F√∂r att arbeta med problemet att balansera en cartpole beh√∂ver vi initiera motsvarande milj√∂. Varje milj√∂ √§r associerad med:

- **Observationsutrymme** som definierar strukturen f√∂r den information vi f√•r fr√•n milj√∂n. F√∂r cartpole-problemet f√•r vi positionen av st√•ngen, hastighet och n√•gra andra v√§rden.

- **Handlingsutrymme** som definierar m√∂jliga handlingar. I v√•rt fall √§r handlingsutrymmet diskret och best√•r av tv√• handlingar - **v√§nster** och **h√∂ger**. (kodblock 2)

1. F√∂r att initiera, skriv f√∂ljande kod:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

F√∂r att se hur milj√∂n fungerar, l√•t oss k√∂ra en kort simulering i 100 steg. Vid varje steg tillhandah√•ller vi en av handlingarna som ska utf√∂ras - i denna simulering v√§ljer vi bara slumpm√§ssigt en handling fr√•n `action_space`.

1. K√∂r koden nedan och se vad det leder till.

    ‚úÖ Kom ih√•g att det √§r att f√∂redra att k√∂ra denna kod p√• en lokal Python-installation! (kodblock 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Du b√∂r se n√•got liknande denna bild:

    ![icke-balanserande cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Under simuleringen beh√∂ver vi f√• observationer f√∂r att kunna best√§mma hur vi ska agera. Faktum √§r att funktionen `step` returnerar aktuella observationer, en bel√∂ningsfunktion och flaggan `done` som indikerar om det √§r meningsfullt att forts√§tta simuleringen eller inte: (kodblock 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    Du kommer att se n√•got liknande detta i notebookens output:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Observationsvektorn som returneras vid varje steg av simuleringen inneh√•ller f√∂ljande v√§rden:
    - Vagnens position
    - Vagnens hastighet
    - St√•ngens vinkel
    - St√•ngens rotationshastighet

1. H√§mta min- och maxv√§rden f√∂r dessa nummer: (kodblock 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Du kanske ocks√• m√§rker att bel√∂ningsv√§rdet vid varje simuleringssteg alltid √§r 1. Detta beror p√• att v√•rt m√•l √§r att √∂verleva s√• l√§nge som m√∂jligt, dvs. h√•lla st√•ngen i en rimligt vertikal position under l√§ngst m√∂jliga tid.

    ‚úÖ Faktum √§r att CartPole-simuleringen anses vara l√∂st om vi lyckas f√• ett genomsnittligt bel√∂ningsv√§rde p√• 195 √∂ver 100 p√• varandra f√∂ljande f√∂rs√∂k.

## Diskretisering av tillst√•nd

I Q-Learning beh√∂ver vi bygga en Q-Tabell som definierar vad vi ska g√∂ra vid varje tillst√•nd. F√∂r att kunna g√∂ra detta m√•ste tillst√•ndet vara **diskret**, mer specifikt, det b√∂r inneh√•lla ett √§ndligt antal diskreta v√§rden. D√§rf√∂r m√•ste vi p√• n√•got s√§tt **diskretisera** v√•ra observationer och mappa dem till en √§ndlig upps√§ttning tillst√•nd.

Det finns n√•gra s√§tt vi kan g√∂ra detta p√•:

- **Dela upp i intervall**. Om vi k√§nner till intervallet f√∂r ett visst v√§rde kan vi dela detta intervall i ett antal **intervall**, och sedan ers√§tta v√§rdet med det intervallnummer det tillh√∂r. Detta kan g√∂ras med hj√§lp av numpy-metoden [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html). I detta fall kommer vi att veta exakt storleken p√• tillst√•ndet, eftersom det beror p√• antalet intervall vi v√§ljer f√∂r digitalisering.

‚úÖ Vi kan anv√§nda linj√§r interpolation f√∂r att f√∂ra v√§rden till ett √§ndligt intervall (s√§g, fr√•n -20 till 20), och sedan konvertera siffror till heltal genom avrundning. Detta ger oss lite mindre kontroll √∂ver storleken p√• tillst√•ndet, s√§rskilt om vi inte k√§nner till de exakta intervallen f√∂r ing√•ngsv√§rdena. Till exempel, i v√•rt fall har 2 av 4 v√§rden inga √∂vre/nedre gr√§nser f√∂r sina v√§rden, vilket kan resultera i ett o√§ndligt antal tillst√•nd.

I v√•rt exempel kommer vi att anv√§nda det andra tillv√§gag√•ngss√§ttet. Som du kanske m√§rker senare, trots obest√§mda √∂vre/nedre gr√§nser, tar dessa v√§rden s√§llan v√§rden utanf√∂r vissa √§ndliga intervall, vilket g√∂r att tillst√•nd med extrema v√§rden blir mycket s√§llsynta.

1. H√§r √§r funktionen som tar observationen fr√•n v√•r modell och producerar en tuple med 4 heltalsv√§rden: (kodblock 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. L√•t oss ocks√• utforska en annan diskretiseringsmetod med hj√§lp av intervall: (kodblock 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. L√•t oss nu k√∂ra en kort simulering och observera dessa diskreta milj√∂v√§rden. Testa g√§rna b√•de `discretize` och `discretize_bins` och se om det finns n√•gon skillnad.

    ‚úÖ `discretize_bins` returnerar intervallnumret, som √§r 0-baserat. F√∂r v√§rden p√• ing√•ngsvariabeln runt 0 returnerar det numret fr√•n mitten av intervallet (10). I `discretize` brydde vi oss inte om intervallet f√∂r utg√•ngsv√§rdena, vilket till√•ter dem att vara negativa, s√• tillst√•ndsv√§rdena √§r inte f√∂rskjutna och 0 motsvarar 0. (kodblock 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Avkommentera raden som b√∂rjar med `env.render` om du vill se hur milj√∂n exekveras. Annars kan du k√∂ra den i bakgrunden, vilket √§r snabbare. Vi kommer att anv√§nda denna "osynliga" exekvering under v√•r Q-Learning-process.

## Q-Tabellens struktur

I v√•r f√∂reg√•ende lektion var tillst√•ndet ett enkelt par av siffror fr√•n 0 till 8, och det var d√§rf√∂r bekv√§mt att representera Q-Tabellen med en numpy-tensor med formen 8x8x2. Om vi anv√§nder intervall-diskretisering √§r storleken p√• v√•r tillst√•ndsvektor ocks√• k√§nd, s√• vi kan anv√§nda samma tillv√§gag√•ngss√§tt och representera tillst√•ndet med en array med formen 20x20x10x10x2 (h√§r √§r 2 dimensionen f√∂r handlingsutrymmet, och de f√∂rsta dimensionerna motsvarar antalet intervall vi har valt att anv√§nda f√∂r varje parameter i observationsutrymmet).

Men ibland √§r de exakta dimensionerna f√∂r observationsutrymmet inte k√§nda. I fallet med funktionen `discretize` kan vi aldrig vara s√§kra p√• att v√•rt tillst√•nd h√•ller sig inom vissa gr√§nser, eftersom vissa av de ursprungliga v√§rdena inte √§r begr√§nsade. D√§rf√∂r kommer vi att anv√§nda ett n√•got annorlunda tillv√§gag√•ngss√§tt och representera Q-Tabellen med en ordbok.

1. Anv√§nd paret *(state,action)* som nyckel i ordboken, och v√§rdet skulle motsvara v√§rdet i Q-Tabellen. (kodblock 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    H√§r definierar vi ocks√• en funktion `qvalues()`, som returnerar en lista med v√§rden fr√•n Q-Tabellen f√∂r ett givet tillst√•nd som motsvarar alla m√∂jliga handlingar. Om posten inte finns i Q-Tabellen kommer vi att returnera 0 som standard.

## L√•t oss b√∂rja med Q-Learning

Nu √§r vi redo att l√§ra Peter att balansera!

1. F√∂rst, l√•t oss st√§lla in n√•gra hyperparametrar: (kodblock 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    H√§r √§r `alpha` **inl√§rningshastigheten** som definierar i vilken utstr√§ckning vi ska justera de aktuella v√§rdena i Q-Tabellen vid varje steg. I den f√∂reg√•ende lektionen b√∂rjade vi med 1 och minskade sedan `alpha` till l√§gre v√§rden under tr√§ningen. I detta exempel kommer vi att h√•lla den konstant f√∂r enkelhetens skull, och du kan experimentera med att justera `alpha`-v√§rden senare.

    `gamma` √§r **diskonteringsfaktorn** som visar i vilken utstr√§ckning vi ska prioritera framtida bel√∂ningar √∂ver nuvarande bel√∂ningar.

    `epsilon` √§r **utforsknings-/utnyttjandefaktorn** som avg√∂r om vi ska f√∂redra utforskning framf√∂r utnyttjande eller vice versa. I v√•r algoritm kommer vi i `epsilon` procent av fallen att v√§lja n√§sta handling enligt Q-Tabellens v√§rden, och i resterande antal fall kommer vi att utf√∂ra en slumpm√§ssig handling. Detta g√∂r att vi kan utforska omr√•den i s√∂kutrymmet som vi aldrig har sett tidigare.

    ‚úÖ N√§r det g√§ller balansering - att v√§lja slumpm√§ssig handling (utforskning) skulle fungera som ett slumpm√§ssigt slag i fel riktning, och st√•ngen skulle beh√∂va l√§ra sig att √•terf√• balansen fr√•n dessa "misstag".

### F√∂rb√§ttra algoritmen

Vi kan ocks√• g√∂ra tv√• f√∂rb√§ttringar av v√•r algoritm fr√•n den f√∂reg√•ende lektionen:

- **Ber√§kna genomsnittlig kumulativ bel√∂ning** √∂ver ett antal simuleringar. Vi kommer att skriva ut framstegen var 5000:e iteration och vi kommer att ta genomsnittet av v√•r kumulativa bel√∂ning under den tidsperioden. Det betyder att om vi f√•r mer √§n 195 po√§ng kan vi anse problemet l√∂st, med √§nnu h√∂gre kvalitet √§n vad som kr√§vs.

- **Ber√§kna maximal genomsnittlig kumulativ bel√∂ning**, `Qmax`, och vi kommer att lagra Q-Tabellen som motsvarar det resultatet. N√§r du k√∂r tr√§ningen kommer du att m√§rka att ibland b√∂rjar det genomsnittliga kumulativa resultatet sjunka, och vi vill beh√•lla v√§rdena i Q-Tabellen som motsvarar den b√§sta modellen som observerats under tr√§ningen.

1. Samla alla kumulativa bel√∂ningar vid varje simulering i vektorn `rewards` f√∂r vidare plottning. (kodblock 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Vad du kan m√§rka fr√•n dessa resultat:

- **N√§ra v√•rt m√•l**. Vi √§r mycket n√§ra att uppn√• m√•let att f√• 195 kumulativa bel√∂ningar √∂ver 100+ p√• varandra f√∂ljande k√∂rningar av simuleringen, eller s√• har vi faktiskt uppn√•tt det! √Ñven om vi f√•r mindre siffror vet vi fortfarande inte, eftersom vi tar genomsnittet √∂ver 5000 k√∂rningar, och endast 100 k√∂rningar kr√§vs enligt de formella kriterierna.

- **Bel√∂ningen b√∂rjar sjunka**. Ibland b√∂rjar bel√∂ningen sjunka, vilket betyder att vi kan "f√∂rst√∂ra" redan inl√§rda v√§rden i Q-Tabellen med de som g√∂r situationen s√§mre.

Denna observation blir tydligare om vi plottar tr√§ningsframstegen.

## Plotta tr√§ningsframsteg

Under tr√§ningen har vi samlat det kumulativa bel√∂ningsv√§rdet vid varje iteration i vektorn `rewards`. S√• h√§r ser det ut n√§r vi plottar det mot iterationsnumret:

```python
plt.plot(rewards)
```

![r√•a framsteg](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

Fr√•n denna graf √§r det inte m√∂jligt att s√§ga n√•got, eftersom l√§ngden p√• tr√§ningssessionerna varierar kraftigt p√• grund av den stokastiska tr√§ningsprocessens natur. F√∂r att g√∂ra grafen mer meningsfull kan vi ber√§kna **rullande medelv√§rde** √∂ver en serie experiment, l√•t s√§ga 100. Detta kan g√∂ras bekv√§mt med `np.convolve`: (kodblock 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![tr√§ningsframsteg](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Variera hyperparametrar

F√∂r att g√∂ra inl√§rningen mer stabil kan det vara vettigt att justera n√•gra av v√•ra hyperparametrar under tr√§ningen. I synnerhet:

- **F√∂r inl√§rningshastigheten**, `alpha`, kan vi b√∂rja med v√§rden n√§ra 1 och sedan forts√§tta att minska parametern. Med tiden kommer vi att f√• bra sannolikhetsv√§rden i Q-Tabellen, och d√§rf√∂r b√∂r vi justera dem f√∂rsiktigt och inte helt skriva √∂ver med nya v√§rden.

- **√ñka epsilon**. Vi kanske vill √∂ka `epsilon` l√•ngsamt f√∂r att utforska mindre och utnyttja mer. Det kan vara vettigt att b√∂rja med ett l√§gre v√§rde f√∂r `epsilon` och sedan √∂ka det till n√§stan 1.
> **Uppgift 1**: Testa att √§ndra hyperparameterv√§rden och se om du kan uppn√• h√∂gre kumulativ bel√∂ning. Kommer du √∂ver 195?
> **Uppgift 2**: F√∂r att formellt l√∂sa problemet beh√∂ver du uppn√• ett genomsnittligt bel√∂ningsv√§rde p√• 195 √∂ver 100 p√• varandra f√∂ljande k√∂rningar. M√§t detta under tr√§ningen och s√§kerst√§ll att du formellt har l√∂st problemet!

## Se resultatet i praktiken

Det skulle vara intressant att faktiskt se hur den tr√§nade modellen beter sig. L√•t oss k√∂ra simuleringen och f√∂lja samma strategi f√∂r val av handlingar som under tr√§ningen, d√§r vi samplar enligt sannolikhetsf√∂rdelningen i Q-Tabellen: (kodblock 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

Du b√∂r se n√•got liknande detta:

![en balanserande cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄUtmaning

> **Uppgift 3**: H√§r anv√§nde vi den slutgiltiga versionen av Q-Tabellen, som kanske inte √§r den b√§sta. Kom ih√•g att vi har sparat den b√§st presterande Q-Tabellen i variabeln `Qbest`! Testa samma exempel med den b√§st presterande Q-Tabellen genom att kopiera `Qbest` till `Q` och se om du m√§rker n√•gon skillnad.

> **Uppgift 4**: H√§r valde vi inte den b√§sta handlingen vid varje steg, utan samplade ist√§llet enligt motsvarande sannolikhetsf√∂rdelning. Skulle det vara mer logiskt att alltid v√§lja den b√§sta handlingen, med det h√∂gsta v√§rdet i Q-Tabellen? Detta kan g√∂ras genom att anv√§nda funktionen `np.argmax` f√∂r att hitta handlingsnumret som motsvarar det h√∂gsta v√§rdet i Q-Tabellen. Implementera denna strategi och se om det f√∂rb√§ttrar balansen.

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

## Uppgift
[Tr√§na en Mountain Car](assignment.md)

## Slutsats

Vi har nu l√§rt oss hur man tr√§nar agenter f√∂r att uppn√• bra resultat genom att bara tillhandah√•lla en bel√∂ningsfunktion som definierar √∂nskat tillst√•nd i spelet, och genom att ge dem m√∂jlighet att intelligent utforska s√∂kutrymmet. Vi har framg√•ngsrikt till√§mpat Q-Learning-algoritmen i fall med diskreta och kontinuerliga milj√∂er, men med diskreta handlingar.

Det √§r ocks√• viktigt att studera situationer d√§r handlingsutrymmet ocks√• √§r kontinuerligt, och n√§r observationsutrymmet √§r mycket mer komplext, som en bild fr√•n sk√§rmen i ett Atari-spel. I dessa problem beh√∂ver vi ofta anv√§nda mer kraftfulla maskininl√§rningstekniker, s√•som neurala n√§tverk, f√∂r att uppn√• bra resultat. Dessa mer avancerade √§mnen √§r f√∂rem√•l f√∂r v√•r kommande mer avancerade AI-kurs.

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller brister. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell human √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.