<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9a6b702d1437c0467e3c5c28d763dac2",
  "translation_date": "2025-09-05T21:38:29+00:00",
  "source_file": "1-Introduction/3-fairness/README.md",
  "language_code": "sv"
}
-->
# Bygga maskininl√§rningsl√∂sningar med ansvarsfull AI

![Sammanfattning av ansvarsfull AI i maskininl√§rning i en sketchnote](../../../../sketchnotes/ml-fairness.png)
> Sketchnote av [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

## Introduktion

I denna kursplan kommer du att b√∂rja uppt√§cka hur maskininl√§rning kan och redan p√•verkar v√•ra dagliga liv. Redan nu √§r system och modeller involverade i dagliga beslutsprocesser, som exempelvis h√§lsov√•rdsdiagnoser, l√•neans√∂kningar eller att uppt√§cka bedr√§gerier. D√§rf√∂r √§r det viktigt att dessa modeller fungerar v√§l f√∂r att ge resultat som √§r p√•litliga. Precis som med vilken mjukvaruapplikation som helst kommer AI-system att missa f√∂rv√§ntningar eller ge o√∂nskade resultat. Det √§r d√§rf√∂r avg√∂rande att kunna f√∂rst√• och f√∂rklara beteendet hos en AI-modell.

F√∂rest√§ll dig vad som kan h√§nda n√§r data som anv√§nds f√∂r att bygga dessa modeller saknar vissa demografiska grupper, som exempelvis ras, k√∂n, politiska √•sikter eller religion, eller n√§r dessa grupper √§r oproportionerligt representerade. Vad h√§nder om modellens resultat tolkas som att gynna vissa grupper? Vilka blir konsekvenserna f√∂r applikationen? Dessutom, vad h√§nder n√§r modellen ger ett negativt resultat som skadar m√§nniskor? Vem √§r ansvarig f√∂r AI-systemets beteende? Dessa √§r n√•gra av de fr√•gor vi kommer att utforska i denna kursplan.

I denna lektion kommer du att:

- √ñka din medvetenhet om vikten av r√§ttvisa i maskininl√§rning och skador relaterade till or√§ttvisa.
- Bli bekant med att utforska avvikelser och ovanliga scenarier f√∂r att s√§kerst√§lla tillf√∂rlitlighet och s√§kerhet.
- F√• f√∂rst√•else f√∂r behovet av att st√§rka alla genom att designa inkluderande system.
- Utforska hur viktigt det √§r att skydda integritet och s√§kerhet f√∂r data och m√§nniskor.
- Se vikten av att ha en "glasl√•da"-metod f√∂r att f√∂rklara AI-modellers beteende.
- Vara medveten om hur ansvarstagande √§r avg√∂rande f√∂r att bygga f√∂rtroende f√∂r AI-system.

## F√∂rkunskaper

Som f√∂rkunskap, v√§nligen ta "Principer f√∂r ansvarsfull AI" p√• Learn Path och titta p√• videon nedan om √§mnet:

L√§r dig mer om ansvarsfull AI genom att f√∂lja denna [Learning Path](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Microsofts syn p√• ansvarsfull AI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Microsofts syn p√• ansvarsfull AI")

> üé• Klicka p√• bilden ovan f√∂r en video: Microsofts syn p√• ansvarsfull AI

## R√§ttvisa

AI-system b√∂r behandla alla r√§ttvist och undvika att p√•verka liknande grupper av m√§nniskor p√• olika s√§tt. Till exempel, n√§r AI-system ger v√§gledning om medicinsk behandling, l√•neans√∂kningar eller anst√§llning, b√∂r de ge samma rekommendationer till alla med liknande symptom, ekonomiska f√∂rh√•llanden eller yrkeskvalifikationer. Vi m√§nniskor b√§r alla p√• √§rvda f√∂rdomar som p√•verkar v√•ra beslut och handlingar. Dessa f√∂rdomar kan ocks√• √•terspeglas i data som anv√§nds f√∂r att tr√§na AI-system. S√•dan manipulation kan ibland ske oavsiktligt. Det √§r ofta sv√•rt att medvetet veta n√§r man introducerar f√∂rdomar i data.

**"Or√§ttvisa"** omfattar negativa effekter, eller "skador", f√∂r en grupp m√§nniskor, s√•som de definierade utifr√•n ras, k√∂n, √•lder eller funktionsneds√§ttning. De huvudsakliga skadorna relaterade till r√§ttvisa kan klassificeras som:

- **Allokering**, om exempelvis ett k√∂n eller en etnicitet gynnas √∂ver en annan.
- **Kvalitet p√• tj√§nsten**. Om du tr√§nar data f√∂r ett specifikt scenario men verkligheten √§r mycket mer komplex, leder det till en d√•ligt fungerande tj√§nst. Till exempel en tv√•lautomat som inte kunde k√§nna av personer med m√∂rk hud. [Referens](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Nedv√§rdering**. Att or√§ttvist kritisera och m√§rka n√•got eller n√•gon. Till exempel, en bildm√§rkningsteknik som √∂k√§ndes f√∂r att felaktigt m√§rka bilder av m√∂rkhyade personer som gorillor.
- **√ñver- eller underrepresentation**. Id√©n att en viss grupp inte syns i ett visst yrke, och att alla tj√§nster eller funktioner som forts√§tter att fr√§mja detta bidrar till skada.
- **Stereotyper**. Att associera en viss grupp med f√∂rutbest√§mda attribut. Till exempel kan ett spr√•k√∂vers√§ttningssystem mellan engelska och turkiska ha felaktigheter p√• grund av ord med stereotypa kopplingar till k√∂n.

![√∂vers√§ttning till turkiska](../../../../1-Introduction/3-fairness/images/gender-bias-translate-en-tr.png)
> √∂vers√§ttning till turkiska

![√∂vers√§ttning tillbaka till engelska](../../../../1-Introduction/3-fairness/images/gender-bias-translate-tr-en.png)
> √∂vers√§ttning tillbaka till engelska

N√§r vi designar och testar AI-system m√•ste vi s√§kerst√§lla att AI √§r r√§ttvis och inte programmerad att fatta partiska eller diskriminerande beslut, vilket √§ven m√§nniskor √§r f√∂rbjudna att g√∂ra. Att garantera r√§ttvisa i AI och maskininl√§rning f√∂rblir en komplex socio-teknisk utmaning.

### Tillf√∂rlitlighet och s√§kerhet

F√∂r att bygga f√∂rtroende m√•ste AI-system vara tillf√∂rlitliga, s√§kra och konsekventa under normala och ov√§ntade f√∂rh√•llanden. Det √§r viktigt att veta hur AI-system beter sig i olika situationer, s√§rskilt n√§r de √§r avvikande. N√§r vi bygger AI-l√∂sningar m√•ste vi l√§gga stor vikt vid hur vi hanterar en m√§ngd olika omst√§ndigheter som AI-l√∂sningarna kan st√∂ta p√•. Till exempel m√•ste en sj√§lvk√∂rande bil prioritera m√§nniskors s√§kerhet. D√§rf√∂r m√•ste AI som driver bilen ta h√§nsyn till alla m√∂jliga scenarier som bilen kan st√∂ta p√•, s√•som natt, √•skv√§der eller sn√∂stormar, barn som springer √∂ver gatan, husdjur, v√§garbeten etc. Hur v√§l ett AI-system kan hantera en m√§ngd olika f√∂rh√•llanden p√• ett tillf√∂rlitligt och s√§kert s√§tt √•terspeglar graden av f√∂rutseende som dataforskaren eller AI-utvecklaren hade under systemets design eller testning.

> [üé• Klicka h√§r f√∂r en video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inkludering

AI-system b√∂r designas f√∂r att engagera och st√§rka alla. N√§r dataforskare och AI-utvecklare designar och implementerar AI-system identifierar och adresserar de potentiella hinder i systemet som oavsiktligt kan exkludera m√§nniskor. Till exempel finns det 1 miljard m√§nniskor med funktionsneds√§ttningar v√§rlden √∂ver. Med AI:s framsteg kan de f√• tillg√•ng till en m√§ngd information och m√∂jligheter enklare i sina dagliga liv. Genom att adressera dessa hinder skapas m√∂jligheter att innovera och utveckla AI-produkter med b√§ttre upplevelser som gynnar alla.

> [üé• Klicka h√§r f√∂r en video: inkludering i AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### S√§kerhet och integritet

AI-system b√∂r vara s√§kra och respektera m√§nniskors integritet. M√§nniskor har mindre f√∂rtroende f√∂r system som √§ventyrar deras integritet, information eller liv. N√§r vi tr√§nar maskininl√§rningsmodeller f√∂rlitar vi oss p√• data f√∂r att producera de b√§sta resultaten. I detta arbete m√•ste vi √∂verv√§ga datans ursprung och integritet. Till exempel, var datan anv√§ndargenererad eller offentligt tillg√§nglig? Vidare, n√§r vi arbetar med data, √§r det avg√∂rande att utveckla AI-system som kan skydda konfidentiell information och motst√• attacker. N√§r AI blir allt vanligare blir det allt viktigare och mer komplext att skydda integritet och s√§kra viktig personlig och aff√§rsm√§ssig information. Integritets- och datas√§kerhetsfr√•gor kr√§ver s√§rskilt noggrann uppm√§rksamhet f√∂r AI eftersom tillg√•ng till data √§r avg√∂rande f√∂r att AI-system ska kunna g√∂ra korrekta och informerade f√∂ruts√§gelser och beslut om m√§nniskor.

> [üé• Klicka h√§r f√∂r en video: s√§kerhet i AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Som bransch har vi gjort betydande framsteg inom integritet och s√§kerhet, mycket tack vare regleringar som GDPR (General Data Protection Regulation).
- Med AI-system m√•ste vi dock erk√§nna sp√§nningen mellan behovet av mer personlig data f√∂r att g√∂ra systemen mer personliga och effektiva ‚Äì och integritet.
- Precis som med internets f√∂delse ser vi ocks√• en stor √∂kning av s√§kerhetsproblem relaterade till AI.
- Samtidigt har vi sett AI anv√§ndas f√∂r att f√∂rb√§ttra s√§kerheten. Till exempel drivs de flesta moderna antivirusprogram idag av AI-heuristik.
- Vi m√•ste s√§kerst√§lla att v√•ra dataforskningsprocesser harmoniserar med de senaste integritets- och s√§kerhetspraxis.

### Transparens

AI-system b√∂r vara f√∂rst√•eliga. En viktig del av transparens √§r att f√∂rklara AI-systemens beteende och deras komponenter. Att f√∂rb√§ttra f√∂rst√•elsen av AI-system kr√§ver att intressenter f√∂rst√•r hur och varf√∂r de fungerar s√• att de kan identifiera potentiella prestandaproblem, s√§kerhets- och integritetsproblem, f√∂rdomar, exkluderande praxis eller oavsiktliga resultat. Vi tror ocks√• att de som anv√§nder AI-system b√∂r vara √§rliga och √∂ppna om n√§r, varf√∂r och hur de v√§ljer att anv√§nda dem, samt om systemens begr√§nsningar. Till exempel, om en bank anv√§nder ett AI-system f√∂r att st√∂dja sina beslut om konsumentl√•n, √§r det viktigt att granska resultaten och f√∂rst√• vilken data som p√•verkar systemets rekommendationer. Regeringar b√∂rjar reglera AI inom olika branscher, s√• dataforskare och organisationer m√•ste f√∂rklara om ett AI-system uppfyller regulatoriska krav, s√§rskilt n√§r det finns ett o√∂nskat resultat.

> [üé• Klicka h√§r f√∂r en video: transparens i AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Eftersom AI-system √§r s√• komplexa √§r det sv√•rt att f√∂rst√• hur de fungerar och tolka resultaten.
- Denna brist p√• f√∂rst√•else p√•verkar hur dessa system hanteras, operationaliseras och dokumenteras.
- Denna brist p√• f√∂rst√•else p√•verkar √§nnu viktigare de beslut som fattas baserat p√• resultaten dessa system producerar.

### Ansvarstagande

De personer som designar och implementerar AI-system m√•ste vara ansvariga f√∂r hur deras system fungerar. Behovet av ansvarstagande √§r s√§rskilt viktigt med k√§nsliga teknologier som ansiktsigenk√§nning. P√• senare tid har efterfr√•gan p√• ansiktsigenk√§nningsteknik √∂kat, s√§rskilt fr√•n brottsbek√§mpande organisationer som ser potentialen i tekniken f√∂r anv√§ndning som att hitta f√∂rsvunna barn. Dessa teknologier kan dock potentiellt anv√§ndas av en regering f√∂r att √§ventyra medborgarnas grundl√§ggande friheter, exempelvis genom att m√∂jligg√∂ra kontinuerlig √∂vervakning av specifika individer. D√§rf√∂r m√•ste dataforskare och organisationer vara ansvariga f√∂r hur deras AI-system p√•verkar individer eller samh√§llet.

[![Ledande AI-forskare varnar f√∂r mass√∂vervakning genom ansiktsigenk√§nning](../../../../1-Introduction/3-fairness/images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Microsofts syn p√• ansvarsfull AI")

> üé• Klicka p√• bilden ovan f√∂r en video: Varningar om mass√∂vervakning genom ansiktsigenk√§nning

Slutligen √§r en av de st√∂rsta fr√•gorna f√∂r v√•r generation, som den f√∂rsta generationen som introducerar AI i samh√§llet, hur vi s√§kerst√§ller att datorer f√∂rblir ansvariga inf√∂r m√§nniskor och hur vi s√§kerst√§ller att de som designar datorer f√∂rblir ansvariga inf√∂r alla andra.

## Konsekvensbed√∂mning

Innan du tr√§nar en maskininl√§rningsmodell √§r det viktigt att genomf√∂ra en konsekvensbed√∂mning f√∂r att f√∂rst√• syftet med AI-systemet; vad den avsedda anv√§ndningen √§r; var det kommer att implementeras; och vem som kommer att interagera med systemet. Dessa √§r hj√§lpsamma f√∂r granskare eller testare som utv√§rderar systemet f√∂r att veta vilka faktorer som ska beaktas vid identifiering av potentiella risker och f√∂rv√§ntade konsekvenser.

F√∂ljande √§r fokusomr√•den vid genomf√∂rande av en konsekvensbed√∂mning:

* **Negativ p√•verkan p√• individer**. Att vara medveten om eventuella restriktioner eller krav, otill√•ten anv√§ndning eller k√§nda begr√§nsningar som hindrar systemets prestanda √§r avg√∂rande f√∂r att s√§kerst√§lla att systemet inte anv√§nds p√• ett s√§tt som kan skada individer.
* **Datakrav**. Att f√∂rst√• hur och var systemet kommer att anv√§nda data g√∂r det m√∂jligt f√∂r granskare att utforska eventuella datakrav som du m√•ste vara medveten om (t.ex. GDPR eller HIPPA-regler). Dessutom, unders√∂k om datak√§llan eller m√§ngden data √§r tillr√§cklig f√∂r tr√§ning.
* **Sammanfattning av p√•verkan**. Samla en lista √∂ver potentiella skador som kan uppst√• vid anv√§ndning av systemet. Under hela ML-livscykeln, granska om de identifierade problemen har √•tg√§rdats eller hanterats.
* **Till√§mpliga m√•l** f√∂r var och en av de sex k√§rnprinciperna. Bed√∂m om m√•len fr√•n varje princip uppfylls och om det finns n√•gra luckor.

## Fels√∂kning med ansvarsfull AI

Precis som att fels√∂ka en mjukvaruapplikation √§r fels√∂kning av ett AI-system en n√∂dv√§ndig process f√∂r att identifiera och l√∂sa problem i systemet. Det finns m√•nga faktorer som kan p√•verka att en modell inte presterar som f√∂rv√§ntat eller ansvarsfullt. De flesta traditionella modellprestandam√•tt √§r kvantitativa sammanst√§llningar av en modells prestanda, vilket inte √§r tillr√§ckligt f√∂r att analysera hur en modell bryter mot principerna f√∂r ansvarsfull AI. Dessutom √§r en maskininl√§rningsmodell en "svart l√•da" som g√∂r det sv√•rt att f√∂rst√• vad som driver dess resultat eller att ge en f√∂rklaring n√§r den g√∂r ett misstag. Senare i denna kurs kommer vi att l√§ra oss hur man anv√§nder Responsible AI-dashboarden f√∂r att hj√§lpa till att fels√∂ka AI-system. Dashboarden erbjuder ett holistiskt verktyg f√∂r dataforskare och AI-utvecklare att utf√∂ra:

* **Fels√∂kningsanalys**. F√∂r att identifiera felens f√∂rdelning i modellen som kan p√•verka systemets r√§ttvisa eller tillf√∂rlitlighet.
* **Modell√∂versikt**. F√∂r att uppt√§cka var det finns skillnader i modellens prestanda √∂ver olika datakohorter.
* **Dataanalys**. F√∂r att f√∂rst√• datadistributionen och identifiera eventuella f√∂rdomar i data som kan leda till problem med r√§ttvisa, inkludering och tillf√∂rlitlighet.
* **Modelltolkning**. F√∂r att f√∂rst√• vad som p√•verkar eller styr modellens f√∂ruts√§gelser. Detta hj√§lper till att f√∂rklara modellens beteende, vilket √§r viktigt f√∂r transparens och ansvarstagande.

## üöÄ Utmaning

F√∂r att f√∂rhindra att skador introduceras fr√•n b√∂rjan b√∂r vi:

- ha en m√•ngfald av bakgrunder och perspektiv bland de personer som arbetar med systemen
- investera i dataset som speglar m√•ngfalden i v√•rt samh√§lle
- utveckla b√§ttre metoder under hela maskininl√§rningslivscykeln f√∂r att uppt√§cka och korrigera ansvarsfull AI n√§r det intr√§ffar

T√§nk p√• verkliga scenarier d√§r en modells op√•litlighet √§r uppenbar i modellbyggande och anv√§ndning. Vad mer b√∂r vi √∂verv√§ga?

## [Quiz efter f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ml/)

## Granskning och sj√§lvstudier

I denna lektion har du l√§rt dig n√•gra grunder om begreppen r√§ttvisa och or√§ttvisa i maskininl√§rning.
Titta p√• denna workshop f√∂r att f√∂rdjupa dig i √§mnena:

- I jakten p√• ansvarsfull AI: Att oms√§tta principer i praktiken av Besmira Nushi, Mehrnoosh Sameki och Amit Sharma

[![Responsible AI Toolbox: En √∂ppen k√§llkodsram f√∂r att bygga ansvarsfull AI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: En √∂ppen k√§llkodsram f√∂r att bygga ansvarsfull AI")


> üé• Klicka p√• bilden ovan f√∂r en video: RAI Toolbox: En √∂ppen k√§llkodsram f√∂r att bygga ansvarsfull AI av Besmira Nushi, Mehrnoosh Sameki och Amit Sharma

L√§s ocks√•:

- Microsofts RAI-resurscenter: [Responsible AI Resources ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) 

- Microsofts FATE-forskningsgrupp: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) 

RAI Toolbox:

- [Responsible AI Toolbox GitHub-repository](https://github.com/microsoft/responsible-ai-toolbox)

L√§s om Azure Machine Learnings verktyg f√∂r att s√§kerst√§lla r√§ttvisa:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) 

## Uppgift

[Utforska RAI Toolbox](assignment.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, v√§nligen notera att automatiska √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.