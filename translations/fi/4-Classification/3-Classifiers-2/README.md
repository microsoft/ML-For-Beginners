<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "49047911108adc49d605cddfb455749c",
  "translation_date": "2025-09-05T00:50:41+00:00",
  "source_file": "4-Classification/3-Classifiers-2/README.md",
  "language_code": "fi"
}
-->
# Ruokakulttuuriluokittelijat 2

T√§ss√§ toisessa luokittelutunnissa tutustut tarkemmin tapoihin luokitella numeerista dataa. Opit my√∂s, mit√§ seurauksia on sill√§, ett√§ valitset yhden luokittelijan toisen sijaan.

## [Esiluennon kysely](https://ff-quizzes.netlify.app/en/ml/)

### Esitiedot

Oletamme, ett√§ olet suorittanut aiemmat oppitunnit ja sinulla on puhdistettu datasetti `data`-kansiossasi nimelt√§ _cleaned_cuisines.csv_, joka sijaitsee t√§m√§n nelj√§n oppitunnin kansion juurihakemistossa.

### Valmistelut

Olemme ladanneet _notebook.ipynb_-tiedostosi puhdistetulla datasetill√§ ja jakaneet sen X- ja y-datafreimeihin, jotka ovat valmiita mallin rakennusprosessia varten.

## Luokittelukartta

Aiemmin opit eri vaihtoehdoista datan luokitteluun Microsoftin huijauslistan avulla. Scikit-learn tarjoaa vastaavan, mutta tarkemman huijauslistan, joka voi auttaa kaventamaan valintaa luokittelijoiden (toinen termi estimointimenetelmille) v√§lill√§:

![ML-kartta Scikit-learnista](../../../../4-Classification/3-Classifiers-2/images/map.png)
> Vinkki: [vieraile kartassa verkossa](https://scikit-learn.org/stable/tutorial/machine_learning_map/) ja klikkaa polkuja lukeaksesi dokumentaatiota.

### Suunnitelma

T√§m√§ kartta on eritt√§in hy√∂dyllinen, kun ymm√§rr√§t datasi hyvin, sill√§ voit "kulkea" sen polkuja pitkin p√§√§t√∂kseen:

- Meill√§ on >50 n√§ytett√§
- Haluamme ennustaa kategorian
- Meill√§ on merkitty data
- Meill√§ on alle 100K n√§ytett√§
- ‚ú® Voimme valita Linear SVC:n
- Jos se ei toimi, koska meill√§ on numeerista dataa
    - Voimme kokeilla ‚ú® KNeighbors-luokittelijaa 
      - Jos se ei toimi, kokeile ‚ú® SVC:t√§ ja ‚ú® Ensemble-luokittelijoita

T√§m√§ on eritt√§in hy√∂dyllinen polku seurattavaksi.

## Harjoitus - jaa data

Seuraamalla t√§t√§ polkua meid√§n tulisi aloittaa tarvittavien kirjastojen tuonnilla.

1. Tuo tarvittavat kirjastot:

    ```python
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
    import numpy as np
    ```

1. Jaa koulutus- ja testidatasi:

    ```python
    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
    ```

## Linear SVC -luokittelija

Support-Vector Clustering (SVC) kuuluu Support-Vector Machines -perheeseen ML-tekniikoissa (lis√§tietoja alla). T√§ss√§ menetelm√§ss√§ voit valita "kernelin" p√§√§tt√§√§ksesi, miten etiketit ryhmitell√§√§n. 'C'-parametri viittaa 'regularisointiin', joka s√§√§telee parametrien vaikutusta. Kernel voi olla yksi [useista](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); t√§ss√§ asetamme sen 'lineaariseksi' varmistaaksemme, ett√§ hy√∂dynn√§mme lineaarista SVC:t√§. Todenn√§k√∂isyys oletuksena on 'false'; t√§ss√§ asetamme sen 'true' saadaksemme todenn√§k√∂isyysarvioita. Asetamme satunnaistilan '0':ksi sekoittaaksemme datan todenn√§k√∂isyyksien saamiseksi.

### Harjoitus - k√§yt√§ lineaarista SVC:t√§

Aloita luomalla luokittelijoiden taulukko. Lis√§√§t t√§h√§n taulukkoon asteittain, kun testaamme.

1. Aloita lineaarisella SVC:ll√§:

    ```python
    C = 10
    # Create different classifiers.
    classifiers = {
        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
    }
    ```

2. Kouluta mallisi k√§ytt√§en lineaarista SVC:t√§ ja tulosta raportti:

    ```python
    n_classifiers = len(classifiers)
    
    for index, (name, classifier) in enumerate(classifiers.items()):
        classifier.fit(X_train, np.ravel(y_train))
    
        y_pred = classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
        print(classification_report(y_test,y_pred))
    ```

    Tulokset ovat melko hyvi√§:

    ```output
    Accuracy (train) for Linear SVC: 78.6% 
                  precision    recall  f1-score   support
    
         chinese       0.71      0.67      0.69       242
          indian       0.88      0.86      0.87       234
        japanese       0.79      0.74      0.76       254
          korean       0.85      0.81      0.83       242
            thai       0.71      0.86      0.78       227
    
        accuracy                           0.79      1199
       macro avg       0.79      0.79      0.79      1199
    weighted avg       0.79      0.79      0.79      1199
    ```

## K-Neighbors -luokittelija

K-Neighbors kuuluu ML-menetelmien "naapurit"-perheeseen, jota voidaan k√§ytt√§√§ sek√§ valvottuun ett√§ valvomattomaan oppimiseen. T√§ss√§ menetelm√§ss√§ m√§√§ritell√§√§n ennalta m√§√§r√§tty m√§√§r√§ pisteit√§, ja data ker√§t√§√§n n√§iden pisteiden ymp√§rille siten, ett√§ yleistetyt etiketit voidaan ennustaa datalle.

### Harjoitus - k√§yt√§ K-Neighbors -luokittelijaa

Edellinen luokittelija oli hyv√§ ja toimi hyvin datan kanssa, mutta ehk√§ voimme saada paremman tarkkuuden. Kokeile K-Neighbors -luokittelijaa.

1. Lis√§√§ rivi luokittelijataulukkoon (lis√§√§ pilkku Linear SVC -kohdan j√§lkeen):

    ```python
    'KNN classifier': KNeighborsClassifier(C),
    ```

    Tulokset ovat hieman huonommat:

    ```output
    Accuracy (train) for KNN classifier: 73.8% 
                  precision    recall  f1-score   support
    
         chinese       0.64      0.67      0.66       242
          indian       0.86      0.78      0.82       234
        japanese       0.66      0.83      0.74       254
          korean       0.94      0.58      0.72       242
            thai       0.71      0.82      0.76       227
    
        accuracy                           0.74      1199
       macro avg       0.76      0.74      0.74      1199
    weighted avg       0.76      0.74      0.74      1199
    ```

    ‚úÖ Lue lis√§√§ [K-Neighborsista](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)

## Support Vector -luokittelija

Support-Vector -luokittelijat kuuluvat [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) -perheeseen ML-menetelmiss√§, joita k√§ytet√§√§n luokittelu- ja regressioteht√§viin. SVM:t "karttavat koulutusesimerkit pisteiksi avaruudessa" maksimoidakseen et√§isyyden kahden kategorian v√§lill√§. Seuraava data kartataan t√§h√§n avaruuteen, jotta sen kategoria voidaan ennustaa.

### Harjoitus - k√§yt√§ Support Vector -luokittelijaa

Kokeillaan hieman parempaa tarkkuutta Support Vector -luokittelijalla.

1. Lis√§√§ pilkku K-Neighbors -kohdan j√§lkeen ja lis√§√§ t√§m√§ rivi:

    ```python
    'SVC': SVC(),
    ```

    Tulokset ovat eritt√§in hyvi√§!

    ```output
    Accuracy (train) for SVC: 83.2% 
                  precision    recall  f1-score   support
    
         chinese       0.79      0.74      0.76       242
          indian       0.88      0.90      0.89       234
        japanese       0.87      0.81      0.84       254
          korean       0.91      0.82      0.86       242
            thai       0.74      0.90      0.81       227
    
        accuracy                           0.83      1199
       macro avg       0.84      0.83      0.83      1199
    weighted avg       0.84      0.83      0.83      1199
    ```

    ‚úÖ Lue lis√§√§ [Support-Vectorsista](https://scikit-learn.org/stable/modules/svm.html#svm)

## Ensemble-luokittelijat

Seurataan polkua aivan loppuun asti, vaikka edellinen testi oli eritt√§in hyv√§. Kokeillaan joitakin 'Ensemble-luokittelijoita', erityisesti Random Forestia ja AdaBoostia:

```python
  'RFST': RandomForestClassifier(n_estimators=100),
  'ADA': AdaBoostClassifier(n_estimators=100)
```

Tulokset ovat eritt√§in hyvi√§, erityisesti Random Forestin osalta:

```output
Accuracy (train) for RFST: 84.5% 
              precision    recall  f1-score   support

     chinese       0.80      0.77      0.78       242
      indian       0.89      0.92      0.90       234
    japanese       0.86      0.84      0.85       254
      korean       0.88      0.83      0.85       242
        thai       0.80      0.87      0.83       227

    accuracy                           0.84      1199
   macro avg       0.85      0.85      0.84      1199
weighted avg       0.85      0.84      0.84      1199

Accuracy (train) for ADA: 72.4% 
              precision    recall  f1-score   support

     chinese       0.64      0.49      0.56       242
      indian       0.91      0.83      0.87       234
    japanese       0.68      0.69      0.69       254
      korean       0.73      0.79      0.76       242
        thai       0.67      0.83      0.74       227

    accuracy                           0.72      1199
   macro avg       0.73      0.73      0.72      1199
weighted avg       0.73      0.72      0.72      1199
```

‚úÖ Lue lis√§√§ [Ensemble-luokittelijoista](https://scikit-learn.org/stable/modules/ensemble.html)

T√§m√§ koneoppimismenetelm√§ "yhdist√§√§ useiden perusestimointimenetelmien ennusteet" parantaakseen mallin laatua. Esimerkiss√§mme k√§ytimme Random Trees -menetelm√§√§ ja AdaBoostia. 

- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), keskiarvomenetelm√§, rakentaa "mets√§n" "p√§√§t√∂spuista", joihin lis√§t√§√§n satunnaisuutta ylisovituksen v√§ltt√§miseksi. n_estimators-parametri asetetaan puiden m√§√§r√§ksi.

- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) sovittaa luokittelijan datasettiin ja sovittaa kopioita t√§st√§ luokittelijasta samaan datasettiin. Se keskittyy v√§√§rin luokiteltujen kohteiden painoihin ja s√§√§t√§√§ seuraavan luokittelijan sovitusta korjatakseen.

---

## üöÄHaaste

Jokaisella n√§ist√§ tekniikoista on suuri m√§√§r√§ parametreja, joita voit s√§√§t√§√§. Tutki kunkin oletusparametreja ja mieti, mit√§ n√§iden parametrien s√§√§t√§minen tarkoittaisi mallin laadulle.

## [J√§lkiluennon kysely](https://ff-quizzes.netlify.app/en/ml/)

## Kertaus ja itseopiskelu

N√§iss√§ oppitunneissa on paljon ammattikielt√§, joten ota hetki aikaa tarkastellaksesi [t√§t√§ listaa](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) hy√∂dyllisist√§ termeist√§!

## Teht√§v√§ 

[Parametrien s√§√§t√∂](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.