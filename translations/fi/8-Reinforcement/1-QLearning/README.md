<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "911efd5e595089000cb3c16fce1beab8",
  "translation_date": "2025-09-05T01:11:06+00:00",
  "source_file": "8-Reinforcement/1-QLearning/README.md",
  "language_code": "fi"
}
-->
# Johdanto vahvistusoppimiseen ja Q-oppimiseen

![Yhteenveto vahvistusoppimisesta koneoppimisessa sketchnotena](../../../../sketchnotes/ml-reinforcement.png)
> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)

Vahvistusoppiminen sis√§lt√§√§ kolme t√§rke√§√§ k√§sitett√§: agentti, tilat ja joukko toimintoja per tila. Kun agentti suorittaa toiminnon tietyss√§ tilassa, se saa palkkion. Kuvittele tietokonepeli Super Mario. Sin√§ olet Mario, olet pelitasolla, seisot kallion reunalla. Yl√§puolellasi on kolikko. Sin√§, Mario, pelitasolla, tietyss√§ sijainnissa... se on tilasi. Yksi askel oikealle (toiminto) vie sinut reunalta alas, ja se antaisi sinulle matalan numeerisen pisteen. Kuitenkin hyppynapin painaminen antaisi sinulle pisteen ja pysyisit hengiss√§. Se on positiivinen lopputulos, ja sen pit√§isi palkita sinut positiivisella numeerisella pisteell√§.

K√§ytt√§m√§ll√§ vahvistusoppimista ja simulaattoria (peli√§) voit oppia pelaamaan peli√§ maksimoidaksesi palkkion, joka on pysy√§ hengiss√§ ja ker√§t√§ mahdollisimman paljon pisteit√§.

[![Johdanto vahvistusoppimiseen](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> üé• Klikkaa yll√§ olevaa kuvaa kuullaksesi Dmitryn keskustelua vahvistusoppimisesta

## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ml/)

## Esivaatimukset ja asennus

T√§ss√§ oppitunnissa kokeilemme Python-koodia. Sinun pit√§isi pysty√§ suorittamaan t√§m√§n oppitunnin Jupyter Notebook -koodi joko omalla tietokoneellasi tai pilvess√§.

Voit avata [oppitunnin notebookin](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) ja k√§yd√§ l√§pi t√§m√§n oppitunnin rakentaaksesi.

> **Huom:** Jos avaat t√§m√§n koodin pilvest√§, sinun t√§ytyy my√∂s hakea [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) -tiedosto, jota k√§ytet√§√§n notebook-koodissa. Lis√§√§ se samaan hakemistoon kuin notebook.

## Johdanto

T√§ss√§ oppitunnissa tutkimme **[Pekka ja susi](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)** -maailmaa, joka on saanut inspiraationsa ven√§l√§isen s√§velt√§j√§n [Sergei Prokofjevin](https://en.wikipedia.org/wiki/Sergei_Prokofiev) musiikillisesta sadusta. K√§yt√§mme **vahvistusoppimista** antaaksemme Pekalle mahdollisuuden tutkia ymp√§rist√∂√§√§n, ker√§t√§ herkullisia omenoita ja v√§ltt√§√§ kohtaamista suden kanssa.

**Vahvistusoppiminen** (RL) on oppimistekniikka, joka mahdollistaa optimaalisen k√§ytt√§ytymisen oppimisen **agentille** jossain **ymp√§rist√∂ss√§** suorittamalla lukuisia kokeita. Agentilla t√§ss√§ ymp√§rist√∂ss√§ tulisi olla jokin **tavoite**, joka m√§√§ritell√§√§n **palkkiofunktiolla**.

## Ymp√§rist√∂

Yksinkertaisuuden vuoksi kuvitelkaamme Pekan maailma neli√∂taulukoksi, jonka koko on `leveys` x `korkeus`, kuten t√§ss√§:

![Pekan ymp√§rist√∂](../../../../8-Reinforcement/1-QLearning/images/environment.png)

Jokainen solu t√§ss√§ taulukossa voi olla:

* **maa**, jolla Pekka ja muut olennot voivat k√§vell√§.
* **vesi**, jolla ei tietenk√§√§n voi k√§vell√§.
* **puu** tai **ruoho**, paikka, jossa voi lev√§t√§.
* **omena**, joka edustaa jotain, mit√§ Pekka mielell√§√§n l√∂yt√§isi ravinnokseen.
* **susi**, joka on vaarallinen ja tulisi v√§ltt√§√§.

On olemassa erillinen Python-moduuli, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), joka sis√§lt√§√§ koodin t√§m√§n ymp√§rist√∂n kanssa ty√∂skentelyyn. Koska t√§m√§ koodi ei ole t√§rke√§ k√§sitteidemme ymm√§rt√§misen kannalta, tuomme moduulin ja k√§yt√§mme sit√§ luodaksemme esimerkkitaulukon (koodilohko 1):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

T√§m√§n koodin pit√§isi tulostaa ymp√§rist√∂n kuva, joka on samanlainen kuin yll√§.

## Toiminnot ja politiikka

Esimerkiss√§mme Pekan tavoitteena olisi l√∂yt√§√§ omena samalla v√§ltt√§en suden ja muut esteet. T√§t√§ varten h√§n voi k√§yt√§nn√∂ss√§ k√§vell√§ ymp√§riins√§, kunnes l√∂yt√§√§ omenan.

Siksi miss√§ tahansa sijainnissa h√§n voi valita yhden seuraavista toiminnoista: yl√∂s, alas, vasemmalle ja oikealle.

M√§√§rittelemme n√§m√§ toiminnot sanakirjana ja yhdist√§mme ne vastaaviin koordinaattimuutoksiin. Esimerkiksi oikealle siirtyminen (`R`) vastaisi paria `(1,0)`. (koodilohko 2):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

Yhteenvetona t√§m√§n skenaarion strategia ja tavoite ovat seuraavat:

- **Strategia**, agenttimme (Pekan) strategia m√§√§ritell√§√§n niin sanotulla **politiikalla**. Politiikka on funktio, joka palauttaa toiminnon miss√§ tahansa tilassa. Meid√§n tapauksessamme ongelman tila esitet√§√§n taulukolla, mukaan lukien pelaajan nykyinen sijainti.

- **Tavoite**, vahvistusoppimisen tavoite on lopulta oppia hyv√§ politiikka, joka mahdollistaa ongelman tehokkaan ratkaisemisen. Perustana tarkastelemme kuitenkin yksinkertaisinta politiikkaa, jota kutsutaan **satunnaiseksi k√§velyksi**.

## Satunnainen k√§vely

Ratkaistaan ensin ongelmamme toteuttamalla satunnaisen k√§velyn strategia. Satunnaisessa k√§velyss√§ valitsemme seuraavan toiminnon satunnaisesti sallituista toiminnoista, kunnes saavumme omenalle (koodilohko 3).

1. Toteuta satunnainen k√§vely alla olevalla koodilla:

    ```python
    def random_policy(m):
        return random.choice(list(actions))
    
    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position 
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1
    
    walk(m,random_policy)
    ```

    `walk`-kutsun pit√§isi palauttaa vastaavan polun pituus, joka voi vaihdella eri suorituskerroilla.

1. Suorita k√§velykokeilu useita kertoja (esimerkiksi 100) ja tulosta tuloksena saadut tilastot (koodilohko 4):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")
    
    print_statistics(random_policy)
    ```

    Huomaa, ett√§ polun keskim√§√§r√§inen pituus on noin 30‚Äì40 askelta, mik√§ on melko paljon, kun otetaan huomioon, ett√§ keskim√§√§r√§inen et√§isyys l√§himp√§√§n omenaan on noin 5‚Äì6 askelta.

    Voit my√∂s n√§hd√§, milt√§ Pekan liikkuminen n√§ytt√§√§ satunnaisen k√§velyn aikana:

    ![Pekan satunnainen k√§vely](../../../../8-Reinforcement/1-QLearning/images/random_walk.gif)

## Palkkiofunktio

Jotta politiikkamme olisi √§lykk√§√§mpi, meid√§n t√§ytyy ymm√§rt√§√§, mitk√§ siirrot ovat "parempia" kuin toiset. T√§t√§ varten meid√§n t√§ytyy m√§√§ritell√§ tavoitteemme.

Tavoite voidaan m√§√§ritell√§ **palkkiofunktion** avulla, joka palauttaa jonkin pistem√§√§r√§n jokaiselle tilalle. Mit√§ korkeampi numero, sit√§ parempi palkkiofunktio. (koodilohko 5)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

Mielenkiintoinen asia palkkiofunktioissa on, ett√§ useimmissa tapauksissa *merkitt√§v√§ palkkio annetaan vasta pelin lopussa*. T√§m√§ tarkoittaa, ett√§ algoritmimme pit√§isi jotenkin muistaa "hyv√§t" askeleet, jotka johtavat positiiviseen palkkioon lopussa, ja lis√§t√§ niiden merkityst√§. Samoin kaikki siirrot, jotka johtavat huonoihin tuloksiin, tulisi est√§√§.

## Q-oppiminen

Algoritmi, jota k√§sittelemme t√§ss√§, on nimelt√§√§n **Q-oppiminen**. T√§ss√§ algoritmissa politiikka m√§√§ritell√§√§n funktiolla (tai tietorakenteella), jota kutsutaan **Q-taulukoksi**. Se tallentaa kunkin toiminnon "hyvyyden" tietyss√§ tilassa.

Sit√§ kutsutaan Q-taulukoksi, koska se on usein k√§tev√§√§ esitt√§√§ taulukkona tai monidimensionaalisena matriisina. Koska taulukkomme mitat ovat `leveys` x `korkeus`, voimme esitt√§√§ Q-taulukon numpy-matriisina, jonka muoto on `leveys` x `korkeus` x `len(toiminnot)`: (koodilohko 6)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

Huomaa, ett√§ alustamme kaikki Q-taulukon arvot samalla arvolla, t√§ss√§ tapauksessa - 0.25. T√§m√§ vastaa "satunnaisen k√§velyn" politiikkaa, koska kaikki siirrot jokaisessa tilassa ovat yht√§ hyvi√§. Voimme v√§litt√§√§ Q-taulukon `plot`-funktiolle visualisoidaksemme taulukon taulukossa: `m.plot(Q)`.

![Pekan ymp√§rist√∂](../../../../8-Reinforcement/1-QLearning/images/env_init.png)

Jokaisen solun keskell√§ on "nuoli", joka osoittaa suositellun liikkumissuunnan. Koska kaikki suunnat ovat yht√§ hyvi√§, n√§ytet√§√§n piste.

Nyt meid√§n t√§ytyy suorittaa simulaatio, tutkia ymp√§rist√∂√§mme ja oppia parempi Q-taulukon arvojen jakauma, joka mahdollistaa omenan l√∂yt√§misen paljon nopeammin.

## Q-oppimisen ydin: Bellmanin yht√§l√∂

Kun alamme liikkua, jokaisella toiminnolla on vastaava palkkio, eli voimme teoriassa valita seuraavan toiminnon korkeimman v√§litt√∂m√§n palkkion perusteella. Useimmissa tiloissa siirto ei kuitenkaan saavuta tavoitettamme, eli omenan l√∂yt√§mist√§, joten emme voi heti p√§√§tt√§√§, mik√§ suunta on parempi.

> Muista, ett√§ v√§lit√∂n tulos ei ole t√§rkein, vaan lopullinen tulos, jonka saamme simulaation lopussa.

Jotta voimme ottaa huomioon viiv√§styneen palkkion, meid√§n t√§ytyy k√§ytt√§√§ **[dynaamisen ohjelmoinnin](https://en.wikipedia.org/wiki/Dynamic_programming)** periaatteita, jotka mahdollistavat ongelman tarkastelun rekursiivisesti.

Oletetaan, ett√§ olemme nyt tilassa *s*, ja haluamme siirty√§ seuraavaan tilaan *s'*. Tekem√§ll√§ niin saamme v√§litt√∂m√§n palkkion *r(s,a)*, joka m√§√§ritell√§√§n palkkiofunktiolla, plus jonkin tulevan palkkion. Jos oletamme, ett√§ Q-taulukkomme heijastaa oikein kunkin toiminnon "houkuttelevuuden", niin tilassa *s'* valitsemme toiminnon *a*, joka vastaa maksimiarvoa *Q(s',a')*. N√§in ollen paras mahdollinen tuleva palkkio, jonka voimme saada tilassa *s*, m√§√§ritell√§√§n `max`

## Politiikan tarkistaminen

Koska Q-taulukko listaa kunkin toiminnon "houkuttelevuuden" kussakin tilassa, sen avulla on melko helppoa m√§√§ritell√§ tehokas navigointi maailmassamme. Yksinkertaisimmassa tapauksessa voimme valita toiminnon, joka vastaa korkeinta Q-taulukon arvoa: (koodilohko 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Jos kokeilet yll√§ olevaa koodia useita kertoja, saatat huomata, ett√§ se joskus "jumittuu", ja sinun t√§ytyy painaa STOP-painiketta keskeytt√§√§ksesi sen. T√§m√§ johtuu siit√§, ett√§ voi olla tilanteita, joissa kaksi tilaa "osoittavat" toisiaan optimaalisen Q-arvon suhteen, jolloin agentti p√§√§tyy liikkumaan n√§iden tilojen v√§lill√§ loputtomasti.

## üöÄHaaste

> **Teht√§v√§ 1:** Muokkaa `walk`-funktiota rajoittamaan polun maksimipituus tiettyyn askelm√§√§r√§√§n (esimerkiksi 100), ja katso, kuinka yll√§ oleva koodi palauttaa t√§m√§n arvon ajoittain.

> **Teht√§v√§ 2:** Muokkaa `walk`-funktiota niin, ettei se palaa paikkoihin, joissa se on jo aiemmin k√§ynyt. T√§m√§ est√§√§ `walk`-toiminnon silmukoinnin, mutta agentti voi silti p√§√§ty√§ "jumiin" paikkaan, josta se ei p√§√§se pois.

## Navigointi

Parempi navigointipolitiikka olisi se, jota k√§ytimme harjoittelun aikana, ja joka yhdist√§√§ hy√∂dynt√§misen ja tutkimisen. T√§ss√§ politiikassa valitsemme kunkin toiminnon tietyll√§ todenn√§k√∂isyydell√§, suhteessa Q-taulukon arvoihin. T√§m√§ strategia voi silti johtaa siihen, ett√§ agentti palaa jo tutkittuun paikkaan, mutta kuten alla olevasta koodista n√§et, se johtaa hyvin lyhyeen keskim√§√§r√§iseen polkuun haluttuun sijaintiin (muista, ett√§ `print_statistics` suorittaa simulaation 100 kertaa): (koodilohko 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Kun suoritat t√§m√§n koodin, saat paljon lyhyemm√§n keskim√§√§r√§isen polun pituuden kuin aiemmin, noin 3-6 v√§lill√§.

## Oppimisprosessin tutkiminen

Kuten mainitsimme, oppimisprosessi on tasapaino tutkimisen ja ongelmatilan rakenteesta saadun tiedon hy√∂dynt√§misen v√§lill√§. Olemme n√§hneet, ett√§ oppimisen tulokset (kyky auttaa agenttia l√∂yt√§m√§√§n lyhyt polku tavoitteeseen) ovat parantuneet, mutta on my√∂s mielenkiintoista tarkastella, miten keskim√§√§r√§inen polun pituus k√§ytt√§ytyy oppimisprosessin aikana:

## Oppimisen yhteenveto:

- **Keskim√§√§r√§inen polun pituus kasvaa**. Aluksi keskim√§√§r√§inen polun pituus kasvaa. T√§m√§ johtuu todenn√§k√∂isesti siit√§, ett√§ kun emme tied√§ ymp√§rist√∂st√§ mit√§√§n, olemme todenn√§k√∂isesti jumissa huonoissa tiloissa, kuten vedess√§ tai suden luona. Kun opimme lis√§√§ ja alamme k√§ytt√§√§ t√§t√§ tietoa, voimme tutkia ymp√§rist√∂√§ pidemp√§√§n, mutta emme silti tied√§ kovin hyvin, miss√§ omenat ovat.

- **Polun pituus lyhenee oppimisen edetess√§**. Kun opimme tarpeeksi, agentin on helpompi saavuttaa tavoite, ja polun pituus alkaa lyhenty√§. Olemme kuitenkin edelleen avoimia tutkimiselle, joten poikkeamme usein parhaasta polusta ja tutkimme uusia vaihtoehtoja, mik√§ tekee polusta pidemm√§n kuin optimaalinen.

- **Pituus kasvaa √§killisesti**. Graafista n√§emme my√∂s, ett√§ jossain vaiheessa pituus kasvaa √§killisesti. T√§m√§ osoittaa prosessin satunnaisen luonteen, ja ett√§ voimme jossain vaiheessa "pilata" Q-taulukon kertoimet korvaamalla ne uusilla arvoilla. T√§m√§ tulisi ihanteellisesti minimoida pienent√§m√§ll√§ oppimisnopeutta (esimerkiksi harjoittelun loppuvaiheessa s√§√§d√§mme Q-taulukon arvoja vain pienell√§ arvolla).

Kaiken kaikkiaan on t√§rke√§√§ muistaa, ett√§ oppimisprosessin onnistuminen ja laatu riippuvat merkitt√§v√§sti parametreista, kuten oppimisnopeudesta, oppimisnopeuden v√§henemisest√§ ja diskonttauskerroimesta. N√§it√§ kutsutaan usein **hyperparametreiksi**, jotta ne erotetaan **parametreista**, joita optimoimme harjoittelun aikana (esimerkiksi Q-taulukon kertoimet). Parhaiden hyperparametriarvojen l√∂yt√§mist√§ kutsutaan **hyperparametrien optimoinniksi**, ja se ansaitsee oman aiheensa.

## [Luennon j√§lkeinen kysely](https://ff-quizzes.netlify.app/en/ml/)

## Teht√§v√§ 
[Realistisempi maailma](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§ist√§ asiakirjaa sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.