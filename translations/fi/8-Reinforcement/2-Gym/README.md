<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "107d5bb29da8a562e7ae72262d251a75",
  "translation_date": "2025-09-05T01:17:30+00:00",
  "source_file": "8-Reinforcement/2-Gym/README.md",
  "language_code": "fi"
}
-->
## Esivaatimukset

T√§ss√§ oppitunnissa k√§yt√§mme kirjastoa nimelt√§ **OpenAI Gym** simuloimaan erilaisia **ymp√§rist√∂j√§**. Voit ajaa oppitunnin koodin paikallisesti (esim. Visual Studio Codessa), jolloin simulaatio avautuu uuteen ikkunaan. Jos suoritat koodin verkossa, sinun t√§ytyy ehk√§ tehd√§ joitakin muutoksia koodiin, kuten kuvataan [t√§ss√§](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).

## OpenAI Gym

Edellisess√§ oppitunnissa pelin s√§√§nn√∂t ja tila m√§√§riteltiin itse luomassamme `Board`-luokassa. T√§ss√§ k√§yt√§mme erityist√§ **simulaatioymp√§rist√∂√§**, joka simuloi tasapainottavan tangon fysiikkaa. Yksi suosituimmista simulaatioymp√§rist√∂ist√§ vahvistusoppimisalgoritmien kouluttamiseen on nimelt√§√§n [Gym](https://gym.openai.com/), jota yll√§pit√§√§ [OpenAI](https://openai.com/). T√§m√§n Gymin avulla voimme luoda erilaisia **ymp√§rist√∂j√§**, kuten cartpole-simulaation tai Atari-pelej√§.

> **Huomio**: Voit n√§hd√§ muita OpenAI Gymin tarjoamia ymp√§rist√∂j√§ [t√§√§lt√§](https://gym.openai.com/envs/#classic_control).

Aloitetaan asentamalla Gym ja tuomalla tarvittavat kirjastot (koodilohko 1):

```python
import sys
!{sys.executable} -m pip install gym 

import gym
import matplotlib.pyplot as plt
import numpy as np
import random
```

## Harjoitus - alustetaan cartpole-ymp√§rist√∂

Cartpole-tasapainotusongelman kanssa ty√∂skentely√§ varten meid√§n t√§ytyy alustaa vastaava ymp√§rist√∂. Jokainen ymp√§rist√∂ liittyy:

- **Havaintotilaan**, joka m√§√§ritt√§√§ rakenteen tiedoille, joita saamme ymp√§rist√∂st√§. Cartpole-ongelmassa saamme tangon sijainnin, nopeuden ja joitakin muita arvoja.

- **Toimintatilaan**, joka m√§√§ritt√§√§ mahdolliset toiminnot. Meid√§n tapauksessamme toimintatila on diskreetti ja sis√§lt√§√§ kaksi toimintoa - **vasemmalle** ja **oikealle**. (koodilohko 2)

1. Alustamiseen kirjoita seuraava koodi:

    ```python
    env = gym.make("CartPole-v1")
    print(env.action_space)
    print(env.observation_space)
    print(env.action_space.sample())
    ```

N√§emme, miten ymp√§rist√∂ toimii, suorittamalla lyhyen simulaation 100 askeleen ajan. Jokaisella askeleella annamme yhden toiminnon suoritettavaksi - t√§ss√§ simulaatiossa valitsemme toiminnon satunnaisesti `action_space`-tilasta.

1. Suorita alla oleva koodi ja katso, mihin se johtaa.

    ‚úÖ Muista, ett√§ on suositeltavaa ajaa t√§m√§ koodi paikallisessa Python-asennuksessa! (koodilohko 3)

    ```python
    env.reset()
    
    for i in range(100):
       env.render()
       env.step(env.action_space.sample())
    env.close()
    ```

    Sinun pit√§isi n√§hd√§ jotain t√§m√§n kuvan kaltaista:

    ![ei tasapainottava cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-nobalance.gif)

1. Simulaation aikana meid√§n t√§ytyy saada havaintoja p√§√§tt√§√§ksemme, miten toimia. Itse asiassa `step`-funktio palauttaa nykyiset havainnot, palkintofunktion ja `done`-lipun, joka osoittaa, onko j√§rkev√§√§ jatkaa simulaatiota vai ei: (koodilohko 4)

    ```python
    env.reset()
    
    done = False
    while not done:
       env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       print(f"{obs} -> {rew}")
    env.close()
    ```

    N√§et jotain t√§llaista notebookin tulosteessa:

    ```text
    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0
    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0
    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0
    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0
    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0
    ...
    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0
    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0
    ```

    Simulaation jokaisella askeleella palautettu havaintovektori sis√§lt√§√§ seuraavat arvot:
    - K√§rryn sijainti
    - K√§rryn nopeus
    - Tangon kulma
    - Tangon py√∂rimisnopeus

1. Hanki n√§iden lukujen minimi- ja maksimiarvot: (koodilohko 5)

    ```python
    print(env.observation_space.low)
    print(env.observation_space.high)
    ```

    Saatat my√∂s huomata, ett√§ palkintoarvo jokaisella simulaatioaskeleella on aina 1. T√§m√§ johtuu siit√§, ett√§ tavoitteemme on selviyty√§ mahdollisimman pitk√§√§n, eli pit√§√§ tanko kohtuullisen pystysuorassa mahdollisimman pitk√§√§n.

    ‚úÖ Itse asiassa CartPole-simulaatio katsotaan ratkaistuksi, jos onnistumme saamaan keskim√§√§r√§isen palkinnon 195 yli 100 per√§kk√§isen kokeilun aikana.

## Tilojen diskretisointi

Q-Learningissa meid√§n t√§ytyy rakentaa Q-taulukko, joka m√§√§ritt√§√§, mit√§ tehd√§ kussakin tilassa. Jotta voimme tehd√§ t√§m√§n, tilan t√§ytyy olla **diskreetti**, tarkemmin sanottuna sen t√§ytyy sis√§lt√§√§ rajallinen m√§√§r√§ diskreettej√§ arvoja. Siksi meid√§n t√§ytyy jollain tavalla **diskretisoida** havaintomme, kartoittaen ne rajalliseen joukkoon tiloja.

T√§h√§n on muutamia tapoja:

- **Jakaminen osiin**. Jos tied√§mme tietyn arvon v√§lin, voimme jakaa t√§m√§n v√§lin useisiin **osiin** ja korvata arvon sen osan numerolla, johon se kuuluu. T√§m√§ voidaan tehd√§ numpy-kirjaston [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html)-menetelm√§ll√§. T√§ss√§ tapauksessa tied√§mme tarkasti tilan koon, koska se riippuu valitsemastamme osien m√§√§r√§st√§ digitalisointia varten.

‚úÖ Voimme k√§ytt√§√§ lineaarista interpolointia tuodaksemme arvot rajalliseen v√§liin (esim. -20:st√§ 20:een) ja sitten muuntaa numerot kokonaisluvuiksi py√∂rist√§m√§ll√§. T√§m√§ antaa meille hieman v√§hemm√§n kontrollia tilan koosta, erityisesti jos emme tied√§ tarkkoja sy√∂tearvojen rajoja. Esimerkiksi meid√§n tapauksessamme 2 nelj√§st√§ arvosta ei ole yl√§-/alarajoja, mik√§ voi johtaa √§√§rett√∂m√§√§n m√§√§r√§√§n tiloja.

Esimerkiss√§mme k√§yt√§mme toista l√§hestymistapaa. Kuten saatat my√∂hemmin huomata, huolimatta m√§√§rittelem√§tt√∂mist√§ yl√§-/alarajoista, n√§m√§ arvot harvoin saavuttavat tiettyjen rajallisten v√§liarvojen ulkopuolisia arvoja, joten tilat, joilla on √§√§rimm√§isi√§ arvoja, ovat hyvin harvinaisia.

1. T√§ss√§ on funktio, joka ottaa mallimme havainnon ja tuottaa 4 kokonaislukuarvon tuplen: (koodilohko 6)

    ```python
    def discretize(x):
        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))
    ```

1. Tutkitaan my√∂s toinen diskretisointimenetelm√§ k√§ytt√§en osia: (koodilohko 7)

    ```python
    def create_bins(i,num):
        return np.arange(num+1)*(i[1]-i[0])/num+i[0]
    
    print("Sample bins for interval (-5,5) with 10 bins\n",create_bins((-5,5),10))
    
    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter
    nbins = [20,20,10,10] # number of bins for each parameter
    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]
    
    def discretize_bins(x):
        return tuple(np.digitize(x[i],bins[i]) for i in range(4))
    ```

1. Suoritetaan lyhyt simulaatio ja tarkkaillaan n√§it√§ diskreettej√§ ymp√§rist√∂arvoja. Voit kokeilla sek√§ `discretize`- ett√§ `discretize_bins`-funktioita ja n√§hd√§, onko niiss√§ eroa.

    ‚úÖ `discretize_bins` palauttaa osan numeron, joka alkaa nollasta. N√§in ollen sy√∂tearvojen ollessa l√§hell√§ 0 se palauttaa numeron v√§lin keskelt√§ (10). `discretize`-funktiossa emme v√§litt√§neet l√§ht√∂arvojen v√§list√§, jolloin arvot voivat olla negatiivisia, ja 0 vastaa 0:aa. (koodilohko 8)

    ```python
    env.reset()
    
    done = False
    while not done:
       #env.render()
       obs, rew, done, info = env.step(env.action_space.sample())
       #print(discretize_bins(obs))
       print(discretize(obs))
    env.close()
    ```

    ‚úÖ Poista kommentti rivilt√§, joka alkaa `env.render`, jos haluat n√§hd√§, miten ymp√§rist√∂ suoritetaan. Muutoin voit suorittaa sen taustalla, mik√§ on nopeampaa. K√§yt√§mme t√§t√§ "n√§kym√§t√∂nt√§" suoritusta Q-Learning-prosessimme aikana.

## Q-taulukon rakenne

Edellisess√§ oppitunnissa tila oli yksinkertainen pariluku v√§lill√§ 0‚Äì8, ja siksi oli k√§tev√§√§ esitt√§√§ Q-taulukko numpy-tensorina, jonka muoto oli 8x8x2. Jos k√§yt√§mme osien diskretisointia, tilavektorimme koko on my√∂s tiedossa, joten voimme k√§ytt√§√§ samaa l√§hestymistapaa ja esitt√§√§ tilan taulukolla, jonka muoto on 20x20x10x10x2 (t√§ss√§ 2 on toimintatilan ulottuvuus, ja ensimm√§iset ulottuvuudet vastaavat osien m√§√§r√§√§, joita olemme valinneet kullekin havaintotilan parametrille).

Joskus havaintotilan tarkat ulottuvuudet eiv√§t kuitenkaan ole tiedossa. `discretize`-funktion tapauksessa emme voi koskaan olla varmoja, ett√§ tilamme pysyy tiettyjen rajojen sis√§ll√§, koska jotkut alkuper√§isist√§ arvoista eiv√§t ole rajattuja. Siksi k√§yt√§mme hieman erilaista l√§hestymistapaa ja esittelemme Q-taulukon sanakirjana.

1. K√§yt√§ paria *(tila, toiminto)* sanakirjan avaimena, ja arvo vastaisi Q-taulukon arvoa. (koodilohko 9)

    ```python
    Q = {}
    actions = (0,1)
    
    def qvalues(state):
        return [Q.get((state,a),0) for a in actions]
    ```

    T√§ss√§ m√§√§rittelemme my√∂s funktion `qvalues()`, joka palauttaa listan Q-taulukon arvoista tietylle tilalle, joka vastaa kaikkia mahdollisia toimintoja. Jos merkint√§√§ ei ole Q-taulukossa, palautamme oletusarvona 0.

## Aloitetaan Q-Learning

Nyt olemme valmiita opettamaan Peteri√§ tasapainottamaan!

1. Asetetaan ensin joitakin hyperparametreja: (koodilohko 10)

    ```python
    # hyperparameters
    alpha = 0.3
    gamma = 0.9
    epsilon = 0.90
    ```

    T√§ss√§ `alpha` on **oppimisnopeus**, joka m√§√§ritt√§√§, miss√§ m√§√§rin meid√§n pit√§isi s√§√§t√§√§ Q-taulukon nykyisi√§ arvoja jokaisella askeleella. Edellisess√§ oppitunnissa aloitimme arvolla 1 ja sitten v√§hensimme `alpha`-arvoa koulutuksen aikana. T√§ss√§ esimerkiss√§ pid√§mme sen vakiona yksinkertaisuuden vuoksi, ja voit kokeilla `alpha`-arvojen s√§√§t√§mist√§ my√∂hemmin.

    `gamma` on **diskonttaustekij√§**, joka osoittaa, miss√§ m√§√§rin meid√§n pit√§isi priorisoida tulevaa palkintoa nykyisen palkinnon yli.

    `epsilon` on **tutkimus/hy√∂dynt√§mistekij√§**, joka m√§√§ritt√§√§, pit√§isik√∂ meid√§n suosia tutkimista vai hy√∂dynt√§mist√§. Algoritmissamme valitsemme `epsilon`-prosentissa tapauksista seuraavan toiminnon Q-taulukon arvojen mukaan, ja j√§ljell√§ olevissa tapauksissa suoritamme satunnaisen toiminnon. T√§m√§ antaa meille mahdollisuuden tutkia hakutilan alueita, joita emme ole koskaan n√§hneet.

    ‚úÖ Tasapainottamisen kannalta satunnaisen toiminnon valitseminen (tutkiminen) toimisi kuin satunnainen isku v√§√§r√§√§n suuntaan, ja tangon t√§ytyisi oppia, miten palauttaa tasapaino n√§ist√§ "virheist√§".

### Paranna algoritmia

Voimme my√∂s tehd√§ kaksi parannusta algoritmiimme edellisest√§ oppitunnista:

- **Laske keskim√§√§r√§inen kumulatiivinen palkinto** useiden simulaatioiden aikana. Tulostamme edistymisen joka 5000 iteraation j√§lkeen ja keskiarvoistamme kumulatiivisen palkinnon tuolta ajalta. T√§m√§ tarkoittaa, ett√§ jos saamme yli 195 pistett√§, voimme pit√§√§ ongelman ratkaistuna, jopa vaadittua korkeammalla laadulla.

- **Laske maksimaalinen keskim√§√§r√§inen kumulatiivinen tulos**, `Qmax`, ja tallennamme Q-taulukon, joka vastaa kyseist√§ tulosta. Kun suoritat koulutuksen, huomaat, ett√§ joskus keskim√§√§r√§inen kumulatiivinen tulos alkaa laskea, ja haluamme s√§ilytt√§√§ Q-taulukon arvot, jotka vastaavat parasta mallia, joka havaittiin koulutuksen aikana.

1. Ker√§√§ kaikki kumulatiiviset palkinnot jokaisessa simulaatiossa `rewards`-vektoriin my√∂hemp√§√§ visualisointia varten. (koodilohko 11)

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    
    Qmax = 0
    cum_rewards = []
    rewards = []
    for epoch in range(100000):
        obs = env.reset()
        done = False
        cum_reward=0
        # == do the simulation ==
        while not done:
            s = discretize(obs)
            if random.random()<epsilon:
                # exploitation - chose the action according to Q-Table probabilities
                v = probs(np.array(qvalues(s)))
                a = random.choices(actions,weights=v)[0]
            else:
                # exploration - randomly chose the action
                a = np.random.randint(env.action_space.n)
    
            obs, rew, done, info = env.step(a)
            cum_reward+=rew
            ns = discretize(obs)
            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))
        cum_rewards.append(cum_reward)
        rewards.append(cum_reward)
        # == Periodically print results and calculate average reward ==
        if epoch%5000==0:
            print(f"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}")
            if np.average(cum_rewards) > Qmax:
                Qmax = np.average(cum_rewards)
                Qbest = Q
            cum_rewards=[]
    ```

Mit√§ voit huomata n√§ist√§ tuloksista:

- **L√§hell√§ tavoitettamme**. Olemme hyvin l√§hell√§ tavoitteen saavuttamista, eli 195 kumulatiivista palkintoa yli 100+ per√§kk√§isen simulaation aikana, tai olemme ehk√§ jo saavuttaneet sen! Vaikka saisimme pienempi√§ lukuja, emme silti tied√§, koska keskiarvoistamme 5000 suorituksen aikana, ja virallinen kriteeri vaatii vain 100 suoritusta.

- **Palkinto alkaa laskea**. Joskus palkinto alkaa laskea, mik√§ tarkoittaa, ett√§ voimme "tuhota" jo opitut arvot Q-taulukossa arvoilla, jotka pahentavat tilannetta.

T√§m√§ havainto n√§kyy selke√§mmin, jos piirr√§mme koulutuksen edistymisen.

## Koulutuksen edistymisen visualisointi

Koulutuksen aikana olemme ker√§nneet kumulatiivisen palkintoarvon jokaisessa iteraatiossa `rewards`-vektoriin. T√§ss√§ on, milt√§ se n√§ytt√§√§, kun piirr√§mme sen iteraation numeroa vastaan:

```python
plt.plot(rewards)
```

![raaka edistyminen](../../../../8-Reinforcement/2-Gym/images/train_progress_raw.png)

T√§st√§ graafista ei ole mahdollista p√§√§tell√§ mit√§√§n, koska stokastisen koulutusprosessin luonteen vuoksi koulutussessioiden pituus vaihtelee suuresti. Jotta graafi olisi ymm√§rrett√§v√§mpi, voimme laskea **juoksevan keskiarvon** sarjasta kokeita, esimerkiksi 100. T√§m√§ voidaan tehd√§ k√§tev√§sti `np.convolve`-menetelm√§ll√§: (koodilohko 12)

```python
def running_average(x,window):
    return np.convolve(x,np.ones(window)/window,mode='valid')

plt.plot(running_average(rewards,100))
```

![koulutuksen edistyminen](../../../../8-Reinforcement/2-Gym/images/train_progress_runav.png)

## Hyperparametrien s√§√§t√§minen

Jotta oppiminen olisi vakaampaa, kannattaa s√§√§t√§√§ joitakin hyperparametreja koulutuksen aikana. Erityisesti:

- **Oppimisnopeuden**, `alpha`, osalta voimme aloittaa arvoilla l√§hell√§ 1 ja sitten v√§hent√§√§ parametria. Ajan my√∂t√§ saamme hyvi√§ todenn√§k√∂isyysarvoja Q-taulukkoon, ja siksi meid√§n pit√§isi s√§√§t√§√§ niit√§ hieman, eik√§ korvata kokonaan uusilla arvoilla.

- **Lis√§√§ epsilonia**. Voimme haluta lis√§t√§ `epsilon`-arvoa hitaasti, jotta tutkimme v√§hemm√§n ja hy√∂dynn√§mme enemm√§n. Todenn√§k√∂isesti kannattaa aloittaa pienemm√§ll√§ `epsilon`-arvolla ja nostaa se l√§hes 1:een.
> **Teht√§v√§ 1**: Kokeile hyperparametrien arvoja ja katso, voitko saavuttaa suuremman kumulatiivisen palkkion. P√§√§setk√∂ yli 195?
> **Teht√§v√§ 2**: Jotta ongelma ratkaistaan virallisesti, sinun t√§ytyy saavuttaa 195 keskim√§√§r√§inen palkinto 100 per√§kk√§isen ajon aikana. Mittaa t√§m√§ koulutuksen aikana ja varmista, ett√§ olet ratkaissut ongelman virallisesti!

## Tulosten tarkastelu k√§yt√§nn√∂ss√§

Olisi mielenkiintoista n√§hd√§, miten koulutettu malli k√§ytt√§ytyy. Suoritetaan simulaatio ja k√§ytet√§√§n samaa toimintojen valintastrategiaa kuin koulutuksen aikana, n√§ytteist√§m√§ll√§ Q-taulukon todenn√§k√∂isyysjakauman mukaan: (koodilohko 13)

```python
obs = env.reset()
done = False
while not done:
   s = discretize(obs)
   env.render()
   v = probs(np.array(qvalues(s)))
   a = random.choices(actions,weights=v)[0]
   obs,_,done,_ = env.step(a)
env.close()
```

N√§et jotain t√§llaista:

![tasapainottava cartpole](../../../../8-Reinforcement/2-Gym/images/cartpole-balance.gif)

---

## üöÄHaaste

> **Teht√§v√§ 3**: T√§ss√§ k√§ytimme Q-taulukon lopullista versiota, joka ei v√§ltt√§m√§tt√§ ole paras. Muista, ett√§ olemme tallentaneet parhaiten toimivan Q-taulukon `Qbest`-muuttujaan! Kokeile samaa esimerkki√§ parhaiten toimivalla Q-taulukolla kopioimalla `Qbest` `Q`:n tilalle ja katso, huomaatko eroa.

> **Teht√§v√§ 4**: T√§ss√§ emme valinneet parasta toimintoa jokaisella askeleella, vaan n√§ytteistimme vastaavan todenn√§k√∂isyysjakauman mukaan. Olisiko j√§rkev√§mp√§√§ aina valita paras toiminto, jolla on korkein Q-taulukon arvo? T√§m√§ voidaan tehd√§ k√§ytt√§m√§ll√§ `np.argmax`-funktiota l√∂yt√§√§kseen toimintojen numeron, joka vastaa korkeinta Q-taulukon arvoa. Toteuta t√§m√§ strategia ja katso, parantaako se tasapainottamista.

## [Luentoj√§lkeinen kysely](https://ff-quizzes.netlify.app/en/ml/)

## Teht√§v√§
[Harjoittele Mountain Car](assignment.md)

## Yhteenveto

Olemme nyt oppineet, kuinka kouluttaa agentteja saavuttamaan hyvi√§ tuloksia pelk√§st√§√§n tarjoamalla heille palkintofunktion, joka m√§√§rittelee pelin halutun tilan, ja antamalla heille mahdollisuuden √§lykk√§√§sti tutkia hakutilaa. Olemme onnistuneesti soveltaneet Q-Learning-algoritmia sek√§ diskreeteiss√§ ett√§ jatkuvissa ymp√§rist√∂iss√§, mutta diskreeteill√§ toiminnoilla.

On my√∂s t√§rke√§√§ tutkia tilanteita, joissa toimintotila on jatkuva ja havaintotila paljon monimutkaisempi, kuten kuva Atari-pelin n√§yt√∂st√§. N√§iss√§ ongelmissa tarvitsemme usein tehokkaampia koneoppimistekniikoita, kuten neuroverkkoja, saavuttaaksemme hyvi√§ tuloksia. N√§m√§ edistyneemm√§t aiheet ovat tulevan kehittyneemm√§n teko√§lykurssimme aiheena.

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.