<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "20ca019012b1725de956681d036d8b18",
  "translation_date": "2025-09-05T01:04:31+00:00",
  "source_file": "8-Reinforcement/README.md",
  "language_code": "fi"
}
-->
# Johdatus vahvistusoppimiseen

Vahvistusoppiminen, RL, n√§hd√§√§n yhten√§ koneoppimisen perusparadigmoista, yhdess√§ ohjatun oppimisen ja ohjaamattoman oppimisen kanssa. RL keskittyy p√§√§t√∂ksentekoon: oikeiden p√§√§t√∂sten tekemiseen tai ainakin oppimiseen niist√§.

Kuvittele, ett√§ sinulla on simuloitu ymp√§rist√∂, kuten osakemarkkinat. Mit√§ tapahtuu, jos asetat tietyn s√§√§ntelyn? Onko sill√§ positiivinen vai negatiivinen vaikutus? Jos jotain negatiivista tapahtuu, sinun t√§ytyy ottaa t√§m√§ _negatiivinen vahvistus_, oppia siit√§ ja muuttaa suuntaa. Jos tulos on positiivinen, sinun t√§ytyy rakentaa sen _positiivisen vahvistuksen_ varaan.

![peter ja susi](../../../8-Reinforcement/images/peter.png)

> Peter ja h√§nen yst√§v√§ns√§ yritt√§v√§t paeta n√§lk√§ist√§ sutta! Kuva: [Jen Looper](https://twitter.com/jenlooper)

## Alueellinen aihe: Peter ja susi (Ven√§j√§)

[Peter ja susi](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) on ven√§l√§isen s√§velt√§j√§n [Sergei Prokofjevin](https://en.wikipedia.org/wiki/Sergei_Prokofiev) kirjoittama musiikillinen satu. Se kertoo nuoresta pioneeri Peterist√§, joka rohkeasti l√§htee talostaan mets√§n aukealle jahtamaan sutta. T√§ss√§ osiossa koulutamme koneoppimisalgoritmeja, jotka auttavat Peteri√§:

- **Tutkimaan** ymp√§r√∂iv√§√§ aluetta ja rakentamaan optimaalisen navigointikartan
- **Oppimaan** k√§ytt√§m√§√§n skeittilautaa ja tasapainottamaan sill√§, jotta h√§n voi liikkua nopeammin.

[![Peter ja susi](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Klikkaa yll√§ olevaa kuvaa kuunnellaksesi Prokofjevin Peter ja susi

## Vahvistusoppiminen

Aiemmissa osioissa olet n√§hnyt kaksi esimerkki√§ koneoppimisongelmista:

- **Ohjattu oppiminen**, jossa meill√§ on datakokonaisuuksia, jotka ehdottavat esimerkkiratkaisuja ongelmaan, jonka haluamme ratkaista. [Luokittelu](../4-Classification/README.md) ja [regressio](../2-Regression/README.md) ovat ohjatun oppimisen teht√§vi√§.
- **Ohjaamaton oppiminen**, jossa meill√§ ei ole merkitty√§ harjoitusdataa. T√§rkein esimerkki ohjaamattomasta oppimisesta on [Klusterointi](../5-Clustering/README.md).

T√§ss√§ osiossa esittelemme uuden tyyppisen oppimisongelman, joka ei vaadi merkitty√§ harjoitusdataa. T√§llaisia ongelmia on useita:

- **[Puoliohjattu oppiminen](https://wikipedia.org/wiki/Semi-supervised_learning)**, jossa meill√§ on paljon merkitsem√§t√∂nt√§ dataa, jota voidaan k√§ytt√§√§ mallin esikoulutukseen.
- **[Vahvistusoppiminen](https://wikipedia.org/wiki/Reinforcement_learning)**, jossa agentti oppii k√§ytt√§ytym√§√§n tekem√§ll√§ kokeita jossain simuloidussa ymp√§rist√∂ss√§.

### Esimerkki - tietokonepeli

Oletetaan, ett√§ haluat opettaa tietokoneen pelaamaan peli√§, kuten shakkia tai [Super Mario](https://wikipedia.org/wiki/Super_Mario). Jotta tietokone voisi pelata peli√§, sen t√§ytyy ennustaa, mik√§ siirto tehd√§√§n kussakin pelitilanteessa. Vaikka t√§m√§ saattaa vaikuttaa luokitteluongelmalta, se ei ole sit√§ - koska meill√§ ei ole datakokonaisuutta, jossa olisi tilat ja vastaavat toiminnot. Vaikka meill√§ saattaisi olla dataa, kuten olemassa olevia shakkimatseja tai pelaajien Super Mario -pelitallenteita, on todenn√§k√∂ist√§, ett√§ t√§m√§ data ei riitt√§v√§sti kata suurta m√§√§r√§√§ mahdollisia tiloja.

Sen sijaan, ett√§ etsisimme olemassa olevaa pelidataa, **Vahvistusoppiminen** (RL) perustuu ideaan, ett√§ *tietokone pelaa* monta kertaa ja tarkkailee tulosta. N√§in ollen vahvistusoppimisen soveltamiseen tarvitaan kaksi asiaa:

- **Ymp√§rist√∂** ja **simulaattori**, jotka mahdollistavat pelin pelaamisen monta kertaa. T√§m√§ simulaattori m√§√§rittelisi kaikki pelis√§√§nn√∂t sek√§ mahdolliset tilat ja toiminnot.

- **Palkintofunktio**, joka kertoo, kuinka hyvin suoriuduimme kunkin siirron tai pelin aikana.

Suurin ero muiden koneoppimisen tyyppien ja RL:n v√§lill√§ on se, ett√§ RL:ss√§ emme yleens√§ tied√§, voitammeko vai h√§vi√§mme, ennen kuin peli on ohi. N√§in ollen emme voi sanoa, onko tietty siirto yksin√§√§n hyv√§ vai ei - saamme palkinnon vasta pelin lopussa. Tavoitteemme on suunnitella algoritmeja, jotka mahdollistavat mallin kouluttamisen ep√§varmoissa olosuhteissa. Opimme yhdest√§ RL-algoritmista nimelt√§ **Q-oppiminen**.

## Oppitunnit

1. [Johdatus vahvistusoppimiseen ja Q-oppimiseen](1-QLearning/README.md)
2. [Simulaatioymp√§rist√∂n k√§ytt√∂ Gymiss√§](2-Gym/README.md)

## Tekij√§t

"Johdatus vahvistusoppimiseen" on kirjoitettu ‚ô•Ô∏è:lla [Dmitry Soshnikov](http://soshnikov.com)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.