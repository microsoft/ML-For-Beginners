<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "abf86d845c84330bce205a46b382ec88",
  "translation_date": "2025-09-04T23:33:10+00:00",
  "source_file": "2-Regression/4-Logistic/README.md",
  "language_code": "fi"
}
-->
# Logistinen regressio kategorioiden ennustamiseen

![Logistinen vs. lineaarinen regressio -infografiikka](../../../../2-Regression/4-Logistic/images/linear-vs-logistic.png)

## [Esiluentakysely](https://ff-quizzes.netlify.app/en/ml/)

> ### [T√§m√§ oppitunti on saatavilla my√∂s R-kielell√§!](../../../../2-Regression/4-Logistic/solution/R/lesson_4.html)

## Johdanto

T√§ss√§ viimeisess√§ oppitunnissa regressiosta, joka on yksi perinteisist√§ _klassisen_ koneoppimisen tekniikoista, tarkastelemme logistista regressiota. T√§t√§ tekniikkaa k√§ytet√§√§n mallintamaan ja ennustamaan bin√§√§risi√§ kategorioita. Onko t√§m√§ karkki suklaata vai ei? Onko t√§m√§ tauti tarttuva vai ei? Valitseeko asiakas t√§m√§n tuotteen vai ei?

T√§ss√§ oppitunnissa opit:

- Uuden kirjaston datan visualisointiin
- Logistisen regression tekniikoita

‚úÖ Syvenn√§ ymm√§rryst√§si t√§m√§n tyyppisest√§ regressiosta t√§ss√§ [Learn-moduulissa](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)

## Esitiedot

Ty√∂skennelty√§mme kurpitsadataa k√§ytt√§en olemme nyt riitt√§v√§n tuttuja sen kanssa huomataksemme, ett√§ siin√§ on yksi bin√§√§rinen kategoria, jonka kanssa voimme ty√∂skennell√§: `Color`.

Rakennetaan logistinen regressiomalli ennustamaan, _mink√§ v√§rinen tietty kurpitsa todenn√§k√∂isesti on_ (oranssi üéÉ vai valkoinen üëª).

> Miksi puhumme bin√§√§risest√§ luokittelusta regressiota k√§sittelev√§ss√§ oppitunnissa? Ainoastaan kielellisen mukavuuden vuoksi, sill√§ logistinen regressio on [todellisuudessa luokittelumenetelm√§](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), vaikkakin lineaarinen sellainen. Opit muista tavoista luokitella dataa seuraavassa oppituntiryhm√§ss√§.

## M√§√§rittele kysymys

T√§ss√§ yhteydess√§ ilmaisemme t√§m√§n bin√§√§risesti: 'Valkoinen' tai 'Ei valkoinen'. Datasetiss√§mme on my√∂s 'raidallinen' kategoria, mutta sen esiintymi√§ on v√§h√§n, joten emme k√§yt√§ sit√§. Se katoaa joka tapauksessa, kun poistamme datasetist√§ puuttuvat arvot.

> üéÉ Hauska fakta: kutsumme joskus valkoisia kurpitsoja 'aavekurpitsoiksi'. Niit√§ ei ole kovin helppo kaivertaa, joten ne eiv√§t ole yht√§ suosittuja kuin oranssit, mutta ne n√§ytt√§v√§t siisteilt√§! Voisimme siis my√∂s muotoilla kysymyksemme n√§in: 'Aave' vai 'Ei aave'. üëª

## Tietoa logistisesta regressiosta

Logistinen regressio eroaa aiemmin oppimastasi lineaarisesta regressiosta muutamalla t√§rke√§ll√§ tavalla.

[![ML aloittelijoille - Logistisen regression ymm√§rt√§minen koneoppimisen luokittelussa](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY "ML aloittelijoille - Logistisen regression ymm√§rt√§minen koneoppimisen luokittelussa")

> üé• Klikkaa yll√§ olevaa kuvaa saadaksesi lyhyen videoesittelyn logistisesta regressiosta.

### Bin√§√§rinen luokittelu

Logistinen regressio ei tarjoa samoja ominaisuuksia kuin lineaarinen regressio. Edellinen tarjoaa ennusteen bin√§√§risest√§ kategoriasta ("valkoinen tai ei valkoinen"), kun taas j√§lkimm√§inen pystyy ennustamaan jatkuvia arvoja, esimerkiksi kurpitsan alkuper√§n ja sadonkorjuuajan perusteella, _kuinka paljon sen hinta nousee_.

![Kurpitsan luokittelumalli](../../../../2-Regression/4-Logistic/images/pumpkin-classifier.png)
> Infografiikka: [Dasani Madipalli](https://twitter.com/dasani_decoded)

### Muut luokittelut

Logistisessa regressiossa on my√∂s muita tyyppej√§, kuten multinomiaalinen ja ordinaalinen:

- **Multinomiaalinen**, jossa on useampi kuin yksi kategoria - "Oranssi, Valkoinen ja Raidallinen".
- **Ordinaalinen**, jossa on j√§rjestettyj√§ kategorioita, hy√∂dyllinen, jos haluamme j√§rjest√§√§ tulokset loogisesti, kuten kurpitsat, jotka on j√§rjestetty kokojen mukaan (mini, sm, med, lg, xl, xxl).

![Multinomiaalinen vs. ordinaalinen regressio](../../../../2-Regression/4-Logistic/images/multinomial-vs-ordinal.png)

### Muuttujien EI tarvitse korreloida

Muistatko, kuinka lineaarinen regressio toimi paremmin korreloivien muuttujien kanssa? Logistinen regressio on p√§invastainen - muuttujien ei tarvitse olla linjassa. T√§m√§ sopii t√§lle datalle, jossa korrelaatiot ovat melko heikkoja.

### Tarvitset paljon puhdasta dataa

Logistinen regressio antaa tarkempia tuloksia, jos k√§yt√§t enemm√§n dataa; pieni datasetimme ei ole optimaalinen t√§h√§n teht√§v√§√§n, joten pid√§ t√§m√§ mieless√§.

[![ML aloittelijoille - Datan analysointi ja valmistelu logistista regressiota varten](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs "ML aloittelijoille - Datan analysointi ja valmistelu logistista regressiota varten")

> üé• Klikkaa yll√§ olevaa kuvaa saadaksesi lyhyen videoesittelyn datan valmistelusta lineaarista regressiota varten.

‚úÖ Mieti, millaiset datatyypit sopisivat hyvin logistiseen regressioon.

## Harjoitus - siivoa data

Ensiksi siivoa dataa hieman, poista puuttuvat arvot ja valitse vain tietyt sarakkeet:

1. Lis√§√§ seuraava koodi:

    ```python
  
    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']
    pumpkins = full_pumpkins.loc[:, columns_to_select]

    pumpkins.dropna(inplace=True)
    ```

    Voit aina vilkaista uutta dataframeasi:

    ```python
    pumpkins.info
    ```

### Visualisointi - kategorinen kaavio

Olet nyt ladannut [aloitusmuistion](../../../../2-Regression/4-Logistic/notebook.ipynb) kurpitsadatalla ja siivonnut sen niin, ett√§ datasetiss√§ on muutama muuttuja, mukaan lukien `Color`. Visualisoidaan dataframe muistiossa k√§ytt√§en eri kirjastoa: [Seaborn](https://seaborn.pydata.org/index.html), joka on rakennettu aiemmin k√§ytt√§m√§mme Matplotlibin p√§√§lle.

Seaborn tarjoaa k√§tevi√§ tapoja visualisoida dataa. Esimerkiksi voit verrata datan jakaumia jokaiselle `Variety`- ja `Color`-arvolle kategorisessa kaaviossa.

1. Luo t√§llainen kaavio k√§ytt√§m√§ll√§ `catplot`-funktiota, k√§ytt√§en kurpitsadataamme `pumpkins` ja m√§√§ritt√§m√§ll√§ v√§rikartta jokaiselle kurpitsakategorialle (oranssi tai valkoinen):

    ```python
    import seaborn as sns
    
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }

    sns.catplot(
    data=pumpkins, y="Variety", hue="Color", kind="count",
    palette=palette, 
    )
    ```

    ![Visualisoidun datan ruudukko](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_1.png)

    Tarkastelemalla dataa voit n√§hd√§, miten `Color`-data liittyy `Variety`-arvoon.

    ‚úÖ Mit√§ mielenkiintoisia tutkimusaiheita voit keksi√§ t√§m√§n kategorisen kaavion perusteella?

### Datan esik√§sittely: ominaisuuksien ja luokkien koodaus

Datasetiss√§mme kaikki sarakkeet sis√§lt√§v√§t merkkijonoarvoja. Kategorisen datan k√§sittely on ihmisille intuitiivista, mutta ei koneille. Koneoppimisalgoritmit toimivat hyvin numeroiden kanssa. Siksi koodaus on eritt√§in t√§rke√§ vaihe datan esik√§sittelyss√§, koska se mahdollistaa kategorisen datan muuttamisen numeeriseksi dataksi ilman tietojen menetyst√§. Hyv√§ koodaus johtaa hyv√§n mallin rakentamiseen.

Ominaisuuksien koodaukseen on kaksi p√§√§tyyppi√§:

1. Ordinaalikoodaus: sopii hyvin ordinaalisille muuttujille, jotka ovat kategorisia muuttujia, joiden data noudattaa loogista j√§rjestyst√§, kuten datasetimme `Item Size` -sarake. T√§m√§ luo kartoituksen, jossa jokainen kategoria esitet√§√§n numerolla, joka on sarakkeen kategorian j√§rjestysnumero.

    ```python
    from sklearn.preprocessing import OrdinalEncoder

    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]
    ordinal_features = ['Item Size']
    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)
    ```

2. Kategorinen koodaus: sopii hyvin nominaalisille muuttujille, jotka ovat kategorisia muuttujia, joiden data ei noudata loogista j√§rjestyst√§, kuten kaikki muut datasetin ominaisuudet paitsi `Item Size`. T√§m√§ on yksi-hot-koodaus, mik√§ tarkoittaa, ett√§ jokainen kategoria esitet√§√§n bin√§√§risell√§ sarakkeella: koodattu muuttuja on yht√§ kuin 1, jos kurpitsa kuuluu kyseiseen `Variety`-arvoon, ja 0 muuten.

    ```python
    from sklearn.preprocessing import OneHotEncoder

    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']
    categorical_encoder = OneHotEncoder(sparse_output=False)
    ```

Sitten `ColumnTransformer`-luokkaa k√§ytet√§√§n yhdist√§m√§√§n useita koodauksia yhdeksi vaiheeksi ja soveltamaan niit√§ sopiviin sarakkeisiin.

```python
    from sklearn.compose import ColumnTransformer
    
    ct = ColumnTransformer(transformers=[
        ('ord', ordinal_encoder, ordinal_features),
        ('cat', categorical_encoder, categorical_features)
        ])
    
    ct.set_output(transform='pandas')
    encoded_features = ct.fit_transform(pumpkins)
```

Toisaalta luokan koodaamiseen k√§yt√§mme scikit-learnin `LabelEncoder`-luokkaa, joka on apuluokka normalisoimaan luokat siten, ett√§ ne sis√§lt√§v√§t vain arvoja v√§lill√§ 0 ja n_classes-1 (t√§ss√§ 0 ja 1).

```python
    from sklearn.preprocessing import LabelEncoder

    label_encoder = LabelEncoder()
    encoded_label = label_encoder.fit_transform(pumpkins['Color'])
```

Kun olemme koodanneet ominaisuudet ja luokan, voimme yhdist√§√§ ne uuteen dataframeen `encoded_pumpkins`.

```python
    encoded_pumpkins = encoded_features.assign(Color=encoded_label)
```

‚úÖ Mitk√§ ovat ordinaalikoodauksen edut `Item Size` -sarakkeelle?

### Analysoi muuttujien v√§lisi√§ suhteita

Nyt kun olemme esik√§sitelleet datamme, voimme analysoida ominaisuuksien ja luokan v√§lisi√§ suhteita saadaksemme k√§sityksen siit√§, kuinka hyvin malli pystyy ennustamaan luokan annettujen ominaisuuksien perusteella. Paras tapa suorittaa t√§m√§ntyyppinen analyysi on datan visualisointi. K√§yt√§mme j√§lleen Seabornin `catplot`-funktiota visualisoidaksemme suhteet `Item Size`-, `Variety`- ja `Color`-arvojen v√§lill√§ kategorisessa kaaviossa. Visualisointia varten k√§yt√§mme koodattua `Item Size` -saraketta ja koodaamatonta `Variety`-saraketta.

```python
    palette = {
    'ORANGE': 'orange',
    'WHITE': 'wheat',
    }
    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']

    g = sns.catplot(
        data=pumpkins,
        x="Item Size", y="Color", row='Variety',
        kind="box", orient="h",
        sharex=False, margin_titles=True,
        height=1.8, aspect=4, palette=palette,
    )
    g.set(xlabel="Item Size", ylabel="").set(xlim=(0,6))
    g.set_titles(row_template="{row_name}")
```

![Visualisoidun datan kaavio](../../../../2-Regression/4-Logistic/images/pumpkins_catplot_2.png)

### K√§yt√§ swarm-kaaviota

Koska `Color` on bin√§√§rinen kategoria (valkoinen tai ei), se tarvitsee '[erikoistuneen l√§hestymistavan](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar)' visualisointiin. On olemassa muita tapoja visualisoida t√§m√§n kategorian suhde muihin muuttujiin.

Voit visualisoida muuttujia rinnakkain Seabornin kaavioilla.

1. Kokeile 'swarm'-kaaviota arvojen jakauman n√§ytt√§miseksi:

    ```python
    palette = {
    0: 'orange',
    1: 'wheat'
    }
    sns.swarmplot(x="Color", y="ord__Item Size", data=encoded_pumpkins, palette=palette)
    ```

    ![Visualisoidun datan swarm-kaavio](../../../../2-Regression/4-Logistic/images/swarm_2.png)

**Huomio**: Yll√§ oleva koodi saattaa tuottaa varoituksen, koska Seaborn ei pysty esitt√§m√§√§n n√§in suurta m√§√§r√§√§ datapisteit√§ swarm-kaaviossa. Mahdollinen ratkaisu on pienent√§√§ merkkien kokoa k√§ytt√§m√§ll√§ 'size'-parametria. Huomaa kuitenkin, ett√§ t√§m√§ voi vaikuttaa kaavion luettavuuteen.

> **üßÆ N√§yt√§ matematiikka**
>
> Logistinen regressio perustuu 'maksimim√§√§r√§n todenn√§k√∂isyyden' k√§sitteeseen k√§ytt√§en [sigmoidifunktioita](https://wikipedia.org/wiki/Sigmoid_function). 'Sigmoidifunktio' n√§ytt√§√§ graafilla 'S'-muotoiselta. Se ottaa arvon ja muuntaa sen v√§lill√§ 0 ja 1. Sen k√§yr√§√§ kutsutaan my√∂s 'logistiseksi k√§yr√§ksi'. Sen kaava n√§ytt√§√§ t√§lt√§:
>
> ![logistinen funktio](../../../../2-Regression/4-Logistic/images/sigmoid.png)
>
> miss√§ sigmoidin keskipiste on x:n 0-pisteess√§, L on k√§yr√§n maksimiarvo ja k on k√§yr√§n jyrkkyys. Jos funktion tulos on yli 0,5, kyseinen luokka saa bin√§√§risen valinnan arvon '1'. Muussa tapauksessa se luokitellaan arvoksi '0'.

## Rakenna mallisi

Bin√§√§risen luokittelun mallin rakentaminen on yll√§tt√§v√§n suoraviivaista Scikit-learnilla.

[![ML aloittelijoille - Logistinen regressio datan luokitteluun](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 "ML aloittelijoille - Logistinen regressio datan luokitteluun")

> üé• Klikkaa yll√§ olevaa kuvaa saadaksesi lyhyen videoesittelyn logistisen regressiomallin rakentamisesta.

1. Valitse muuttujat, joita haluat k√§ytt√§√§ luokittelumallissasi, ja jaa data koulutus- ja testijoukkoihin kutsumalla `train_test_split()`:

    ```python
    from sklearn.model_selection import train_test_split
    
    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]
    y = encoded_pumpkins['Color']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    
    ```

2. Nyt voit kouluttaa mallisi kutsumalla `fit()` koulutusdatalla ja tulostaa sen tuloksen:

    ```python
    from sklearn.metrics import f1_score, classification_report 
    from sklearn.linear_model import LogisticRegression

    model = LogisticRegression()
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    print(classification_report(y_test, predictions))
    print('Predicted labels: ', predictions)
    print('F1-score: ', f1_score(y_test, predictions))
    ```

    Tarkastele mallisi tulostaulukkoa. Se ei ole huono, kun otetaan huomioon, ett√§ sinulla on vain noin 1000 rivi√§ dataa:

    ```output
                       precision    recall  f1-score   support
    
                    0       0.94      0.98      0.96       166
                    1       0.85      0.67      0.75        33
    
        accuracy                                0.92       199
        macro avg           0.89      0.82      0.85       199
        weighted avg        0.92      0.92      0.92       199
    
        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0
        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0
        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0
        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
        0 0 0 1 0 0 0 0 0 0 0 0 1 1]
        F1-score:  0.7457627118644068
    ```

## Parempi ymm√§rrys virhemaatriksin avulla

Vaikka voit saada tulostaulukon raportin [termeist√§](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) tulostamalla yll√§ olevat kohteet, saatat ymm√§rt√§√§ malliasi helpommin k√§ytt√§m√§ll√§ [virhemaatriksia](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) mallin suorituskyvyn arvioimiseen.

> üéì '[Virhemaatriksi](https://wikipedia.org/wiki/Confusion_matrix)' (tai 'virhemaatriksi') on taulukko, joka ilmaisee mallisi todelliset vs. v√§√§r√§t positiiviset ja negatiiviset arvot, ja siten arvioi ennusteiden tarkkuutta.

1. K√§yt√§ virhemaatriksia kutsumalla `confusion_matrix()`:

    ```python
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_test, predictions)
    ```

    Tarkastele mallisi virhemaatriksia:

    ```output
    array([[162,   4],
           [ 11,  22]])
    ```

Scikit-learnissa virhemaatriksin rivit (akseli 0) ovat todellisia luokkia ja sarakkeet (akseli 1) ovat ennustettuja luokkia.

|       |   0   |   1   |
| :---: | :---: | :---: |
|   0   |  TN   |  FP   |
|   1   |  FN   |  TP   |

Mit√§ t√§ss√§ tapahtuu? Oletetaan, ett√§ malliamme pyydet√§√§n luokittelemaan kurpitsat kahteen bin√§√§riseen kategoriaan, kategoriaan 'valkoinen' ja kategoriaan 'ei-valkoinen'.

- Jos mallisi ennustaa kurpitsan ei-valkoiseksi ja se kuuluu todellisuudessa kategoriaan 'ei-valkoinen', kutsumme sit√§ oikeaksi negatiiviseksi (TN), joka n√§kyy vasemmassa yl√§kulmassa.
- Jos mallisi ennustaa kurpitsan valkoiseksi ja se kuuluu todellisuudessa kategoriaan 'ei-valkoinen', kutsumme sit√§ v√§√§r√§ksi positiiviseksi (FP), joka n√§kyy oikeassa yl√§kulmassa.
- Jos mallisi ennustaa kurpitsan ei-valkoiseksi ja se kuuluu todellisuudessa kategoriaan 'valkoinen', kutsumme sit√§ v√§√§r√§ksi negatiiviseksi (FN), joka n√§kyy vasemmassa alakulmassa.
- Jos mallisi ennustaa kurpitsan valkoiseksi ja se kuuluu todellisuudessa kategoriaan 'valkoinen', kutsumme sit√§ oikeaksi positiiviseksi (TP), joka n√§kyy oikeassa alakulmassa.

Kuten arvata saattaa, on toivottavaa, ett√§ oikeiden positiivisten ja oikeiden negatiivisten m√§√§r√§ on suuri ja v√§√§rien positiivisten ja v√§√§rien negatiivisten m√§√§r√§ pieni, mik√§ tarkoittaa, ett√§ malli toimii paremmin.
Miten sekaannusmatriisi liittyy tarkkuuteen ja kattavuuteen? Muista, ett√§ yll√§ tulostettu luokitteluraportti n√§ytti tarkkuuden (0.85) ja kattavuuden (0.67).

Tarkkuus = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461

Kattavuus = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666

‚úÖ K: Miten malli suoriutui sekaannusmatriisin perusteella? V: Ei huono; on hyv√§ m√§√§r√§ oikeita negatiivisia, mutta my√∂s joitakin v√§√§ri√§ negatiivisia.

Palataan aiemmin n√§htyihin termeihin sekaannusmatriisin TP/TN ja FP/FN -kartoituksen avulla:

üéì Tarkkuus: TP/(TP + FP) Relevanttien tapausten osuus haetuista tapauksista (esim. mitk√§ tunnisteet merkittiin oikein)

üéì Kattavuus: TP/(TP + FN) Relevanttien tapausten osuus, jotka haettiin, riippumatta siit√§, merkittiink√∂ ne oikein vai ei

üéì f1-pisteet: (2 * tarkkuus * kattavuus)/(tarkkuus + kattavuus) Painotettu keskiarvo tarkkuudesta ja kattavuudesta, paras arvo on 1 ja huonoin 0

üéì Tuki: Haettujen tunnisteiden esiintymien lukum√§√§r√§

üéì Tarkkuus: (TP + TN)/(TP + TN + FP + FN) Oikein ennustettujen tunnisteiden prosenttiosuus n√§ytteest√§.

üéì Makrokeskiarvo: Painottamattomien keskiarvojen laskeminen kullekin tunnisteelle, ottamatta huomioon tunnisteiden ep√§tasapainoa.

üéì Painotettu keskiarvo: Keskiarvojen laskeminen kullekin tunnisteelle, ottaen huomioon tunnisteiden ep√§tasapaino painottamalla niit√§ niiden tuen (kunkin tunnisteen oikeiden tapausten lukum√§√§r√§n) mukaan.

‚úÖ Voitko mietti√§, mit√§ metriikkaa sinun tulisi seurata, jos haluat v√§hent√§√§ v√§√§rien negatiivisten m√§√§r√§√§?

## Visualisoi t√§m√§n mallin ROC-k√§yr√§

[![ML aloittelijoille - Logistisen regressiomallin suorituskyvyn analysointi ROC-k√§yrill√§](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 "ML aloittelijoille - Logistisen regressiomallin suorituskyvyn analysointi ROC-k√§yrill√§")

> üé• Klikkaa yll√§ olevaa kuvaa saadaksesi lyhyen videokatsauksen ROC-k√§yrist√§

Tehd√§√§n viel√§ yksi visualisointi, jotta n√§emme niin sanotun 'ROC'-k√§yr√§n:

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline

y_scores = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])

fig = plt.figure(figsize=(6, 6))
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()
```

K√§ytt√§m√§ll√§ Matplotlibia, piirr√§ mallin [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) eli ROC. ROC-k√§yri√§ k√§ytet√§√§n usein luokittelijan tulosten tarkasteluun oikeiden ja v√§√§rien positiivisten osalta. "ROC-k√§yrill√§ Y-akselilla on yleens√§ oikeiden positiivisten osuus ja X-akselilla v√§√§rien positiivisten osuus." K√§yr√§n jyrkkyys ja v√§li keskilinjan ja k√§yr√§n v√§lill√§ ovat t√§rkeit√§: haluat k√§yr√§n, joka nopeasti nousee ja menee linjan yli. Meid√§n tapauksessamme on aluksi v√§√§ri√§ positiivisia, ja sitten linja nousee ja menee kunnolla yli:

![ROC](../../../../2-Regression/4-Logistic/images/ROC_2.png)

Lopuksi, k√§yt√§ Scikit-learnin [`roc_auc_score` APIa](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) laskeaksesi todellinen 'Area Under the Curve' (AUC):

```python
auc = roc_auc_score(y_test,y_scores[:,1])
print(auc)
```
Tulos on `0.9749908725812341`. Koska AUC vaihtelee v√§lill√§ 0‚Äì1, haluat korkean pistem√§√§r√§n, sill√§ malli, joka on 100 % oikeassa ennusteissaan, saa AUC-arvon 1; t√§ss√§ tapauksessa malli on _melko hyv√§_.

Tulevissa luokitteluun liittyviss√§ oppitunneissa opit, kuinka voit parantaa mallisi pisteit√§ iteratiivisesti. Mutta nyt, onneksi olkoon! Olet suorittanut n√§m√§ regressio-oppitunnit!

---
## üöÄHaaste

Logistiseen regressioon liittyy paljon enemm√§n! Mutta paras tapa oppia on kokeilla. Etsi datasetti, joka sopii t√§m√§n tyyppiseen analyysiin, ja rakenna malli sen avulla. Mit√§ opit? vinkki: kokeile [Kagglea](https://www.kaggle.com/search?q=logistic+regression+datasets) l√∂yt√§√§ksesi mielenkiintoisia datasettej√§.

## [Luennon j√§lkeinen kysely](https://ff-quizzes.netlify.app/en/ml/)

## Kertaus & Itseopiskelu

Lue [t√§m√§n Stanfordin artikkelin](https://web.stanford.edu/~jurafsky/slp3/5.pdf) ensimm√§iset sivut logistisen regression k√§yt√§nn√∂n sovelluksista. Mieti teht√§vi√§, jotka sopivat paremmin jommallekummalle regressiotyypille, joita olemme t√§h√§n menness√§ opiskelleet. Mik√§ toimisi parhaiten?

## Teht√§v√§

[Uudelleen yritt√§minen t√§ss√§ regressiossa](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.