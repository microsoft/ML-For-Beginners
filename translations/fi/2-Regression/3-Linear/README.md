<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "40e64f004f3cb50aa1d8661672d3cd92",
  "translation_date": "2025-09-04T23:25:03+00:00",
  "source_file": "2-Regression/3-Linear/README.md",
  "language_code": "fi"
}
-->
# Rakenna regressiomalli Scikit-learnilla: nelj√§ tapaa regressioon

![Lineaarinen vs polynominen regressio infografiikka](../../../../2-Regression/3-Linear/images/linear-polynomial.png)
> Infografiikka: [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Ennakkokysely](https://ff-quizzes.netlify.app/en/ml/)

> ### [T√§m√§ oppitunti on saatavilla my√∂s R-kielell√§!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Johdanto 

T√§h√§n menness√§ olet tutustunut regressioon k√§ytt√§en esimerkkidataa kurpitsan hinnoitteludatasta, jota k√§yt√§mme koko t√§m√§n oppitunnin ajan. Olet my√∂s visualisoinut dataa Matplotlibin avulla.

Nyt olet valmis sukeltamaan syvemm√§lle koneoppimisen regressioon. Vaikka visualisointi auttaa ymm√§rt√§m√§√§n dataa, koneoppimisen todellinen voima tulee _mallien kouluttamisesta_. Mallit koulutetaan historiallisella datalla, jotta ne voivat automaattisesti tunnistaa datan riippuvuuksia, ja niiden avulla voidaan ennustaa tuloksia uudelle datalle, jota malli ei ole aiemmin n√§hnyt.

T√§ss√§ oppitunnissa opit lis√§√§ kahdesta regressiotyypist√§: _perus lineaarisesta regressiosta_ ja _polynomisesta regressiosta_, sek√§ n√§iden tekniikoiden taustalla olevasta matematiikasta. N√§iden mallien avulla voimme ennustaa kurpitsan hintoja eri sy√∂tt√∂datasta riippuen.

[![Koneoppimisen perusteet - Lineaarisen regression ymm√§rt√§minen](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "Koneoppimisen perusteet - Lineaarisen regression ymm√§rt√§minen")

> üé• Klikkaa yll√§ olevaa kuvaa lyhytt√§ videota varten lineaarisesta regressiosta.

> T√§ss√§ oppimateriaalissa oletamme vain v√§h√§ist√§ matematiikan osaamista ja pyrimme tekem√§√§n sen helposti l√§hestytt√§v√§ksi opiskelijoille, jotka tulevat muilta aloilta. Huomioi muistiinpanot, üßÆ matemaattiset esimerkit, diagrammit ja muut oppimisty√∂kalut, jotka auttavat ymm√§rt√§misess√§.

### Esitiedot

Sinun tulisi nyt olla perehtynyt kurpitsadatan rakenteeseen, jota tutkimme. L√∂yd√§t sen esiladattuna ja esipuhdistettuna t√§m√§n oppitunnin _notebook.ipynb_-tiedostosta. Tiedostossa kurpitsan hinta on esitetty per bushel uudessa dataframessa. Varmista, ett√§ voit ajaa n√§m√§ notebookit Visual Studio Coden kernelleiss√§.

### Valmistelu

Muistutuksena, lataat t√§t√§ dataa voidaksesi esitt√§√§ kysymyksi√§ siit√§. 

- Milloin on paras aika ostaa kurpitsoja? 
- Mink√§ hinnan voin odottaa miniatyyrikurpitsojen laatikolle?
- Pit√§isik√∂ minun ostaa ne puolibushelin koreissa vai 1 1/9 bushelin laatikoissa?
Jatketaan datan tutkimista.

Edellisess√§ oppitunnissa loit Pandas-dataframen ja t√§ytit sen osalla alkuper√§ist√§ datasetti√§, standardisoiden hinnoittelun bushelin mukaan. T√§ll√§ tavalla pystyit kuitenkin ker√§√§m√§√§n vain noin 400 datapistett√§ ja vain syksyn kuukausilta.

Tutustu dataan, joka on esiladattu t√§m√§n oppitunnin mukana tulevassa notebookissa. Data on esiladattu ja alkuper√§inen hajontakaavio on piirretty n√§ytt√§m√§√§n kuukausidataa. Ehk√§ voimme saada hieman enemm√§n yksityiskohtia datan luonteesta puhdistamalla sit√§ lis√§√§.

## Lineaarinen regressioviiva

Kuten opit oppitunnilla 1, lineaarisen regressioharjoituksen tavoitteena on pysty√§ piirt√§m√§√§n viiva, joka:

- **N√§ytt√§√§ muuttujien suhteet**. N√§ytt√§√§ muuttujien v√§lisen suhteen
- **Tekee ennusteita**. Tekee tarkkoja ennusteita siit√§, mihin uusi datapiste sijoittuisi suhteessa viivaan. 
 
On tyypillist√§ k√§ytt√§√§ **Least-Squares Regression** -menetelm√§√§ t√§m√§n tyyppisen viivan piirt√§miseen. Termi 'least-squares' tarkoittaa, ett√§ kaikki regressioviivan ymp√§rill√§ olevat datapisteet neli√∂id√§√§n ja sitten summataan. Ihanteellisesti lopullinen summa on mahdollisimman pieni, koska haluamme v√§h√§n virheit√§ eli `least-squares`.

Teemme n√§in, koska haluamme mallintaa viivan, jolla on pienin kumulatiivinen et√§isyys kaikista datapisteist√§mme. Neli√∂imme termit ennen niiden yhteenlaskemista, koska olemme kiinnostuneita niiden suuruudesta, emmek√§ suunnasta.

> **üßÆ N√§yt√§ matematiikka** 
> 
> T√§m√§ viiva, jota kutsutaan _parhaiten sopivaksi viivaksi_, voidaan ilmaista [yht√§l√∂ll√§](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` on 'selitt√§v√§ muuttuja'. `Y` on 'riippuva muuttuja'. Viivan kulmakerroin on `b` ja `a` on y-akselin leikkauspiste, joka viittaa `Y`:n arvoon, kun `X = 0`. 
>
>![kulmakertoimen laskeminen](../../../../2-Regression/3-Linear/images/slope.png)
>
> Ensin lasketaan kulmakerroin `b`. Infografiikka: [Jen Looper](https://twitter.com/jenlooper)
>
> Toisin sanoen, viitaten alkuper√§iseen kurpitsadataan liittyv√§√§n kysymykseen: "ennusta kurpitsan hinta per bushel kuukauden mukaan", `X` viittaa hintaan ja `Y` viittaa myyntikuukauteen. 
>
>![yht√§l√∂n t√§ydent√§minen](../../../../2-Regression/3-Linear/images/calculation.png)
>
> Laske `Y`:n arvo. Jos maksat noin $4, sen t√§ytyy olla huhtikuu! Infografiikka: [Jen Looper](https://twitter.com/jenlooper)
>
> Matematiikka, joka laskee viivan, t√§ytyy osoittaa viivan kulmakerroin, joka riippuu my√∂s leikkauspisteest√§, eli siit√§, miss√§ `Y` sijaitsee, kun `X = 0`.
>
> Voit tarkastella n√§iden arvojen laskentamenetelm√§√§ [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) -sivustolla. K√§y my√∂s [Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) -sivustolla n√§hd√§ksesi, miten lukuarvot vaikuttavat viivaan.

## Korrelaatio

Yksi termi, joka on hyv√§ ymm√§rt√§√§, on **korrelaatiokerroin** annettujen X- ja Y-muuttujien v√§lill√§. Hajontakaavion avulla voit nopeasti visualisoida t√§m√§n kertoimen. Kaavio, jossa datapisteet ovat siistiss√§ linjassa, omaa korkean korrelaation, mutta kaavio, jossa datapisteet ovat hajallaan X:n ja Y:n v√§lill√§, omaa matalan korrelaation.

Hyv√§ lineaarinen regressiomalli on sellainen, jolla on korkea (l√§hemp√§n√§ 1 kuin 0) korrelaatiokerroin k√§ytt√§en Least-Squares Regression -menetelm√§√§ ja regressioviivaa.

‚úÖ Aja t√§m√§n oppitunnin mukana tuleva notebook ja katso Kuukausi-Hinta hajontakaaviota. Vaikuttaako data, joka yhdist√§√§ Kuukauden ja Hinnan kurpitsamyynniss√§, olevan korkea vai matala korrelaatio visuaalisen tulkintasi mukaan hajontakaaviosta? Muuttuuko t√§m√§, jos k√§yt√§t tarkempaa mittaa kuin `Kuukausi`, esim. *vuoden p√§iv√§* (eli p√§ivien lukum√§√§r√§ vuoden alusta)?

Alla olevassa koodissa oletamme, ett√§ olemme puhdistaneet datan ja saaneet dataframen nimelt√§ `new_pumpkins`, joka n√§ytt√§√§ seuraavalta:

ID | Kuukausi | VuodenP√§iv√§ | Lajike | Kaupunki | Pakkaus | Alin Hinta | Korkein Hinta | Hinta
---|----------|-------------|--------|----------|---------|------------|---------------|------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> Koodi datan puhdistamiseen l√∂ytyy tiedostosta [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). Olemme suorittaneet samat puhdistusvaiheet kuin edellisess√§ oppitunnissa ja laskeneet `VuodenP√§iv√§`-sarakkeen seuraavalla lausekkeella:

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Nyt kun ymm√§rr√§t lineaarisen regression taustalla olevan matematiikan, luodaan regressiomalli n√§hd√§ksesi, voimmeko ennustaa, mik√§ kurpitsapaketti tarjoaa parhaat hinnat. Joku, joka ostaa kurpitsoja juhlap√§iv√§n kurpitsapellolle, saattaa haluta t√§t√§ tietoa voidakseen optimoida kurpitsapakettien ostot pellolle.

## Korrelaation etsiminen

[![Koneoppimisen perusteet - Korrelaation etsiminen: Lineaarisen regression avain](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "Koneoppimisen perusteet - Korrelaation etsiminen: Lineaarisen regression avain")

> üé• Klikkaa yll√§ olevaa kuvaa lyhytt√§ videota varten korrelaatiosta.

Edellisest√§ oppitunnista olet todenn√§k√∂isesti n√§hnyt, ett√§ keskim√§√§r√§inen hinta eri kuukausina n√§ytt√§√§ t√§lt√§:

<img alt="Keskim√§√§r√§inen hinta kuukauden mukaan" src="../2-Data/images/barchart.png" width="50%"/>

T√§m√§ viittaa siihen, ett√§ korrelaatiota saattaa olla, ja voimme yritt√§√§ kouluttaa lineaarisen regressiomallin ennustamaan suhdetta `Kuukausi` ja `Hinta` v√§lill√§ tai `VuodenP√§iv√§` ja `Hinta` v√§lill√§. T√§ss√§ on hajontakaavio, joka n√§ytt√§√§ j√§lkimm√§isen suhteen:

<img alt="Hajontakaavio Hinta vs. Vuoden P√§iv√§" src="images/scatter-dayofyear.png" width="50%" /> 

Katsotaan, onko korrelaatiota k√§ytt√§m√§ll√§ `corr`-funktiota:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

N√§ytt√§√§ silt√§, ett√§ korrelaatio on melko pieni, -0.15 `Kuukauden` mukaan ja -0.17 `VuodenP√§iv√§n` mukaan, mutta saattaa olla toinen t√§rke√§ suhde. N√§ytt√§√§ silt√§, ett√§ eri kurpitsalajikkeiden hinnat muodostavat erilaisia klustereita. Vahvistaaksemme t√§m√§n hypoteesin, piirret√§√§n jokainen kurpitsakategoria eri v√§rill√§. K√§ytt√§m√§ll√§ `ax`-parametria `scatter`-piirtofunktiossa voimme piirt√§√§ kaikki pisteet samaan kaavioon:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Hajontakaavio Hinta vs. Vuoden P√§iv√§" src="images/scatter-dayofyear-color.png" width="50%" /> 

Tutkimuksemme viittaa siihen, ett√§ lajikkeella on suurempi vaikutus kokonaishintaan kuin varsinaisella myyntip√§iv√§ll√§. Voimme n√§hd√§ t√§m√§n pylv√§sdiagrammilla:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Pylv√§sdiagrammi hinta vs lajike" src="images/price-by-variety.png" width="50%" /> 

Keskityt√§√§n hetkeksi vain yhteen kurpitsalajikkeeseen, 'pie type', ja katsotaan, mit√§ vaikutusta p√§iv√§m√§√§r√§ll√§ on hintaan:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Hajontakaavio Hinta vs. Vuoden P√§iv√§" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Jos nyt laskemme korrelaation `Hinta` ja `VuodenP√§iv√§` v√§lill√§ k√§ytt√§en `corr`-funktiota, saamme jotain kuten `-0.27` - mik√§ tarkoittaa, ett√§ ennustavan mallin kouluttaminen on j√§rkev√§√§.

> Ennen lineaarisen regressiomallin kouluttamista on t√§rke√§√§ varmistaa, ett√§ datamme on puhdasta. Lineaarinen regressio ei toimi hyvin puuttuvien arvojen kanssa, joten on j√§rkev√§√§ poistaa kaikki tyhj√§t solut:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Toinen l√§hestymistapa olisi t√§ytt√§√§ tyhj√§t arvot vastaavan sarakkeen keskiarvoilla.

## Yksinkertainen lineaarinen regressio

[![Koneoppimisen perusteet - Lineaarinen ja polynominen regressio Scikit-learnilla](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "Koneoppimisen perusteet - Lineaarinen ja polynominen regressio Scikit-learnilla")

> üé• Klikkaa yll√§ olevaa kuvaa lyhytt√§ videota varten lineaarisesta ja polynomisesta regressiosta.

Lineaarisen regressiomallin kouluttamiseen k√§yt√§mme **Scikit-learn**-kirjastoa.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Aloitamme erottamalla sy√∂tt√∂arvot (ominaisuudet) ja odotetut tulokset (label) erillisiin numpy-taulukoihin:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Huomaa, ett√§ meid√§n t√§ytyi suorittaa `reshape` sy√∂tt√∂datalle, jotta lineaarisen regression paketti ymm√§rt√§isi sen oikein. Lineaarinen regressio odottaa 2D-taulukkoa sy√∂tteen√§, jossa taulukon jokainen rivi vastaa sy√∂tt√∂ominaisuuksien vektoria. Meid√§n tapauksessamme, koska meill√§ on vain yksi sy√∂te, tarvitsemme taulukon, jonka muoto on N√ó1, miss√§ N on datasetin koko.

Seuraavaksi meid√§n t√§ytyy jakaa data koulutus- ja testidatasettiin, jotta voimme validoida mallimme koulutuksen j√§lkeen:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Lopuksi varsinaisen lineaarisen regressiomallin kouluttaminen vie vain kaksi koodirivi√§. M√§√§rittelemme `LinearRegression`-objektin ja sovitamme sen dataamme k√§ytt√§m√§ll√§ `fit`-metodia:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

`LinearRegression`-objekti sis√§lt√§√§ `fit`-vaiheen j√§lkeen kaikki regression kertoimet, jotka voidaan hakea `.coef_`-ominaisuuden avulla. Meid√§n tapauksessamme on vain yksi kerroin, jonka pit√§isi olla noin `-0.017`. T√§m√§ tarkoittaa, ett√§ hinnat n√§ytt√§v√§t laskevan hieman ajan my√∂t√§, mutta eiv√§t kovin paljon, noin 2 sentti√§ p√§iv√§ss√§. Voimme my√∂s hakea regressioviivan y-akselin leikkauspisteen `lin_reg.intercept_`-ominaisuuden avulla - se on noin `21` meid√§n tapauksessamme, mik√§ osoittaa hinnan vuoden alussa.

Mallimme tarkkuuden n√§kemiseksi voimme ennustaa hinnat testidatasetilla ja mitata, kuinka l√§hell√§ ennusteemme ovat odotettuja arvoja. T√§m√§ voidaan tehd√§ k√§ytt√§m√§ll√§ keskim√§√§r√§isen neli√∂virheen (MSE) mittaria, joka on kaikkien odotettujen ja ennustettujen arvojen neli√∂ityjen erojen keskiarvo.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```
Virheemme n√§ytt√§√§ olevan noin 2 pisteen kohdalla, mik√§ on ~17 %. Ei kovin hyv√§. Toinen indikaattori mallin laadusta on **determinointikerroin**, joka voidaan laskea n√§in:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Jos arvo on 0, se tarkoittaa, ett√§ malli ei ota sy√∂tt√∂tietoja huomioon ja toimii *huonoimpana lineaarisena ennustajana*, joka on yksinkertaisesti tuloksen keskiarvo. Arvo 1 tarkoittaa, ett√§ voimme t√§ydellisesti ennustaa kaikki odotetut tulokset. Meid√§n tapauksessamme kerroin on noin 0.06, mik√§ on melko alhainen.

Voimme my√∂s piirt√§√§ testidatan yhdess√§ regressioviivan kanssa, jotta n√§emme paremmin, miten regressio toimii meid√§n tapauksessamme:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Lineaarinen regressio" src="images/linear-results.png" width="50%" />

## Polynominen regressio

Toinen lineaarisen regression tyyppi on polynominen regressio. Vaikka joskus muuttujien v√§lill√§ on lineaarinen suhde ‚Äì mit√§ suurempi kurpitsa tilavuudeltaan, sit√§ korkeampi hinta ‚Äì joskus n√§it√§ suhteita ei voida kuvata tasolla tai suoralla viivalla.

‚úÖ T√§ss√§ on [joitakin esimerkkej√§](https://online.stat.psu.edu/stat501/lesson/9/9.8) datasta, joka voisi hy√∂dynt√§√§ polynomista regressiota.

Katso uudelleen suhdetta P√§iv√§m√§√§r√§n ja Hinnan v√§lill√§. Vaikuttaako t√§m√§ hajontakuvio silt√§, ett√§ sit√§ pit√§isi v√§ltt√§m√§tt√§ analysoida suoralla viivalla? Eiv√§tk√∂ hinnat voi vaihdella? T√§ss√§ tapauksessa voit kokeilla polynomista regressiota.

‚úÖ Polynomit ovat matemaattisia lausekkeita, jotka voivat koostua yhdest√§ tai useammasta muuttujasta ja kertoimesta.

Polynominen regressio luo kaarevan viivan, joka sopii paremmin ep√§lineaariseen dataan. Meid√§n tapauksessamme, jos sis√§llyt√§mme neli√∂idyn `DayOfYear`-muuttujan sy√∂tt√∂tietoihin, meid√§n pit√§isi pysty√§ sovittamaan datamme parabolisella k√§yr√§ll√§, jolla on minimi tiettyn√§ ajankohtana vuoden aikana.

Scikit-learn sis√§lt√§√§ hy√∂dyllisen [pipeline-rajapinnan](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline), joka yhdist√§√§ eri datank√§sittelyvaiheet yhteen. **Pipeline** on ketju **estimaattoreita**. Meid√§n tapauksessamme luomme pipelinen, joka ensin lis√§√§ polynomisia ominaisuuksia malliin ja sitten kouluttaa regression:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

K√§ytt√§m√§ll√§ `PolynomialFeatures(2)` tarkoittaa, ett√§ sis√§llyt√§mme kaikki toisen asteen polynomit sy√∂tt√∂tiedoista. Meid√§n tapauksessamme t√§m√§ tarkoittaa vain `DayOfYear`<sup>2</sup>, mutta kahden sy√∂tt√∂muuttujan X ja Y tapauksessa t√§m√§ lis√§√§ X<sup>2</sup>, XY ja Y<sup>2</sup>. Voimme my√∂s k√§ytt√§√§ korkeamman asteen polynomeja, jos haluamme.

Pipelinea voidaan k√§ytt√§√§ samalla tavalla kuin alkuper√§ist√§ `LinearRegression`-objektia, eli voimme `fit` pipelinea ja sitten k√§ytt√§√§ `predict` saadaksemme ennustetulokset. T√§ss√§ on graafi, joka n√§ytt√§√§ testidatan ja approksimaatiok√§yr√§n:

<img alt="Polynominen regressio" src="images/poly-results.png" width="50%" />

Polynomista regressiota k√§ytt√§m√§ll√§ voimme saada hieman pienemm√§n MSE:n ja korkeamman determinointikertoimen, mutta ei merkitt√§v√§sti. Meid√§n t√§ytyy ottaa huomioon muita ominaisuuksia!

> Voit n√§hd√§, ett√§ kurpitsan hinnat ovat alhaisimmillaan jossain Halloweenin tienoilla. Miten selitt√§isit t√§m√§n?

üéÉ Onnittelut, loit juuri mallin, joka voi auttaa ennustamaan piirakkakurpitsojen hinnan. Voit todenn√§k√∂isesti toistaa saman prosessin kaikille kurpitsatyypeille, mutta se olisi ty√∂l√§st√§. Opitaan nyt, miten ottaa kurpitsan lajike huomioon mallissamme!

## Kategoriset ominaisuudet

Ihanteellisessa maailmassa haluaisimme pysty√§ ennustamaan hinnat eri kurpitsalajikkeille k√§ytt√§m√§ll√§ samaa mallia. Kuitenkin `Variety`-sarake on hieman erilainen kuin sarakkeet kuten `Month`, koska se sis√§lt√§√§ ei-numeerisia arvoja. T√§llaisia sarakkeita kutsutaan **kategorisiksi**.

[![ML aloittelijoille - Kategoristen ominaisuuksien ennustaminen lineaarisella regressiolla](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML aloittelijoille - Kategoristen ominaisuuksien ennustaminen lineaarisella regressiolla")

> üé• Klikkaa yll√§ olevaa kuvaa saadaksesi lyhyen videokatsauksen kategoristen ominaisuuksien k√§yt√∂st√§.

T√§ss√§ n√§et, miten keskim√§√§r√§inen hinta riippuu lajikkeesta:

<img alt="Keskim√§√§r√§inen hinta lajikkeen mukaan" src="images/price-by-variety.png" width="50%" />

Jotta voimme ottaa lajikkeen huomioon, meid√§n t√§ytyy ensin muuntaa se numeeriseen muotoon eli **koodata** se. On olemassa useita tapoja tehd√§ t√§m√§:

* Yksinkertainen **numeerinen koodaus** rakentaa taulukon eri lajikkeista ja korvaa lajikenimen taulukon indeksill√§. T√§m√§ ei ole paras idea lineaariselle regressiolle, koska lineaarinen regressio k√§ytt√§√§ indeksin todellista numeerista arvoa ja lis√§√§ sen tulokseen, kertomalla sen jollain kertoimella. Meid√§n tapauksessamme indeksin numeron ja hinnan v√§linen suhde on selv√§sti ep√§lineaarinen, vaikka varmistaisimme, ett√§ indeksit ovat j√§rjestetty jollain erityisell√§ tavalla.
* **One-hot-koodaus** korvaa `Variety`-sarakkeen nelj√§ll√§ eri sarakkeella, yksi kullekin lajikkeelle. Jokainen sarake sis√§lt√§√§ `1`, jos vastaava rivi on tietty√§ lajiketta, ja `0` muuten. T√§m√§ tarkoittaa, ett√§ lineaarisessa regressiossa on nelj√§ kerrointa, yksi kullekin kurpitsalajikkeelle, jotka vastaavat kyseisen lajikkeen "aloitushintaa" (tai pikemminkin "lis√§hintaa").

Alla oleva koodi n√§ytt√§√§, miten voimme tehd√§ one-hot-koodauksen lajikkeelle:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Jotta voimme kouluttaa lineaarisen regression k√§ytt√§en one-hot-koodattua lajiketta sy√∂tteen√§, meid√§n t√§ytyy vain alustaa `X` ja `y` data oikein:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Loppu koodi on sama kuin mit√§ k√§ytimme aiemmin kouluttaaksemme lineaarisen regression. Jos kokeilet sit√§, huomaat, ett√§ keskim√§√§r√§inen neli√∂virhe (MSE) on suunnilleen sama, mutta saamme paljon korkeamman determinointikertoimen (~77 %). Jotta ennusteet olisivat viel√§ tarkempia, voimme ottaa huomioon enemm√§n kategorisia ominaisuuksia sek√§ numeerisia ominaisuuksia, kuten `Month` tai `DayOfYear`. Jotta saamme yhden suuren ominaisuusjoukon, voimme k√§ytt√§√§ `join`-toimintoa:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

T√§ss√§ otamme my√∂s huomioon `City` ja `Package`-tyypin, mik√§ antaa meille MSE:n 2.84 (10 %) ja determinointikertoimen 0.94!

## Yhdist√§minen

Parhaan mallin luomiseksi voimme k√§ytt√§√§ yhdistetty√§ (one-hot-koodattua kategorista + numeerista) dataa yll√§ olevasta esimerkist√§ yhdess√§ polynomisen regression kanssa. T√§ss√§ on t√§ydellinen koodi k√§tev√§ksi viitteeksi:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

T√§m√§n pit√§isi antaa meille paras determinointikerroin, l√§hes 97 %, ja MSE=2.23 (~8 % ennustevirhe).

| Malli | MSE | Determinointi |
|-------|-----|---------------|
| `DayOfYear` Lineaarinen | 2.77 (17.2 %) | 0.07 |
| `DayOfYear` Polynominen | 2.73 (17.0 %) | 0.08 |
| `Variety` Lineaarinen | 5.24 (19.7 %) | 0.77 |
| Kaikki ominaisuudet Lineaarinen | 2.84 (10.5 %) | 0.94 |
| Kaikki ominaisuudet Polynominen | 2.23 (8.25 %) | 0.97 |

üèÜ Hyvin tehty! Loit nelj√§ regressiomallia yhdess√§ oppitunnissa ja paransit mallin laatua 97 %:iin. Regressiota k√§sittelev√§n osion viimeisess√§ osassa opit logistisesta regressiosta kategorioiden m√§√§ritt√§miseksi.

---
## üöÄHaaste

Testaa useita eri muuttujia t√§ss√§ muistikirjassa ja katso, miten korrelaatio vastaa mallin tarkkuutta.

## [Luennon j√§lkeinen kysely](https://ff-quizzes.netlify.app/en/ml/)

## Kertaus ja itseopiskelu

T√§ss√§ oppitunnissa opimme lineaarisesta regressiosta. On olemassa muita t√§rkeit√§ regressiotyyppej√§. Lue Stepwise-, Ridge-, Lasso- ja Elasticnet-tekniikoista. Hyv√§ kurssi lis√§opiskeluun on [Stanfordin tilastollisen oppimisen kurssi](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Teht√§v√§

[Rakenna malli](assignment.md)

---

**Vastuuvapauslauseke**:  
T√§m√§ asiakirja on k√§√§nnetty k√§ytt√§m√§ll√§ teko√§lypohjaista k√§√§nn√∂spalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, ett√§ automaattiset k√§√§nn√∂kset voivat sis√§lt√§√§ virheit√§ tai ep√§tarkkuuksia. Alkuper√§inen asiakirja sen alkuper√§isell√§ kielell√§ tulisi pit√§√§ ensisijaisena l√§hteen√§. Kriittisen tiedon osalta suositellaan ammattimaista ihmisk√§√§nn√∂st√§. Emme ole vastuussa v√§√§rink√§sityksist√§ tai virhetulkinnoista, jotka johtuvat t√§m√§n k√§√§nn√∂ksen k√§yt√∂st√§.