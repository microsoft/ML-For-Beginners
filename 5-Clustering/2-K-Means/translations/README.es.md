# Agrupamiento K-Medias

[![Andrew Ng explica el agrupamiento](https://img.youtube.com/vi/hDmNF9JG3lo/0.jpg)](https://youtu.be/hDmNF9JG3lo "Andrew Ng explica el agrupamiento")

> üé• Haz clic en la imagen de arriba para ver el video: Andrew Ng explica el agrupamiento"

## [Examen previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29?loc=es)

En esta lecci√≥n, aprender√°s c√≥mo crear grupos usando Scikit-learn y el conjunto de datos de m√∫sica Nigeriana que importaste anteriormente. Cubriremos los conceptos b√°sicos de K-Medias para agrupamiento. Ten en mente que, como aprendiste en lecciones anteriores, hay muchas formas de de trabajar con grupos y el m√©todo que uses depende de tus datos. Probaremos K-medias ya que es la t√©cnica de agrupamiento m√°s com√∫n. ¬°Comencemos!

T√©rminos que sobre los que aprender√°s:

- Puntaje de silueta
- M√©todo del codo
- Inercia
- Varianza

## Introducci√≥n

[El agrupamiento K-medias](https://wikipedia.org/wiki/K-means_clustering) es un m√©todo derivado del dominio del procesamiento de se√±ales. Se usa para dividir y particionar grupos de datos en 'k' grupos usando una serie de observaciones. Cada observaci√≥n funciona para agrupar un punto de datos m√°s cercano a su 'media' m√°s cercana, o el punto central de un grupo.

Los grupos pueden ser visualizados como [diagramas Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), los cuales incluye un punto (o 'semilla') y su regi√≥n correspondiente.

![diagrama Voronoi](../images/voronoi.png)

> Infograf√≠a de [Jen Looper](https://twitter.com/jenlooper)

El proceso de agrupamiento K-medias [se ejecuta en un proceso de tres pasos](https://scikit-learn.org/stable/modules/clustering.html#k-means):

1. El algoritmo selecciona el k-n√∫mero de puntos centrales al hacer muestreo del conjunto de datos. Despu√©s de esto, se repite:
    1. Se asigna cada muestra al centroide m√°s cercano.
    2. Se crean nuevos centroides al tomar el valor medio de todas las muestras asignadas a los centroides previos.
    3. Luego, se calcula la diferencia entre los centroides nuevos y viejos y se repite hasta que los centroides se estabilizan.

Un inconveniente de usar K-medias incluye el hecho que necesitar√°s establecer 'k', que es el n√∫mero de centroides. Afortunadamente el 'm√©todo del codo' ayuda a estimar un buen valor inicial para 'k'. Lo probar√°s en un minuto.

## Prerrequisitos

Trabajar√°s en el archivo _notebook.ipynb_ de esta lecci√≥n, que incluye la importaci√≥n de datos y limpieza preliminar que hiciste en la √∫ltima lecci√≥n.

## Ejercicio - preparaci√≥n

Comienza por darle otro vistazo a los datos de canciones.

1. Crea un gr√°fico de caja, llamando a `boxplot()` para cada columna:

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Estos datos son un poco ruidosos: al observar cada columna como un gr√°fico de caja, puedes ver los valores at√≠picos.

    ![Valores at√≠picos](../images/boxplots.png)

Podr√≠as revisar el conjunto de datos y remover estos valores at√≠picos, pero eso har√≠a que quedara un m√≠nimo de datos.

1. Por ahora, elege qu√© columnas usar√°s para tu ejercicio de agrupamiento. Elige unas con rangos similares y codifica la columna `artist_top_genre` como datos num√©ricos:

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Ahora necesitas elegir a cu√°ntos grupos apuntar. Sabes que hay 3 g√©neros de canciones que extrajimos de el conjunto de datos, as√≠ que probemos con 3:

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Ves un arreglo impreso con los grupos predichos (0, 1, 0 2) para cada fila del dataframe.

1. Usa este arreglo para calcular una 'puntaje de silueta':

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Puntaje de silueta

Busca un puntaje de silueta m√°s cercano a 1. Este puntaje var√≠a de -1 a 1, y si el puntaje es 1, el grupo es denso y bien separado de otros grupos. Un valor cercano a 0 representa grupos superpuestos con muestras muy cercanas al l√≠mite de decisi√≥n de los grupos vecinos. [Fuente](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam).

Nuestro puntaje es de **.53**, justo a la mitad. Esto indica que nuestros datos no son particularmente adecuados para este tipo de agrupamiento, pero continuemos.

### Ejercicio - construye un modelo

1. Importa `KMeans` e inicia el proceso de agrupamiento.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Hay algunas partes que requieren explicaci√≥n.

    > üéì range: Estas son las iteraciones del proceso de agrupamiento

    > üéì random_state: "Determina la generaci√≥n de n√∫meros aleatorios para la inicializaci√≥n del centroide." [Fuente](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS: "within-cluster sums of squares (suma de cuadrados dentro del grupo)" mide la distancia cuadr√°tica promedio de todos los puntos dentro de un grupo al centroide dle grupo. [Fuente](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce).

    > üéì Inertia: Los algoritmos K-medias intentan elegir los centroides para minimizar la 'inertia (inercia)', "una medida de cu√°nta coherencia interna  tienen los grupos." [Fuente](https://scikit-learn.org/stable/modules/clustering.html). El valor se agrega a la variable wcss en cada iteraci√≥n.

    > üéì k-means++: En [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) puedes usar la optimizaci√≥n 'k-means++', la cual "inicializa los centroides para que sean (generalmente) distantes uno de otro, llevando a probablemente mejores resultados que la inicializaci√≥n aleatoria".

### M√©todo del codo

Anteriormente, supusiste que, porque has apuntado a 3 g√©neros de canciones, deber√≠as elegir 3 grupos. ¬øPero es el caso?

1. Usa el 'm√©todo del codo' para asegurarte.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(range(1, 11), wcss,marker='o',color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Usa la variable `wcss` que construiste en el paso anterior para crear una gr√°fica que muestre d√≥nde se est√° 'la curva' en el codo, la cual indica el n√∫mero √≥ptimo de grupos. ¬°Quiz√° **es** 3!

    ![M√©todo del codo](../images/elbow.png)

## Ejercicio - muestra los grupos

1. Prueba el proceso de nuevo, esta vez configurando 3 grupos, y muestra los grupos como un gr√°fico de dispersi√≥n:

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. Revisa la precisi√≥n del modelo:

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    La precisi√≥n de este modelo no es tan buena, y la forma de los grupos te dar√° una pista del por qu√©.

    ![Grupos](../images/clusters.png)

    Estos datos est√°n demasiado desequilibrados, muy poco correlacionados y tienen demasiada varianza entre los valores de columna para agrupar bien. De hecho, los grupos que forman est√°n probablemente fuertemente influenciados o sesgados por las tres categor√≠as de g√©neros que definimos arriba. ¬°Eso fue un proceso de aprendizaje!

    En la documentaci√≥n de Scikit-learn, puedes ver que un modelo como este, con grupos no muy bien demarcados, tienen un problema de 'varianza':

    ![Modelos de problemas](../images/problems.png)
    > Infograf√≠a de Scikit-learn

## Varianza

La varianza se define como "ep promedio de diferencias cuadr√°ticas de la media". [Fuente](https://www.mathsisfun.com/data/standard-deviation.html). En el contexto de este problema de agrupamiento, se refiere a los datos en los que los n√∫meros de nuestro conjunto de datos tienden a divergir demasiado de la media.

‚úÖ Este es un buen momento para pensar acerca de todas las formas en que podr√≠as corregir este problema. ¬øModificar los datos un poco m√°s? 'Usar columnas distintas? ¬øUsar un algoritmo diferente? Intenta [escalando tus datos](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) para normalizarlos y probar otras columnas.

> Prueba esta '[calculadora de varianza](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' para entender un poca m√°s el concepto.

---

## üöÄDesaf√≠o

Dedica algo de tiempo a este notebook, ajustando los par√°metros. ¬øPuedes mejorar la precisi√≥n del modelo al limpiar m√°s los datos (eliminando valores at√≠picos, por ejemplo)? Puedes usar pesos para dar mayor ponderaci√≥n a las muestras de datos proporcionadas. ¬øQu√© m√°s puedes hacer para crear mejores grupos?

Pista: Prueba escalar tus datos. Hay c√≥digo comentado en el notebook que agrega escalado est√°ndar para hacer que las columnas de datos se parezcan m√°s entre s√≠ en t√©rminos de rango. Encontrar√°s que mientras el puntaje de silueta disminuye el 'pliegue' en la gr√°fica de codo se suaviza. Esto es por qu√© al dejar los datos sin escalar le permite a los datos con menos variaci√≥n tengan m√°s peso. Lee un poco m√°s de este problema [aqu√≠](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Examen posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30?loc=es)

## Revisi√≥n y auto-estudio

Da un vistazo a un simulador K-Medias [como este](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Puedes usar esta herramienta para visualizar puntos de datos de muestra y determina sus centroides. Puedes editar la aleatoriedad de los datos, el n√∫mero de grupos y el n√∫mero de centroides. ¬øEsto te ayuda para tener una idea de c√≥mo se pueden agrupar los datos?

Tambi√©n, da un vistazo a [este folleto de K-Medias](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de Stanford.

## Asignaci√≥n

[Prueba distintos m√©todos de agrupamiento](assignment.es.md)
