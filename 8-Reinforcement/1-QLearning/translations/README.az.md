# GÃ¼clÉ™ndirici Ã–yrÉ™nmÉ™ vÉ™ Q-Ã–yrÉ™nmÉ™sinÉ™ GiriÅŸ

![GÃ¼clÉ™ndirici Ã¶yrÉ™nmÉ™nin icmalÄ±nÄ±n eskizi](../../../sketchnotes/ml-reinforcement.png)
> [Tomomi Imura](https://www.twitter.com/girlie_mac) tÉ™rÉ™findÉ™n Ã§É™kilmiÅŸ eskiz

GÃ¼clÉ™ndirici Ã¶yrÉ™nmÉ™ Ã¼Ã§ mÃ¼hÃ¼m anlayÄ±ÅŸÄ± Ã¶zÃ¼ndÉ™ birlÉ™ÅŸdirir: agent, vÉ™ziyyÉ™tlÉ™r vÉ™ hÉ™r vÉ™ziyyÉ™t Ã¼Ã§Ã¼n icra yÄ±ÄŸÄ±nÄ±. MÃ¼É™yyÉ™n edilmiÅŸ vÉ™ziyyÉ™tdÉ™ hÉ™rÉ™kÉ™ti icra etmÉ™klÉ™ agentÉ™ mÃ¼kafat verilir. Super Mario oyununu yadÄ±nÄ±za salÄ±n. TÉ™sÉ™vvÃ¼r edin ki, siz Mariosunuz vÉ™ hansÄ±sa mÉ™rhÉ™lÉ™dÉ™ uÃ§urum kÉ™narÄ±nda dayanmÄ±sÄ±nÄ±z. Sizin Ã¼stÃ¼nÃ¼zdÉ™ isÉ™ qÉ™pik var. HansÄ±sa bir mÉ™rhÉ™lÉ™dÉ™ Mario olmaÄŸÄ±nÄ±z sizin vÉ™ziyyÉ™tinizdir. Bir addÄ±m ataraq saÄŸa hÉ™rÉ™kÉ™t etmÉ™k sizi uÃ§uruma aparacaq vÉ™ aÅŸaÄŸÄ± xal toplayacaqsÄ±nÄ±z. Amma atlamaq dÃ¼ymÉ™sinÉ™ basmaÄŸÄ±nÄ±z xal toplamanÄ±za vÉ™ saÄŸ qalmanÄ±za sÉ™bÉ™b olacaqdÄ±. Dediyimiz mÃ¼sbÉ™t nÉ™ticÉ™ olduÄŸuna gÃ¶rÉ™ sizÉ™ mÃ¼sbÉ™t xal verilmÉ™lidir.

Oyunda saÄŸ qalaraq vÉ™ mÃ¼mkÃ¼n olduÄŸu qÉ™dÉ™r yuxarÄ± xal toplamaqla mÃ¼kafatÄ± maksimuma Ã§atdÄ±rmaÄŸÄ± gÃ¼clÉ™ndirici Ã¶yrÉ™nmÉ™dÉ™n vÉ™ simulyator(oyun) istifadÉ™ edÉ™rÉ™k Ã¶yrÉ™nÉ™ bilÉ™rsiniz.

[![GÃ¼clÉ™ndirici Ã–yrÉ™nmÉ™yÉ™ GiriÅŸ](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)

> ğŸ¥ Dmitrinin GÃ¼clÉ™ndirici Ã–yrÉ™nmÉ™ barÉ™dÉ™ olan mÃ¼zakirÉ™sini dinlÉ™mÉ™k Ã¼Ã§Ã¼n yuxarÄ±dakÄ± ÅŸÉ™kilÉ™ kliklÉ™yin

## [MÃ¼hazirÉ™dÉ™n É™vvÉ™l test](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/?loc=az)

## Ä°lkin ÅŸÉ™rtlÉ™r vÉ™ quraÅŸdÄ±rma

Bu gÃ¼n Python-da bÉ™zi kodlarÄ± sÄ±naqdan keÃ§irÉ™cÉ™yik. DÉ™rsÉ™ aid olan Jupyter Notebook kodunu hÉ™m kompÃ¼terinizdÉ™, hÉ™m dÉ™ buludda iÅŸlÉ™dÉ™ bilmÉ™lisiniz.

Siz [dÉ™rs notbukunu](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) aÃ§a vÉ™ mÃ¼hiti qurmaq Ã¼Ã§Ã¼n bu dÉ™rsi izlÉ™yÉ™ bilÉ™rsiniz.

> **Qeyd:** Bu kodu buludda aÃ§Ä±rsÄ±nÄ±zsa, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) faylÄ±nÄ± da notbuk kodunda istifadÉ™ olunduÄŸuna gÃ¶rÉ™ yÃ¼klÉ™mÉ™yiniz lazÄ±mdÄ±r. Onu notbuk faylÄ± ilÉ™ eyni qovluÄŸa É™lavÉ™ edin.

## GiriÅŸ

Bu dÉ™rsdÉ™ biz rus bÉ™stÉ™karÄ± [Sergei Prokofyevin](https://en.wikipedia.org/wiki/Sergei_Prokofiev) musiqili naÄŸÄ±lÄ±ndan ilhamlanaraq **[Piter vÉ™ Canavar](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)** dÃ¼nyasÄ±nÄ± araÅŸdÄ±racaÄŸÄ±q. PeterÉ™ É™trafÄ±nÄ± araÅŸdÄ±rmaq, dadlÄ± alma toplamaq vÉ™ canavarla gÃ¶rÃ¼ÅŸmÉ™mÉ™k Ã¼Ã§Ã¼n **GÃ¼clÉ™ndirici Ã–yrÉ™nmÉ™dÉ™n** istifadÉ™ edÉ™cÉ™yik.

**GÃ¼clÉ™ndirici Ã–yrÉ™nmÉ™** (RL â€“ Reinforcement Learning) Ã§oxlu tÉ™crÃ¼bÉ™lÉ™r keÃ§irÉ™rÉ™k, **mÃ¼hit** daxilindÉ™ **agentin** optimal davranÄ±ÅŸÄ±nÄ± Ã¶yrÉ™nmÉ™yÉ™ imkan verÉ™n Ã¶yrÉ™nmÉ™ texnikasÄ±dÄ±r. Bu mÃ¼hitdÉ™ki agentin **mÃ¼kafat funksiyasÄ±** ilÉ™ mÃ¼É™yyÉ™n edilmiÅŸ **hÉ™dÉ™fi** olmalÄ±dÄ±r.

## MÃ¼hit

SadÉ™lik Ã¼Ã§Ã¼n hesab edÉ™k ki, Peterin dÃ¼nyasÄ± `en` x `hÃ¼ndÃ¼rlÃ¼k` Ã¶lÃ§Ã¼lÃ¼ kvadrat bir lÃ¶vhÉ™dir:

![Piterin MÃ¼hiti](../images/environment.png)

Bu lÃ¶vhÉ™dÉ™ki hÉ™r bir xana aÅŸaÄŸÄ±dakÄ±lardan biri ola bilÉ™r:

* **torpaq** â€“ Ã¼zÉ™rindÉ™ Piter vÉ™ digÉ™r canlÄ±lar gÉ™zÉ™ bilÉ™r.
* **su** â€“ Ã¼stÃ¼ndÉ™ gÉ™zÉ™ bilmÉ™yÉ™cÉ™ksiniz.
* **aÄŸac** vÉ™ ya **ot** â€“ istirahÉ™t edÉ™ bilÉ™cÉ™yiniz yer.
* **alma** â€“ Piterin Ã¶zÃ¼nÃ¼ qidalandÄ±rmaq Ã¼Ã§Ã¼n tapmaqdan mÉ™mnun qalacaÄŸÄ± bir ÅŸeyi tÉ™msil edir.
* **canvar** â€“ bu tÉ™hlÃ¼kÉ™lidir vÉ™ ondan qaÃ§Ä±nmaq lazÄ±mdÄ±r.

Bu mÃ¼hitlÉ™ iÅŸlÉ™mÉ™k Ã¼Ã§Ã¼n iÃ§É™risindÉ™ kodlar olan, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) adlÄ± ayrÄ±ca Python modulu mÃ¶vcuddur. Bu kodlar indiki konseptlÉ™ri baÅŸa dÃ¼ÅŸmÉ™k Ã¼Ã§Ã¼n o qÉ™dÉ™r dÉ™ vacib olmadÄ±ÄŸÄ±ndan, hÉ™min moduldan nÃ¼munÉ™ lÃ¶vhÉ™sini yaratmaq Ã¼Ã§Ã¼n istifadÉ™ edÉ™cÉ™yik (1. kod bloku):

```python
from rlboard import *

width, height = 8,8
m = Board(width,height)
m.randomize(seed=13)
m.plot()
```

Bu kod yuxarÄ±dakÄ± ÅŸÉ™kil kimi É™traf mÃ¼hitin gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ ekrana Ã§ap etmÉ™lidir.

## AddÄ±mlar vÉ™ Taktika

Bizim nÃ¼munÉ™mizdÉ™ Peterin mÉ™qsÉ™di canavardan vÉ™ digÉ™r maneÉ™lÉ™rdÉ™n qaÃ§araq bir alma tapmaq olardÄ±. Bunu etmÉ™k Ã¼Ã§Ã¼n, o, alma tapana qÉ™dÉ™r gÉ™zÉ™ bilÉ™r.

Buna gÃ¶rÉ™ dÉ™, istÉ™nilÉ™n mÃ¶vqedÉ™ o, yuxarÄ±, aÅŸaÄŸÄ±, sola vÉ™ saÄŸa hÉ™rÉ™kÉ™tlÉ™rindÉ™n birini seÃ§É™ bilÉ™r.

Biz bu hÉ™rÉ™kÉ™tlÉ™ri lÃ¼ÄŸÉ™t kimi mÃ¼É™yyÉ™nlÉ™ÅŸdirÉ™cÉ™yik vÉ™ onlarÄ± mÃ¼vafiq koordinat dÉ™yiÅŸikliklÉ™ri cÃ¼tlÉ™rinÉ™ uyÄŸunlaÅŸdÄ±racaÄŸÄ±q. MÉ™sÉ™lÉ™n, saÄŸa hÉ™rÉ™kÉ™t etmÉ™k (`R`) `(1,0)` cÃ¼tÃ¼nÉ™ uyÄŸun olacaq. (2. kod bloku):

```python
actions = { "U" : (0,-1), "D" : (0,1), "L" : (-1,0), "R" : (1,0) }
action_idx = { a : i for i,a in enumerate(actions.keys()) }
```

ÃœmumilÉ™ÅŸdirsÉ™k, bu ssenarinin strategiyasÄ± vÉ™ hÉ™dÉ™fi aÅŸaÄŸÄ±dakÄ±lardÄ±r:

- **Agentimizin(Piter) strategiyasÄ±**, "**taktika**" ilÉ™ mÃ¼É™yyÉ™n edilir. Taktika hÉ™r hansÄ± bir vÉ™ziyyÉ™tdÉ™ atÄ±lmalÄ± olan addÄ±mÄ± qaytaran funksiyadÄ±r. Bizim vÉ™ziyyÉ™timizdÉ™ problemin vÉ™ziyyÉ™ti oyunÃ§unun hazÄ±rkÄ± mÃ¶vqeyi dÉ™ daxil olmaqla lÃ¶vhÉ™ ilÉ™ tÉ™msil olunur.

- **GÃ¼clÉ™ndirici Ã¶yrÉ™nmÉ™nin mÉ™qsÉ™di** problemi sÉ™mÉ™rÉ™li ÅŸÉ™kildÉ™ hÉ™ll etmÉ™yÉ™ imkan verÉ™cÉ™k yaxÅŸÄ± bir taktika Ã¶yrÉ™nmÉ™kdir. Amma indilik, baÅŸlanÄŸÄ±c olaraq **tÉ™sadÃ¼fi gediÅŸ** adlÄ± É™n sadÉ™ strategiyanÄ± nÉ™zÉ™rdÉ™n keÃ§irÉ™k.

## TÉ™sadÃ¼fi gÉ™zinti

GÉ™lin É™vvÉ™lcÉ™ tÉ™sadÃ¼fi gediÅŸ strategiyasÄ±nÄ± hÉ™yata keÃ§irÉ™rÉ™k problemimizi hÉ™ll edÉ™k. TÉ™sadÃ¼fi gediÅŸlÉ™ almaya Ã§atana qÉ™dÉ™r icazÉ™ verilÉ™n addÄ±mlar arasÄ±ndan nÃ¶vbÉ™ti addÄ±mÄ±mÄ±zÄ± tÉ™sadÃ¼fi seÃ§É™cÉ™yik (3. kod bloku).

1. AÅŸaÄŸÄ±dakÄ± kodla tÉ™sadÃ¼fi gediÅŸi hÉ™yata keÃ§irin:

    ```python
    def random_policy(m):
        return random.choice(list(actions))

    def walk(m,policy,start_position=None):
        n = 0 # number of steps
        # set initial position
        if start_position:
            m.human = start_position
        else:
            m.random_start()
        while True:
            if m.at() == Board.Cell.apple:
                return n # success!
            if m.at() in [Board.Cell.wolf, Board.Cell.water]:
                return -1 # eaten by wolf or drowned
            while True:
                a = actions[policy(m)]
                new_pos = m.move_pos(m.human,a)
                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:
                    m.move(a) # do the actual move
                    break
            n+=1

    walk(m,random_policy)
    ```
    `walk` mÃ¼vafiq yolun uzunluÄŸunu qaytarmalÄ±dÄ±r. Amma tÉ™sadÃ¼fi olduÄŸu Ã¼Ã§Ã¼n hÉ™r Ã§aÄŸrÄ±lma zamanÄ± fÉ™rqli uzunluq qaytara bilÉ™r.

1. GÉ™zinti tÉ™crÃ¼bÉ™sini bir neÃ§É™ dÉ™fÉ™ yerinÉ™ yetirin(mÉ™sÉ™lÉ™n, 100) vÉ™ nÉ™ticÉ™dÉ™ É™ldÉ™ edilÉ™n statistikanÄ± Ã§ap edin (4. kod bloku):

    ```python
    def print_statistics(policy):
        s,w,n = 0,0,0
        for _ in range(100):
            z = walk(m,policy)
            if z<0:
                w+=1
            else:
                s += z
                n += 1
        print(f"Average path length = {s/n}, eaten by wolf: {w} times")

    print_statistics(random_policy)
    ```

    DiqqÉ™t etsÉ™k gÃ¶rÉ™rik ki, yolun orta uzunluÄŸu tÉ™xminÉ™n 30-40 addÄ±mdÄ±r, É™n yaxÄ±n almaya qÉ™dÉ™r orta mÉ™safÉ™nin 5-6 addÄ±m civarÄ±nda olduÄŸunu nÉ™zÉ™rÉ™ alsaq, bu, kifayÉ™t qÉ™dÉ™r Ã§oxdur.

    Siz hÉ™mÃ§inin tÉ™sadÃ¼fi gÉ™zinti zamanÄ± Peterin hÉ™rÉ™kÉ™tinin necÉ™ gÃ¶rÃ¼ndÃ¼yÃ¼nÃ¼ gÃ¶rÉ™ bilÉ™rsiniz:

    ![Piterin tÉ™sadÃ¼fi gÉ™zintisi](../images/random_walk.gif)

## MÃ¼kafat funksiyasÄ±

TaktikamÄ±zÄ± daha aÄŸÄ±llÄ± etmÉ™k Ã¼Ã§Ã¼n hansÄ± hÉ™rÉ™kÉ™tlÉ™rin digÉ™rlÉ™rindÉ™n â€œdaha â€‹â€‹yaxÅŸÄ±â€ olduÄŸunu baÅŸa dÃ¼ÅŸmÉ™liyik. Bunun Ã¼Ã§Ã¼n hÉ™dÉ™fimizi mÃ¼É™yyÉ™n etmÉ™liyik.

MÉ™qsÉ™d **mÃ¼kafat funksiyasÄ±** ilÉ™ mÃ¼É™yyÉ™n edilÉ™ bilÉ™r ki, bu da hÉ™r bir vÉ™ziyyÉ™t Ã¼Ã§Ã¼n mÃ¼É™yyÉ™n xal dÉ™yÉ™rini qaytaracaq. SayÄ± nÉ™ qÉ™dÉ™r Ã§ox olarsa, mÃ¼kafat funksiyasÄ± bir o qÉ™dÉ™r yaxÅŸÄ± olar.(5. kod bloku)

```python
move_reward = -0.1
goal_reward = 10
end_reward = -10

def reward(m,pos=None):
    pos = pos or m.human
    if not m.is_valid(pos):
        return end_reward
    x = m.at(pos)
    if x==Board.Cell.water or x == Board.Cell.wolf:
        return end_reward
    if x==Board.Cell.apple:
        return goal_reward
    return move_reward
```

MÃ¼kafat funksiyalarÄ± ilÉ™ baÄŸlÄ± maraqlÄ± mÉ™qam ondan ibarÉ™tdir ki, É™ksÉ™r hallarda *bizÉ™ yalnÄ±z oyunun sonunda É™hÉ™miyyÉ™tli bir mÃ¼kafat verilir*. Bu o demÉ™kdir ki, alqoritmimiz sonda mÃ¼sbÉ™t mÃ¼kafata sÉ™bÉ™b olan â€œyaxÅŸÄ±â€ addÄ±mlarÄ± birtÉ™hÉ™r yadda saxlamalÄ± vÉ™ onlarÄ±n É™hÉ™miyyÉ™tini artÄ±rmalÄ±dÄ±r. Eyni qayda ilÉ™, pis nÉ™ticÉ™lÉ™rÉ™ sÉ™bÉ™b olan bÃ¼tÃ¼n hÉ™rÉ™kÉ™tlÉ™rdÉ™n dÉ™ Ã§É™kinmÉ™si lazÄ±mdÄ±r.

## Q-Ã–yrÉ™nmÉ™si

Burada mÃ¼zakirÉ™ edÉ™cÉ™yimiz alqoritm **Q-Ã–yrÉ™nmÉ™si** adlanÄ±r. Bu alqoritmdÉ™ki taktika **Q-CÉ™dvÉ™l** adlÄ± funksiya(vÉ™ ya data strukturu) ilÉ™ mÃ¼É™yyÉ™n edilir. O, verilmiÅŸ vÉ™ziyyÉ™t Ã¼Ã§Ã¼n hÉ™r bir addÄ±mÄ±n "yaxÅŸÄ±lÄ±ÄŸÄ±nÄ±" qeyd edir.

Bu mÉ™lumatÄ±, cÉ™dvÉ™l vÉ™ ya Ã§oxÃ¶lÃ§Ã¼lÃ¼ massiv kimi tÉ™qdim etmÉ™k É™ksÉ™r hallarda rahat olduÄŸu Ã¼Ã§Ã¼n Q-CÉ™dvÉ™li deyÉ™ adlanÄ±r. LÃ¶vhÉ™mizin `en` x `hÃ¼ndÃ¼rlÃ¼k` Ã¶lÃ§Ã¼lÉ™ri olduÄŸundan, biz Q-CÉ™dvÉ™lini `en` x `hÃ¼ndÃ¼rlÃ¼k` x `len(actions)` formalÄ± numpy massivdÉ™n istifadÉ™ etmÉ™klÉ™ tÉ™msil edÉ™ bilÉ™rik: (6. kod bloku)

```python
Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)
```

DiqqÉ™t edin ki, biz Q-CÉ™dvÉ™linin bÃ¼tÃ¼n dÉ™yÉ™rlÉ™rini eyni dÉ™yÉ™rlÉ™ iÅŸÉ™ salÄ±rÄ±q, indiki halda - 0,25. HÉ™r bir vÉ™ziyyÉ™tdÉ™ bÃ¼tÃ¼n hÉ™rÉ™kÉ™tlÉ™r eyni dÉ™rÉ™cÉ™dÉ™ yaxÅŸÄ± olduÄŸu Ã¼Ã§Ã¼n "tÉ™sadÃ¼fi gÉ™zinti" taktikasÄ±na uyÄŸundur. LÃ¶vhÉ™dÉ™ cÉ™dvÉ™li vizuallaÅŸdÄ±rmaq Ã¼Ã§Ã¼n Q-CÉ™dvÉ™lini `plot` funksiyasÄ±na Ã¶tÃ¼rÉ™ bilÉ™rik: `m.plot(Q)`.

![Piterin MÃ¼hiti](../images/env_init.png)

HÉ™r bir hÃ¼ceyrÉ™nin mÉ™rkÉ™zindÉ™ Ã¼stÃ¼nlÃ¼k verilÉ™n hÉ™rÉ™kÉ™t istiqamÉ™tini gÃ¶stÉ™rÉ™n "ox" var. BÃ¼tÃ¼n istiqamÉ™tlÉ™r bÉ™rabÉ™r olduÄŸundan, nÃ¶qtÉ™ ilÉ™ gÃ¶stÉ™rilmiÅŸdir.

Ä°ndi biz, almaya gedÉ™n yolu daha tez tapmaq Ã¼Ã§Ã¼n simulyasiyanÄ± iÅŸÉ™ salmalÄ±, É™trafÄ±mÄ±zÄ± araÅŸdÄ±rmalÄ± vÉ™ Q-CÉ™dvÉ™l dÉ™yÉ™rlÉ™rinin daha yaxÅŸÄ± paylanmasÄ±nÄ± Ã¶yrÉ™nmÉ™liyik.

## Q-Ã¶yrÉ™nmÉ™sinin mahiyyÉ™ti: Bellman tÉ™nliyi

HÉ™rÉ™kÉ™t etmÉ™yÉ™ baÅŸladÄ±qdan sonra hÉ™r bir gediÅŸin mÃ¼vafiq mÃ¼kafatÄ± olacaq. NÉ™zÉ™ri olaraq É™n yÃ¼ksÉ™k ani mÃ¼kafat É™sasÄ±nda nÃ¶vbÉ™ti addÄ±mÄ± seÃ§É™ bilÉ™rik. Bununla belÉ™, É™ksÉ™r vÉ™ziyyÉ™tlÉ™rdÉ™, atÄ±lan addÄ±m almaya Ã§atmaq hÉ™dÉ™fimizÉ™ nail olmayacaq vÉ™ buna gÃ¶rÉ™ dÉ™ hansÄ± istiqamÉ™tin daha yaxÅŸÄ± olduÄŸuna dÉ™rhal qÉ™rar verÉ™ bilmÉ™yÉ™cÉ™yik.

> UnutmayÄ±n ki, vacib olan ani nÉ™ticÉ™ deyil, simulyasiyanÄ±n sonunda É™ldÉ™ edÉ™cÉ™yimiz son nÉ™ticÉ™dir.

Bu gecikmiÅŸ mÃ¼kafatÄ± hesablamaq Ã¼Ã§Ã¼n problemi rekursiv ÅŸÉ™kildÉ™ dÃ¼ÅŸÃ¼nmÉ™yÉ™ imkan verÉ™n **[dinamik proqramlaÅŸdÄ±rma](https://en.wikipedia.org/wiki/Dynamic_programming)** prinsiplÉ™rindÉ™n istifadÉ™ etmÉ™liyik.

Tutaq ki, biz indi *s* â€‹â€‹vÉ™ziyyÉ™tindÉ™yik vÉ™ nÃ¶vbÉ™ti *s'* vÉ™ziyyÉ™tinÉ™ keÃ§mÉ™k istÉ™yirik. Bunu etmÉ™klÉ™, biz mÃ¼kafat funksiyasÄ± ilÉ™ mÃ¼É™yyÉ™n edilÉ™n ani mÃ¼kafatÄ±(*r(s,a)*), vÉ™ gÉ™lÉ™cÉ™k mÃ¼kafatÄ± alacaÄŸÄ±q. Q-CÉ™dvÉ™limizin hÉ™r bir hÉ™rÉ™kÉ™tin "cÉ™lbediciliyini" dÃ¼zgÃ¼n É™ks etdirdiyini fÉ™rz etsÉ™k, *s'* vÉ™ziyyÉ™tindÉ™ *Q(s',a')*-in maksimum dÉ™yÉ™rinÉ™ uyÄŸun gÉ™lÉ™n *a* addÄ±mÄ±nÄ± seÃ§É™cÉ™yik. BelÉ™liklÉ™, *s* vÉ™ziyyÉ™tindÉ™ É™ldÉ™ edÉ™ bilÉ™cÉ™yimiz É™n yaxÅŸÄ± gÉ™lÉ™cÉ™k mÃ¼kafat `max`<sub>a'</sub>*Q(s',a')* kimi mÃ¼É™yyÉ™n edilÉ™cÉ™k(burada maksimum *s'* vÉ™ziyyÉ™tindÉ™ki bÃ¼tÃ¼n mÃ¼mkÃ¼n *a'* addÄ±mlarÄ±nÄ±n Ã¼zÉ™rindÉ™ hesablanÄ±r).

Bu da bizÉ™, verilmiÅŸ *a* addÄ±mÄ± Ã¼Ã§Ã¼n Q-CÉ™dvÉ™linin *s* vÉ™ziyyÉ™tindÉ™ dÉ™yÉ™rinin hesablanmasÄ± Ã¼Ã§Ã¼n olan **Bellman dÃ¼sturunu** vermiÅŸ olur:

<img src="../images/bellman-equation.png"/>

Burada Î³ **endirim faktoru** adlanÄ±r vÉ™ cari mÃ¼kafata vÉ™ ya bunun É™ksinÉ™, gÉ™lÉ™cÉ™k mÃ¼kafata nÉ™ dÉ™rÉ™cÉ™dÉ™ Ã¼stÃ¼nlÃ¼k vermÉ™li olduÄŸunuzu mÃ¼É™yyÉ™n edir.

## Ã–yrÉ™nmÉ™ Alqoritmi

YuxarÄ±dakÄ± tÉ™nliyi nÉ™zÉ™rÉ™ alaraq indi Ã¶yrÉ™nmÉ™ alqoritmimiz Ã¼Ã§Ã¼n psevdokod yaza bilÉ™rik:

* Q-CÉ™dvÉ™li Q-Ã¼ bÃ¼tÃ¼n vÉ™ziyyÉ™t vÉ™ gediÅŸlÉ™r Ã¼Ã§Ã¼n bÉ™rabÉ™r É™dÉ™dlÉ™rlÉ™ yaradÄ±n
* Ã–yrÉ™nmÉ™ dÉ™rÉ™cÉ™sini Î± â† 1 tÉ™yin edin
* SimulyasiyanÄ± dÉ™fÉ™lÉ™rlÉ™ tÉ™krarlayÄ±n
 1. TÉ™sadÃ¼fi mÃ¶vqedÉ™n baÅŸlayÄ±n
 1. TÉ™krar edin
    1. *s* vÉ™ziyyÉ™tindÉ™ *a* addÄ±mÄ±nÄ± seÃ§in
    2. Yeni vÉ™ziyyÉ™tÉ™ keÃ§É™rÉ™k gediÅŸi yerinÉ™ yetirin *s'*
    3. Oyunun sonu vÉ™ziyyÉ™ti ilÉ™ qarÅŸÄ±laÅŸsaq vÉ™ ya Ã¼mumi mÃ¼kafat Ã§ox kiÃ§ik olarsa - simulyasiyadan Ã§Ä±xÄ±n
    4. Yeni vÉ™ziyyÉ™tdÉ™ *r* mÃ¼kafatÄ±nÄ± hesablayÄ±n
    5. Q-FunksiyasÄ±nÄ± Bellman tÉ™nliyinÉ™ uyÄŸun yenilÉ™yin: *Q(s,a)* â† *(1-Î±)Q(s,a)+Î±(r+Î³ max<sub>a'</sub>Q( s',a'))*
    6. *s* â† *s'*
    7. Ãœmumi mÃ¼kafatÄ± yenilÉ™yin vÉ™ Î±-nÄ± azaldÄ±n.

## Ä°stifadÉ™ ilÉ™ KÉ™ÅŸf qarÅŸÄ±-qarÅŸÄ±ya

YuxarÄ±dakÄ± alqoritmdÉ™ 2.1-ci addÄ±mda gediÅŸi necÉ™ dÉ™qiq seÃ§mÉ™li olduÄŸumuzu mÃ¼É™yyÉ™nlÉ™ÅŸdirmÉ™miÅŸik. AddÄ±mÄ± tÉ™sadÃ¼fi seÃ§sÉ™k, É™trafÄ± tÉ™sadÃ¼fi formada **tÉ™dqiq edÉ™cÉ™yimiz** Ã¼Ã§Ã¼n tez-tez Ã¶lmÉ™k, elÉ™cÉ™ dÉ™ normalda getmÉ™diyimiz É™razilÉ™ri araÅŸdÄ±rmaq kimi ehtimallarÄ±mÄ±z var. Alternativ yanaÅŸma isÉ™ artÄ±q bildiyimiz Q-CÉ™dvÉ™l dÉ™yÉ™rlÉ™rini **istifadÉ™ etmÉ™k** vÉ™ bununla da, *s* vÉ™ziyyÉ™tindÉ™ É™n yaxÅŸÄ± hÉ™rÉ™kÉ™ti(daha yÃ¼ksÉ™k Q-CÉ™dvÉ™l dÉ™yÉ™ri ilÉ™) seÃ§mÉ™k olardÄ±. Amma, belÉ™ etmÉ™k digÉ™r vÉ™ziyyÉ™tlÉ™ri araÅŸdÄ±rmaÄŸÄ±mÄ±za mane olacaq vÉ™ Ã§ox gÃ¼man ki optimal hÉ™lli tapa bilmÉ™yÉ™cÉ™yik.

BelÉ™liklÉ™, É™n yaxÅŸÄ± yanaÅŸma kÉ™ÅŸfiyyat vÉ™ istifadÉ™ arasÄ±nda balansÄ± qorumaqdÄ±r. Bu, Q-CÉ™dvÉ™lindÉ™ki dÉ™yÉ™rlÉ™rÉ™ mÃ¼tÉ™nasib ehtimallarla *s* vÉ™ziyyÉ™tindÉ™ hÉ™rÉ™kÉ™ti seÃ§mÉ™klÉ™ edilÉ™ bilÉ™r. BaÅŸlanÄŸÄ±cda Q-CÉ™dvÉ™l dÉ™yÉ™rlÉ™ri eyni olduqda bu, tÉ™sadÃ¼fi seÃ§imÉ™ uyÄŸun olacaq, lakin É™trafÄ±mÄ±z haqqÄ±nda daha Ã§ox Ã¶yrÉ™ndikcÉ™ arabir agentÉ™ araÅŸdÄ±rÄ±lmamÄ±ÅŸ yolu seÃ§mÉ™yÉ™ icazÉ™ verÉ™rkÉ™n optimal marÅŸrutu izlÉ™mÉ™k ehtimalÄ±mÄ±z daha yÃ¼ksÉ™k olacaq.

## Python Ã¼zÉ™rindÉ™n icrasÄ±

Ä°ndi biz Ã¶yrÉ™nmÉ™ alqoritmini hÉ™yata keÃ§irmÉ™yÉ™ hazÄ±rÄ±q. Bunu etmÉ™zdÉ™n É™vvÉ™l bizÉ™ Q-CÉ™dvÉ™lindÉ™ki ixtiyari É™dÉ™dlÉ™ri mÃ¼vafiq hÉ™rÉ™kÉ™tlÉ™r Ã¼Ã§Ã¼n ehtimallar vektoruna Ã§evirÉ™cÉ™k bÉ™zi funksiyalar da lazÄ±mdÄ±r.

1. `probs()` funksiyasÄ± yaradÄ±n:

    ```python
    def probs(v,eps=1e-4):
        v = v-v.min()+eps
        v = v/v.sum()
        return v
    ```

    Ä°lkin halda vektorun bÃ¼tÃ¼n komponentlÉ™ri eyni olduqda 0-a bÃ¶lÃ¼nmÉ™dÉ™n yayÄ±nmaq Ã¼Ã§Ã¼n orijinal vektora bir neÃ§É™ `eps` É™lavÉ™ edirik.

5000 tÉ™crÃ¼bÉ™ vÉ™ ya **dÃ¶vr** vasitÉ™silÉ™ Ã¶yrÉ™nmÉ™ alqoritmini iÅŸÉ™ salÄ±n: (8. kod bloku)

    ```python
    for epoch in range(5000):

        # Pick initial point
        m.random_start()

        # Start travelling
        n=0
        cum_reward = 0
        while True:
            x,y = m.human
            v = probs(Q[x,y])
            a = random.choices(list(actions),weights=v)[0]
            dpos = actions[a]
            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode
            r = reward(m)
            cum_reward += r
            if r==end_reward or cum_reward < -1000:
                lpath.append(n)
                break
            alpha = np.exp(-n / 10e5)
            gamma = 0.5
            ai = action_idx[a]
            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())
            n+=1
    ```

Bu alqoritmi yerinÉ™ yetirdikdÉ™n sonra Q-CÉ™dvÉ™li hÉ™r addÄ±mda mÃ¼xtÉ™lif hÉ™rÉ™kÉ™tlÉ™rin cÉ™lbediciliyini mÃ¼É™yyÉ™n edÉ™n dÉ™yÉ™rlÉ™rlÉ™ yenilÉ™nmÉ™lidir. Ä°stÉ™nilÉ™n hÉ™rÉ™kÉ™t istiqamÉ™tini gÃ¶stÉ™rÉ™cÉ™k hÉ™r bir hÃ¼ceyrÉ™dÉ™ bir vektor Ã§É™kÉ™rÉ™k Q-CÉ™dvÉ™lini vizuallaÅŸdÄ±rmaÄŸa cÉ™hd edÉ™ bilÉ™rik. SadÉ™lik Ã¼Ã§Ã¼n ox ucluÄŸu É™vÉ™zinÉ™ kiÃ§ik bir dairÉ™ Ã§É™kirik.

<img src="../images/learned.png"/>

## TaktikanÄ±n yoxlanÄ±lmasÄ±

Q-CÉ™dvÉ™li hÉ™r bir vÉ™ziyyÉ™tdÉ™ hÉ™r bir addÄ±mÄ±n "cÉ™lbediciliyini" saxladÄ±ÄŸÄ± Ã¼Ã§Ã¼n dÃ¼nyamÄ±zda sÉ™mÉ™rÉ™li naviqasiyanÄ± mÃ¼É™yyÉ™n etmÉ™k Ã¼Ã§Ã¼n ondan istifadÉ™ etmÉ™k olduqca asandÄ±r. Æn sadÉ™ halda, É™n yÃ¼ksÉ™k Q-CÉ™dvÉ™l dÉ™yÉ™rinÉ™ uyÄŸun hÉ™rÉ™kÉ™ti seÃ§É™ bilÉ™rik: (9. kod bloku)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> YuxarÄ±dakÄ± kodu bir neÃ§É™ dÉ™fÉ™ sÄ±nasanÄ±z, onun bÉ™zÉ™n "donduÄŸunu" gÃ¶rÉ™ bilÉ™rsiniz. BelÉ™ olduqda onu dayandÄ±rmaq Ã¼Ã§Ã¼n noutbukda STOP dÃ¼ymÉ™sini sÄ±xmalÄ±sÄ±nÄ±z. Bu hal iki vÉ™ziyyÉ™tin optimal Q-DÉ™yÉ™ri baxÄ±mÄ±ndan bir-birinÉ™ "iÅŸarÉ™ etdiyi" zaman yarana bilÉ™r. HÉ™min durumda agentlÉ™r bu vÉ™ziyyÉ™tlÉ™r arasÄ±nda qeyri-mÃ¼É™yyÉ™n mÃ¼ddÉ™tÉ™ qÉ™dÉ™r hÉ™rÉ™kÉ™t etmiÅŸ olur.

## ğŸš€ MÉ™ÅŸÄŸÉ™lÉ™

> **TapÅŸÄ±rÄ±q 1:** Yolun maksimum uzunluÄŸunu mÃ¼É™yyÉ™n sayda addÄ±mlarla (mÉ™sÉ™lÉ™n, 100) mÉ™hdudlaÅŸdÄ±rmaq Ã¼Ã§Ã¼n `walk` funksiyasÄ±nÄ± dÉ™yiÅŸdirin vÉ™ yuxarÄ±dakÄ± kodun vaxtaÅŸÄ±rÄ± bu dÉ™yÉ™ri qaytarmasÄ±na baxÄ±n.

> **TapÅŸÄ±rÄ±q 2:** `walk` funksiyasÄ±nÄ± elÉ™ dÉ™yiÅŸdirin ki, o, É™vvÉ™llÉ™r olduÄŸu yerlÉ™rÉ™ qayÄ±tmasÄ±n. BelÉ™ etmÉ™k `walk`-un dÃ¶vrÉ™ girmÉ™sinin qarÅŸÄ±sÄ±nÄ± alsa da, agent yenÉ™ dÉ™ qaÃ§a bilmÉ™yÉ™cÉ™yi yerdÉ™ "tÉ™lÉ™yÉ™" dÃ¼ÅŸÉ™ bilÉ™r.

## Naviqasiya

Daha yaxÅŸÄ± naviqasiya taktikasÄ± tÉ™lim zamanÄ± istifadÉ™ etdiyimiz istismar vÉ™ kÉ™ÅŸfiyyatÄ±n birlÉ™ÅŸdirmiÅŸ olan versiyasÄ± olardÄ±. Bu taktikada biz hÉ™r bir hÉ™rÉ™kÉ™ti Q-CÉ™dvÉ™lindÉ™ki dÉ™yÉ™rlÉ™rÉ™ mÃ¼tÉ™nasib olaraq mÃ¼É™yyÉ™n bir ehtimalla seÃ§É™cÉ™yik. Bu strategiya hÉ™lÉ™ dÉ™ agentin artÄ±q tÉ™dqiq etdiyi mÃ¶vqeyÉ™ qayÄ±tmasÄ± ilÉ™ nÉ™ticÉ™lÉ™nÉ™ bilÉ™r, lakin, aÅŸaÄŸÄ±dakÄ± koddan da gÃ¶rÃ¼ndÃ¼yÃ¼ kimi bu sizÉ™ istÉ™diyiniz mÉ™kana olan Ã§ox qÄ±sa orta yolu verir(unutmayÄ±n ki, `print_statistics` simulyasiyanÄ± 100 dÉ™fÉ™ hÉ™yata keÃ§irir): (10. kod bloku)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Bu kodu iÅŸlÉ™tdikdÉ™n sonra É™vvÉ™lkindÉ™n daha kiÃ§ik, 3-6 aralÄ±ÄŸÄ±nda dÉ™yiÅŸÉ™n orta yol uzunluÄŸu É™ldÉ™ etmÉ™lisiniz.

## Ã–yrÉ™nmÉ™ prosesinin tÉ™dqiqi

Qeyd etdiyimiz kimi, tÉ™lim prosesi problem mÉ™kanÄ±nÄ±n strukturu haqqÄ±nda É™ldÉ™ edilmiÅŸ biliklÉ™rin tÉ™dqiqi vÉ™ Ã¼mumi kÉ™ÅŸfi arasÄ±ndakÄ± balansdÄ±r. Ã–yrÉ™nmÉ™nin nÉ™ticÉ™lÉ™rinin(agentÉ™ hÉ™dÉ™fÉ™ olan qÄ±sa yolu tapmaqda kÃ¶mÉ™k etmÉ™k qabiliyyÉ™ti) yaxÅŸÄ±laÅŸdÄ±ÄŸÄ±nÄ± gÃ¶rmÃ¼ÅŸ olsaq da, Ã¶yrÉ™nmÉ™ prosesi zamanÄ± orta yol uzunluÄŸunun necÉ™ davrandÄ±ÄŸÄ±nÄ± mÃ¼ÅŸahidÉ™ etmÉ™k dÉ™ bir o qÉ™dÉ™r maraqlÄ± olardÄ±:

<img src="../images/lpathlen1.png"/>

Ã–yrÉ™nilÉ™nlÉ™ri belÉ™ Ã¼mumilÉ™ÅŸdirmÉ™k olar:

- **Orta yol uzunluÄŸu artÄ±r**. Burada gÃ¶rdÃ¼yÃ¼mÃ¼z odur ki, É™vvÉ™lcÉ™ orta yol uzunluÄŸu artÄ±r. Bu, yÉ™qin ki, É™traf mÃ¼hit haqqÄ±nda heÃ§ nÉ™ bilmÉ™diyimiz zaman pis vÉ™ziyyÉ™tlÉ™rdÉ™, suda vÉ™ ya canavarda tÉ™lÉ™yÉ™ dÃ¼ÅŸmÉ™k ehtimalÄ±mÄ±zla baÄŸlÄ±dÄ±r. Daha Ã§ox Ã¶yrÉ™ndikcÉ™ vÉ™ bu biliklÉ™rdÉ™n istifadÉ™ etmÉ™yÉ™ baÅŸladÄ±qda É™traf mÃ¼hiti daha uzun mÃ¼ddÉ™t araÅŸdÄ±ra bilÉ™rik. Bununla belÉ™, almalarÄ±n harada daha Ã§ox olduÄŸunu hÉ™lÉ™ dÉ™ bilmiÅŸ olmuruq.

- **Daha Ã§ox Ã¶yrÉ™ndikcÉ™ yol uzunluÄŸu azalÄ±r**. KifayÉ™t qÉ™dÉ™r Ã¶yrÉ™ndikdÉ™n sonra agentin hÉ™dÉ™fÉ™ Ã§atmasÄ± asanlaÅŸÄ±r vÉ™ yol uzunluÄŸu azalmaÄŸa baÅŸlayÄ±r. Amma, biz hÉ™lÉ™ dÉ™ kÉ™ÅŸfiyyata aÃ§Ä±ÄŸÄ±q, ona gÃ¶rÉ™ dÉ™ biz tez-tez É™n yaxÅŸÄ± yoldan uzaqlaÅŸÄ±rÄ±q vÉ™ yeni variantlarÄ± araÅŸdÄ±raraq yolu optimaldan daha uzun edirik.

- **UzunluÄŸun kÉ™skin artmasÄ±**. Bu qrafikdÉ™ dÉ™ mÃ¼ÅŸahidÉ™ etdiyimiz odur ki, mÃ¼É™yyÉ™n bir nÃ¶qtÉ™dÉ™ uzunluq kÉ™skin ÅŸÉ™kildÉ™ artÄ±r. Bu, prosesin stoxastik xarakterini gÃ¶stÉ™rir vÉ™ biz nÉ™ vaxtsa Q-CÉ™dvÉ™l É™msallarÄ±nÄ± yeni dÉ™yÉ™rlÉ™rlÉ™ É™vÉ™z etmÉ™klÉ™ onlarÄ± "korlaya" bilÉ™rik. Ä°deal olaraq, bu, Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tini azaltmaqla minimuma endirilmÉ™lidir(mÉ™sÉ™lÉ™n, tÉ™limin sonuna doÄŸru biz Q-CÉ™dvÉ™l dÉ™yÉ™rlÉ™rini yalnÄ±z kiÃ§ik bir dÉ™yÉ™rlÉ™ tÉ™nzimlÉ™yirik).

ÃœmumiyyÉ™tlÉ™, yadda saxlamaq lazÄ±mdÄ±r ki, tÉ™lim prosesinin uÄŸuru vÉ™ keyfiyyÉ™ti Ã¶yrÉ™nmÉ™ sÃ¼rÉ™ti, Ã¶yrÉ™nmÉ™ sÃ¼rÉ™tinin azalmasÄ± vÉ™ endirim faktoru kimi parametrlÉ™rdÉ™n É™hÉ™miyyÉ™tli dÉ™rÉ™cÉ™dÉ™ asÄ±lÄ±dÄ±r. TÉ™lim zamanÄ± optimallaÅŸdÄ±rdÄ±ÄŸÄ±mÄ±z(mÉ™sÉ™lÉ™n, Q-CÉ™dvÉ™l É™msallarÄ±) **parametrlÉ™rdÉ™n** fÉ™rqlÉ™ndirmÉ™k Ã¼Ã§Ã¼n onlarÄ± tez-tez **hiperparametrlÉ™r** adlandÄ±rÄ±rlar. Æn yaxÅŸÄ± hiperparametr dÉ™yÉ™rlÉ™rinin tapÄ±lmasÄ± prosesi **hiperparametrlÉ™rin optimallaÅŸdÄ±rÄ±lmasÄ±** adlanÄ±r vÉ™ bu ayrÄ±ca mÃ¶vzu sÉ™viyyÉ™sindÉ™dir.

## [MÃ¼hazirÉ™ sonrasÄ± test](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/?loc=az)

## TapÅŸÄ±rÄ±q
[Daha Real DÃ¼nya](assignment.az.md)